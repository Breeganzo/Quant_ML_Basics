{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545af19",
   "metadata": {},
   "source": [
    "# Day 6: Variational Inference for Scalable Bayesian Learning\n",
    "\n",
    "## Week 20: Bayesian Methods | Quant ML Learning Path\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** why Variational Inference (VI) is essential for scalable Bayesian learning\n",
    "2. **Master** the Evidence Lower Bound (ELBO) derivation and optimization\n",
    "3. **Implement** Mean-Field Variational Inference from scratch\n",
    "4. **Apply** Coordinate Ascent VI (CAVI) and Stochastic VI (SVI) algorithms\n",
    "5. **Use** Automatic Differentiation VI (ADVI) with PyMC\n",
    "6. **Compare** VI vs MCMC for practical applications in finance\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Table of Contents\n",
    "\n",
    "1. [Introduction to Variational Inference](#1-introduction)\n",
    "2. [Understanding KL Divergence](#2-kl-divergence)\n",
    "3. [Evidence Lower Bound (ELBO) Derivation](#3-elbo)\n",
    "4. [Mean-Field Variational Inference](#4-mean-field)\n",
    "5. [VI for Gaussian Mixture Models](#5-gmm)\n",
    "6. [Coordinate Ascent Variational Inference (CAVI)](#6-cavi)\n",
    "7. [Stochastic Variational Inference (SVI)](#7-svi)\n",
    "8. [Automatic Differentiation VI (ADVI)](#8-advi)\n",
    "9. [Variational Inference with PyMC](#9-pymc)\n",
    "10. [Comparing VI to MCMC](#10-comparison)\n",
    "11. [Financial Application: Scalable Factor Models](#11-finance)\n",
    "12. [Exercises & Interview Questions](#12-exercises)\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Key Insight\n",
    "\n",
    "> **\"Variational Inference transforms intractable Bayesian inference into an optimization problem, enabling scalability to millions of data points while MCMC struggles with thousands.\"**\n",
    "\n",
    "In quantitative finance, where we deal with massive datasets of tick data, order book snapshots, and alternative data streams, VI provides the scalability necessary for production-grade Bayesian models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8b876",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries <a name=\"1-introduction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Scientific Computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import digamma, gammaln, logsumexp\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "# Probabilistic Programming\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Deep Learning for VI (optional - for ADVI from scratch)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.distributions import Normal, MultivariateNormal, kl_divergence\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"PyTorch not available - some examples will be skipped\")\n",
    "\n",
    "# Styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"PyTorch available: {TORCH_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090757",
   "metadata": {},
   "source": [
    "### Why Variational Inference?\n",
    "\n",
    "**The Scalability Problem with MCMC:**\n",
    "\n",
    "Traditional Bayesian inference using MCMC (Markov Chain Monte Carlo) faces significant challenges:\n",
    "- **Computational Cost**: $O(N)$ per iteration, where $N$ is the dataset size\n",
    "- **Convergence**: Requires many iterations to explore the posterior\n",
    "- **Parallelization**: Sequential nature limits GPU utilization\n",
    "- **Large Data**: Impractical for datasets with millions of observations\n",
    "\n",
    "**Variational Inference Solution:**\n",
    "\n",
    "VI reframes Bayesian inference as an **optimization problem**:\n",
    "- Instead of sampling from $p(\\theta|D)$, we find the closest approximation $q(\\theta)$\n",
    "- Leverages gradient descent and automatic differentiation\n",
    "- Enables **mini-batch training** for massive datasets\n",
    "- Naturally parallelizable on GPUs\n",
    "\n",
    "$$\\text{MCMC: Sample from } p(\\theta|D) \\quad \\rightarrow \\quad \\text{VI: Find } q^*(\\theta) = \\arg\\min_q D_{KL}(q(\\theta) \\| p(\\theta|D))$$\n",
    "\n",
    "| Aspect | MCMC | Variational Inference |\n",
    "|--------|------|----------------------|\n",
    "| Output | Samples from posterior | Parameters of approximating distribution |\n",
    "| Convergence | Asymptotically exact | Approximate (but fast) |\n",
    "| Scalability | $O(N \\times T)$ | $O(M \\times T)$ with minibatches |\n",
    "| GPU-friendly | Limited | Excellent |\n",
    "| Uncertainty | Full posterior | Approximation (often underestimates) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f78b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding KL Divergence <a name=\"2-kl-divergence\"></a>\n",
    "\n",
    "The **Kullback-Leibler (KL) Divergence** is the foundation of variational inference. It measures how different one probability distribution is from another.\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For two distributions $p$ and $q$:\n",
    "\n",
    "$$D_{KL}(q \\| p) = \\mathbb{E}_q\\left[\\log \\frac{q(z)}{p(z)}\\right] = \\int q(z) \\log \\frac{q(z)}{p(z)} dz$$\n",
    "\n",
    "**Key Properties:**\n",
    "1. **Non-negative**: $D_{KL}(q \\| p) \\geq 0$\n",
    "2. **Zero iff equal**: $D_{KL}(q \\| p) = 0 \\Leftrightarrow q = p$\n",
    "3. **Asymmetric**: $D_{KL}(q \\| p) \\neq D_{KL}(p \\| q)$\n",
    "\n",
    "### Forward vs Reverse KL\n",
    "\n",
    "The asymmetry is crucial for understanding VI behavior:\n",
    "\n",
    "- **Forward KL** $D_{KL}(p \\| q)$: \"Mode-covering\" - $q$ covers all modes of $p$\n",
    "- **Reverse KL** $D_{KL}(q \\| p)$: \"Mode-seeking\" - $q$ concentrates on a single mode\n",
    "\n",
    "VI minimizes **Reverse KL**, leading to posterior approximations that may miss modes but are computationally tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_gaussians(mu_q, sigma_q, mu_p, sigma_p):\n",
    "    \"\"\"\n",
    "    Compute KL divergence between two univariate Gaussians.\n",
    "    \n",
    "    D_KL(q || p) = log(œÉ_p/œÉ_q) + (œÉ_q¬≤ + (Œº_q - Œº_p)¬≤) / (2œÉ_p¬≤) - 1/2\n",
    "    \"\"\"\n",
    "    return (np.log(sigma_p / sigma_q) + \n",
    "            (sigma_q**2 + (mu_q - mu_p)**2) / (2 * sigma_p**2) - 0.5)\n",
    "\n",
    "\n",
    "def kl_divergence_numerical(q_samples, p_pdf, q_pdf):\n",
    "    \"\"\"\n",
    "    Numerical approximation of KL divergence using samples.\n",
    "    D_KL(q || p) ‚âà (1/N) Œ£ [log q(x_i) - log p(x_i)]\n",
    "    \"\"\"\n",
    "    log_q = np.log(q_pdf(q_samples) + 1e-10)\n",
    "    log_p = np.log(p_pdf(q_samples) + 1e-10)\n",
    "    return np.mean(log_q - log_p)\n",
    "\n",
    "\n",
    "# Demonstrate KL divergence asymmetry\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Target distribution: bimodal mixture\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "p_bimodal = 0.5 * stats.norm.pdf(x, -2, 0.7) + 0.5 * stats.norm.pdf(x, 2, 0.7)\n",
    "\n",
    "# Different approximations\n",
    "approximations = [\n",
    "    (\"Mode-covering (Wide Gaussian)\", 0, 2.5),\n",
    "    (\"Mode-seeking (Left mode)\", -2, 0.7),\n",
    "    (\"Mode-seeking (Right mode)\", 2, 0.7)\n",
    "]\n",
    "\n",
    "for ax, (title, mu, sigma) in zip(axes, approximations):\n",
    "    q = stats.norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    ax.fill_between(x, p_bimodal, alpha=0.3, color='blue', label='True posterior p(z|x)')\n",
    "    ax.fill_between(x, q, alpha=0.3, color='red', label=f'Approximation q(z)')\n",
    "    ax.plot(x, p_bimodal, 'b-', lw=2)\n",
    "    ax.plot(x, q, 'r--', lw=2)\n",
    "    \n",
    "    # Compute KL divergences numerically\n",
    "    samples = np.random.normal(mu, sigma, 10000)\n",
    "    q_pdf = lambda z: stats.norm.pdf(z, mu, sigma)\n",
    "    p_pdf = lambda z: 0.5 * stats.norm.pdf(z, -2, 0.7) + 0.5 * stats.norm.pdf(z, 2, 0.7)\n",
    "    \n",
    "    kl_q_p = kl_divergence_numerical(samples, p_pdf, q_pdf)\n",
    "    \n",
    "    ax.set_title(f'{title}\\nKL(q||p) ‚âà {kl_q_p:.3f}')\n",
    "    ax.set_xlabel('z')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xlim(-5, 5)\n",
    "\n",
    "plt.suptitle('KL Divergence: Mode-Covering vs Mode-Seeking Behavior', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insight:\")\n",
    "print(\"   - Forward KL (mode-covering): Places mass everywhere p has mass ‚Üí wide approximation\")\n",
    "print(\"   - Reverse KL (mode-seeking): Concentrates on high-density regions ‚Üí may miss modes\")\n",
    "print(\"   - VI uses reverse KL, explaining why it can underestimate posterior uncertainty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f732a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration of KL divergence between Gaussians\n",
    "def explore_kl_divergence():\n",
    "    \"\"\"\n",
    "    Visualize how KL divergence changes as q deviates from p.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Fixed target p ~ N(0, 1)\n",
    "    mu_p, sigma_p = 0, 1\n",
    "    x = np.linspace(-5, 5, 500)\n",
    "    p = stats.norm.pdf(x, mu_p, sigma_p)\n",
    "    \n",
    "    # Vary mean of q\n",
    "    ax = axes[0]\n",
    "    mu_range = np.linspace(-3, 3, 100)\n",
    "    kl_values = [kl_divergence_gaussians(mu_q, 1, mu_p, sigma_p) for mu_q in mu_range]\n",
    "    \n",
    "    ax.plot(mu_range, kl_values, 'b-', lw=2)\n",
    "    ax.axvline(0, color='red', linestyle='--', label='Optimal Œº_q = Œº_p')\n",
    "    ax.set_xlabel('Œº_q (mean of q)')\n",
    "    ax.set_ylabel('KL(q || p)')\n",
    "    ax.set_title('KL Divergence vs Mean Shift\\n(œÉ_q = œÉ_p = 1)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Vary variance of q\n",
    "    ax = axes[1]\n",
    "    sigma_range = np.linspace(0.1, 3, 100)\n",
    "    kl_values = [kl_divergence_gaussians(0, sigma_q, mu_p, sigma_p) for sigma_q in sigma_range]\n",
    "    \n",
    "    ax.plot(sigma_range, kl_values, 'b-', lw=2)\n",
    "    ax.axvline(1, color='red', linestyle='--', label='Optimal œÉ_q = œÉ_p')\n",
    "    ax.set_xlabel('œÉ_q (std of q)')\n",
    "    ax.set_ylabel('KL(q || p)')\n",
    "    ax.set_title('KL Divergence vs Variance Mismatch\\n(Œº_q = Œº_p = 0)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "explore_kl_divergence()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"   - KL divergence is minimized when q matches p exactly\")\n",
    "print(\"   - Mean shifts cause quadratic increase in KL\")\n",
    "print(\"   - Both underestimating and overestimating variance increases KL\")\n",
    "print(\"   - But underestimating variance (narrow q) is more heavily penalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413ad77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evidence Lower Bound (ELBO) Derivation <a name=\"3-elbo\"></a>\n",
    "\n",
    "The **Evidence Lower Bound (ELBO)** is the cornerstone of variational inference. We derive it by showing that maximizing ELBO is equivalent to minimizing KL divergence.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We want to compute the posterior $p(z|x)$ but it's intractable because of the normalizing constant:\n",
    "\n",
    "$$p(z|x) = \\frac{p(x|z)p(z)}{p(x)} = \\frac{p(x|z)p(z)}{\\int p(x|z)p(z)dz}$$\n",
    "\n",
    "The marginal likelihood $p(x) = \\int p(x|z)p(z)dz$ is often intractable.\n",
    "\n",
    "### ELBO Derivation\n",
    "\n",
    "Starting from the log marginal likelihood (log evidence):\n",
    "\n",
    "$$\\log p(x) = \\log \\int p(x, z) dz$$\n",
    "\n",
    "For any distribution $q(z)$:\n",
    "\n",
    "$$\\log p(x) = \\log \\int \\frac{p(x, z)}{q(z)} q(z) dz$$\n",
    "\n",
    "By Jensen's inequality ($\\log \\mathbb{E}[X] \\geq \\mathbb{E}[\\log X]$):\n",
    "\n",
    "$$\\log p(x) \\geq \\int q(z) \\log \\frac{p(x, z)}{q(z)} dz = \\mathcal{L}(q)$$\n",
    "\n",
    "This gives us the **ELBO**:\n",
    "\n",
    "$$\\boxed{\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x, z)] - \\mathbb{E}_q[\\log q(z)] = \\mathbb{E}_q[\\log p(x|z)] - D_{KL}(q(z) \\| p(z))}$$\n",
    "\n",
    "### Two Equivalent Forms of ELBO\n",
    "\n",
    "**Form 1 (Reconstruction + KL):**\n",
    "$$\\mathcal{L}(q) = \\underbrace{\\mathbb{E}_q[\\log p(x|z)]}_{\\text{Reconstruction}} - \\underbrace{D_{KL}(q(z) \\| p(z))}_{\\text{Regularization}}$$\n",
    "\n",
    "**Form 2 (Joint - Entropy):**\n",
    "$$\\mathcal{L}(q) = \\underbrace{\\mathbb{E}_q[\\log p(x, z)]}_{\\text{Expected log joint}} + \\underbrace{\\mathcal{H}[q(z)]}_{\\text{Entropy of q}}$$\n",
    "\n",
    "### Connection to KL Divergence\n",
    "\n",
    "$$\\log p(x) = \\mathcal{L}(q) + D_{KL}(q(z) \\| p(z|x))$$\n",
    "\n",
    "Since $D_{KL} \\geq 0$, maximizing $\\mathcal{L}(q)$ is equivalent to minimizing $D_{KL}(q \\| p(z|x))$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43579cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELBOVisualization:\n",
    "    \"\"\"\n",
    "    Visualize the ELBO and its relationship to the log evidence and KL divergence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, true_mu=2.0, true_sigma=1.0, prior_mu=0.0, prior_sigma=2.0, n_obs=50):\n",
    "        \"\"\"\n",
    "        Simple Bayesian inference problem: estimate mean of Gaussian with known variance.\n",
    "        \n",
    "        Likelihood: x_i | Œº ~ N(Œº, œÉ¬≤_known)\n",
    "        Prior: Œº ~ N(Œº_0, œÉ¬≤_0)\n",
    "        Posterior: Œº | x ~ N(Œº_n, œÉ¬≤_n) [conjugate, analytically tractable]\n",
    "        \"\"\"\n",
    "        self.prior_mu = prior_mu\n",
    "        self.prior_sigma = prior_sigma\n",
    "        self.likelihood_sigma = 1.0  # Known observation noise\n",
    "        \n",
    "        # Generate data\n",
    "        np.random.seed(42)\n",
    "        self.data = np.random.normal(true_mu, self.likelihood_sigma, n_obs)\n",
    "        self.n = len(self.data)\n",
    "        self.data_mean = np.mean(self.data)\n",
    "        \n",
    "        # Analytical posterior (conjugate)\n",
    "        self.posterior_precision = 1/prior_sigma**2 + self.n/self.likelihood_sigma**2\n",
    "        self.posterior_sigma = 1/np.sqrt(self.posterior_precision)\n",
    "        self.posterior_mu = (prior_mu/prior_sigma**2 + \n",
    "                            self.n * self.data_mean/self.likelihood_sigma**2) / self.posterior_precision\n",
    "        \n",
    "        # Log evidence (analytically computable for this simple case)\n",
    "        self._compute_log_evidence()\n",
    "    \n",
    "    def _compute_log_evidence(self):\n",
    "        \"\"\"Compute log p(x) analytically for Gaussian-Gaussian model.\"\"\"\n",
    "        # p(x) = ‚à´ p(x|Œº)p(Œº) dŒº\n",
    "        # For Gaussian conjugate: p(x) ~ N(Œº_0, œÉ¬≤_0 + œÉ¬≤_likelihood/n)\n",
    "        predictive_var = self.prior_sigma**2 + self.likelihood_sigma**2/self.n\n",
    "        self.log_evidence = stats.norm.logpdf(self.data_mean, self.prior_mu, np.sqrt(predictive_var))\n",
    "        # Adjust for full data\n",
    "        self.log_evidence = np.sum(stats.norm.logpdf(\n",
    "            self.data, self.data_mean, self.likelihood_sigma\n",
    "        )) + self.log_evidence\n",
    "    \n",
    "    def compute_elbo(self, q_mu, q_sigma):\n",
    "        \"\"\"\n",
    "        Compute ELBO for variational distribution q(Œº) = N(q_mu, q_sigma¬≤).\n",
    "        \n",
    "        ELBO = E_q[log p(x|Œº)] + E_q[log p(Œº)] - E_q[log q(Œº)]\n",
    "        \"\"\"\n",
    "        # E_q[log p(x|Œº)] - expected log likelihood\n",
    "        # = -n/2 log(2œÄ) - n/2 log(œÉ¬≤) - 1/(2œÉ¬≤) E_q[Œ£(x_i - Œº)¬≤]\n",
    "        # = -n/2 log(2œÄœÉ¬≤) - 1/(2œÉ¬≤) [Œ£(x_i - E[Œº])¬≤ + n*Var[Œº]]\n",
    "        \n",
    "        sum_sq = np.sum((self.data - q_mu)**2)\n",
    "        expected_log_lik = (-self.n/2 * np.log(2*np.pi*self.likelihood_sigma**2) \n",
    "                          - 1/(2*self.likelihood_sigma**2) * (sum_sq + self.n * q_sigma**2))\n",
    "        \n",
    "        # E_q[log p(Œº)] - expected log prior\n",
    "        # = -1/2 log(2œÄ) - log(œÉ_0) - 1/(2œÉ_0¬≤) E_q[(Œº - Œº_0)¬≤]\n",
    "        expected_log_prior = (-0.5 * np.log(2*np.pi*self.prior_sigma**2)\n",
    "                             - 1/(2*self.prior_sigma**2) * ((q_mu - self.prior_mu)**2 + q_sigma**2))\n",
    "        \n",
    "        # -E_q[log q(Œº)] = H[q] (entropy)\n",
    "        entropy = 0.5 * np.log(2*np.pi*np.e*q_sigma**2)\n",
    "        \n",
    "        elbo = expected_log_lik + expected_log_prior + entropy\n",
    "        return elbo\n",
    "    \n",
    "    def compute_kl_to_posterior(self, q_mu, q_sigma):\n",
    "        \"\"\"Compute KL(q || posterior).\"\"\"\n",
    "        return kl_divergence_gaussians(q_mu, q_sigma, self.posterior_mu, self.posterior_sigma)\n",
    "    \n",
    "    def visualize(self):\n",
    "        \"\"\"Visualize ELBO landscape and its relationship to KL divergence.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Plot 1: Distributions\n",
    "        ax = axes[0, 0]\n",
    "        x = np.linspace(-2, 5, 500)\n",
    "        ax.plot(x, stats.norm.pdf(x, self.prior_mu, self.prior_sigma), \n",
    "                'g--', lw=2, label=f'Prior: N({self.prior_mu}, {self.prior_sigma}¬≤)')\n",
    "        ax.plot(x, stats.norm.pdf(x, self.posterior_mu, self.posterior_sigma), \n",
    "                'b-', lw=2, label=f'Posterior: N({self.posterior_mu:.2f}, {self.posterior_sigma:.3f}¬≤)')\n",
    "        ax.axvline(self.data_mean, color='red', linestyle=':', label=f'Data mean: {self.data_mean:.2f}')\n",
    "        ax.set_xlabel('Œº')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_title('Prior and True Posterior')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: ELBO surface\n",
    "        ax = axes[0, 1]\n",
    "        mu_range = np.linspace(0, 4, 100)\n",
    "        sigma_range = np.linspace(0.05, 1.5, 100)\n",
    "        MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "        ELBO = np.zeros_like(MU)\n",
    "        \n",
    "        for i in range(len(sigma_range)):\n",
    "            for j in range(len(mu_range)):\n",
    "                ELBO[i, j] = self.compute_elbo(MU[i, j], SIGMA[i, j])\n",
    "        \n",
    "        contour = ax.contourf(MU, SIGMA, ELBO, levels=50, cmap='viridis')\n",
    "        ax.scatter(self.posterior_mu, self.posterior_sigma, c='red', s=100, \n",
    "                   marker='*', label=f'Optimal: ({self.posterior_mu:.2f}, {self.posterior_sigma:.3f})')\n",
    "        ax.set_xlabel('q_Œº')\n",
    "        ax.set_ylabel('q_œÉ')\n",
    "        ax.set_title('ELBO Surface')\n",
    "        ax.legend()\n",
    "        plt.colorbar(contour, ax=ax, label='ELBO')\n",
    "        \n",
    "        # Plot 3: ELBO vs KL relationship\n",
    "        ax = axes[1, 0]\n",
    "        elbo_values = []\n",
    "        kl_values = []\n",
    "        \n",
    "        for sigma in np.linspace(0.1, 1.0, 20):\n",
    "            for mu in np.linspace(0.5, 3.5, 20):\n",
    "                elbo_values.append(self.compute_elbo(mu, sigma))\n",
    "                kl_values.append(self.compute_kl_to_posterior(mu, sigma))\n",
    "        \n",
    "        ax.scatter(elbo_values, kl_values, alpha=0.5, c=kl_values, cmap='coolwarm')\n",
    "        ax.set_xlabel('ELBO')\n",
    "        ax.set_ylabel('KL(q || posterior)')\n",
    "        ax.set_title('ELBO vs KL Divergence\\n(Higher ELBO ‚Üí Lower KL)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: ELBO decomposition during optimization\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        # Simulate optimization path\n",
    "        q_mus = np.linspace(0, self.posterior_mu, 50)\n",
    "        q_sigmas = np.linspace(1.0, self.posterior_sigma, 50)\n",
    "        \n",
    "        elbos = [self.compute_elbo(m, s) for m, s in zip(q_mus, q_sigmas)]\n",
    "        kls = [self.compute_kl_to_posterior(m, s) for m, s in zip(q_mus, q_sigmas)]\n",
    "        \n",
    "        ax.plot(elbos, 'b-', lw=2, label='ELBO')\n",
    "        ax.axhline(self.log_evidence, color='green', linestyle='--', \n",
    "                   label=f'log p(x) = {self.log_evidence:.2f}')\n",
    "        ax.fill_between(range(len(elbos)), elbos, self.log_evidence, \n",
    "                        alpha=0.3, color='red', label='Gap = KL(q||p)')\n",
    "        ax.set_xlabel('Optimization Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.set_title('ELBO Optimization: log p(x) = ELBO + KL')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"   True posterior: N({self.posterior_mu:.4f}, {self.posterior_sigma:.4f}¬≤)\")\n",
    "        print(f\"   Log evidence: {self.log_evidence:.4f}\")\n",
    "        print(f\"   Optimal ELBO: {self.compute_elbo(self.posterior_mu, self.posterior_sigma):.4f}\")\n",
    "        print(f\"   Gap (should be ‚âà0): {self.log_evidence - self.compute_elbo(self.posterior_mu, self.posterior_sigma):.6f}\")\n",
    "\n",
    "\n",
    "# Run visualization\n",
    "elbo_viz = ELBOVisualization()\n",
    "elbo_viz.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99bf565",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Mean-Field Variational Inference <a name=\"4-mean-field\"></a>\n",
    "\n",
    "The **Mean-Field Approximation** is the most common form of VI. It assumes the variational distribution factorizes over all latent variables:\n",
    "\n",
    "$$q(z_1, z_2, \\ldots, z_K) = \\prod_{k=1}^K q_k(z_k)$$\n",
    "\n",
    "This independence assumption makes optimization tractable but limits the approximation's expressiveness.\n",
    "\n",
    "### Optimal Update Equations\n",
    "\n",
    "For mean-field VI, the optimal $q_j^*(z_j)$ that maximizes ELBO is:\n",
    "\n",
    "$$\\log q_j^*(z_j) = \\mathbb{E}_{q_{-j}}[\\log p(z, x)] + \\text{const}$$\n",
    "\n",
    "Where $q_{-j}$ denotes all factors except $q_j$.\n",
    "\n",
    "### Application: Bayesian Linear Regression\n",
    "\n",
    "Consider the model:\n",
    "- **Likelihood**: $y | X, w, \\tau \\sim \\mathcal{N}(Xw, \\tau^{-1}I)$\n",
    "- **Prior on weights**: $w \\sim \\mathcal{N}(0, \\alpha^{-1}I)$  \n",
    "- **Prior on precision**: $\\tau \\sim \\text{Gamma}(a_0, b_0)$\n",
    "\n",
    "We seek: $q(w, \\tau) = q(w)q(\\tau)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef820664",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanFieldBayesianLinearRegression:\n",
    "    \"\"\"\n",
    "    Mean-Field Variational Inference for Bayesian Linear Regression.\n",
    "    \n",
    "    Model:\n",
    "        y = Xw + Œµ, Œµ ~ N(0, œÑ‚Åª¬πI)\n",
    "        w ~ N(0, Œ±‚Åª¬πI)\n",
    "        œÑ ~ Gamma(a‚ÇÄ, b‚ÇÄ)\n",
    "    \n",
    "    Variational family:\n",
    "        q(w, œÑ) = q(w)q(œÑ)\n",
    "        q(w) = N(Œº_w, Œ£_w)\n",
    "        q(œÑ) = Gamma(a_n, b_n)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, a0=1.0, b0=1.0):\n",
    "        \"\"\"\n",
    "        Initialize hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha: Precision of weight prior\n",
    "            a0, b0: Parameters of Gamma prior on noise precision\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.a0 = a0\n",
    "        self.b0 = b0\n",
    "        self.elbo_history = []\n",
    "        \n",
    "    def fit(self, X, y, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Fit the model using coordinate ascent variational inference.\n",
    "        \n",
    "        The update equations are derived from:\n",
    "        log q*(w) ‚àù E_q(œÑ)[log p(y|X,w,œÑ)] + log p(w)\n",
    "        log q*(œÑ) ‚àù E_q(w)[log p(y|X,w,œÑ)] + log p(œÑ)\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Initialize variational parameters\n",
    "        # q(w) = N(Œº_w, Œ£_w)\n",
    "        self.mu_w = np.zeros(D)\n",
    "        self.Sigma_w = np.eye(D) / self.alpha\n",
    "        \n",
    "        # q(œÑ) = Gamma(a_n, b_n)\n",
    "        self.a_n = self.a0\n",
    "        self.b_n = self.b0\n",
    "        \n",
    "        # Precompute\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "        yty = y.T @ y\n",
    "        \n",
    "        prev_elbo = -np.inf\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # E[œÑ] from current q(œÑ)\n",
    "            E_tau = self.a_n / self.b_n\n",
    "            \n",
    "            # Update q(w) = N(Œº_w, Œ£_w)\n",
    "            # Œ£_w = (Œ±I + E[œÑ]X'X)‚Åª¬π\n",
    "            # Œº_w = E[œÑ] Œ£_w X'y\n",
    "            self.Sigma_w = np.linalg.inv(self.alpha * np.eye(D) + E_tau * XtX)\n",
    "            self.mu_w = E_tau * self.Sigma_w @ Xty\n",
    "            \n",
    "            # Update q(œÑ) = Gamma(a_n, b_n)\n",
    "            # a_n = a‚ÇÄ + N/2\n",
    "            # b_n = b‚ÇÄ + (1/2) E_q(w)[(y - Xw)'(y - Xw)]\n",
    "            #     = b‚ÇÄ + (1/2) [y'y - 2Œº'_w X'y + tr(X'X(Œ£_w + Œº_w Œº'_w))]\n",
    "            self.a_n = self.a0 + N / 2\n",
    "            \n",
    "            E_wtXtXw = np.trace(XtX @ self.Sigma_w) + self.mu_w @ XtX @ self.mu_w\n",
    "            self.b_n = self.b0 + 0.5 * (yty - 2 * self.mu_w @ Xty + E_wtXtXw)\n",
    "            \n",
    "            # Compute ELBO\n",
    "            elbo = self._compute_elbo(X, y)\n",
    "            self.elbo_history.append(elbo)\n",
    "            \n",
    "            # Check convergence\n",
    "            if abs(elbo - prev_elbo) < tol:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_elbo = elbo\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_elbo(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the Evidence Lower Bound.\n",
    "        \n",
    "        ELBO = E_q[log p(y|X,w,œÑ)] + E_q[log p(w)] + E_q[log p(œÑ)]\n",
    "               - E_q[log q(w)] - E_q[log q(œÑ)]\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        E_tau = self.a_n / self.b_n\n",
    "        E_log_tau = digamma(self.a_n) - np.log(self.b_n)\n",
    "        \n",
    "        # E_q[log p(y|X,w,œÑ)]\n",
    "        XtX = X.T @ X\n",
    "        Xty = X.T @ y\n",
    "        yty = y.T @ y\n",
    "        \n",
    "        E_wtXtXw = np.trace(XtX @ self.Sigma_w) + self.mu_w @ XtX @ self.mu_w\n",
    "        \n",
    "        E_log_lik = (N/2 * E_log_tau - N/2 * np.log(2*np.pi) \n",
    "                   - E_tau/2 * (yty - 2*self.mu_w @ Xty + E_wtXtXw))\n",
    "        \n",
    "        # E_q[log p(w)]\n",
    "        E_wtw = np.trace(self.Sigma_w) + self.mu_w @ self.mu_w\n",
    "        E_log_prior_w = (-D/2 * np.log(2*np.pi/self.alpha) - self.alpha/2 * E_wtw)\n",
    "        \n",
    "        # E_q[log p(œÑ)]\n",
    "        E_log_prior_tau = ((self.a0 - 1) * E_log_tau - self.b0 * E_tau \n",
    "                          + self.a0 * np.log(self.b0) - gammaln(self.a0))\n",
    "        \n",
    "        # -E_q[log q(w)] = entropy of Gaussian\n",
    "        H_q_w = 0.5 * D * (1 + np.log(2*np.pi)) + 0.5 * np.linalg.slogdet(self.Sigma_w)[1]\n",
    "        \n",
    "        # -E_q[log q(œÑ)] = entropy of Gamma\n",
    "        H_q_tau = (self.a_n - np.log(self.b_n) + gammaln(self.a_n) \n",
    "                  + (1 - self.a_n) * digamma(self.a_n))\n",
    "        \n",
    "        elbo = E_log_lik + E_log_prior_w + E_log_prior_tau + H_q_w + H_q_tau\n",
    "        return elbo\n",
    "    \n",
    "    def predict(self, X_new, return_std=False):\n",
    "        \"\"\"\n",
    "        Predict using the variational posterior.\n",
    "        \n",
    "        Returns:\n",
    "            y_pred: Posterior predictive mean\n",
    "            y_std: Posterior predictive standard deviation (if return_std=True)\n",
    "        \"\"\"\n",
    "        y_pred = X_new @ self.mu_w\n",
    "        \n",
    "        if return_std:\n",
    "            # Predictive variance = E[œÑ‚Åª¬π] + X_new Œ£_w X_new'\n",
    "            E_tau_inv = self.b_n / (self.a_n - 1) if self.a_n > 1 else np.inf\n",
    "            var_pred = E_tau_inv + np.sum(X_new @ self.Sigma_w * X_new, axis=1)\n",
    "            return y_pred, np.sqrt(var_pred)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Generate synthetic data for Bayesian Linear Regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters\n",
    "true_w = np.array([2.0, -1.5, 0.5])\n",
    "true_tau = 4.0  # Noise precision (variance = 0.25)\n",
    "\n",
    "# Generate features\n",
    "N_train = 200\n",
    "N_test = 50\n",
    "X_train = np.random.randn(N_train, 3)\n",
    "X_test = np.random.randn(N_test, 3)\n",
    "\n",
    "# Generate targets with noise\n",
    "y_train = X_train @ true_w + np.random.normal(0, 1/np.sqrt(true_tau), N_train)\n",
    "y_test = X_test @ true_w + np.random.normal(0, 1/np.sqrt(true_tau), N_test)\n",
    "\n",
    "# Fit Mean-Field VI model\n",
    "vi_model = MeanFieldBayesianLinearRegression(alpha=1.0, a0=1.0, b0=1.0)\n",
    "vi_model.fit(X_train, y_train, max_iter=100)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train, y_std_train = vi_model.predict(X_train, return_std=True)\n",
    "y_pred_test, y_std_test = vi_model.predict(X_test, return_std=True)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: ELBO convergence\n",
    "ax = axes[0, 0]\n",
    "ax.plot(vi_model.elbo_history, 'b-', lw=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('ELBO')\n",
    "ax.set_title('ELBO Convergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Weight posterior\n",
    "ax = axes[0, 1]\n",
    "x_range = np.linspace(-3, 4, 500)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (true_val, mu, sigma, color) in enumerate(zip(\n",
    "    true_w, vi_model.mu_w, np.sqrt(np.diag(vi_model.Sigma_w)), colors\n",
    ")):\n",
    "    ax.plot(x_range, stats.norm.pdf(x_range, mu, sigma), \n",
    "            color=color, lw=2, label=f'q(w_{i}): Œº={mu:.2f}, œÉ={sigma:.2f}')\n",
    "    ax.axvline(true_val, color=color, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Weight value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Distributions over Weights\\n(dashed = true values)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Predictions vs Actuals\n",
    "ax = axes[1, 0]\n",
    "ax.errorbar(y_test, y_pred_test, yerr=2*y_std_test, fmt='o', alpha=0.5, \n",
    "            capsize=3, label='Predictions ¬± 2œÉ')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "        'r--', lw=2, label='Perfect prediction')\n",
    "ax.set_xlabel('True y')\n",
    "ax.set_ylabel('Predicted y')\n",
    "ax.set_title('Test Set: Predictions vs Actuals')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Noise precision posterior\n",
    "ax = axes[1, 1]\n",
    "tau_range = np.linspace(0.1, 10, 500)\n",
    "ax.plot(tau_range, stats.gamma.pdf(tau_range, vi_model.a_n, scale=1/vi_model.b_n), \n",
    "        'b-', lw=2, label=f'q(œÑ): Gamma({vi_model.a_n:.1f}, {vi_model.b_n:.1f})')\n",
    "ax.axvline(true_tau, color='red', linestyle='--', lw=2, label=f'True œÑ = {true_tau}')\n",
    "ax.axvline(vi_model.a_n/vi_model.b_n, color='blue', linestyle=':', lw=2, \n",
    "           label=f'E[œÑ] = {vi_model.a_n/vi_model.b_n:.2f}')\n",
    "ax.set_xlabel('œÑ (noise precision)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Posterior Distribution over Noise Precision')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä Mean-Field VI Results:\")\n",
    "print(f\"   True weights: {true_w}\")\n",
    "print(f\"   VI posterior means: {vi_model.mu_w}\")\n",
    "print(f\"   VI posterior stds: {np.sqrt(np.diag(vi_model.Sigma_w))}\")\n",
    "print(f\"\\n   True noise precision: {true_tau}\")\n",
    "print(f\"   VI E[œÑ]: {vi_model.a_n/vi_model.b_n:.3f}\")\n",
    "print(f\"\\n   Test RMSE: {np.sqrt(np.mean((y_test - y_pred_test)**2)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca4958",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Variational Inference for Gaussian Mixture Models <a name=\"5-gmm\"></a>\n",
    "\n",
    "Gaussian Mixture Models (GMMs) are a classic application of VI, where we infer:\n",
    "- **Cluster assignments** $z_n$ for each data point\n",
    "- **Cluster means** $\\mu_k$\n",
    "- **Cluster precisions** $\\Lambda_k$\n",
    "- **Mixing proportions** $\\pi$\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "$$p(x, z, \\mu, \\Lambda, \\pi) = p(\\pi) \\prod_k p(\\mu_k) p(\\Lambda_k) \\prod_n p(z_n|\\pi) p(x_n|z_n, \\mu, \\Lambda)$$\n",
    "\n",
    "### Mean-Field Factorization\n",
    "\n",
    "$$q(z, \\mu, \\Lambda, \\pi) = q(\\pi) \\prod_k q(\\mu_k) q(\\Lambda_k) \\prod_n q(z_n)$$\n",
    "\n",
    "This leads to iterative updates similar to EM, but with full posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b928ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGMM:\n",
    "    \"\"\"\n",
    "    Variational Inference for Gaussian Mixture Models.\n",
    "    \n",
    "    Implements mean-field VI with:\n",
    "    - Dirichlet prior on mixing proportions\n",
    "    - Normal-Wishart prior on component parameters\n",
    "    \n",
    "    This is similar to sklearn's BayesianGaussianMixture but from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=3, alpha_0=1.0, beta_0=1.0, \n",
    "                 nu_0=None, m_0=None, W_0=None):\n",
    "        \"\"\"\n",
    "        Initialize prior hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            n_components: Number of mixture components\n",
    "            alpha_0: Dirichlet concentration parameter\n",
    "            beta_0: Scale for mean prior\n",
    "            nu_0: Degrees of freedom for Wishart (default: D)\n",
    "            m_0: Prior mean (default: zeros)\n",
    "            W_0: Prior precision matrix (default: identity)\n",
    "        \"\"\"\n",
    "        self.K = n_components\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "        self.nu_0 = nu_0\n",
    "        self.m_0 = m_0\n",
    "        self.W_0 = W_0\n",
    "        self.elbo_history = []\n",
    "        \n",
    "    def fit(self, X, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Fit the Variational GMM using coordinate ascent.\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Set default hyperparameters if not provided\n",
    "        if self.nu_0 is None:\n",
    "            self.nu_0 = D\n",
    "        if self.m_0 is None:\n",
    "            self.m_0 = np.zeros(D)\n",
    "        if self.W_0 is None:\n",
    "            self.W_0 = np.eye(D)\n",
    "        \n",
    "        # Initialize variational parameters\n",
    "        # q(œÄ) = Dir(Œ±), q(Œº_k, Œõ_k) = NW(m_k, Œ≤_k, W_k, ŒΩ_k)\n",
    "        self.alpha = np.ones(self.K) * self.alpha_0\n",
    "        self.beta = np.ones(self.K) * self.beta_0\n",
    "        self.nu = np.ones(self.K) * self.nu_0\n",
    "        self.m = X[np.random.choice(N, self.K, replace=False)]  # Initialize to random data points\n",
    "        self.W = np.array([self.W_0.copy() for _ in range(self.K)])\n",
    "        \n",
    "        # Responsibilities q(z_n = k)\n",
    "        self.r = np.random.dirichlet(np.ones(self.K), N)\n",
    "        \n",
    "        prev_elbo = -np.inf\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # E-step: Update responsibilities\n",
    "            self._update_responsibilities(X)\n",
    "            \n",
    "            # M-step: Update variational parameters for œÄ, Œº, Œõ\n",
    "            self._update_parameters(X)\n",
    "            \n",
    "            # Compute ELBO\n",
    "            elbo = self._compute_elbo(X)\n",
    "            self.elbo_history.append(elbo)\n",
    "            \n",
    "            if abs(elbo - prev_elbo) < tol:\n",
    "                print(f\"Converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_elbo = elbo\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _update_responsibilities(self, X):\n",
    "        \"\"\"\n",
    "        Update q(z_n) - the cluster responsibilities.\n",
    "        \n",
    "        log r_nk ‚àù E[log œÄ_k] + (1/2)E[log|Œõ_k|] \n",
    "                   - (D/2)log(2œÄ) - (1/2)E[(x_n - Œº_k)'Œõ_k(x_n - Œº_k)]\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # E[log œÄ_k] from Dirichlet\n",
    "        E_log_pi = digamma(self.alpha) - digamma(np.sum(self.alpha))\n",
    "        \n",
    "        # E[log |Œõ_k|] from Wishart\n",
    "        E_log_det_Lambda = np.zeros(self.K)\n",
    "        for k in range(self.K):\n",
    "            E_log_det_Lambda[k] = (D * np.log(2) + np.linalg.slogdet(self.W[k])[1] +\n",
    "                                  np.sum(digamma((self.nu[k] + 1 - np.arange(1, D+1))/2)))\n",
    "        \n",
    "        # E[(x - Œº)'Œõ(x - Œº)]\n",
    "        log_rho = np.zeros((N, self.K))\n",
    "        for k in range(self.K):\n",
    "            diff = X - self.m[k]\n",
    "            # E[(x-Œº)'Œõ(x-Œº)] = D/Œ≤ + ŒΩ*(x-m)'W(x-m)\n",
    "            E_quad = D/self.beta[k] + self.nu[k] * np.sum(diff @ self.W[k] * diff, axis=1)\n",
    "            log_rho[:, k] = E_log_pi[k] + 0.5*E_log_det_Lambda[k] - D/2*np.log(2*np.pi) - 0.5*E_quad\n",
    "        \n",
    "        # Normalize responsibilities\n",
    "        log_rho_max = np.max(log_rho, axis=1, keepdims=True)\n",
    "        self.r = np.exp(log_rho - log_rho_max)\n",
    "        self.r /= np.sum(self.r, axis=1, keepdims=True)\n",
    "        \n",
    "        # Statistics\n",
    "        self.N_k = np.sum(self.r, axis=0) + 1e-10  # Avoid division by zero\n",
    "        self.x_bar = (self.r.T @ X) / self.N_k[:, np.newaxis]\n",
    "        \n",
    "    def _update_parameters(self, X):\n",
    "        \"\"\"\n",
    "        Update variational parameters for œÄ and (Œº, Œõ).\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Update Dirichlet parameters\n",
    "        self.alpha = self.alpha_0 + self.N_k\n",
    "        \n",
    "        # Update Normal-Wishart parameters for each component\n",
    "        for k in range(self.K):\n",
    "            # Sufficient statistics\n",
    "            x_bar_k = self.x_bar[k]\n",
    "            diff = X - x_bar_k\n",
    "            S_k = (self.r[:, k:k+1] * diff).T @ diff / self.N_k[k]\n",
    "            \n",
    "            # Update parameters\n",
    "            self.beta[k] = self.beta_0 + self.N_k[k]\n",
    "            self.m[k] = (self.beta_0 * self.m_0 + self.N_k[k] * x_bar_k) / self.beta[k]\n",
    "            self.nu[k] = self.nu_0 + self.N_k[k]\n",
    "            \n",
    "            diff_m = x_bar_k - self.m_0\n",
    "            self.W[k] = np.linalg.inv(\n",
    "                np.linalg.inv(self.W_0) + self.N_k[k] * S_k +\n",
    "                (self.beta_0 * self.N_k[k] / self.beta[k]) * np.outer(diff_m, diff_m)\n",
    "            )\n",
    "    \n",
    "    def _compute_elbo(self, X):\n",
    "        \"\"\"\n",
    "        Compute the Evidence Lower Bound.\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # This is a simplified ELBO computation\n",
    "        # Full computation involves multiple expectation terms\n",
    "        \n",
    "        # E[log p(X|Z,Œº,Œõ)]\n",
    "        E_log_lik = 0\n",
    "        for k in range(self.K):\n",
    "            E_log_det = D*np.log(2) + np.linalg.slogdet(self.W[k])[1] + \\\n",
    "                       np.sum(digamma((self.nu[k]+1-np.arange(1,D+1))/2))\n",
    "            diff = X - self.m[k]\n",
    "            E_quad = D/self.beta[k] + self.nu[k] * np.sum(diff @ self.W[k] * diff, axis=1)\n",
    "            E_log_lik += np.sum(self.r[:, k] * (0.5*E_log_det - D/2*np.log(2*np.pi) - 0.5*E_quad))\n",
    "        \n",
    "        # E[log p(Z|œÄ)]\n",
    "        E_log_pi = digamma(self.alpha) - digamma(np.sum(self.alpha))\n",
    "        E_log_pz = np.sum(self.r * E_log_pi)\n",
    "        \n",
    "        # -E[log q(Z)]\n",
    "        H_qz = -np.sum(self.r * np.log(self.r + 1e-10))\n",
    "        \n",
    "        elbo = E_log_lik + E_log_pz + H_qz\n",
    "        return elbo\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return cluster assignments.\"\"\"\n",
    "        self._update_responsibilities(X)\n",
    "        return np.argmax(self.r, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return cluster probabilities.\"\"\"\n",
    "        self._update_responsibilities(X)\n",
    "        return self.r\n",
    "\n",
    "\n",
    "# Generate synthetic data with 3 clusters\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# True cluster parameters\n",
    "true_means = [[-3, -3], [0, 4], [4, 0]]\n",
    "true_covs = [[[0.5, 0.2], [0.2, 0.5]], \n",
    "             [[1.0, 0], [0, 0.3]], \n",
    "             [[0.3, -0.1], [-0.1, 0.8]]]\n",
    "true_weights = [0.3, 0.4, 0.3]\n",
    "\n",
    "# Sample from mixture\n",
    "X_gmm = []\n",
    "true_labels = []\n",
    "for i, (mean, cov, weight) in enumerate(zip(true_means, true_covs, true_weights)):\n",
    "    n = int(n_samples * weight)\n",
    "    X_gmm.append(np.random.multivariate_normal(mean, cov, n))\n",
    "    true_labels.extend([i] * n)\n",
    "\n",
    "X_gmm = np.vstack(X_gmm)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Shuffle\n",
    "shuffle_idx = np.random.permutation(len(X_gmm))\n",
    "X_gmm = X_gmm[shuffle_idx]\n",
    "true_labels = true_labels[shuffle_idx]\n",
    "\n",
    "# Fit Variational GMM\n",
    "vgmm = VariationalGMM(n_components=3)\n",
    "vgmm.fit(X_gmm, max_iter=100)\n",
    "\n",
    "# Get cluster assignments\n",
    "pred_labels = vgmm.predict(X_gmm)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: True clusters\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(X_gmm[:, 0], X_gmm[:, 1], c=true_labels, cmap='viridis', alpha=0.6, s=20)\n",
    "for mean in true_means:\n",
    "    ax.scatter(*mean, c='red', marker='x', s=200, linewidths=3)\n",
    "ax.set_title('True Clusters')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "# Plot 2: VI inferred clusters\n",
    "ax = axes[1]\n",
    "ax.scatter(X_gmm[:, 0], X_gmm[:, 1], c=pred_labels, cmap='viridis', alpha=0.6, s=20)\n",
    "for k in range(vgmm.K):\n",
    "    ax.scatter(*vgmm.m[k], c='red', marker='*', s=200, linewidths=2)\n",
    "    # Draw covariance ellipse\n",
    "    cov_k = np.linalg.inv(vgmm.nu[k] * vgmm.W[k])  # Expected covariance\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_k)\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "    width, height = 2 * 2 * np.sqrt(eigenvalues)  # 2 std\n",
    "    ellipse = Ellipse(vgmm.m[k], width, height, angle=angle, \n",
    "                     fill=False, color='red', linewidth=2)\n",
    "    ax.add_patch(ellipse)\n",
    "ax.set_title('VI Inferred Clusters')\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "\n",
    "# Plot 3: ELBO convergence\n",
    "ax = axes[2]\n",
    "ax.plot(vgmm.elbo_history, 'b-', lw=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('ELBO')\n",
    "ax.set_title('ELBO Convergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print mixing proportions\n",
    "print(\"\\nüìä Variational GMM Results:\")\n",
    "print(f\"   True mixing proportions: {true_weights}\")\n",
    "print(f\"   VI posterior E[œÄ]: {vgmm.alpha / np.sum(vgmm.alpha)}\")\n",
    "print(f\"\\n   Effective components (N_k > 1): {np.sum(vgmm.N_k > 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dfc71a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Coordinate Ascent Variational Inference (CAVI) <a name=\"6-cavi\"></a>\n",
    "\n",
    "**Coordinate Ascent Variational Inference (CAVI)** is a systematic algorithm for optimizing the ELBO by iteratively updating each variational factor while holding others fixed.\n",
    "\n",
    "### CAVI Algorithm\n",
    "\n",
    "For mean-field factorization $q(z) = \\prod_j q_j(z_j)$:\n",
    "\n",
    "**repeat until convergence:**\n",
    "- **for** each latent variable $j$:\n",
    "  $$\\log q_j^*(z_j) \\leftarrow \\mathbb{E}_{-j}[\\log p(z, x)] + \\text{const}$$\n",
    "\n",
    "The key insight is that the optimal form of each factor is determined by the expected complete conditional.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Monotonic increase**: Each update is guaranteed to increase (or maintain) the ELBO\n",
    "- **Coordinate-wise**: Only one factor changes at a time\n",
    "- **Closed-form**: For conjugate exponential family models, updates have analytical solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAVI_BayesianFactorModel:\n",
    "    \"\"\"\n",
    "    Coordinate Ascent VI for a simple Bayesian Factor Model.\n",
    "    \n",
    "    Model: Asset returns decompose into factor exposures and idiosyncratic risk\n",
    "        r_t = B @ f_t + Œµ_t\n",
    "        \n",
    "    Where:\n",
    "        r_t: N-dimensional asset returns at time t\n",
    "        B: N √ó K factor loading matrix\n",
    "        f_t: K-dimensional latent factors\n",
    "        Œµ_t ~ N(0, Œ®) idiosyncratic noise\n",
    "    \n",
    "    Priors:\n",
    "        B_nk ~ N(0, œÉ_B¬≤)\n",
    "        f_tk ~ N(0, 1)\n",
    "        œà_n ~ InvGamma(a, b)\n",
    "    \n",
    "    This is a simplified factor model for demonstration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors=3, sigma_B=1.0, a_psi=1.0, b_psi=1.0):\n",
    "        self.K = n_factors\n",
    "        self.sigma_B = sigma_B\n",
    "        self.a_psi = a_psi\n",
    "        self.b_psi = b_psi\n",
    "        self.elbo_history = []\n",
    "        \n",
    "    def fit(self, R, max_iter=100, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Fit the factor model using CAVI.\n",
    "        \n",
    "        Args:\n",
    "            R: T √ó N matrix of returns (T time periods, N assets)\n",
    "        \"\"\"\n",
    "        T, N = R.shape\n",
    "        K = self.K\n",
    "        \n",
    "        # Initialize variational parameters\n",
    "        # q(B) = N(m_B, S_B) - each row independently\n",
    "        self.m_B = np.random.randn(N, K) * 0.1\n",
    "        self.S_B = np.array([np.eye(K) for _ in range(N)])\n",
    "        \n",
    "        # q(F) = N(m_F, S_F) - each row independently\n",
    "        self.m_F = np.random.randn(T, K) * 0.1\n",
    "        self.S_F = np.array([np.eye(K) for _ in range(T)])\n",
    "        \n",
    "        # q(œà) = InvGamma(a_n, b_n)\n",
    "        self.a_n = np.ones(N) * (self.a_psi + T/2)\n",
    "        self.b_n = np.ones(N) * self.b_psi\n",
    "        \n",
    "        prev_elbo = -np.inf\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # CAVI updates\n",
    "            self._update_factors(R)      # Update q(F)\n",
    "            self._update_loadings(R)     # Update q(B)\n",
    "            self._update_precisions(R)   # Update q(œà)\n",
    "            \n",
    "            # Compute ELBO\n",
    "            elbo = self._compute_elbo(R)\n",
    "            self.elbo_history.append(elbo)\n",
    "            \n",
    "            if iteration > 0 and abs(elbo - prev_elbo) < tol:\n",
    "                print(f\"CAVI converged at iteration {iteration + 1}\")\n",
    "                break\n",
    "            prev_elbo = elbo\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _update_factors(self, R):\n",
    "        \"\"\"Update q(F) = ‚àè_t q(f_t).\"\"\"\n",
    "        T, N = R.shape\n",
    "        \n",
    "        # E[Œ®‚Åª¬π] diagonal\n",
    "        E_psi_inv = self.a_n / self.b_n\n",
    "        \n",
    "        for t in range(T):\n",
    "            # S_F_t = (I + E[B'Œ®‚Åª¬πB])‚Åª¬π\n",
    "            E_BtPsiB = np.zeros((self.K, self.K))\n",
    "            E_BtPsi_r = np.zeros(self.K)\n",
    "            \n",
    "            for n in range(N):\n",
    "                # E[B_n' Œ®_n‚Åª¬π B_n] = Œ®_n‚Åª¬π (S_B_n + m_B_n m_B_n')\n",
    "                E_BtPsiB += E_psi_inv[n] * (self.S_B[n] + np.outer(self.m_B[n], self.m_B[n]))\n",
    "                E_BtPsi_r += E_psi_inv[n] * self.m_B[n] * R[t, n]\n",
    "            \n",
    "            self.S_F[t] = np.linalg.inv(np.eye(self.K) + E_BtPsiB)\n",
    "            self.m_F[t] = self.S_F[t] @ E_BtPsi_r\n",
    "    \n",
    "    def _update_loadings(self, R):\n",
    "        \"\"\"Update q(B) = ‚àè_n q(B_n).\"\"\"\n",
    "        T, N = R.shape\n",
    "        \n",
    "        E_psi_inv = self.a_n / self.b_n\n",
    "        \n",
    "        # E[F'F] = Œ£_t (S_F_t + m_F_t m_F_t')\n",
    "        E_FtF = np.sum([self.S_F[t] + np.outer(self.m_F[t], self.m_F[t]) for t in range(T)], axis=0)\n",
    "        \n",
    "        for n in range(N):\n",
    "            # S_B_n = (1/œÉ_B¬≤ I + œà_n‚Åª¬π E[F'F])‚Åª¬π\n",
    "            self.S_B[n] = np.linalg.inv(np.eye(self.K)/self.sigma_B**2 + E_psi_inv[n] * E_FtF)\n",
    "            \n",
    "            # m_B_n = S_B_n œà_n‚Åª¬π E[F]' r_n\n",
    "            self.m_B[n] = E_psi_inv[n] * self.S_B[n] @ self.m_F.T @ R[:, n]\n",
    "    \n",
    "    def _update_precisions(self, R):\n",
    "        \"\"\"Update q(œà_n) = InvGamma(a_n, b_n).\"\"\"\n",
    "        T, N = R.shape\n",
    "        \n",
    "        self.a_n = np.ones(N) * (self.a_psi + T/2)\n",
    "        \n",
    "        for n in range(N):\n",
    "            # E[(r_n - B_n f)'(r_n - B_n f)] = Œ£_t E[(r_nt - B_n f_t)¬≤]\n",
    "            expected_sq_error = 0\n",
    "            for t in range(T):\n",
    "                # E[(r - Bf)¬≤] = r¬≤ - 2r*m_B'm_F + E[f'B'Bf]\n",
    "                # E[f'B'Bf] = tr(E[BB']E[ff']) = tr((S_B + m_Bm_B')(S_F + m_Fm_F'))\n",
    "                E_BB = self.S_B[n] + np.outer(self.m_B[n], self.m_B[n])\n",
    "                E_ff = self.S_F[t] + np.outer(self.m_F[t], self.m_F[t])\n",
    "                \n",
    "                expected_sq_error += (R[t, n]**2 \n",
    "                                     - 2 * R[t, n] * self.m_B[n] @ self.m_F[t]\n",
    "                                     + np.trace(E_BB @ E_ff))\n",
    "            \n",
    "            self.b_n[n] = self.b_psi + 0.5 * expected_sq_error\n",
    "    \n",
    "    def _compute_elbo(self, R):\n",
    "        \"\"\"Compute ELBO (simplified version).\"\"\"\n",
    "        T, N = R.shape\n",
    "        \n",
    "        E_psi_inv = self.a_n / self.b_n\n",
    "        E_log_psi_inv = digamma(self.a_n) - np.log(self.b_n)\n",
    "        \n",
    "        # Expected log likelihood\n",
    "        elbo = 0.5 * T * np.sum(E_log_psi_inv) - T * N / 2 * np.log(2*np.pi)\n",
    "        \n",
    "        for n in range(N):\n",
    "            for t in range(T):\n",
    "                E_BB = self.S_B[n] + np.outer(self.m_B[n], self.m_B[n])\n",
    "                E_ff = self.S_F[t] + np.outer(self.m_F[t], self.m_F[t])\n",
    "                expected_sq = (R[t, n]**2 - 2*R[t, n]*self.m_B[n]@self.m_F[t] + np.trace(E_BB @ E_ff))\n",
    "                elbo -= 0.5 * E_psi_inv[n] * expected_sq\n",
    "        \n",
    "        # Add entropy terms (simplified)\n",
    "        for n in range(N):\n",
    "            elbo += 0.5 * np.linalg.slogdet(self.S_B[n])[1]\n",
    "        for t in range(T):\n",
    "            elbo += 0.5 * np.linalg.slogdet(self.S_F[t])[1]\n",
    "        \n",
    "        return elbo\n",
    "    \n",
    "    def get_factor_loadings(self):\n",
    "        \"\"\"Return posterior mean of factor loadings.\"\"\"\n",
    "        return self.m_B\n",
    "    \n",
    "    def get_factors(self):\n",
    "        \"\"\"Return posterior mean of latent factors.\"\"\"\n",
    "        return self.m_F\n",
    "\n",
    "\n",
    "# Generate synthetic factor model data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters\n",
    "T, N, K_true = 250, 10, 3  # 250 days, 10 assets, 3 factors\n",
    "\n",
    "# True factor loadings (sparse structure)\n",
    "B_true = np.array([\n",
    "    [1.0, 0.2, 0.0],   # Asset 1: High market beta\n",
    "    [0.8, 0.3, 0.1],   # Asset 2\n",
    "    [0.6, 0.7, 0.0],   # Asset 3: Value factor\n",
    "    [0.5, 0.5, 0.5],   # Asset 4: Balanced\n",
    "    [0.3, 0.1, 0.9],   # Asset 5: Momentum factor\n",
    "    [0.9, 0.0, 0.2],   # Asset 6\n",
    "    [0.4, 0.8, 0.1],   # Asset 7\n",
    "    [0.7, 0.3, 0.4],   # Asset 8\n",
    "    [0.2, 0.6, 0.6],   # Asset 9\n",
    "    [0.5, 0.4, 0.3],   # Asset 10\n",
    "])\n",
    "\n",
    "# True latent factors (standard normal)\n",
    "F_true = np.random.randn(T, K_true)\n",
    "\n",
    "# Idiosyncratic noise\n",
    "psi_true = np.array([0.05, 0.08, 0.04, 0.06, 0.07, 0.05, 0.09, 0.06, 0.08, 0.05])\n",
    "epsilon = np.random.randn(T, N) * np.sqrt(psi_true)\n",
    "\n",
    "# Observed returns\n",
    "R_observed = F_true @ B_true.T + epsilon\n",
    "\n",
    "# Fit CAVI factor model\n",
    "cavi_model = CAVI_BayesianFactorModel(n_factors=3)\n",
    "cavi_model.fit(R_observed, max_iter=100)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: ELBO convergence\n",
    "ax = axes[0, 0]\n",
    "ax.plot(cavi_model.elbo_history, 'b-', lw=2)\n",
    "ax.set_xlabel('CAVI Iteration')\n",
    "ax.set_ylabel('ELBO')\n",
    "ax.set_title('CAVI Convergence for Factor Model')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Recovered factor loadings\n",
    "ax = axes[0, 1]\n",
    "B_recovered = cavi_model.get_factor_loadings()\n",
    "# Need to handle factor ordering/sign ambiguity\n",
    "for k in range(K_true):\n",
    "    if np.corrcoef(B_true[:, k], B_recovered[:, k])[0, 1] < 0:\n",
    "        B_recovered[:, k] *= -1\n",
    "\n",
    "im = ax.imshow(np.hstack([B_true, B_recovered]), aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xticks([0, 1, 2, 4, 5, 6])\n",
    "ax.set_xticklabels(['F1', 'F2', 'F3', 'F1', 'F2', 'F3'])\n",
    "ax.axvline(2.5, color='black', lw=2)\n",
    "ax.set_ylabel('Asset')\n",
    "ax.set_title('Factor Loadings: True (left) vs Recovered (right)')\n",
    "plt.colorbar(im, ax=ax, label='Loading')\n",
    "\n",
    "# Plot 3: Recovered factors\n",
    "ax = axes[1, 0]\n",
    "F_recovered = cavi_model.get_factors()\n",
    "for k in range(K_true):\n",
    "    if np.corrcoef(F_true[:, k], F_recovered[:, k])[0, 1] < 0:\n",
    "        F_recovered[:, k] *= -1\n",
    "        \n",
    "ax.plot(F_true[:50, 0], 'b-', alpha=0.7, label='True F1')\n",
    "ax.plot(F_recovered[:50, 0], 'r--', alpha=0.7, label='Recovered F1')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Factor Value')\n",
    "ax.set_title('Latent Factor Recovery (First 50 periods)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Idiosyncratic variance recovery\n",
    "ax = axes[1, 1]\n",
    "psi_recovered = cavi_model.b_n / (cavi_model.a_n - 1)  # E[œà] for InvGamma\n",
    "x_pos = np.arange(N)\n",
    "width = 0.35\n",
    "ax.bar(x_pos - width/2, psi_true, width, label='True œà', alpha=0.7)\n",
    "ax.bar(x_pos + width/2, psi_recovered, width, label='Recovered E[œà]', alpha=0.7)\n",
    "ax.set_xlabel('Asset')\n",
    "ax.set_ylabel('Idiosyncratic Variance')\n",
    "ax.set_title('Idiosyncratic Variance Recovery')\n",
    "ax.legend()\n",
    "ax.set_xticks(x_pos)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä CAVI Factor Model Results:\")\n",
    "print(f\"   Factor loading correlation (per factor):\")\n",
    "for k in range(K_true):\n",
    "    corr = np.abs(np.corrcoef(B_true[:, k], cavi_model.m_B[:, k])[0, 1])\n",
    "    print(f\"      Factor {k+1}: {corr:.4f}\")\n",
    "print(f\"\\n   Mean idiosyncratic variance error: {np.mean(np.abs(psi_true - psi_recovered)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f186e5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Stochastic Variational Inference (SVI) <a name=\"7-svi\"></a>\n",
    "\n",
    "**Stochastic Variational Inference** enables VI to scale to massive datasets by using:\n",
    "1. **Mini-batches** instead of full data\n",
    "2. **Stochastic gradient descent** on the ELBO\n",
    "3. **Natural gradients** for faster convergence\n",
    "\n",
    "### The Problem with Batch VI\n",
    "\n",
    "For $N$ data points:\n",
    "$$\\nabla_\\lambda \\mathcal{L} = \\sum_{i=1}^N \\nabla_\\lambda \\mathbb{E}_q[\\log p(x_i, z)] - \\nabla_\\lambda \\mathbb{E}_q[\\log q(z)]$$\n",
    "\n",
    "Computing gradients requires a pass over all data ‚Äî $O(N)$ per iteration!\n",
    "\n",
    "### SVI Solution\n",
    "\n",
    "Use an unbiased estimate with mini-batch $\\mathcal{B}$:\n",
    "$$\\hat{\\nabla}_\\lambda \\mathcal{L} = \\frac{N}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\nabla_\\lambda \\mathbb{E}_q[\\log p(x_i, z)] - \\nabla_\\lambda \\mathbb{E}_q[\\log q(z)]$$\n",
    "\n",
    "### Natural Gradients\n",
    "\n",
    "Standard gradients ignore the geometry of probability distributions. **Natural gradients** account for the Riemannian geometry:\n",
    "\n",
    "$$\\tilde{\\nabla}_\\lambda \\mathcal{L} = F^{-1} \\nabla_\\lambda \\mathcal{L}$$\n",
    "\n",
    "Where $F$ is the Fisher information matrix. For exponential families, this simplifies beautifully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc616188",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticVI_BayesianLogistic:\n",
    "    \"\"\"\n",
    "    Stochastic Variational Inference for Bayesian Logistic Regression.\n",
    "    \n",
    "    Model:\n",
    "        y | x, w ~ Bernoulli(sigmoid(w'x))\n",
    "        w ~ N(0, œÉ¬≤I)\n",
    "    \n",
    "    Uses:\n",
    "        - Mini-batch gradient estimates\n",
    "        - Reparameterization trick for gradients\n",
    "        - Adam optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, prior_var=1.0):\n",
    "        self.lr = learning_rate\n",
    "        self.prior_var = prior_var\n",
    "        self.elbo_history = []\n",
    "        \n",
    "    def fit(self, X, y, batch_size=64, n_epochs=100, n_samples=10):\n",
    "        \"\"\"\n",
    "        Fit using SVI with mini-batches.\n",
    "        \n",
    "        Args:\n",
    "            X: Features (N √ó D)\n",
    "            y: Binary labels (N,)\n",
    "            batch_size: Mini-batch size\n",
    "            n_epochs: Number of epochs\n",
    "            n_samples: Monte Carlo samples for gradient estimation\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # Initialize variational parameters: q(w) = N(Œº, diag(œÉ¬≤))\n",
    "        self.mu = np.zeros(D)\n",
    "        self.log_sigma = np.zeros(D)  # log(œÉ) for stability\n",
    "        \n",
    "        # Adam optimizer state\n",
    "        m_mu, v_mu = np.zeros(D), np.zeros(D)\n",
    "        m_logsig, v_logsig = np.zeros(D), np.zeros(D)\n",
    "        beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
    "        t = 0\n",
    "        \n",
    "        n_batches = N // batch_size\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Shuffle data\n",
    "            perm = np.random.permutation(N)\n",
    "            X_shuffled = X[perm]\n",
    "            y_shuffled = y[perm]\n",
    "            \n",
    "            epoch_elbo = 0\n",
    "            \n",
    "            for batch_idx in range(n_batches):\n",
    "                t += 1\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                # Compute stochastic gradients\n",
    "                grad_mu, grad_logsig, batch_elbo = self._compute_gradients(\n",
    "                    X_batch, y_batch, N, n_samples\n",
    "                )\n",
    "                epoch_elbo += batch_elbo\n",
    "                \n",
    "                # Adam updates for mu\n",
    "                m_mu = beta1 * m_mu + (1 - beta1) * grad_mu\n",
    "                v_mu = beta2 * v_mu + (1 - beta2) * grad_mu**2\n",
    "                m_hat = m_mu / (1 - beta1**t)\n",
    "                v_hat = v_mu / (1 - beta2**t)\n",
    "                self.mu += self.lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "                \n",
    "                # Adam updates for log_sigma\n",
    "                m_logsig = beta1 * m_logsig + (1 - beta1) * grad_logsig\n",
    "                v_logsig = beta2 * v_logsig + (1 - beta2) * grad_logsig**2\n",
    "                m_hat = m_logsig / (1 - beta1**t)\n",
    "                v_hat = v_logsig / (1 - beta2**t)\n",
    "                self.log_sigma += self.lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "            \n",
    "            self.elbo_history.append(epoch_elbo / n_batches)\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                print(f\"Epoch {epoch}: ELBO = {self.elbo_history[-1]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_gradients(self, X_batch, y_batch, N, n_samples):\n",
    "        \"\"\"\n",
    "        Compute stochastic gradients using reparameterization trick.\n",
    "        \n",
    "        The reparameterization trick:\n",
    "            w = Œº + œÉ ‚äô Œµ,  where Œµ ~ N(0, I)\n",
    "        \n",
    "        Allows us to compute ‚àá_Œº,œÉ E_q[f(w)] = E_Œµ[‚àá_Œº,œÉ f(Œº + œÉŒµ)]\n",
    "        \"\"\"\n",
    "        M = len(y_batch)\n",
    "        D = len(self.mu)\n",
    "        sigma = np.exp(self.log_sigma)\n",
    "        \n",
    "        grad_mu = np.zeros(D)\n",
    "        grad_logsig = np.zeros(D)\n",
    "        total_elbo = 0\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            # Reparameterization: w = Œº + œÉ ‚äô Œµ\n",
    "            eps = np.random.randn(D)\n",
    "            w = self.mu + sigma * eps\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = X_batch @ w\n",
    "            probs = 1 / (1 + np.exp(-np.clip(logits, -500, 500)))\n",
    "            \n",
    "            # Log likelihood gradient (scaled for full dataset)\n",
    "            log_lik_grad_w = (N / M) * X_batch.T @ (y_batch - probs)\n",
    "            \n",
    "            # KL gradient (prior is N(0, œÉ¬≤_prior I))\n",
    "            kl_grad_w = -w / self.prior_var\n",
    "            \n",
    "            # Total gradient w.r.t. w\n",
    "            grad_w = log_lik_grad_w + kl_grad_w\n",
    "            \n",
    "            # Backprop through reparameterization\n",
    "            grad_mu += grad_w\n",
    "            grad_logsig += grad_w * eps * sigma  # Chain rule: ‚àÇw/‚àÇlog_œÉ = œÉŒµ\n",
    "            \n",
    "            # ELBO contribution\n",
    "            log_lik = np.sum(y_batch * np.log(probs + 1e-10) + \n",
    "                           (1 - y_batch) * np.log(1 - probs + 1e-10))\n",
    "            log_prior = -0.5 * np.sum(w**2) / self.prior_var\n",
    "            log_q = -0.5 * np.sum(eps**2)  # Entropy term cancels in expectation\n",
    "            total_elbo += (N / M) * log_lik + log_prior\n",
    "        \n",
    "        # Average over samples\n",
    "        grad_mu /= n_samples\n",
    "        grad_logsig /= n_samples\n",
    "        total_elbo /= n_samples\n",
    "        \n",
    "        # Add entropy gradient: ‚àÇH[q]/‚àÇlog_œÉ = 1 (since H = D/2(1 + log(2œÄ)) + Œ£log(œÉ))\n",
    "        grad_logsig += 1.0\n",
    "        \n",
    "        return grad_mu, grad_logsig, total_elbo\n",
    "    \n",
    "    def predict_proba(self, X, n_samples=100):\n",
    "        \"\"\"\n",
    "        Predict probabilities with uncertainty.\n",
    "        \"\"\"\n",
    "        sigma = np.exp(self.log_sigma)\n",
    "        probs = np.zeros(len(X))\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            w = self.mu + sigma * np.random.randn(len(self.mu))\n",
    "            logits = X @ w\n",
    "            probs += 1 / (1 + np.exp(-np.clip(logits, -500, 500)))\n",
    "        \n",
    "        return probs / n_samples\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        return (self.predict_proba(X) > 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Generate synthetic classification data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Large dataset to demonstrate scalability\n",
    "N_large = 10000\n",
    "D = 20\n",
    "\n",
    "# True weights (sparse)\n",
    "w_true = np.zeros(D)\n",
    "w_true[:5] = np.array([2.0, -1.5, 1.0, -0.5, 0.8])\n",
    "\n",
    "X_large = np.random.randn(N_large, D)\n",
    "logits_true = X_large @ w_true\n",
    "y_large = (np.random.rand(N_large) < 1/(1 + np.exp(-logits_true))).astype(int)\n",
    "\n",
    "# Split train/test\n",
    "train_idx = np.random.choice(N_large, int(0.8 * N_large), replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(N_large), train_idx)\n",
    "\n",
    "X_train, y_train = X_large[train_idx], y_large[train_idx]\n",
    "X_test, y_test = X_large[test_idx], y_large[test_idx]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Fit SVI model\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "svi_model = StochasticVI_BayesianLogistic(learning_rate=0.05, prior_var=10.0)\n",
    "svi_model.fit(X_train, y_train, batch_size=128, n_epochs=100, n_samples=5)\n",
    "svi_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred_proba = svi_model.predict_proba(X_test)\n",
    "y_pred = svi_model.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(f\"\\nSVI completed in {svi_time:.2f} seconds\")\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdeaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVI results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: ELBO convergence\n",
    "ax = axes[0, 0]\n",
    "ax.plot(svi_model.elbo_history, 'b-', lw=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('ELBO (mini-batch average)')\n",
    "ax.set_title('SVI ELBO Convergence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Weight recovery\n",
    "ax = axes[0, 1]\n",
    "sigma = np.exp(svi_model.log_sigma)\n",
    "x_pos = np.arange(D)\n",
    "ax.bar(x_pos, w_true, alpha=0.5, label='True weights', color='blue')\n",
    "ax.errorbar(x_pos, svi_model.mu, yerr=2*sigma, fmt='o', color='red', \n",
    "            capsize=3, label='VI posterior ¬± 2œÉ')\n",
    "ax.set_xlabel('Weight index')\n",
    "ax.set_ylabel('Weight value')\n",
    "ax.set_title('Weight Recovery: True vs VI Posterior')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 3: Uncertainty vs correctness\n",
    "ax = axes[1, 0]\n",
    "entropy = -y_pred_proba * np.log(y_pred_proba + 1e-10) - (1-y_pred_proba) * np.log(1-y_pred_proba + 1e-10)\n",
    "correct = (y_pred == y_test)\n",
    "\n",
    "# Bin by entropy\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, np.log(2), n_bins + 1)\n",
    "bin_indices = np.digitize(entropy, bins)\n",
    "\n",
    "accuracy_by_entropy = []\n",
    "entropy_centers = []\n",
    "for i in range(1, n_bins + 1):\n",
    "    mask = bin_indices == i\n",
    "    if np.sum(mask) > 0:\n",
    "        accuracy_by_entropy.append(np.mean(correct[mask]))\n",
    "        entropy_centers.append((bins[i-1] + bins[i]) / 2)\n",
    "\n",
    "ax.bar(entropy_centers, accuracy_by_entropy, width=0.05, alpha=0.7)\n",
    "ax.set_xlabel('Predictive Entropy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy vs Predictive Uncertainty\\n(Lower entropy ‚Üí More confident ‚Üí Higher accuracy)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Calibration plot\n",
    "ax = axes[1, 1]\n",
    "n_bins = 10\n",
    "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "bin_indices = np.digitize(y_pred_proba, bin_edges)\n",
    "\n",
    "mean_predicted = []\n",
    "fraction_positive = []\n",
    "bin_counts = []\n",
    "\n",
    "for i in range(1, n_bins + 1):\n",
    "    mask = bin_indices == i\n",
    "    if np.sum(mask) > 0:\n",
    "        mean_predicted.append(np.mean(y_pred_proba[mask]))\n",
    "        fraction_positive.append(np.mean(y_test[mask]))\n",
    "        bin_counts.append(np.sum(mask))\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "ax.scatter(mean_predicted, fraction_positive, s=100, c='red', alpha=0.7, label='SVI')\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration Plot')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä SVI Results Summary:\")\n",
    "print(f\"   Dataset size: {N_large} samples\")\n",
    "print(f\"   Training time: {svi_time:.2f} seconds\")\n",
    "print(f\"   Test accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Mean posterior std: {np.mean(sigma):.4f}\")\n",
    "print(f\"   Weight recovery correlation: {np.corrcoef(w_true, svi_model.mu)[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24668da6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Automatic Differentiation Variational Inference (ADVI) <a name=\"8-advi\"></a>\n",
    "\n",
    "**ADVI** automates variational inference for any differentiable probabilistic model by:\n",
    "\n",
    "1. **Transforming** constrained parameters to unconstrained space\n",
    "2. **Applying** a Gaussian variational family in the transformed space\n",
    "3. **Using** automatic differentiation to compute ELBO gradients\n",
    "4. **Optimizing** with standard gradient-based methods\n",
    "\n",
    "### The Reparameterization Trick\n",
    "\n",
    "The key innovation enabling gradient-based VI is the **reparameterization trick**:\n",
    "\n",
    "Instead of sampling from $q_\\phi(z)$, we write:\n",
    "$$z = g(\\epsilon, \\phi) \\quad \\text{where} \\quad \\epsilon \\sim p(\\epsilon)$$\n",
    "\n",
    "For Gaussians: $z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "This allows gradients to flow through the sampling process:\n",
    "$$\\nabla_\\phi \\mathbb{E}_{q_\\phi}[f(z)] = \\nabla_\\phi \\mathbb{E}_{p(\\epsilon)}[f(g(\\epsilon, \\phi))] = \\mathbb{E}_{p(\\epsilon)}[\\nabla_\\phi f(g(\\epsilon, \\phi))]$$\n",
    "\n",
    "### Transformation to Unconstrained Space\n",
    "\n",
    "| Parameter Constraint | Transformation |\n",
    "|---------------------|----------------|\n",
    "| $\\theta \\in \\mathbb{R}$ | Identity |\n",
    "| $\\theta > 0$ | $\\log(\\theta) \\in \\mathbb{R}$ |\n",
    "| $\\theta \\in (0, 1)$ | $\\text{logit}(\\theta) \\in \\mathbb{R}$ |\n",
    "| $\\theta \\in \\Delta^K$ (simplex) | Stick-breaking |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a646e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TORCH_AVAILABLE:\n",
    "    class ADVI_BayesianNN(nn.Module):\n",
    "        \"\"\"\n",
    "        Automatic Differentiation VI for a Bayesian Neural Network.\n",
    "        \n",
    "        Uses the reparameterization trick with diagonal Gaussian posteriors\n",
    "        over all weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, input_dim, hidden_dim=32, output_dim=1, prior_std=1.0):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.prior_std = prior_std\n",
    "            \n",
    "            # Variational parameters for first layer\n",
    "            self.w1_mu = nn.Parameter(torch.randn(input_dim, hidden_dim) * 0.1)\n",
    "            self.w1_logsigma = nn.Parameter(torch.zeros(input_dim, hidden_dim) - 2)\n",
    "            self.b1_mu = nn.Parameter(torch.zeros(hidden_dim))\n",
    "            self.b1_logsigma = nn.Parameter(torch.zeros(hidden_dim) - 2)\n",
    "            \n",
    "            # Variational parameters for second layer\n",
    "            self.w2_mu = nn.Parameter(torch.randn(hidden_dim, output_dim) * 0.1)\n",
    "            self.w2_logsigma = nn.Parameter(torch.zeros(hidden_dim, output_dim) - 2)\n",
    "            self.b2_mu = nn.Parameter(torch.zeros(output_dim))\n",
    "            self.b2_logsigma = nn.Parameter(torch.zeros(output_dim) - 2)\n",
    "            \n",
    "            # Observation noise (learned)\n",
    "            self.log_noise_std = nn.Parameter(torch.tensor(0.0))\n",
    "        \n",
    "        def sample_weights(self):\n",
    "            \"\"\"Sample weights using reparameterization trick.\"\"\"\n",
    "            eps_w1 = torch.randn_like(self.w1_mu)\n",
    "            eps_b1 = torch.randn_like(self.b1_mu)\n",
    "            eps_w2 = torch.randn_like(self.w2_mu)\n",
    "            eps_b2 = torch.randn_like(self.b2_mu)\n",
    "            \n",
    "            w1 = self.w1_mu + torch.exp(self.w1_logsigma) * eps_w1\n",
    "            b1 = self.b1_mu + torch.exp(self.b1_logsigma) * eps_b1\n",
    "            w2 = self.w2_mu + torch.exp(self.w2_logsigma) * eps_w2\n",
    "            b2 = self.b2_mu + torch.exp(self.b2_logsigma) * eps_b2\n",
    "            \n",
    "            return w1, b1, w2, b2\n",
    "        \n",
    "        def forward(self, x, w1, b1, w2, b2):\n",
    "            \"\"\"Forward pass with given weights.\"\"\"\n",
    "            h = torch.tanh(x @ w1 + b1)\n",
    "            out = h @ w2 + b2\n",
    "            return out\n",
    "        \n",
    "        def kl_divergence(self):\n",
    "            \"\"\"Compute KL(q || prior) for all parameters.\"\"\"\n",
    "            kl = 0.0\n",
    "            \n",
    "            # KL for each parameter group\n",
    "            for mu, logsigma in [(self.w1_mu, self.w1_logsigma),\n",
    "                                  (self.b1_mu, self.b1_logsigma),\n",
    "                                  (self.w2_mu, self.w2_logsigma),\n",
    "                                  (self.b2_mu, self.b2_logsigma)]:\n",
    "                sigma = torch.exp(logsigma)\n",
    "                # KL(N(Œº,œÉ¬≤) || N(0, prior_std¬≤))\n",
    "                kl += 0.5 * torch.sum(\n",
    "                    (sigma**2 + mu**2) / self.prior_std**2 \n",
    "                    - 1 \n",
    "                    - 2*logsigma \n",
    "                    + 2*np.log(self.prior_std)\n",
    "                )\n",
    "            \n",
    "            return kl\n",
    "        \n",
    "        def elbo(self, x, y, n_samples=5):\n",
    "            \"\"\"Compute ELBO using Monte Carlo estimate.\"\"\"\n",
    "            batch_size = len(x)\n",
    "            log_lik = 0.0\n",
    "            noise_std = torch.exp(self.log_noise_std)\n",
    "            \n",
    "            for _ in range(n_samples):\n",
    "                w1, b1, w2, b2 = self.sample_weights()\n",
    "                pred = self.forward(x, w1, b1, w2, b2)\n",
    "                \n",
    "                # Log likelihood: y ~ N(pred, noise_std¬≤)\n",
    "                log_lik += torch.sum(\n",
    "                    -0.5 * np.log(2*np.pi) \n",
    "                    - self.log_noise_std \n",
    "                    - 0.5 * ((y - pred) / noise_std)**2\n",
    "                )\n",
    "            \n",
    "            log_lik /= n_samples\n",
    "            kl = self.kl_divergence()\n",
    "            \n",
    "            return log_lik - kl\n",
    "        \n",
    "        def predict(self, x, n_samples=100):\n",
    "            \"\"\"Predict with uncertainty quantification.\"\"\"\n",
    "            preds = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(n_samples):\n",
    "                    w1, b1, w2, b2 = self.sample_weights()\n",
    "                    pred = self.forward(x, w1, b1, w2, b2)\n",
    "                    preds.append(pred.numpy())\n",
    "            \n",
    "            preds = np.array(preds)\n",
    "            return preds.mean(axis=0), preds.std(axis=0)\n",
    "\n",
    "    \n",
    "    # Generate nonlinear regression data\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    N = 500\n",
    "    X_nl = np.random.uniform(-3, 3, (N, 1)).astype(np.float32)\n",
    "    y_true_func = np.sin(X_nl) + 0.3 * np.cos(3*X_nl)  # Nonlinear function\n",
    "    y_nl = y_true_func + np.random.normal(0, 0.2, (N, 1)).astype(np.float32)  # Add noise\n",
    "    \n",
    "    # Train/test split\n",
    "    train_mask = np.random.rand(N) < 0.8\n",
    "    X_train_nl = torch.tensor(X_nl[train_mask])\n",
    "    y_train_nl = torch.tensor(y_nl[train_mask])\n",
    "    X_test_nl = torch.tensor(X_nl[~train_mask])\n",
    "    y_test_nl = torch.tensor(y_nl[~train_mask])\n",
    "    \n",
    "    # Train ADVI BNN\n",
    "    bnn = ADVI_BayesianNN(input_dim=1, hidden_dim=50, output_dim=1, prior_std=1.0)\n",
    "    optimizer = optim.Adam(bnn.parameters(), lr=0.01)\n",
    "    \n",
    "    elbo_history_bnn = []\n",
    "    \n",
    "    print(\"Training ADVI Bayesian Neural Network...\")\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -bnn.elbo(X_train_nl, y_train_nl, n_samples=5)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        elbo_history_bnn.append(-loss.item())\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: ELBO = {-loss.item():.4f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    X_plot = torch.tensor(np.linspace(-4, 4, 200).reshape(-1, 1).astype(np.float32))\n",
    "    y_mean, y_std = bnn.predict(X_plot, n_samples=200)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    ax.plot(elbo_history_bnn, 'b-', lw=1)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('ELBO')\n",
    "    ax.set_title('ADVI BNN Training: ELBO Convergence')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    ax.scatter(X_train_nl.numpy(), y_train_nl.numpy(), alpha=0.3, s=10, label='Training data')\n",
    "    ax.scatter(X_test_nl.numpy(), y_test_nl.numpy(), alpha=0.5, s=20, c='green', label='Test data')\n",
    "    \n",
    "    X_plot_np = X_plot.numpy().flatten()\n",
    "    ax.plot(X_plot_np, y_mean.flatten(), 'r-', lw=2, label='Posterior mean')\n",
    "    ax.fill_between(X_plot_np, \n",
    "                    y_mean.flatten() - 2*y_std.flatten(),\n",
    "                    y_mean.flatten() + 2*y_std.flatten(),\n",
    "                    alpha=0.3, color='red', label='¬±2œÉ uncertainty')\n",
    "    \n",
    "    # True function\n",
    "    y_true_plot = np.sin(X_plot_np) + 0.3 * np.cos(3*X_plot_np)\n",
    "    ax.plot(X_plot_np, y_true_plot, 'k--', lw=2, label='True function')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('ADVI Bayesian Neural Network: Predictions with Uncertainty')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä ADVI BNN Results:\")\n",
    "    print(f\"   Learned noise std: {np.exp(bnn.log_noise_std.item()):.4f} (true: 0.2)\")\n",
    "    \n",
    "else:\n",
    "    print(\"PyTorch not available. Skipping ADVI BNN example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2098b35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Variational Inference with PyMC <a name=\"9-pymc\"></a>\n",
    "\n",
    "**PyMC** provides built-in VI methods that make it easy to apply variational inference to complex Bayesian models:\n",
    "\n",
    "- **ADVI**: Automatic Differentiation VI with diagonal Gaussian\n",
    "- **FullRankADVI**: ADVI with full covariance Gaussian\n",
    "- **SVGD**: Stein Variational Gradient Descent\n",
    "\n",
    "### Variational Families\n",
    "\n",
    "| Method | Variational Family | Complexity | Quality |\n",
    "|--------|-------------------|------------|---------|\n",
    "| ADVI | $\\mathcal{N}(\\mu, \\text{diag}(\\sigma^2))$ | $O(D)$ | Fast, may underfit correlations |\n",
    "| FullRankADVI | $\\mathcal{N}(\\mu, \\Sigma)$ | $O(D^2)$ | Captures correlations |\n",
    "| SVGD | Particle-based | $O(M^2)$ | Non-parametric, flexible |\n",
    "\n",
    "Let's apply PyMC's VI to a financial time series model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aafb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic stock returns with regime-switching volatility\n",
    "np.random.seed(42)\n",
    "\n",
    "T = 1000  # Trading days\n",
    "n_regimes = 2\n",
    "\n",
    "# True regime parameters\n",
    "true_mu = np.array([0.0005, -0.0002])  # Daily returns: 12.5% vs -5% annualized\n",
    "true_sigma = np.array([0.01, 0.03])    # Low vol vs high vol regime\n",
    "true_transition = np.array([[0.98, 0.02],  # Stay in low vol\n",
    "                           [0.05, 0.95]])   # Stay in high vol\n",
    "\n",
    "# Generate regime sequence (Markov chain)\n",
    "regimes = np.zeros(T, dtype=int)\n",
    "regimes[0] = 0  # Start in low vol regime\n",
    "for t in range(1, T):\n",
    "    regimes[t] = np.random.choice([0, 1], p=true_transition[regimes[t-1]])\n",
    "\n",
    "# Generate returns\n",
    "returns = np.random.normal(true_mu[regimes], true_sigma[regimes])\n",
    "\n",
    "print(f\"Simulated {T} days of returns\")\n",
    "print(f\"Time in low vol regime: {100*np.mean(regimes == 0):.1f}%\")\n",
    "print(f\"Time in high vol regime: {100*np.mean(regimes == 1):.1f}%\")\n",
    "\n",
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(returns, 'b-', alpha=0.7, lw=0.5)\n",
    "ax.fill_between(range(T), returns.min(), returns.max(), \n",
    "                where=regimes == 1, alpha=0.3, color='red', label='High vol regime')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Daily Return')\n",
    "ax.set_title('Simulated Stock Returns with Regime-Switching Volatility')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "rolling_vol = pd.Series(returns).rolling(21).std() * np.sqrt(252)\n",
    "ax.plot(rolling_vol, 'b-', lw=1.5, label='21-day rolling vol (annualized)')\n",
    "ax.axhline(true_sigma[0] * np.sqrt(252), color='green', linestyle='--', \n",
    "           label=f'True low vol: {true_sigma[0]*np.sqrt(252):.1%}')\n",
    "ax.axhline(true_sigma[1] * np.sqrt(252), color='red', linestyle='--',\n",
    "           label=f'True high vol: {true_sigma[1]*np.sqrt(252):.1%}')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Annualized Volatility')\n",
    "ax.set_title('Rolling Volatility')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Bayesian stochastic volatility model with PyMC\n",
    "# We'll use a simpler model than full HMM for VI compatibility\n",
    "\n",
    "with pm.Model() as sv_model:\n",
    "    # Priors for volatility parameters\n",
    "    # Log-volatility follows AR(1) process: h_t = mu + phi*(h_{t-1} - mu) + sigma_h * eps\n",
    "    \n",
    "    mu_h = pm.Normal('mu_h', mu=-4, sigma=1)  # Mean log-vol (exp(-4) ‚âà 0.018)\n",
    "    phi = pm.Beta('phi', alpha=20, beta=1.5)   # Persistence (prior mode ~0.97)\n",
    "    sigma_h = pm.Exponential('sigma_h', lam=10)  # Vol of vol\n",
    "    \n",
    "    # Initial log-volatility\n",
    "    h_init = pm.Normal('h_init', mu=mu_h, sigma=sigma_h / np.sqrt(1 - phi**2))\n",
    "    \n",
    "    # Log-volatility process (simplified - using random walk approximation for VI)\n",
    "    # In practice, we'd use a scan, but for VI demo we discretize\n",
    "    h = pm.AR('h', rho=[phi], sigma=sigma_h, constant=True, \n",
    "              init_dist=pm.Normal.dist(mu=mu_h, sigma=0.5),\n",
    "              shape=T)\n",
    "    \n",
    "    # Observation model\n",
    "    sigma_t = pm.math.exp(h / 2)\n",
    "    mu_r = pm.Normal('mu_r', mu=0, sigma=0.001)  # Mean return\n",
    "    \n",
    "    # Likelihood\n",
    "    y = pm.Normal('y', mu=mu_r, sigma=sigma_t, observed=returns)\n",
    "\n",
    "print(\"PyMC Stochastic Volatility Model defined\")\n",
    "print(f\"Model has {sv_model.point_logps().keys()} parameters\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
