{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72fcf4b",
   "metadata": {},
   "source": [
    "# Day 5: Particle Filters for Nonlinear/Non-Gaussian Systems\n",
    "\n",
    "## Week 20: Bayesian Methods in Finance\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand** why Kalman filters fail for nonlinear/non-Gaussian systems\n",
    "2. **Master** Sequential Monte Carlo (SMC) fundamentals\n",
    "3. **Implement** Bootstrap Particle Filter from scratch\n",
    "4. **Apply** resampling strategies (Multinomial, Systematic, Stratified)\n",
    "5. **Build** particle filters for financial applications (stochastic volatility, regime switching)\n",
    "6. **Diagnose** particle degeneracy and implement solutions\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [Motivation: Beyond Kalman Filters](#1-motivation)\n",
    "2. [Sequential Monte Carlo Fundamentals](#2-smc-fundamentals)\n",
    "3. [The Bootstrap Particle Filter](#3-bootstrap-pf)\n",
    "4. [Resampling Strategies](#4-resampling)\n",
    "5. [Financial Applications](#5-finance-applications)\n",
    "6. [Advanced Topics: SIR, Auxiliary PF, Rao-Blackwellization](#6-advanced)\n",
    "7. [Interview Questions & Exercises](#7-interview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import logsumexp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Environment ready for Particle Filter analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96df0de2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Motivation: Beyond Kalman Filters <a name=\"1-motivation\"></a>\n",
    "\n",
    "### 1.1 The Filtering Problem\n",
    "\n",
    "**State-Space Model:**\n",
    "\n",
    "$$\\begin{align}\n",
    "x_t &= f(x_{t-1}, u_t) \\quad \\text{(State Transition)} \\\\\n",
    "y_t &= h(x_t, v_t) \\quad \\text{(Observation)}\n",
    "\\end{align}$$\n",
    "\n",
    "Where:\n",
    "- $x_t$ = hidden state (e.g., true volatility, regime)\n",
    "- $y_t$ = observation (e.g., returns, prices)\n",
    "- $u_t, v_t$ = noise terms (not necessarily Gaussian!)\n",
    "\n",
    "**Goal:** Compute the posterior distribution $p(x_t | y_{1:t})$\n",
    "\n",
    "### 1.2 When Kalman Filters Fail\n",
    "\n",
    "| Assumption | Kalman Filter | Particle Filter |\n",
    "|------------|---------------|------------------|\n",
    "| State transition | Linear | **Any nonlinear** |\n",
    "| Observation model | Linear | **Any nonlinear** |\n",
    "| Noise distribution | Gaussian | **Any distribution** |\n",
    "| Posterior | Gaussian (exact) | **Approximated by particles** |\n",
    "\n",
    "**Financial Examples Where KF Fails:**\n",
    "1. **Stochastic Volatility Models** - Volatility evolves nonlinearly\n",
    "2. **Regime-Switching Models** - Discrete state space\n",
    "3. **Jump-Diffusion Models** - Non-Gaussian innovations\n",
    "4. **Option Pricing** - Nonlinear observation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c53c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Why Kalman Filters Fail for Nonlinear Systems\n",
    "\n",
    "def demonstrate_kalman_failure():\n",
    "    \"\"\"\n",
    "    Show a simple nonlinear system where Kalman filter fails.\n",
    "    Model: x_t = 0.5*x_{t-1} + 25*x_{t-1}/(1+x_{t-1}^2) + 8*cos(1.2*t) + w_t\n",
    "           y_t = x_t^2/20 + v_t\n",
    "    \"\"\"\n",
    "    T = 100\n",
    "    Q = 10  # Process noise variance\n",
    "    R = 1   # Observation noise variance\n",
    "    \n",
    "    # True states and observations\n",
    "    x_true = np.zeros(T)\n",
    "    y = np.zeros(T)\n",
    "    x_true[0] = 0.1\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Nonlinear state transition\n",
    "        x_true[t] = (0.5 * x_true[t-1] + \n",
    "                     25 * x_true[t-1] / (1 + x_true[t-1]**2) + \n",
    "                     8 * np.cos(1.2 * t) + \n",
    "                     np.sqrt(Q) * np.random.randn())\n",
    "    \n",
    "    # Nonlinear observation\n",
    "    y = x_true**2 / 20 + np.sqrt(R) * np.random.randn(T)\n",
    "    \n",
    "    # Naive Kalman Filter (treating as linear - WRONG!)\n",
    "    x_kf = np.zeros(T)\n",
    "    P = np.zeros(T)\n",
    "    x_kf[0] = 0\n",
    "    P[0] = 1\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Linearized (incorrectly) prediction\n",
    "        x_pred = 0.5 * x_kf[t-1]  # Missing nonlinear terms!\n",
    "        P_pred = 0.25 * P[t-1] + Q\n",
    "        \n",
    "        # Linearized update (also wrong)\n",
    "        H = x_pred / 10  # Linearized observation Jacobian\n",
    "        K = P_pred * H / (H**2 * P_pred + R)\n",
    "        x_kf[t] = x_pred + K * (y[t] - x_pred**2/20)\n",
    "        P[t] = (1 - K * H) * P_pred\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    axes[0].plot(x_true, 'b-', lw=2, label='True State', alpha=0.8)\n",
    "    axes[0].plot(x_kf, 'r--', lw=2, label='Kalman Filter (Linearized)', alpha=0.8)\n",
    "    axes[0].fill_between(range(T), x_kf - 2*np.sqrt(P), x_kf + 2*np.sqrt(P), \n",
    "                         alpha=0.2, color='red', label='¬±2œÉ Confidence')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('State')\n",
    "    axes[0].set_title('Kalman Filter Fails for Nonlinear System')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(y, 'g.', alpha=0.5, label='Observations')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Observation')\n",
    "    axes[1].set_title('Nonlinear Observations: y = x¬≤/20 + noise')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((x_true - x_kf)**2))\n",
    "    print(f\"\\nüìä Kalman Filter RMSE: {rmse:.2f}\")\n",
    "    print(\"‚ö†Ô∏è High error due to nonlinearity - Particle Filter needed!\")\n",
    "    \n",
    "    return x_true, y, Q, R\n",
    "\n",
    "x_true, y_obs, Q, R = demonstrate_kalman_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc83b86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Sequential Monte Carlo Fundamentals <a name=\"2-smc-fundamentals\"></a>\n",
    "\n",
    "### 2.1 The Key Idea\n",
    "\n",
    "**Represent the posterior with weighted samples (particles):**\n",
    "\n",
    "$$p(x_t | y_{1:t}) \\approx \\sum_{i=1}^{N} w_t^{(i)} \\delta_{x_t^{(i)}}(x_t)$$\n",
    "\n",
    "Where:\n",
    "- $\\{x_t^{(i)}\\}_{i=1}^N$ = particles (samples)\n",
    "- $\\{w_t^{(i)}\\}_{i=1}^N$ = normalized importance weights\n",
    "- $\\delta_{x}$ = Dirac delta function\n",
    "\n",
    "### 2.2 Recursive Bayesian Filtering\n",
    "\n",
    "**Prediction Step:**\n",
    "$$p(x_t | y_{1:t-1}) = \\int p(x_t | x_{t-1}) p(x_{t-1} | y_{1:t-1}) dx_{t-1}$$\n",
    "\n",
    "**Update Step:**\n",
    "$$p(x_t | y_{1:t}) = \\frac{p(y_t | x_t) p(x_t | y_{1:t-1})}{p(y_t | y_{1:t-1})}$$\n",
    "\n",
    "### 2.3 Importance Sampling\n",
    "\n",
    "Since we can't sample directly from the posterior, we use a **proposal distribution** $q(x_t | x_{t-1}, y_t)$:\n",
    "\n",
    "$$w_t^{(i)} \\propto w_{t-1}^{(i)} \\frac{p(y_t | x_t^{(i)}) p(x_t^{(i)} | x_{t-1}^{(i)})}{q(x_t^{(i)} | x_{t-1}^{(i)}, y_t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Importance Sampling Concept\n",
    "\n",
    "def visualize_importance_sampling():\n",
    "    \"\"\"\n",
    "    Demonstrate importance sampling for a non-Gaussian target.\n",
    "    \"\"\"\n",
    "    # Target: Mixture of Gaussians (non-Gaussian)\n",
    "    def target_pdf(x):\n",
    "        return 0.3 * stats.norm.pdf(x, -2, 0.5) + 0.7 * stats.norm.pdf(x, 2, 1)\n",
    "    \n",
    "    # Proposal: Simple Gaussian\n",
    "    proposal_mean, proposal_std = 0, 2.5\n",
    "    \n",
    "    # Draw samples from proposal\n",
    "    N = 1000\n",
    "    samples = np.random.normal(proposal_mean, proposal_std, N)\n",
    "    \n",
    "    # Compute importance weights\n",
    "    target_vals = target_pdf(samples)\n",
    "    proposal_vals = stats.norm.pdf(samples, proposal_mean, proposal_std)\n",
    "    weights = target_vals / proposal_vals\n",
    "    weights_normalized = weights / weights.sum()\n",
    "    \n",
    "    # Effective Sample Size\n",
    "    ESS = 1 / np.sum(weights_normalized**2)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    x_range = np.linspace(-6, 6, 500)\n",
    "    \n",
    "    # Plot 1: Target and Proposal\n",
    "    axes[0].plot(x_range, target_pdf(x_range), 'b-', lw=2, label='Target p(x)')\n",
    "    axes[0].plot(x_range, stats.norm.pdf(x_range, proposal_mean, proposal_std), \n",
    "                 'r--', lw=2, label='Proposal q(x)')\n",
    "    axes[0].fill_between(x_range, target_pdf(x_range), alpha=0.3)\n",
    "    axes[0].set_xlabel('x')\n",
    "    axes[0].set_ylabel('Density')\n",
    "    axes[0].set_title('Target vs Proposal Distribution')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot 2: Unweighted Samples\n",
    "    axes[1].hist(samples, bins=50, density=True, alpha=0.6, color='red', \n",
    "                 label='Unweighted Samples')\n",
    "    axes[1].plot(x_range, target_pdf(x_range), 'b-', lw=2, label='Target')\n",
    "    axes[1].set_xlabel('x')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].set_title('Unweighted Samples (from Proposal)')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot 3: Weighted Samples (Resampled)\n",
    "    resampled_idx = np.random.choice(N, size=N, p=weights_normalized)\n",
    "    resampled = samples[resampled_idx]\n",
    "    axes[2].hist(resampled, bins=50, density=True, alpha=0.6, color='green',\n",
    "                 label='Resampled')\n",
    "    axes[2].plot(x_range, target_pdf(x_range), 'b-', lw=2, label='Target')\n",
    "    axes[2].set_xlabel('x')\n",
    "    axes[2].set_ylabel('Density')\n",
    "    axes[2].set_title(f'After Resampling (ESS={ESS:.0f}/{N})')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Importance Sampling Results:\")\n",
    "    print(f\"   ‚Ä¢ Number of particles: {N}\")\n",
    "    print(f\"   ‚Ä¢ Effective Sample Size (ESS): {ESS:.1f} ({100*ESS/N:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ True mean: {0.3*(-2) + 0.7*2:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Weighted estimate: {np.sum(weights_normalized * samples):.2f}\")\n",
    "\n",
    "visualize_importance_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff5ff95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. The Bootstrap Particle Filter <a name=\"3-bootstrap-pf\"></a>\n",
    "\n",
    "### 3.1 Algorithm (Sequential Importance Resampling - SIR)\n",
    "\n",
    "The Bootstrap Filter uses the **state transition** as the proposal: $q(x_t | x_{t-1}, y_t) = p(x_t | x_{t-1})$\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "```\n",
    "1. INITIALIZE: Sample x_0^(i) ~ p(x_0), set w_0^(i) = 1/N\n",
    "\n",
    "2. FOR t = 1, 2, ..., T:\n",
    "   \n",
    "   a. PREDICT: Sample x_t^(i) ~ p(x_t | x_{t-1}^(i))\n",
    "   \n",
    "   b. UPDATE: Compute weights w_t^(i) ‚àù p(y_t | x_t^(i))\n",
    "   \n",
    "   c. NORMALIZE: w_t^(i) = w_t^(i) / Œ£ w_t^(j)\n",
    "   \n",
    "   d. RESAMPLE: If ESS < threshold, resample particles\n",
    "   \n",
    "   e. ESTIMATE: E[x_t] ‚âà Œ£ w_t^(i) * x_t^(i)\n",
    "```\n",
    "\n",
    "### 3.2 Effective Sample Size (ESS)\n",
    "\n",
    "$$\\text{ESS} = \\frac{1}{\\sum_{i=1}^N (w_t^{(i)})^2}$$\n",
    "\n",
    "- ESS ‚âà N: All weights similar (good)\n",
    "- ESS ‚âà 1: One particle dominates (particle degeneracy - bad!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee43be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BootstrapParticleFilter:\n",
    "    \"\"\"\n",
    "    Bootstrap Particle Filter (SIR) implementation.\n",
    "    \n",
    "    Features:\n",
    "    - Flexible state transition and observation models\n",
    "    - Multiple resampling strategies\n",
    "    - ESS monitoring and adaptive resampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_particles, state_dim=1, resample_threshold=0.5,\n",
    "                 resample_method='systematic'):\n",
    "        \"\"\"\n",
    "        Initialize the particle filter.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_particles : int\n",
    "            Number of particles\n",
    "        state_dim : int\n",
    "            Dimension of state space\n",
    "        resample_threshold : float\n",
    "            Resample when ESS/N < threshold\n",
    "        resample_method : str\n",
    "            'multinomial', 'systematic', 'stratified', 'residual'\n",
    "        \"\"\"\n",
    "        self.N = n_particles\n",
    "        self.state_dim = state_dim\n",
    "        self.resample_threshold = resample_threshold\n",
    "        self.resample_method = resample_method\n",
    "        \n",
    "        # Storage\n",
    "        self.particles = None\n",
    "        self.weights = None\n",
    "        self.ess_history = []\n",
    "        self.resample_times = []\n",
    "        \n",
    "    def initialize(self, prior_sampler):\n",
    "        \"\"\"\n",
    "        Initialize particles from prior distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_sampler : callable\n",
    "            Function that returns N samples from prior\n",
    "        \"\"\"\n",
    "        self.particles = prior_sampler(self.N)\n",
    "        self.weights = np.ones(self.N) / self.N\n",
    "        self.ess_history = [self.N]\n",
    "        self.resample_times = []\n",
    "        \n",
    "    def predict(self, transition_sampler):\n",
    "        \"\"\"\n",
    "        Prediction step: propagate particles through state transition.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        transition_sampler : callable\n",
    "            Function(particles) -> new_particles\n",
    "        \"\"\"\n",
    "        self.particles = transition_sampler(self.particles)\n",
    "        \n",
    "    def update(self, observation, likelihood_func, t=None):\n",
    "        \"\"\"\n",
    "        Update step: reweight particles based on observation likelihood.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        observation : float or array\n",
    "            Current observation y_t\n",
    "        likelihood_func : callable\n",
    "            Function(particles, observation) -> likelihoods\n",
    "        t : int, optional\n",
    "            Current time step (for tracking)\n",
    "        \"\"\"\n",
    "        # Compute likelihoods\n",
    "        likelihoods = likelihood_func(self.particles, observation)\n",
    "        \n",
    "        # Update weights (in log space for numerical stability)\n",
    "        log_weights = np.log(self.weights + 1e-300) + np.log(likelihoods + 1e-300)\n",
    "        \n",
    "        # Normalize\n",
    "        log_weights -= logsumexp(log_weights)\n",
    "        self.weights = np.exp(log_weights)\n",
    "        \n",
    "        # Compute ESS\n",
    "        ess = self.compute_ess()\n",
    "        self.ess_history.append(ess)\n",
    "        \n",
    "        # Adaptive resampling\n",
    "        if ess < self.resample_threshold * self.N:\n",
    "            self.resample()\n",
    "            if t is not None:\n",
    "                self.resample_times.append(t)\n",
    "                \n",
    "    def compute_ess(self):\n",
    "        \"\"\"Compute Effective Sample Size.\"\"\"\n",
    "        return 1.0 / np.sum(self.weights**2)\n",
    "    \n",
    "    def resample(self):\n",
    "        \"\"\"Resample particles according to their weights.\"\"\"\n",
    "        if self.resample_method == 'multinomial':\n",
    "            indices = self._multinomial_resample()\n",
    "        elif self.resample_method == 'systematic':\n",
    "            indices = self._systematic_resample()\n",
    "        elif self.resample_method == 'stratified':\n",
    "            indices = self._stratified_resample()\n",
    "        elif self.resample_method == 'residual':\n",
    "            indices = self._residual_resample()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown resampling method: {self.resample_method}\")\n",
    "            \n",
    "        self.particles = self.particles[indices]\n",
    "        self.weights = np.ones(self.N) / self.N\n",
    "        \n",
    "    def _multinomial_resample(self):\n",
    "        \"\"\"Standard multinomial resampling.\"\"\"\n",
    "        return np.random.choice(self.N, size=self.N, p=self.weights)\n",
    "    \n",
    "    def _systematic_resample(self):\n",
    "        \"\"\"Systematic resampling (lower variance).\"\"\"\n",
    "        positions = (np.arange(self.N) + np.random.uniform()) / self.N\n",
    "        cumsum = np.cumsum(self.weights)\n",
    "        indices = np.searchsorted(cumsum, positions)\n",
    "        return np.clip(indices, 0, self.N - 1)\n",
    "    \n",
    "    def _stratified_resample(self):\n",
    "        \"\"\"Stratified resampling.\"\"\"\n",
    "        positions = (np.arange(self.N) + np.random.uniform(size=self.N)) / self.N\n",
    "        cumsum = np.cumsum(self.weights)\n",
    "        indices = np.searchsorted(cumsum, positions)\n",
    "        return np.clip(indices, 0, self.N - 1)\n",
    "    \n",
    "    def _residual_resample(self):\n",
    "        \"\"\"Residual resampling.\"\"\"\n",
    "        # Deterministic part\n",
    "        num_copies = np.floor(self.N * self.weights).astype(int)\n",
    "        indices = np.repeat(np.arange(self.N), num_copies)\n",
    "        \n",
    "        # Stochastic part\n",
    "        residuals = self.N * self.weights - num_copies\n",
    "        residuals /= residuals.sum()\n",
    "        remaining = self.N - len(indices)\n",
    "        additional = np.random.choice(self.N, size=remaining, p=residuals)\n",
    "        \n",
    "        return np.concatenate([indices, additional])\n",
    "    \n",
    "    def estimate_mean(self):\n",
    "        \"\"\"Compute weighted mean of particles.\"\"\"\n",
    "        return np.sum(self.weights[:, np.newaxis] * self.particles, axis=0)\n",
    "    \n",
    "    def estimate_variance(self):\n",
    "        \"\"\"Compute weighted variance of particles.\"\"\"\n",
    "        mean = self.estimate_mean()\n",
    "        return np.sum(self.weights[:, np.newaxis] * (self.particles - mean)**2, axis=0)\n",
    "    \n",
    "    def estimate_quantiles(self, quantiles=[0.05, 0.5, 0.95]):\n",
    "        \"\"\"Compute weighted quantiles.\"\"\"\n",
    "        sorted_idx = np.argsort(self.particles.flatten())\n",
    "        sorted_particles = self.particles.flatten()[sorted_idx]\n",
    "        sorted_weights = self.weights[sorted_idx]\n",
    "        cumsum = np.cumsum(sorted_weights)\n",
    "        \n",
    "        result = []\n",
    "        for q in quantiles:\n",
    "            idx = np.searchsorted(cumsum, q)\n",
    "            idx = min(idx, len(sorted_particles) - 1)\n",
    "            result.append(sorted_particles[idx])\n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ BootstrapParticleFilter class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10a6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Bootstrap Particle Filter on the nonlinear system\n",
    "\n",
    "def run_particle_filter_demo():\n",
    "    \"\"\"\n",
    "    Apply Bootstrap PF to the same nonlinear system where Kalman failed.\n",
    "    \"\"\"\n",
    "    # Generate data\n",
    "    T = 100\n",
    "    Q = 10  # Process noise variance\n",
    "    R = 1   # Observation noise variance\n",
    "    \n",
    "    x_true = np.zeros(T)\n",
    "    y = np.zeros(T)\n",
    "    x_true[0] = 0.1\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        x_true[t] = (0.5 * x_true[t-1] + \n",
    "                     25 * x_true[t-1] / (1 + x_true[t-1]**2) + \n",
    "                     8 * np.cos(1.2 * t) + \n",
    "                     np.sqrt(Q) * np.random.randn())\n",
    "    \n",
    "    y = x_true**2 / 20 + np.sqrt(R) * np.random.randn(T)\n",
    "    \n",
    "    # Define state-space model functions\n",
    "    def prior_sampler(N):\n",
    "        return np.random.normal(0, 1, (N, 1))\n",
    "    \n",
    "    def transition_sampler(particles, t):\n",
    "        \"\"\"Nonlinear state transition.\"\"\"\n",
    "        x = particles.flatten()\n",
    "        x_new = (0.5 * x + \n",
    "                 25 * x / (1 + x**2) + \n",
    "                 8 * np.cos(1.2 * t) + \n",
    "                 np.sqrt(Q) * np.random.randn(len(x)))\n",
    "        return x_new.reshape(-1, 1)\n",
    "    \n",
    "    def likelihood(particles, observation):\n",
    "        \"\"\"Observation likelihood p(y|x) = N(y; x^2/20, R).\"\"\"\n",
    "        x = particles.flatten()\n",
    "        pred_obs = x**2 / 20\n",
    "        return stats.norm.pdf(observation, pred_obs, np.sqrt(R))\n",
    "    \n",
    "    # Run particle filter\n",
    "    n_particles = 500\n",
    "    pf = BootstrapParticleFilter(n_particles, state_dim=1, \n",
    "                                  resample_threshold=0.5,\n",
    "                                  resample_method='systematic')\n",
    "    pf.initialize(prior_sampler)\n",
    "    \n",
    "    # Storage for estimates\n",
    "    x_pf_mean = np.zeros(T)\n",
    "    x_pf_std = np.zeros(T)\n",
    "    x_pf_q05 = np.zeros(T)\n",
    "    x_pf_q95 = np.zeros(T)\n",
    "    \n",
    "    x_pf_mean[0] = pf.estimate_mean()[0]\n",
    "    x_pf_std[0] = np.sqrt(pf.estimate_variance()[0])\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Predict\n",
    "        pf.predict(lambda p: transition_sampler(p, t))\n",
    "        \n",
    "        # Update\n",
    "        pf.update(y[t], likelihood, t=t)\n",
    "        \n",
    "        # Store estimates\n",
    "        x_pf_mean[t] = pf.estimate_mean()[0]\n",
    "        x_pf_std[t] = np.sqrt(pf.estimate_variance()[0])\n",
    "        q05, _, q95 = pf.estimate_quantiles([0.05, 0.5, 0.95])\n",
    "        x_pf_q05[t] = q05\n",
    "        x_pf_q95[t] = q95\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: State estimation\n",
    "    axes[0, 0].plot(x_true, 'b-', lw=2, label='True State', alpha=0.8)\n",
    "    axes[0, 0].plot(x_pf_mean, 'r-', lw=2, label='PF Estimate', alpha=0.8)\n",
    "    axes[0, 0].fill_between(range(T), x_pf_q05, x_pf_q95, \n",
    "                            alpha=0.3, color='red', label='90% CI')\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('State')\n",
    "    axes[0, 0].set_title('Particle Filter State Estimation')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Plot 2: Estimation error\n",
    "    error = x_true - x_pf_mean\n",
    "    axes[0, 1].plot(error, 'g-', lw=1, alpha=0.8)\n",
    "    axes[0, 1].axhline(0, color='k', linestyle='--')\n",
    "    axes[0, 1].fill_between(range(T), -2*x_pf_std, 2*x_pf_std, \n",
    "                            alpha=0.2, color='blue', label='¬±2œÉ')\n",
    "    axes[0, 1].set_xlabel('Time')\n",
    "    axes[0, 1].set_ylabel('Error')\n",
    "    axes[0, 1].set_title(f'Estimation Error (RMSE={np.sqrt(np.mean(error**2)):.2f})')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: ESS over time\n",
    "    axes[1, 0].plot(pf.ess_history, 'b-', lw=1)\n",
    "    axes[1, 0].axhline(n_particles, color='g', linestyle='--', label='N particles')\n",
    "    axes[1, 0].axhline(0.5 * n_particles, color='r', linestyle='--', label='Resample threshold')\n",
    "    for rt in pf.resample_times:\n",
    "        axes[1, 0].axvline(rt, color='orange', alpha=0.3, lw=0.5)\n",
    "    axes[1, 0].set_xlabel('Time')\n",
    "    axes[1, 0].set_ylabel('ESS')\n",
    "    axes[1, 0].set_title(f'Effective Sample Size (Resampled {len(pf.resample_times)} times)')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Plot 4: Final particle distribution\n",
    "    axes[1, 1].hist(pf.particles.flatten(), bins=50, density=True, alpha=0.6, color='blue')\n",
    "    axes[1, 1].axvline(x_true[-1], color='r', lw=2, label=f'True: {x_true[-1]:.2f}')\n",
    "    axes[1, 1].axvline(x_pf_mean[-1], color='g', lw=2, linestyle='--', \n",
    "                       label=f'Est: {x_pf_mean[-1]:.2f}')\n",
    "    axes[1, 1].set_xlabel('State')\n",
    "    axes[1, 1].set_ylabel('Density')\n",
    "    axes[1, 1].set_title('Final Particle Distribution')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Particle Filter Performance:\")\n",
    "    print(f\"   ‚Ä¢ RMSE: {np.sqrt(np.mean(error**2)):.3f}\")\n",
    "    print(f\"   ‚Ä¢ Mean ESS: {np.mean(pf.ess_history):.1f} / {n_particles}\")\n",
    "    print(f\"   ‚Ä¢ Resample events: {len(pf.resample_times)}\")\n",
    "    \n",
    "    return pf\n",
    "\n",
    "pf = run_particle_filter_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd5e57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Resampling Strategies <a name=\"4-resampling\"></a>\n",
    "\n",
    "### 4.1 Why Resampling?\n",
    "\n",
    "**Problem: Weight Degeneracy**\n",
    "- Over time, weights become concentrated on few particles\n",
    "- Most particles have negligible weights\n",
    "- ESS ‚Üí 1 (only one effective particle)\n",
    "\n",
    "**Solution: Resampling**\n",
    "- Duplicate high-weight particles\n",
    "- Remove low-weight particles\n",
    "- Reset all weights to 1/N\n",
    "\n",
    "### 4.2 Comparison of Methods\n",
    "\n",
    "| Method | Variance | Complexity | Notes |\n",
    "|--------|----------|------------|-------|\n",
    "| Multinomial | High | O(N log N) | Simple but high variance |\n",
    "| Systematic | Low | O(N) | **Preferred in practice** |\n",
    "| Stratified | Low | O(N) | Good balance |\n",
    "| Residual | Medium | O(N) | Hybrid approach |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99203a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare resampling methods\n",
    "\n",
    "def compare_resampling_methods():\n",
    "    \"\"\"\n",
    "    Compare variance and performance of different resampling strategies.\n",
    "    \"\"\"\n",
    "    # Create a skewed weight distribution\n",
    "    N = 100\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Exponential weights (highly skewed)\n",
    "    raw_weights = np.exp(np.random.randn(N) * 2)\n",
    "    weights = raw_weights / raw_weights.sum()\n",
    "    \n",
    "    # Original particles\n",
    "    particles = np.arange(N)\n",
    "    \n",
    "    # Resampling functions\n",
    "    def multinomial_resample(w):\n",
    "        return np.random.choice(len(w), size=len(w), p=w)\n",
    "    \n",
    "    def systematic_resample(w):\n",
    "        N = len(w)\n",
    "        positions = (np.arange(N) + np.random.uniform()) / N\n",
    "        cumsum = np.cumsum(w)\n",
    "        return np.clip(np.searchsorted(cumsum, positions), 0, N-1)\n",
    "    \n",
    "    def stratified_resample(w):\n",
    "        N = len(w)\n",
    "        positions = (np.arange(N) + np.random.uniform(size=N)) / N\n",
    "        cumsum = np.cumsum(w)\n",
    "        return np.clip(np.searchsorted(cumsum, positions), 0, N-1)\n",
    "    \n",
    "    # Run multiple trials\n",
    "    n_trials = 1000\n",
    "    methods = {\n",
    "        'Multinomial': multinomial_resample,\n",
    "        'Systematic': systematic_resample,\n",
    "        'Stratified': stratified_resample\n",
    "    }\n",
    "    \n",
    "    # True expectation\n",
    "    true_mean = np.sum(weights * particles)\n",
    "    \n",
    "    results = {name: [] for name in methods}\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        for name, func in methods.items():\n",
    "            idx = func(weights)\n",
    "            estimate = np.mean(particles[idx])\n",
    "            results[name].append(estimate)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Plot 1: Original weight distribution\n",
    "    axes[0].bar(range(N), np.sort(weights)[::-1], alpha=0.7)\n",
    "    axes[0].set_xlabel('Particle Index (sorted)')\n",
    "    axes[0].set_ylabel('Weight')\n",
    "    axes[0].set_title(f'Weight Distribution (ESS={1/np.sum(weights**2):.1f})')\n",
    "    \n",
    "    # Plot 2: Histogram of estimates\n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    for (name, estimates), color in zip(results.items(), colors):\n",
    "        axes[1].hist(estimates, bins=30, alpha=0.5, label=name, color=color, density=True)\n",
    "    axes[1].axvline(true_mean, color='red', lw=2, linestyle='--', label='True Mean')\n",
    "    axes[1].set_xlabel('Estimate')\n",
    "    axes[1].set_ylabel('Density')\n",
    "    axes[1].set_title('Distribution of Mean Estimates')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot 3: Variance comparison\n",
    "    variances = [np.var(results[name]) for name in methods]\n",
    "    bars = axes[2].bar(methods.keys(), variances, color=colors, alpha=0.7)\n",
    "    axes[2].set_ylabel('Variance of Estimate')\n",
    "    axes[2].set_title('Resampling Variance Comparison')\n",
    "    \n",
    "    # Add variance values on bars\n",
    "    for bar, var in zip(bars, variances):\n",
    "        axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                     f'{var:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Resampling Method Comparison:\")\n",
    "    print(f\"   True mean: {true_mean:.2f}\")\n",
    "    print(f\"   {'Method':<15} {'Mean':<10} {'Std':<10} {'Bias':<10}\")\n",
    "    print(\"   \" + \"-\" * 45)\n",
    "    for name in methods:\n",
    "        mean_est = np.mean(results[name])\n",
    "        std_est = np.std(results[name])\n",
    "        bias = mean_est - true_mean\n",
    "        print(f\"   {name:<15} {mean_est:<10.3f} {std_est:<10.3f} {bias:<10.4f}\")\n",
    "\n",
    "compare_resampling_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2064510",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Financial Applications <a name=\"5-finance-applications\"></a>\n",
    "\n",
    "### 5.1 Stochastic Volatility Model\n",
    "\n",
    "One of the most important applications in finance!\n",
    "\n",
    "**Model:**\n",
    "$$\\begin{align}\n",
    "r_t &= \\exp(h_t/2) \\epsilon_t \\quad &\\epsilon_t \\sim N(0,1) \\\\\n",
    "h_t &= \\mu + \\phi(h_{t-1} - \\mu) + \\sigma_h \\eta_t \\quad &\\eta_t \\sim N(0,1)\n",
    "\\end{align}$$\n",
    "\n",
    "Where:\n",
    "- $r_t$ = log returns (observed)\n",
    "- $h_t$ = log volatility (hidden state)\n",
    "- $\\mu$ = long-run mean log volatility\n",
    "- $\\phi$ = persistence parameter (close to 1)\n",
    "- $\\sigma_h$ = volatility of volatility\n",
    "\n",
    "**Why Particle Filter?**\n",
    "- Observation equation $r_t = \\exp(h_t/2) \\epsilon_t$ is **nonlinear** in $h_t$\n",
    "- The likelihood $p(r_t | h_t)$ involves a nonlinear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0791af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticVolatilityPF:\n",
    "    \"\"\"\n",
    "    Particle Filter for Stochastic Volatility Model.\n",
    "    \n",
    "    State: h_t (log volatility)\n",
    "    Observation: r_t (returns)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu, phi, sigma_h, n_particles=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        mu : float\n",
    "            Long-run mean of log volatility\n",
    "        phi : float\n",
    "            Persistence parameter (0 < phi < 1)\n",
    "        sigma_h : float\n",
    "            Volatility of volatility\n",
    "        n_particles : int\n",
    "            Number of particles\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.phi = phi\n",
    "        self.sigma_h = sigma_h\n",
    "        self.N = n_particles\n",
    "        \n",
    "        # Unconditional distribution\n",
    "        self.unconditional_var = sigma_h**2 / (1 - phi**2)\n",
    "        \n",
    "    def simulate(self, T):\n",
    "        \"\"\"\n",
    "        Simulate from the SV model.\n",
    "        \"\"\"\n",
    "        h = np.zeros(T)\n",
    "        r = np.zeros(T)\n",
    "        \n",
    "        # Initialize from unconditional distribution\n",
    "        h[0] = self.mu + np.sqrt(self.unconditional_var) * np.random.randn()\n",
    "        r[0] = np.exp(h[0]/2) * np.random.randn()\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            h[t] = self.mu + self.phi * (h[t-1] - self.mu) + self.sigma_h * np.random.randn()\n",
    "            r[t] = np.exp(h[t]/2) * np.random.randn()\n",
    "            \n",
    "        return r, h\n",
    "    \n",
    "    def filter(self, returns):\n",
    "        \"\"\"\n",
    "        Run particle filter on observed returns.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        h_mean : array\n",
    "            Filtered mean of log volatility\n",
    "        h_std : array\n",
    "            Filtered std of log volatility\n",
    "        vol_mean : array\n",
    "            Filtered mean of volatility (exp(h/2))\n",
    "        \"\"\"\n",
    "        T = len(returns)\n",
    "        \n",
    "        # Initialize particles from unconditional distribution\n",
    "        particles = self.mu + np.sqrt(self.unconditional_var) * np.random.randn(self.N)\n",
    "        weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        # Storage\n",
    "        h_mean = np.zeros(T)\n",
    "        h_std = np.zeros(T)\n",
    "        h_q05 = np.zeros(T)\n",
    "        h_q95 = np.zeros(T)\n",
    "        ess_history = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t > 0:\n",
    "                # Prediction step\n",
    "                particles = (self.mu + self.phi * (particles - self.mu) + \n",
    "                            self.sigma_h * np.random.randn(self.N))\n",
    "            \n",
    "            # Update step\n",
    "            # Likelihood: p(r_t | h_t) = N(r_t; 0, exp(h_t))\n",
    "            vol = np.exp(particles / 2)\n",
    "            log_likelihood = -0.5 * np.log(2 * np.pi) - particles/2 - 0.5 * (returns[t]**2) / np.exp(particles)\n",
    "            \n",
    "            # Update weights (log space)\n",
    "            log_weights = np.log(weights + 1e-300) + log_likelihood\n",
    "            log_weights -= logsumexp(log_weights)\n",
    "            weights = np.exp(log_weights)\n",
    "            \n",
    "            # Store estimates\n",
    "            h_mean[t] = np.sum(weights * particles)\n",
    "            h_std[t] = np.sqrt(np.sum(weights * (particles - h_mean[t])**2))\n",
    "            \n",
    "            # Quantiles\n",
    "            sorted_idx = np.argsort(particles)\n",
    "            cumsum = np.cumsum(weights[sorted_idx])\n",
    "            h_q05[t] = particles[sorted_idx][np.searchsorted(cumsum, 0.05)]\n",
    "            h_q95[t] = particles[sorted_idx][min(np.searchsorted(cumsum, 0.95), self.N-1)]\n",
    "            \n",
    "            # ESS\n",
    "            ess = 1.0 / np.sum(weights**2)\n",
    "            ess_history.append(ess)\n",
    "            \n",
    "            # Resample if ESS too low\n",
    "            if ess < self.N / 2:\n",
    "                indices = np.random.choice(self.N, size=self.N, p=weights)\n",
    "                particles = particles[indices]\n",
    "                weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        vol_mean = np.exp(h_mean / 2)\n",
    "        \n",
    "        return {\n",
    "            'h_mean': h_mean,\n",
    "            'h_std': h_std,\n",
    "            'h_q05': h_q05,\n",
    "            'h_q95': h_q95,\n",
    "            'vol_mean': vol_mean,\n",
    "            'ess': np.array(ess_history)\n",
    "        }\n",
    "\n",
    "# Run SV model example\n",
    "def run_sv_example():\n",
    "    # Parameters (typical for daily returns)\n",
    "    mu = -0.5      # Long-run mean log vol (corresponds to ~22% annual vol)\n",
    "    phi = 0.98     # High persistence\n",
    "    sigma_h = 0.15 # Vol of vol\n",
    "    \n",
    "    # Simulate data\n",
    "    T = 500\n",
    "    sv_model = StochasticVolatilityPF(mu, phi, sigma_h, n_particles=2000)\n",
    "    returns, h_true = sv_model.simulate(T)\n",
    "    \n",
    "    # Run particle filter\n",
    "    results = sv_model.filter(returns)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "    \n",
    "    # Plot 1: Returns\n",
    "    axes[0].plot(returns, 'b-', lw=0.5, alpha=0.7)\n",
    "    axes[0].set_ylabel('Returns')\n",
    "    axes[0].set_title('Simulated Returns from Stochastic Volatility Model')\n",
    "    axes[0].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Log volatility\n",
    "    axes[1].plot(h_true, 'b-', lw=1.5, label='True h_t', alpha=0.8)\n",
    "    axes[1].plot(results['h_mean'], 'r-', lw=1.5, label='Filtered h_t', alpha=0.8)\n",
    "    axes[1].fill_between(range(T), results['h_q05'], results['h_q95'],\n",
    "                         alpha=0.3, color='red', label='90% CI')\n",
    "    axes[1].set_ylabel('Log Volatility h_t')\n",
    "    axes[1].set_title('Particle Filter: Log Volatility Estimation')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Plot 3: Volatility\n",
    "    vol_true = np.exp(h_true / 2)\n",
    "    abs_returns = np.abs(returns)\n",
    "    \n",
    "    axes[2].plot(vol_true, 'b-', lw=1.5, label='True œÉ_t', alpha=0.8)\n",
    "    axes[2].plot(results['vol_mean'], 'r-', lw=1.5, label='Filtered œÉ_t', alpha=0.8)\n",
    "    axes[2].plot(abs_returns, 'g.', alpha=0.2, markersize=2, label='|r_t|')\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].set_ylabel('Volatility')\n",
    "    axes[2].set_title('Particle Filter: Volatility Estimation')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Metrics\n",
    "    rmse_h = np.sqrt(np.mean((h_true - results['h_mean'])**2))\n",
    "    rmse_vol = np.sqrt(np.mean((vol_true - results['vol_mean'])**2))\n",
    "    corr_vol = np.corrcoef(vol_true, results['vol_mean'])[0, 1]\n",
    "    \n",
    "    print(f\"\\nüìä Stochastic Volatility Filter Performance:\")\n",
    "    print(f\"   ‚Ä¢ Log-vol RMSE: {rmse_h:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Volatility RMSE: {rmse_vol:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Volatility Correlation: {corr_vol:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Mean ESS: {np.mean(results['ess']):.0f} / {sv_model.N}\")\n",
    "    \n",
    "    return returns, h_true, results\n",
    "\n",
    "returns, h_true, sv_results = run_sv_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d58632",
   "metadata": {},
   "source": [
    "### 5.2 Regime-Switching Model\n",
    "\n",
    "**Hamilton Regime-Switching Model:**\n",
    "\n",
    "$$r_t = \\mu_{s_t} + \\sigma_{s_t} \\epsilon_t$$\n",
    "\n",
    "Where $s_t \\in \\{0, 1\\}$ follows a Markov chain with transition matrix:\n",
    "\n",
    "$$P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix}$$\n",
    "\n",
    "**Why Particle Filter?**\n",
    "- Discrete state space (not Gaussian!)\n",
    "- Mixture distribution for observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegimeSwitchingPF:\n",
    "    \"\"\"\n",
    "    Particle Filter for 2-state Regime-Switching Model.\n",
    "    \n",
    "    State: s_t ‚àà {0, 1} (regime)\n",
    "    Observation: r_t (returns)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu, sigma, P, n_particles=1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        mu : array\n",
    "            Mean returns in each regime [mu_0, mu_1]\n",
    "        sigma : array\n",
    "            Volatility in each regime [sigma_0, sigma_1]\n",
    "        P : array (2x2)\n",
    "            Transition probability matrix\n",
    "        n_particles : int\n",
    "            Number of particles\n",
    "        \"\"\"\n",
    "        self.mu = np.array(mu)\n",
    "        self.sigma = np.array(sigma)\n",
    "        self.P = np.array(P)\n",
    "        self.N = n_particles\n",
    "        \n",
    "        # Stationary distribution\n",
    "        self.pi = self._compute_stationary()\n",
    "        \n",
    "    def _compute_stationary(self):\n",
    "        \"\"\"Compute stationary distribution of Markov chain.\"\"\"\n",
    "        # pi * P = pi, sum(pi) = 1\n",
    "        # pi_0 * P[0,0] + pi_1 * P[1,0] = pi_0\n",
    "        # pi_0 + pi_1 = 1\n",
    "        p01 = self.P[0, 1]\n",
    "        p10 = self.P[1, 0]\n",
    "        pi_0 = p10 / (p01 + p10)\n",
    "        return np.array([pi_0, 1 - pi_0])\n",
    "    \n",
    "    def simulate(self, T):\n",
    "        \"\"\"\n",
    "        Simulate from the regime-switching model.\n",
    "        \"\"\"\n",
    "        s = np.zeros(T, dtype=int)\n",
    "        r = np.zeros(T)\n",
    "        \n",
    "        # Initialize from stationary distribution\n",
    "        s[0] = np.random.choice([0, 1], p=self.pi)\n",
    "        r[0] = self.mu[s[0]] + self.sigma[s[0]] * np.random.randn()\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            # Transition\n",
    "            s[t] = np.random.choice([0, 1], p=self.P[s[t-1]])\n",
    "            # Observation\n",
    "            r[t] = self.mu[s[t]] + self.sigma[s[t]] * np.random.randn()\n",
    "            \n",
    "        return r, s\n",
    "    \n",
    "    def filter(self, returns):\n",
    "        \"\"\"\n",
    "        Run particle filter.\n",
    "        \n",
    "        Returns filtered probability of being in regime 1.\n",
    "        \"\"\"\n",
    "        T = len(returns)\n",
    "        \n",
    "        # Initialize particles\n",
    "        particles = np.random.choice([0, 1], size=self.N, p=self.pi)\n",
    "        weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        # Storage\n",
    "        prob_regime1 = np.zeros(T)\n",
    "        ess_history = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t > 0:\n",
    "                # Prediction: propagate through Markov transition\n",
    "                new_particles = np.zeros(self.N, dtype=int)\n",
    "                for i in range(self.N):\n",
    "                    new_particles[i] = np.random.choice([0, 1], p=self.P[particles[i]])\n",
    "                particles = new_particles\n",
    "            \n",
    "            # Update: compute likelihood\n",
    "            likelihood = np.zeros(self.N)\n",
    "            for i in range(self.N):\n",
    "                likelihood[i] = stats.norm.pdf(returns[t], \n",
    "                                                self.mu[particles[i]], \n",
    "                                                self.sigma[particles[i]])\n",
    "            \n",
    "            # Update weights\n",
    "            log_weights = np.log(weights + 1e-300) + np.log(likelihood + 1e-300)\n",
    "            log_weights -= logsumexp(log_weights)\n",
    "            weights = np.exp(log_weights)\n",
    "            \n",
    "            # Store regime probability\n",
    "            prob_regime1[t] = np.sum(weights * particles)\n",
    "            \n",
    "            # ESS\n",
    "            ess = 1.0 / np.sum(weights**2)\n",
    "            ess_history.append(ess)\n",
    "            \n",
    "            # Resample\n",
    "            if ess < self.N / 2:\n",
    "                indices = np.random.choice(self.N, size=self.N, p=weights)\n",
    "                particles = particles[indices]\n",
    "                weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        return prob_regime1, np.array(ess_history)\n",
    "\n",
    "# Run regime-switching example\n",
    "def run_regime_switching_example():\n",
    "    # Parameters: Bull and Bear market\n",
    "    mu = [0.001, -0.002]     # Bull: +0.1%, Bear: -0.2% daily\n",
    "    sigma = [0.01, 0.025]    # Bull: 1%, Bear: 2.5% daily vol\n",
    "    P = [[0.98, 0.02],       # 2% chance of switching from Bull to Bear\n",
    "         [0.05, 0.95]]       # 5% chance of switching from Bear to Bull\n",
    "    \n",
    "    T = 500\n",
    "    rs_model = RegimeSwitchingPF(mu, sigma, P, n_particles=1000)\n",
    "    returns, regimes = rs_model.simulate(T)\n",
    "    \n",
    "    # Run particle filter\n",
    "    prob_bear, ess = rs_model.filter(returns)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Returns colored by regime\n",
    "    colors = ['green' if s == 0 else 'red' for s in regimes]\n",
    "    axes[0].bar(range(T), returns, color=colors, alpha=0.7, width=1.0)\n",
    "    axes[0].set_ylabel('Returns')\n",
    "    axes[0].set_title('Returns (Green=Bull, Red=Bear)')\n",
    "    \n",
    "    # Plot 2: True regime vs filtered probability\n",
    "    axes[1].fill_between(range(T), 0, regimes, alpha=0.3, color='red', \n",
    "                         label='True Bear Regime', step='mid')\n",
    "    axes[1].plot(prob_bear, 'b-', lw=1.5, label='P(Bear|data)', alpha=0.8)\n",
    "    axes[1].set_ylabel('Probability')\n",
    "    axes[1].set_title('Regime Detection: True vs Filtered')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # Plot 3: Cumulative returns\n",
    "    cumret = np.cumsum(returns)\n",
    "    axes[2].plot(cumret, 'b-', lw=1.5)\n",
    "    \n",
    "    # Shade bear regimes\n",
    "    in_bear = False\n",
    "    bear_start = 0\n",
    "    for t in range(T):\n",
    "        if regimes[t] == 1 and not in_bear:\n",
    "            bear_start = t\n",
    "            in_bear = True\n",
    "        elif regimes[t] == 0 and in_bear:\n",
    "            axes[2].axvspan(bear_start, t, alpha=0.2, color='red')\n",
    "            in_bear = False\n",
    "    if in_bear:\n",
    "        axes[2].axvspan(bear_start, T, alpha=0.2, color='red')\n",
    "        \n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].set_ylabel('Cumulative Return')\n",
    "    axes[2].set_title('Cumulative Returns (Red shading = Bear regime)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance metrics\n",
    "    regime_pred = (prob_bear > 0.5).astype(int)\n",
    "    accuracy = np.mean(regime_pred == regimes)\n",
    "    \n",
    "    print(f\"\\nüìä Regime-Switching Filter Performance:\")\n",
    "    print(f\"   ‚Ä¢ Classification Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"   ‚Ä¢ Mean ESS: {np.mean(ess):.0f} / {rs_model.N}\")\n",
    "    print(f\"   ‚Ä¢ Stationary prob (Bear): {rs_model.pi[1]:.1%}\")\n",
    "    print(f\"   ‚Ä¢ Empirical Bear freq: {np.mean(regimes):.1%}\")\n",
    "    \n",
    "    return returns, regimes, prob_bear\n",
    "\n",
    "rs_returns, rs_regimes, rs_prob = run_regime_switching_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6092c45",
   "metadata": {},
   "source": [
    "### 5.3 Application to Real Financial Data\n",
    "\n",
    "Let's apply the Stochastic Volatility particle filter to real market data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sv_to_real_data():\n",
    "    \"\"\"\n",
    "    Apply Stochastic Volatility PF to simulated \"real\" market data.\n",
    "    (In practice, you'd use yfinance or similar)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Simulate realistic market data with fat tails and volatility clustering\n",
    "    T = 750  # ~3 years of daily data\n",
    "    \n",
    "    # Generate returns with realistic features\n",
    "    # Use a more complex model to simulate\n",
    "    mu_sv = -0.3\n",
    "    phi_sv = 0.985\n",
    "    sigma_h_sv = 0.12\n",
    "    \n",
    "    h = np.zeros(T)\n",
    "    h[0] = mu_sv\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        # Add some jumps to volatility\n",
    "        jump = 0.5 * (np.random.random() < 0.02)  # 2% chance of vol spike\n",
    "        h[t] = mu_sv + phi_sv * (h[t-1] - mu_sv) + sigma_h_sv * np.random.randn() + jump\n",
    "    \n",
    "    # Generate returns with Student-t innovations (fatter tails)\n",
    "    df = 5  # degrees of freedom\n",
    "    returns = np.exp(h/2) * np.random.standard_t(df, T) / np.sqrt(df/(df-2))\n",
    "    \n",
    "    # Convert to price\n",
    "    prices = 100 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Create date index\n",
    "    dates = pd.date_range(start='2023-01-01', periods=T, freq='B')\n",
    "    \n",
    "    # Run particle filter (with parameters we'd typically estimate)\n",
    "    sv_filter = StochasticVolatilityPF(\n",
    "        mu=-0.4,       # Slightly misspecified\n",
    "        phi=0.98,\n",
    "        sigma_h=0.15,\n",
    "        n_particles=3000\n",
    "    )\n",
    "    \n",
    "    results = sv_filter.filter(returns)\n",
    "    \n",
    "    # Compare with realized volatility (20-day rolling)\n",
    "    realized_vol = pd.Series(returns).rolling(20).std() * np.sqrt(252)\n",
    "    filtered_vol_annual = results['vol_mean'] * np.sqrt(252)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 14))\n",
    "    \n",
    "    # Plot 1: Price\n",
    "    axes[0].plot(dates, prices, 'b-', lw=1)\n",
    "    axes[0].set_ylabel('Price')\n",
    "    axes[0].set_title('Simulated Market Data with Stochastic Volatility')\n",
    "    \n",
    "    # Plot 2: Returns\n",
    "    axes[1].plot(dates, returns * 100, 'b-', lw=0.5, alpha=0.7)\n",
    "    axes[1].axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1].set_ylabel('Returns (%)')\n",
    "    axes[1].set_title('Daily Returns')\n",
    "    \n",
    "    # Plot 3: Volatility comparison\n",
    "    axes[2].plot(dates, realized_vol, 'b-', lw=1, label='20-day Realized Vol', alpha=0.7)\n",
    "    axes[2].plot(dates, filtered_vol_annual, 'r-', lw=1.5, label='PF Filtered Vol', alpha=0.8)\n",
    "    axes[2].plot(dates, np.exp(h/2) * np.sqrt(252), 'g--', lw=1, label='True Vol', alpha=0.7)\n",
    "    axes[2].set_ylabel('Annualized Volatility')\n",
    "    axes[2].set_title('Volatility: Particle Filter vs Realized')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    # Plot 4: ESS\n",
    "    axes[3].plot(dates, results['ess'], 'b-', lw=0.5)\n",
    "    axes[3].axhline(sv_filter.N * 0.5, color='r', linestyle='--', label='Resample threshold')\n",
    "    axes[3].set_xlabel('Date')\n",
    "    axes[3].set_ylabel('ESS')\n",
    "    axes[3].set_title('Effective Sample Size')\n",
    "    axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    valid_idx = ~np.isnan(realized_vol)\n",
    "    corr = np.corrcoef(realized_vol[valid_idx], filtered_vol_annual[valid_idx])[0, 1]\n",
    "    \n",
    "    print(f\"\\nüìä Real Data Analysis Results:\")\n",
    "    print(f\"   ‚Ä¢ Correlation (PF vol vs Realized vol): {corr:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Mean filtered vol: {np.mean(filtered_vol_annual)*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Mean realized vol: {np.nanmean(realized_vol)*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Vol of vol (PF): {np.std(filtered_vol_annual)*100:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "real_results = apply_sv_to_real_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d4849",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Advanced Topics <a name=\"6-advanced\"></a>\n",
    "\n",
    "### 6.1 Auxiliary Particle Filter (APF)\n",
    "\n",
    "**Improvement over Bootstrap PF:**\n",
    "- Look ahead using the observation to guide resampling\n",
    "- Better proposal that considers $y_t$\n",
    "\n",
    "**Key Idea:** First resample based on predicted observation likelihood, then propagate.\n",
    "\n",
    "### 6.2 Rao-Blackwellized Particle Filter\n",
    "\n",
    "**For models with linear sub-structure:**\n",
    "- Marginalize out the linear part analytically (Kalman filter)\n",
    "- Use particles only for the nonlinear part\n",
    "- Reduces variance significantly\n",
    "\n",
    "### 6.3 Particle MCMC\n",
    "\n",
    "**For parameter estimation:**\n",
    "- Use particle filter within MCMC\n",
    "- Particle Marginal Metropolis-Hastings (PMMH)\n",
    "- Achieves exact Bayesian inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666473b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryParticleFilter:\n",
    "    \"\"\"\n",
    "    Auxiliary Particle Filter for Stochastic Volatility.\n",
    "    \n",
    "    Improves upon Bootstrap PF by using a look-ahead step.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu, phi, sigma_h, n_particles=1000):\n",
    "        self.mu = mu\n",
    "        self.phi = phi\n",
    "        self.sigma_h = sigma_h\n",
    "        self.N = n_particles\n",
    "        self.unconditional_var = sigma_h**2 / (1 - phi**2)\n",
    "        \n",
    "    def filter(self, returns):\n",
    "        \"\"\"\n",
    "        Run Auxiliary Particle Filter.\n",
    "        \"\"\"\n",
    "        T = len(returns)\n",
    "        \n",
    "        # Initialize\n",
    "        particles = self.mu + np.sqrt(self.unconditional_var) * np.random.randn(self.N)\n",
    "        weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        h_mean = np.zeros(T)\n",
    "        ess_history = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t > 0:\n",
    "                # APF Step 1: Compute auxiliary weights using predicted mean\n",
    "                h_pred_mean = self.mu + self.phi * (particles - self.mu)\n",
    "                \n",
    "                # Likelihood at predicted mean\n",
    "                aux_likelihood = stats.norm.pdf(returns[t], 0, np.exp(h_pred_mean/2))\n",
    "                aux_weights = weights * aux_likelihood\n",
    "                aux_weights /= aux_weights.sum()\n",
    "                \n",
    "                # First-stage resampling based on auxiliary weights\n",
    "                indices = np.random.choice(self.N, size=self.N, p=aux_weights)\n",
    "                particles = particles[indices]\n",
    "                \n",
    "                # APF Step 2: Propagate particles\n",
    "                particles = (self.mu + self.phi * (particles - self.mu) + \n",
    "                            self.sigma_h * np.random.randn(self.N))\n",
    "                \n",
    "                # APF Step 3: Compute corrected weights\n",
    "                likelihood = stats.norm.pdf(returns[t], 0, np.exp(particles/2))\n",
    "                h_pred_resampled = self.mu + self.phi * (particles - self.mu)\n",
    "                aux_likelihood_resampled = stats.norm.pdf(returns[t], 0, np.exp(h_pred_resampled/2))\n",
    "                \n",
    "                weights = likelihood / (aux_likelihood_resampled + 1e-300)\n",
    "                weights /= weights.sum()\n",
    "            else:\n",
    "                # First observation\n",
    "                likelihood = stats.norm.pdf(returns[t], 0, np.exp(particles/2))\n",
    "                weights = weights * likelihood\n",
    "                weights /= weights.sum()\n",
    "            \n",
    "            # Store estimates\n",
    "            h_mean[t] = np.sum(weights * particles)\n",
    "            ess = 1.0 / np.sum(weights**2)\n",
    "            ess_history.append(ess)\n",
    "            \n",
    "            # Resample if needed\n",
    "            if ess < self.N / 2:\n",
    "                indices = np.random.choice(self.N, size=self.N, p=weights)\n",
    "                particles = particles[indices]\n",
    "                weights = np.ones(self.N) / self.N\n",
    "        \n",
    "        return {'h_mean': h_mean, 'vol_mean': np.exp(h_mean/2), 'ess': np.array(ess_history)}\n",
    "\n",
    "# Compare Bootstrap PF vs Auxiliary PF\n",
    "def compare_pf_methods():\n",
    "    # Parameters\n",
    "    mu, phi, sigma_h = -0.5, 0.98, 0.15\n",
    "    T = 300\n",
    "    N_particles = 500\n",
    "    \n",
    "    # Generate data\n",
    "    sv_model = StochasticVolatilityPF(mu, phi, sigma_h, N_particles)\n",
    "    returns, h_true = sv_model.simulate(T)\n",
    "    \n",
    "    # Run both filters multiple times\n",
    "    n_runs = 10\n",
    "    \n",
    "    bpf_rmse = []\n",
    "    apf_rmse = []\n",
    "    bpf_ess = []\n",
    "    apf_ess = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        # Bootstrap PF\n",
    "        bpf = StochasticVolatilityPF(mu, phi, sigma_h, N_particles)\n",
    "        bpf_results = bpf.filter(returns)\n",
    "        bpf_rmse.append(np.sqrt(np.mean((h_true - bpf_results['h_mean'])**2)))\n",
    "        bpf_ess.append(np.mean(bpf_results['ess']))\n",
    "        \n",
    "        # Auxiliary PF\n",
    "        apf = AuxiliaryParticleFilter(mu, phi, sigma_h, N_particles)\n",
    "        apf_results = apf.filter(returns)\n",
    "        apf_rmse.append(np.sqrt(np.mean((h_true - apf_results['h_mean'])**2)))\n",
    "        apf_ess.append(np.mean(apf_results['ess']))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    methods = ['Bootstrap PF', 'Auxiliary PF']\n",
    "    rmses = [np.mean(bpf_rmse), np.mean(apf_rmse)]\n",
    "    rmse_stds = [np.std(bpf_rmse), np.std(apf_rmse)]\n",
    "    \n",
    "    axes[0].bar(methods, rmses, yerr=rmse_stds, capsize=5, color=['blue', 'green'], alpha=0.7)\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].set_title('Log-Volatility RMSE Comparison')\n",
    "    \n",
    "    # ESS comparison\n",
    "    ess_means = [np.mean(bpf_ess), np.mean(apf_ess)]\n",
    "    ess_stds = [np.std(bpf_ess), np.std(apf_ess)]\n",
    "    \n",
    "    axes[1].bar(methods, ess_means, yerr=ess_stds, capsize=5, color=['blue', 'green'], alpha=0.7)\n",
    "    axes[1].axhline(N_particles, color='r', linestyle='--', label=f'N={N_particles}')\n",
    "    axes[1].set_ylabel('Mean ESS')\n",
    "    axes[1].set_title('Effective Sample Size Comparison')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Method Comparison ({n_runs} runs, {N_particles} particles):\")\n",
    "    print(f\"   {'Method':<15} {'RMSE':<15} {'Mean ESS':<15}\")\n",
    "    print(\"   \" + \"-\" * 45)\n",
    "    print(f\"   {'Bootstrap PF':<15} {np.mean(bpf_rmse):.4f} ¬± {np.std(bpf_rmse):.4f}  {np.mean(bpf_ess):.1f} ¬± {np.std(bpf_ess):.1f}\")\n",
    "    print(f\"   {'Auxiliary PF':<15} {np.mean(apf_rmse):.4f} ¬± {np.std(apf_rmse):.4f}  {np.mean(apf_ess):.1f} ¬± {np.std(apf_ess):.1f}\")\n",
    "\n",
    "compare_pf_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301eb769",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Interview Questions & Exercises <a name=\"7-interview\"></a>\n",
    "\n",
    "### üíº Common Interview Questions\n",
    "\n",
    "**Q1: When would you use a particle filter instead of a Kalman filter?**\n",
    "\n",
    "> Use particle filters when:\n",
    "> - State transition is nonlinear\n",
    "> - Observation model is nonlinear\n",
    "> - Noise is non-Gaussian (fat tails, skewness)\n",
    "> - State space is discrete (regime-switching)\n",
    "> - Posterior is multimodal\n",
    "\n",
    "**Q2: What is particle degeneracy and how do you address it?**\n",
    "\n",
    "> Particle degeneracy occurs when few particles have most of the weight (ESS‚Üí1).\n",
    "> Solutions:\n",
    "> - Resampling (multinomial, systematic, stratified)\n",
    "> - Better proposal distributions (auxiliary PF, optimal proposal)\n",
    "> - More particles\n",
    "> - Rao-Blackwellization\n",
    "\n",
    "**Q3: Explain the tradeoff between number of particles and computational cost.**\n",
    "\n",
    "> - Monte Carlo error ‚àù 1/‚àöN (need 4x particles for 2x accuracy)\n",
    "> - Computational cost is O(N) per timestep\n",
    "> - Memory cost is O(N)\n",
    "> - In practice: 100-10,000 particles depending on problem complexity\n",
    "\n",
    "**Q4: How would you use a particle filter for real-time trading?**\n",
    "\n",
    "> Applications:\n",
    "> - Real-time volatility estimation (stochastic vol model)\n",
    "> - Regime detection for strategy switching\n",
    "> - Tracking hidden market state (informed trading)\n",
    "> - Online parameter learning\n",
    "> \n",
    "> Considerations:\n",
    "> - Latency requirements\n",
    "> - Parallelization on GPU\n",
    "> - Fixed-lag smoothing for better estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25526b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a particle filter for jump-diffusion model\n",
    "\n",
    "def exercise_jump_diffusion_pf():\n",
    "    \"\"\"\n",
    "    Exercise: Implement PF for Merton Jump-Diffusion.\n",
    "    \n",
    "    Model:\n",
    "    dS = ŒºS dt + œÉS dW + S dJ\n",
    "    \n",
    "    Where J is a compound Poisson process with:\n",
    "    - Jump intensity Œª\n",
    "    - Jump size ~ N(Œº_J, œÉ_J¬≤)\n",
    "    \n",
    "    Hidden state: Number of jumps in each period\n",
    "    Observation: Log returns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters\n",
    "    mu = 0.0005      # Drift\n",
    "    sigma = 0.01     # Diffusion volatility\n",
    "    lam = 0.05       # Jump intensity (5% chance per day)\n",
    "    mu_J = -0.02     # Mean jump size (negative = crashes)\n",
    "    sigma_J = 0.03   # Jump size volatility\n",
    "    \n",
    "    T = 500\n",
    "    N = 1000  # particles\n",
    "    \n",
    "    # Simulate data\n",
    "    np.random.seed(42)\n",
    "    n_jumps = np.random.poisson(lam, T)\n",
    "    jump_sizes = np.array([np.sum(np.random.normal(mu_J, sigma_J, n)) if n > 0 else 0 \n",
    "                           for n in n_jumps])\n",
    "    returns = mu + sigma * np.random.randn(T) + jump_sizes\n",
    "    \n",
    "    # Particle filter\n",
    "    # State: n_t (number of jumps)\n",
    "    particles = np.zeros(N, dtype=int)  # Start with 0 jumps\n",
    "    weights = np.ones(N) / N\n",
    "    \n",
    "    n_jumps_est = np.zeros(T)\n",
    "    prob_jump = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Prediction: sample number of jumps from Poisson\n",
    "        particles = np.random.poisson(lam, N)\n",
    "        \n",
    "        # Update: compute likelihood\n",
    "        # p(r_t | n_t) = N(r_t; Œº + n_t*Œº_J, œÉ¬≤ + n_t*œÉ_J¬≤)\n",
    "        likelihood = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            mean_r = mu + particles[i] * mu_J\n",
    "            var_r = sigma**2 + particles[i] * sigma_J**2\n",
    "            likelihood[i] = stats.norm.pdf(returns[t], mean_r, np.sqrt(var_r))\n",
    "        \n",
    "        # Update weights\n",
    "        weights = weights * likelihood\n",
    "        weights /= weights.sum()\n",
    "        \n",
    "        # Estimates\n",
    "        n_jumps_est[t] = np.sum(weights * particles)\n",
    "        prob_jump[t] = np.sum(weights * (particles > 0))\n",
    "        \n",
    "        # Resample\n",
    "        ess = 1.0 / np.sum(weights**2)\n",
    "        if ess < N / 2:\n",
    "            indices = np.random.choice(N, size=N, p=weights)\n",
    "            particles = particles[indices]\n",
    "            weights = np.ones(N) / N\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    axes[0].plot(returns * 100, 'b-', lw=0.5, alpha=0.7)\n",
    "    for t in np.where(n_jumps > 0)[0]:\n",
    "        axes[0].axvline(t, color='red', alpha=0.3, lw=1)\n",
    "    axes[0].set_ylabel('Returns (%)')\n",
    "    axes[0].set_title('Returns with Jump Events (red lines = actual jumps)')\n",
    "    \n",
    "    axes[1].plot(n_jumps, 'b-', lw=1, label='True # Jumps', alpha=0.8)\n",
    "    axes[1].plot(n_jumps_est, 'r-', lw=1, label='Estimated # Jumps', alpha=0.8)\n",
    "    axes[1].set_ylabel('Number of Jumps')\n",
    "    axes[1].set_title('Jump Detection')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].fill_between(range(T), 0, (n_jumps > 0).astype(float), \n",
    "                         alpha=0.3, color='blue', label='True Jump', step='mid')\n",
    "    axes[2].plot(prob_jump, 'r-', lw=1, label='P(Jump|data)', alpha=0.8)\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].set_ylabel('Probability')\n",
    "    axes[2].set_title('Jump Probability Detection')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Metrics\n",
    "    pred_jump = (prob_jump > 0.5).astype(int)\n",
    "    true_jump = (n_jumps > 0).astype(int)\n",
    "    \n",
    "    tp = np.sum((pred_jump == 1) & (true_jump == 1))\n",
    "    fp = np.sum((pred_jump == 1) & (true_jump == 0))\n",
    "    fn = np.sum((pred_jump == 0) & (true_jump == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Jump-Diffusion PF Results:\")\n",
    "    print(f\"   ‚Ä¢ Total jumps: {np.sum(n_jumps > 0)}\")\n",
    "    print(f\"   ‚Ä¢ Detected jumps: {np.sum(pred_jump)}\")\n",
    "    print(f\"   ‚Ä¢ Precision: {precision:.1%}\")\n",
    "    print(f\"   ‚Ä¢ Recall: {recall:.1%}\")\n",
    "\n",
    "exercise_jump_diffusion_pf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Multi-asset regime detection\n",
    "\n",
    "def exercise_multivariate_regime_pf():\n",
    "    \"\"\"\n",
    "    Exercise: Detect common regimes across multiple assets.\n",
    "    \n",
    "    Model: All assets share a common hidden regime.\n",
    "    r_{i,t} = Œº_{i,s_t} + œÉ_{i,s_t} * Œµ_{i,t}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters for 3 assets\n",
    "    n_assets = 3\n",
    "    asset_names = ['Equity', 'Bond', 'Commodity']\n",
    "    \n",
    "    # Regime 0: Risk-on (equity up, bond down, commodity up)\n",
    "    # Regime 1: Risk-off (equity down, bond up, commodity down)\n",
    "    mu = np.array([[0.001, -0.0003, 0.0008],   # Risk-on means\n",
    "                   [-0.002, 0.0005, -0.001]])   # Risk-off means\n",
    "    \n",
    "    sigma = np.array([[0.012, 0.004, 0.015],    # Risk-on vols\n",
    "                      [0.025, 0.006, 0.022]])    # Risk-off vols\n",
    "    \n",
    "    P = np.array([[0.97, 0.03],\n",
    "                  [0.05, 0.95]])\n",
    "    \n",
    "    T = 400\n",
    "    N = 500\n",
    "    \n",
    "    # Simulate\n",
    "    np.random.seed(123)\n",
    "    s = np.zeros(T, dtype=int)\n",
    "    s[0] = 0\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        s[t] = np.random.choice([0, 1], p=P[s[t-1]])\n",
    "    \n",
    "    returns = np.zeros((T, n_assets))\n",
    "    for t in range(T):\n",
    "        for i in range(n_assets):\n",
    "            returns[t, i] = mu[s[t], i] + sigma[s[t], i] * np.random.randn()\n",
    "    \n",
    "    # Multivariate particle filter\n",
    "    particles = np.random.choice([0, 1], size=N, p=[0.5, 0.5])\n",
    "    weights = np.ones(N) / N\n",
    "    \n",
    "    prob_riskoff = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        if t > 0:\n",
    "            new_particles = np.zeros(N, dtype=int)\n",
    "            for i in range(N):\n",
    "                new_particles[i] = np.random.choice([0, 1], p=P[particles[i]])\n",
    "            particles = new_particles\n",
    "        \n",
    "        # Multivariate likelihood (product of univariate)\n",
    "        likelihood = np.ones(N)\n",
    "        for i in range(N):\n",
    "            for j in range(n_assets):\n",
    "                likelihood[i] *= stats.norm.pdf(returns[t, j], \n",
    "                                                 mu[particles[i], j],\n",
    "                                                 sigma[particles[i], j])\n",
    "        \n",
    "        weights = weights * likelihood\n",
    "        weights /= weights.sum()\n",
    "        \n",
    "        prob_riskoff[t] = np.sum(weights * particles)\n",
    "        \n",
    "        # Resample\n",
    "        ess = 1.0 / np.sum(weights**2)\n",
    "        if ess < N / 2:\n",
    "            indices = np.random.choice(N, size=N, p=weights)\n",
    "            particles = particles[indices]\n",
    "            weights = np.ones(N) / N\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(n_assets + 2, 1, figsize=(14, 12))\n",
    "    \n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    for i in range(n_assets):\n",
    "        cumret = np.cumsum(returns[:, i])\n",
    "        axes[i].plot(cumret, color=colors[i], lw=1)\n",
    "        # Shade risk-off periods\n",
    "        in_riskoff = False\n",
    "        start = 0\n",
    "        for t in range(T):\n",
    "            if s[t] == 1 and not in_riskoff:\n",
    "                start = t\n",
    "                in_riskoff = True\n",
    "            elif s[t] == 0 and in_riskoff:\n",
    "                axes[i].axvspan(start, t, alpha=0.2, color='red')\n",
    "                in_riskoff = False\n",
    "        if in_riskoff:\n",
    "            axes[i].axvspan(start, T, alpha=0.2, color='red')\n",
    "        axes[i].set_ylabel(asset_names[i])\n",
    "        axes[i].set_title(f'{asset_names[i]} Cumulative Returns (Red = Risk-off)')\n",
    "    \n",
    "    # Regime detection\n",
    "    axes[n_assets].fill_between(range(T), 0, s, alpha=0.3, color='red', \n",
    "                                 label='True Risk-off', step='mid')\n",
    "    axes[n_assets].plot(prob_riskoff, 'b-', lw=1.5, label='P(Risk-off|data)', alpha=0.8)\n",
    "    axes[n_assets].set_ylabel('Probability')\n",
    "    axes[n_assets].set_title('Multivariate Regime Detection')\n",
    "    axes[n_assets].legend()\n",
    "    \n",
    "    # All assets together\n",
    "    for i in range(n_assets):\n",
    "        axes[n_assets+1].plot(np.cumsum(returns[:, i]), color=colors[i], \n",
    "                               lw=1, label=asset_names[i])\n",
    "    axes[n_assets+1].set_xlabel('Time')\n",
    "    axes[n_assets+1].set_ylabel('Cumulative Return')\n",
    "    axes[n_assets+1].set_title('All Assets Overlay')\n",
    "    axes[n_assets+1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = np.mean((prob_riskoff > 0.5) == s)\n",
    "    print(f\"\\nüìä Multivariate Regime Detection:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"   ‚Ä¢ Risk-off frequency (true): {np.mean(s):.1%}\")\n",
    "    print(f\"   ‚Ä¢ Risk-off frequency (detected): {np.mean(prob_riskoff > 0.5):.1%}\")\n",
    "\n",
    "exercise_multivariate_regime_pf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57e4fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Particle filters** solve filtering problems that Kalman filters cannot:\n",
    "   - Nonlinear dynamics\n",
    "   - Non-Gaussian noise\n",
    "   - Discrete state spaces\n",
    "\n",
    "2. **Core algorithm** (Bootstrap PF):\n",
    "   - Predict: Propagate particles through state transition\n",
    "   - Update: Reweight by observation likelihood\n",
    "   - Resample: Avoid weight degeneracy\n",
    "\n",
    "3. **Financial applications**:\n",
    "   - Stochastic volatility estimation\n",
    "   - Regime detection\n",
    "   - Jump detection\n",
    "   - Online parameter learning\n",
    "\n",
    "4. **Practical considerations**:\n",
    "   - Monitor ESS for particle degeneracy\n",
    "   - Use systematic/stratified resampling\n",
    "   - Consider auxiliary PF for better proposals\n",
    "   - GPU parallelization for real-time applications\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Day 6: Gaussian Processes for financial modeling\n",
    "- Explore: Particle MCMC for parameter estimation\n",
    "- Advanced: Rao-Blackwellized PF for mixed linear/nonlinear models\n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "1. Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods in Practice\n",
    "2. Johannes, M., Korteweg, A., & Polson, N. (2014). Sequential Learning, Predictability, and Optimal Portfolio Returns\n",
    "3. Pitt, M. K., & Shephard, N. (1999). Filtering via Simulation: Auxiliary Particle Filters\n",
    "4. Andrieu, C., Doucet, A., & Holenstein, R. (2010). Particle Markov Chain Monte Carlo Methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
