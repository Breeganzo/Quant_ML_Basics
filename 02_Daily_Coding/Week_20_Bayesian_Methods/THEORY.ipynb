{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0e112c",
   "metadata": {},
   "source": [
    "# Week 20: Bayesian Methods for Quantitative Trading\n",
    "\n",
    "## Comprehensive Theory and Implementation Guide\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **Learning Objectives**\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "\n",
    "1. **Bayesian Inference Fundamentals** - Bayes' theorem, priors, likelihoods, and posteriors\n",
    "2. **Conjugate Priors** - Efficient closed-form posterior computation\n",
    "3. **MCMC Methods** - Sampling from complex posterior distributions\n",
    "4. **Bayesian Regression** - Uncertainty quantification in predictions\n",
    "5. **Portfolio Optimization** - Black-Litterman and robust allocation\n",
    "6. **Risk Management** - Bayesian VaR and tail risk estimation\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Why Bayesian Methods in Quantitative Finance?**\n",
    "\n",
    "| Advantage | Description |\n",
    "|-----------|-------------|\n",
    "| **Uncertainty Quantification** | Full probability distributions, not just point estimates |\n",
    "| **Incorporating Prior Knowledge** | Systematic way to include domain expertise |\n",
    "| **Small Sample Robustness** | Better performance with limited data |\n",
    "| **Sequential Learning** | Natural framework for updating beliefs |\n",
    "| **Regularization** | Priors act as regularizers preventing overfitting |\n",
    "| **Model Comparison** | Principled approach via Bayes factors |\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ **Table of Contents**\n",
    "\n",
    "1. Import Required Libraries\n",
    "2. Bayes' Theorem Fundamentals\n",
    "3. Prior Distributions for Trading Parameters\n",
    "4. Likelihood Functions for Market Data\n",
    "5. Posterior Distribution Computation\n",
    "6. Conjugate Priors in Finance\n",
    "7. Bayesian Parameter Estimation for Returns\n",
    "8. Bayesian Updating with New Market Data\n",
    "9. Credible Intervals vs Confidence Intervals\n",
    "10. Bayesian Linear Regression for Price Prediction\n",
    "11. Markov Chain Monte Carlo (MCMC) Sampling\n",
    "12. Bayesian Model Comparison and Selection\n",
    "13. Bayesian Volatility Estimation\n",
    "14. Bayesian Portfolio Optimization (Black-Litterman)\n",
    "15. Bayesian Risk Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad666ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use several key libraries for Bayesian analysis:\n",
    "- **NumPy/SciPy**: Numerical computing and statistical distributions\n",
    "- **PyMC**: Probabilistic programming for Bayesian inference\n",
    "- **ArviZ**: Visualization and diagnostics for Bayesian models\n",
    "- **Matplotlib/Seaborn**: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import gamma as gamma_func, beta as beta_func\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WEEK 20: BAYESIAN METHODS FOR QUANTITATIVE TRADING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úì Libraries imported successfully\")\n",
    "print(\"\\nNote: PyMC and ArviZ will be imported when needed for MCMC sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca93f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Bayes' Theorem Fundamentals\n",
    "\n",
    "### üìê **The Foundation of Bayesian Inference**\n",
    "\n",
    "Bayes' theorem provides the mathematical framework for updating beliefs:\n",
    "\n",
    "$$P(\\theta | D) = \\frac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$\n",
    "\n",
    "Where:\n",
    "- $P(\\theta | D)$ = **Posterior**: Updated belief about parameters after seeing data\n",
    "- $P(D | \\theta)$ = **Likelihood**: Probability of data given parameters\n",
    "- $P(\\theta)$ = **Prior**: Initial belief about parameters before seeing data\n",
    "- $P(D)$ = **Evidence/Marginal Likelihood**: Normalizing constant\n",
    "\n",
    "### üîë **Key Insight for Trading**\n",
    "\n",
    "> \"The posterior is a weighted average of prior beliefs and observed evidence\"\n",
    "\n",
    "$$\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}$$\n",
    "\n",
    "### **Example: Trading Signal Classification**\n",
    "\n",
    "Consider a trading signal that predicts market direction. We want to estimate its true accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_theorem_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate Bayes' theorem with a trading signal example.\n",
    "    \n",
    "    Scenario: You have a trading signal. You observe it making predictions\n",
    "    and want to estimate its true accuracy (probability of correct prediction).\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BAYES' THEOREM: TRADING SIGNAL ACCURACY ESTIMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Grid of possible accuracy values (theta)\n",
    "    theta = np.linspace(0.001, 0.999, 1000)\n",
    "    \n",
    "    # PRIOR: Our initial belief about signal accuracy\n",
    "    # We'll use a Beta(2, 2) prior - mild belief that accuracy is around 50%\n",
    "    prior_alpha, prior_beta = 2, 2\n",
    "    prior = stats.beta.pdf(theta, prior_alpha, prior_beta)\n",
    "    \n",
    "    # OBSERVED DATA: Signal made 60 predictions, 42 were correct\n",
    "    n_predictions = 60\n",
    "    n_correct = 42\n",
    "    \n",
    "    # LIKELIHOOD: Binomial probability\n",
    "    # P(42 correct out of 60 | accuracy = theta)\n",
    "    likelihood = stats.binom.pmf(n_correct, n_predictions, theta)\n",
    "    \n",
    "    # POSTERIOR: Unnormalized posterior = likelihood √ó prior\n",
    "    posterior_unnorm = likelihood * prior\n",
    "    \n",
    "    # Normalize to get proper probability distribution\n",
    "    posterior = posterior_unnorm / np.trapz(posterior_unnorm, theta)\n",
    "    \n",
    "    # For Beta-Binomial conjugate pair, we have closed form:\n",
    "    # Posterior is Beta(alpha + successes, beta + failures)\n",
    "    post_alpha = prior_alpha + n_correct\n",
    "    post_beta = prior_beta + (n_predictions - n_correct)\n",
    "    posterior_exact = stats.beta.pdf(theta, post_alpha, post_beta)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Prior\n",
    "    axes[0].fill_between(theta, prior, alpha=0.3, color='blue')\n",
    "    axes[0].plot(theta, prior, 'b-', linewidth=2, label=f'Beta({prior_alpha}, {prior_beta})')\n",
    "    axes[0].axvline(0.5, color='red', linestyle='--', alpha=0.7, label='Prior mean')\n",
    "    axes[0].set_xlabel('Signal Accuracy (Œ∏)', fontsize=12)\n",
    "    axes[0].set_ylabel('Probability Density', fontsize=12)\n",
    "    axes[0].set_title('PRIOR Distribution\\n(Before seeing data)', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    \n",
    "    # Likelihood\n",
    "    axes[1].fill_between(theta, likelihood / np.max(likelihood), alpha=0.3, color='green')\n",
    "    axes[1].plot(theta, likelihood / np.max(likelihood), 'g-', linewidth=2)\n",
    "    axes[1].axvline(n_correct / n_predictions, color='red', linestyle='--', alpha=0.7, \n",
    "                    label=f'MLE = {n_correct/n_predictions:.3f}')\n",
    "    axes[1].set_xlabel('Signal Accuracy (Œ∏)', fontsize=12)\n",
    "    axes[1].set_ylabel('Likelihood (scaled)', fontsize=12)\n",
    "    axes[1].set_title(f'LIKELIHOOD\\n({n_correct}/{n_predictions} correct predictions)', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    \n",
    "    # Posterior\n",
    "    axes[2].fill_between(theta, posterior_exact, alpha=0.3, color='purple')\n",
    "    axes[2].plot(theta, posterior_exact, 'purple', linewidth=2, \n",
    "                 label=f'Beta({post_alpha}, {post_beta})')\n",
    "    posterior_mean = post_alpha / (post_alpha + post_beta)\n",
    "    axes[2].axvline(posterior_mean, color='red', linestyle='--', alpha=0.7,\n",
    "                    label=f'Posterior mean = {posterior_mean:.3f}')\n",
    "    axes[2].set_xlabel('Signal Accuracy (Œ∏)', fontsize=12)\n",
    "    axes[2].set_ylabel('Probability Density', fontsize=12)\n",
    "    axes[2].set_title('POSTERIOR Distribution\\n(After seeing data)', fontsize=14)\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä SUMMARY STATISTICS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Prior: Beta({prior_alpha}, {prior_beta})\")\n",
    "    print(f\"  - Prior mean: {prior_alpha/(prior_alpha+prior_beta):.3f}\")\n",
    "    print(f\"  - Prior std:  {np.sqrt(prior_alpha*prior_beta/((prior_alpha+prior_beta)**2*(prior_alpha+prior_beta+1))):.3f}\")\n",
    "    print(f\"\\nData observed: {n_correct}/{n_predictions} correct\")\n",
    "    print(f\"  - MLE estimate: {n_correct/n_predictions:.3f}\")\n",
    "    print(f\"\\nPosterior: Beta({post_alpha}, {post_beta})\")\n",
    "    print(f\"  - Posterior mean: {posterior_mean:.3f}\")\n",
    "    post_std = np.sqrt(post_alpha*post_beta/((post_alpha+post_beta)**2*(post_alpha+post_beta+1)))\n",
    "    print(f\"  - Posterior std:  {post_std:.3f}\")\n",
    "    \n",
    "    # 95% Credible Interval\n",
    "    ci_low = stats.beta.ppf(0.025, post_alpha, post_beta)\n",
    "    ci_high = stats.beta.ppf(0.975, post_alpha, post_beta)\n",
    "    print(f\"\\n95% Credible Interval: [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    print(f\"Probability signal accuracy > 60%: {1 - stats.beta.cdf(0.6, post_alpha, post_beta):.3f}\")\n",
    "\n",
    "bayes_theorem_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611bef8",
   "metadata": {},
   "source": [
    "### üìä **Bayesian vs Frequentist Comparison**\n",
    "\n",
    "| Aspect | Frequentist | Bayesian |\n",
    "|--------|-------------|----------|\n",
    "| **Parameters** | Fixed, unknown constants | Random variables with distributions |\n",
    "| **Probability** | Long-run frequency | Degree of belief |\n",
    "| **Inference** | Point estimate + confidence interval | Full posterior distribution |\n",
    "| **Prior information** | Ignored (or implicit) | Explicitly incorporated |\n",
    "| **Small samples** | Often unreliable | More robust via regularization |\n",
    "| **Interpretation** | \"95% of such intervals contain true value\" | \"95% probability parameter is in interval\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe8265",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Prior Distributions for Trading Parameters\n",
    "\n",
    "### üìà **Choosing Priors for Financial Parameters**\n",
    "\n",
    "The choice of prior distribution encodes our beliefs before seeing data. In finance, we often have domain knowledge that should inform our priors.\n",
    "\n",
    "### **Types of Priors**\n",
    "\n",
    "| Prior Type | Description | Use Case |\n",
    "|------------|-------------|----------|\n",
    "| **Non-informative** | Minimal assumptions (uniform, Jeffreys) | Little domain knowledge |\n",
    "| **Weakly informative** | Gentle regularization | Most common in practice |\n",
    "| **Informative** | Strong domain knowledge | Expert elicitation, previous studies |\n",
    "| **Skeptical** | Centered at null hypothesis | Testing \"significant\" effects |\n",
    "\n",
    "### **Common Priors for Trading Parameters**\n",
    "\n",
    "| Parameter | Prior | Justification |\n",
    "|-----------|-------|---------------|\n",
    "| Expected return Œº | Normal(0, œÉ) | Returns are roughly symmetric around zero |\n",
    "| Volatility œÉ | Half-Normal, InverseGamma | Must be positive |\n",
    "| Sharpe ratio | Normal(0, 1) | Most strategies cluster around 0-2 |\n",
    "| Win rate | Beta(Œ±, Œ≤) | Bounded between 0 and 1 |\n",
    "| Correlation œÅ | Uniform(-1, 1) or Beta-transformed | Bounded |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trading_priors():\n",
    "    \"\"\"\n",
    "    Visualize common prior distributions used for trading parameters.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PRIOR DISTRIBUTIONS FOR TRADING PARAMETERS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Prior for Expected Return (annualized)\n",
    "    ax = axes[0, 0]\n",
    "    mu_values = np.linspace(-0.5, 0.5, 1000)\n",
    "    \n",
    "    # Different prior choices\n",
    "    prior_uninformative = stats.uniform.pdf(mu_values, -0.5, 1)\n",
    "    prior_skeptical = stats.norm.pdf(mu_values, 0, 0.05)  # Skeptical - centered at 0\n",
    "    prior_informative = stats.norm.pdf(mu_values, 0.07, 0.10)  # Historical equity premium\n",
    "    \n",
    "    ax.plot(mu_values, prior_uninformative, 'b-', label='Uniform (non-informative)', alpha=0.7)\n",
    "    ax.plot(mu_values, prior_skeptical, 'r-', label='N(0, 0.05) - Skeptical', linewidth=2)\n",
    "    ax.plot(mu_values, prior_informative, 'g-', label='N(0.07, 0.10) - Informative', linewidth=2)\n",
    "    ax.set_xlabel('Expected Annual Return (Œº)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Expected Return', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 2. Prior for Volatility\n",
    "    ax = axes[0, 1]\n",
    "    sigma_values = np.linspace(0.001, 0.6, 1000)\n",
    "    \n",
    "    prior_halfnormal = stats.halfnorm.pdf(sigma_values, scale=0.2)\n",
    "    prior_invgamma = stats.invgamma.pdf(sigma_values, a=3, scale=0.3)\n",
    "    prior_lognormal = stats.lognorm.pdf(sigma_values, s=0.5, scale=0.15)\n",
    "    \n",
    "    ax.plot(sigma_values, prior_halfnormal, 'b-', label='Half-Normal(0.2)', linewidth=2)\n",
    "    ax.plot(sigma_values, prior_invgamma, 'r-', label='InvGamma(3, 0.3)', linewidth=2)\n",
    "    ax.plot(sigma_values, prior_lognormal, 'g-', label='LogNormal(s=0.5)', linewidth=2)\n",
    "    ax.set_xlabel('Annual Volatility (œÉ)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Volatility', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0.2, color='gray', linestyle='--', alpha=0.5, label='Typical equity vol')\n",
    "    \n",
    "    # 3. Prior for Sharpe Ratio\n",
    "    ax = axes[0, 2]\n",
    "    sr_values = np.linspace(-2, 4, 1000)\n",
    "    \n",
    "    prior_sr_skeptical = stats.norm.pdf(sr_values, 0, 0.5)\n",
    "    prior_sr_moderate = stats.norm.pdf(sr_values, 0.5, 1.0)\n",
    "    prior_sr_optimistic = stats.norm.pdf(sr_values, 1.0, 0.5)\n",
    "    \n",
    "    ax.plot(sr_values, prior_sr_skeptical, 'r-', label='N(0, 0.5) - Skeptical', linewidth=2)\n",
    "    ax.plot(sr_values, prior_sr_moderate, 'b-', label='N(0.5, 1.0) - Moderate', linewidth=2)\n",
    "    ax.plot(sr_values, prior_sr_optimistic, 'g-', label='N(1.0, 0.5) - Optimistic', linewidth=2)\n",
    "    ax.set_xlabel('Sharpe Ratio', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Sharpe Ratio', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 4. Prior for Win Rate (probability of profitable trade)\n",
    "    ax = axes[1, 0]\n",
    "    p_values = np.linspace(0.001, 0.999, 1000)\n",
    "    \n",
    "    prior_uniform = stats.beta.pdf(p_values, 1, 1)\n",
    "    prior_symmetric = stats.beta.pdf(p_values, 5, 5)  # Centered at 0.5\n",
    "    prior_skilled = stats.beta.pdf(p_values, 6, 4)  # Slight edge\n",
    "    \n",
    "    ax.plot(p_values, prior_uniform, 'b-', label='Beta(1,1) - Uniform', linewidth=2)\n",
    "    ax.plot(p_values, prior_symmetric, 'r-', label='Beta(5,5) - 50% prior', linewidth=2)\n",
    "    ax.plot(p_values, prior_skilled, 'g-', label='Beta(6,4) - 60% prior', linewidth=2)\n",
    "    ax.set_xlabel('Win Rate', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Trade Win Rate', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 5. Prior for Correlation\n",
    "    ax = axes[1, 1]\n",
    "    rho_values = np.linspace(-0.99, 0.99, 1000)\n",
    "    \n",
    "    # Transform to use beta distribution on [-1, 1]\n",
    "    def beta_on_correlation(rho, a, b):\n",
    "        # Transform rho from [-1,1] to [0,1]\n",
    "        x = (rho + 1) / 2\n",
    "        return stats.beta.pdf(x, a, b) / 2  # Divide by 2 for Jacobian\n",
    "    \n",
    "    prior_uniform_rho = np.ones_like(rho_values) * 0.5\n",
    "    prior_near_zero = beta_on_correlation(rho_values, 5, 5)\n",
    "    prior_positive = beta_on_correlation(rho_values, 6, 4)\n",
    "    \n",
    "    ax.plot(rho_values, prior_uniform_rho, 'b-', label='Uniform(-1, 1)', linewidth=2)\n",
    "    ax.plot(rho_values, prior_near_zero, 'r-', label='Centered at 0', linewidth=2)\n",
    "    ax.plot(rho_values, prior_positive, 'g-', label='Slightly positive', linewidth=2)\n",
    "    ax.set_xlabel('Correlation (œÅ)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Correlation', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 6. Prior for Maximum Drawdown (as fraction)\n",
    "    ax = axes[1, 2]\n",
    "    dd_values = np.linspace(0.01, 0.8, 1000)\n",
    "    \n",
    "    prior_dd_mild = stats.beta.pdf(dd_values, 2, 5)  # Expect small drawdowns\n",
    "    prior_dd_moderate = stats.beta.pdf(dd_values, 2, 3)  # Moderate drawdowns\n",
    "    prior_dd_heavy = stats.beta.pdf(dd_values, 3, 2)  # Expect larger drawdowns\n",
    "    \n",
    "    ax.plot(dd_values, prior_dd_mild, 'g-', label='Beta(2,5) - Mild', linewidth=2)\n",
    "    ax.plot(dd_values, prior_dd_moderate, 'b-', label='Beta(2,3) - Moderate', linewidth=2)\n",
    "    ax.plot(dd_values, prior_dd_heavy, 'r-', label='Beta(3,2) - Heavy', linewidth=2)\n",
    "    ax.set_xlabel('Maximum Drawdown', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title('Priors for Maximum Drawdown', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.axvline(0.2, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(\"\\nüìã PRIOR SELECTION GUIDELINES FOR TRADING\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\"\"\n",
    "    1. EXPECTED RETURNS:\n",
    "       - Skeptical prior N(0, small œÉ) prevents overfitting\n",
    "       - Historical equity premium ~7% can inform prior\n",
    "       - For alpha research, center at 0 (market efficiency prior)\n",
    "    \n",
    "    2. VOLATILITY:\n",
    "       - Must be positive: use Half-Normal, InverseGamma\n",
    "       - Typical equity volatility ~15-25% annually\n",
    "       - Crypto: prior allowing 50%+ volatility\n",
    "    \n",
    "    3. SHARPE RATIO:\n",
    "       - Most real strategies: 0 to 2\n",
    "       - Skeptical prior N(0, 1) is common\n",
    "       - Sharpe > 2 sustained is rare in practice\n",
    "    \n",
    "    4. WIN RATE:\n",
    "       - Random: 50%, so Beta(5,5) is reasonable\n",
    "       - Skilled trader: Beta(6,4) gives slight edge\n",
    "       - Avoid uniform - it's too weak\n",
    "    \n",
    "    5. CORRELATIONS:\n",
    "       - Use LKJ prior for correlation matrices\n",
    "       - Individual correlations: center near 0\n",
    "       - Market regime affects correlations\n",
    "    \"\"\")\n",
    "\n",
    "visualize_trading_priors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e27fe8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Likelihood Functions for Market Data\n",
    "\n",
    "### üìä **The Data Generating Process**\n",
    "\n",
    "The likelihood function $P(D|\\theta)$ describes how data is generated given parameters. For financial returns, common choices include:\n",
    "\n",
    "### **Distribution Models for Returns**\n",
    "\n",
    "| Distribution | PDF | Use Case |\n",
    "|--------------|-----|----------|\n",
    "| **Gaussian** | $\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(r-\\mu)^2}{2\\sigma^2}}$ | First approximation, tractable |\n",
    "| **Student-t** | $\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{x^2}{\\nu})^{-\\frac{\\nu+1}{2}}$ | Fat tails, better fit |\n",
    "| **Mixture** | $\\sum_k \\pi_k \\cdot f_k(r)$ | Regime switching |\n",
    "| **Skew-Normal** | $2\\phi(x)\\Phi(\\alpha x)$ | Asymmetric returns |\n",
    "\n",
    "### **Log-Likelihood for Numerical Stability**\n",
    "\n",
    "We work with log-likelihoods to avoid numerical underflow:\n",
    "\n",
    "$$\\log L(\\theta | D) = \\sum_{i=1}^{n} \\log P(r_i | \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_functions_demo():\n",
    "    \"\"\"\n",
    "    Compare different likelihood functions for modeling financial returns.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LIKELIHOOD FUNCTIONS FOR FINANCIAL RETURNS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate synthetic returns with fat tails\n",
    "    np.random.seed(42)\n",
    "    n_obs = 500\n",
    "    \n",
    "    # Mix of normal returns with occasional extreme moves\n",
    "    normal_returns = np.random.normal(0.0005, 0.015, n_obs)\n",
    "    extreme_idx = np.random.choice(n_obs, size=int(n_obs * 0.05), replace=False)\n",
    "    returns = normal_returns.copy()\n",
    "    returns[extreme_idx] = np.random.normal(0, 0.05, len(extreme_idx))\n",
    "    \n",
    "    # Define likelihood functions\n",
    "    class LikelihoodFunctions:\n",
    "        @staticmethod\n",
    "        def gaussian_loglik(params, data):\n",
    "            \"\"\"Gaussian log-likelihood\"\"\"\n",
    "            mu, sigma = params\n",
    "            if sigma <= 0:\n",
    "                return -np.inf\n",
    "            return np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "        \n",
    "        @staticmethod\n",
    "        def student_t_loglik(params, data):\n",
    "            \"\"\"Student-t log-likelihood\"\"\"\n",
    "            mu, sigma, nu = params\n",
    "            if sigma <= 0 or nu <= 2:\n",
    "                return -np.inf\n",
    "            return np.sum(stats.t.logpdf(data, df=nu, loc=mu, scale=sigma))\n",
    "        \n",
    "        @staticmethod\n",
    "        def mixture_loglik(params, data):\n",
    "            \"\"\"Mixture of two Gaussians log-likelihood\"\"\"\n",
    "            mu1, sigma1, mu2, sigma2, pi = params\n",
    "            if sigma1 <= 0 or sigma2 <= 0 or not (0 < pi < 1):\n",
    "                return -np.inf\n",
    "            \n",
    "            # Log-sum-exp trick for numerical stability\n",
    "            log_p1 = np.log(pi) + stats.norm.logpdf(data, mu1, sigma1)\n",
    "            log_p2 = np.log(1 - pi) + stats.norm.logpdf(data, mu2, sigma2)\n",
    "            \n",
    "            # log(exp(a) + exp(b)) = a + log(1 + exp(b-a))\n",
    "            max_log = np.maximum(log_p1, log_p2)\n",
    "            log_sum = max_log + np.log(np.exp(log_p1 - max_log) + np.exp(log_p2 - max_log))\n",
    "            return np.sum(log_sum)\n",
    "    \n",
    "    # Fit models using MLE\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    # Gaussian fit\n",
    "    def neg_gaussian_ll(params):\n",
    "        return -LikelihoodFunctions.gaussian_loglik(params, returns)\n",
    "    \n",
    "    gauss_result = minimize(neg_gaussian_ll, [0, 0.02], method='L-BFGS-B',\n",
    "                           bounds=[(-0.1, 0.1), (0.001, 0.1)])\n",
    "    \n",
    "    # Student-t fit\n",
    "    def neg_student_ll(params):\n",
    "        return -LikelihoodFunctions.student_t_loglik(params, returns)\n",
    "    \n",
    "    t_result = minimize(neg_student_ll, [0, 0.015, 5], method='L-BFGS-B',\n",
    "                       bounds=[(-0.1, 0.1), (0.001, 0.1), (2.1, 30)])\n",
    "    \n",
    "    # Mixture fit\n",
    "    def neg_mixture_ll(params):\n",
    "        return -LikelihoodFunctions.mixture_loglik(params, returns)\n",
    "    \n",
    "    mix_result = minimize(neg_mixture_ll, [0, 0.01, 0, 0.04, 0.9], method='L-BFGS-B',\n",
    "                         bounds=[(-0.1, 0.1), (0.001, 0.1), (-0.1, 0.1), (0.001, 0.1), (0.01, 0.99)])\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of returns\n",
    "    ax = axes[0]\n",
    "    x_range = np.linspace(returns.min() - 0.01, returns.max() + 0.01, 200)\n",
    "    \n",
    "    ax.hist(returns, bins=50, density=True, alpha=0.6, color='gray', label='Data')\n",
    "    \n",
    "    # Fitted distributions\n",
    "    mu_g, sig_g = gauss_result.x\n",
    "    ax.plot(x_range, stats.norm.pdf(x_range, mu_g, sig_g), \n",
    "            'b-', linewidth=2, label=f'Gaussian (Œº={mu_g:.4f}, œÉ={sig_g:.4f})')\n",
    "    \n",
    "    mu_t, sig_t, nu_t = t_result.x\n",
    "    ax.plot(x_range, stats.t.pdf(x_range, df=nu_t, loc=mu_t, scale=sig_t),\n",
    "            'r-', linewidth=2, label=f'Student-t (ŒΩ={nu_t:.1f})')\n",
    "    \n",
    "    mu1, sig1, mu2, sig2, pi = mix_result.x\n",
    "    mixture_pdf = pi * stats.norm.pdf(x_range, mu1, sig1) + (1-pi) * stats.norm.pdf(x_range, mu2, sig2)\n",
    "    ax.plot(x_range, mixture_pdf, 'g-', linewidth=2, label=f'Mixture (œÄ={pi:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Return', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Fitted Distributions', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # QQ plots\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # Gaussian QQ\n",
    "    sorted_returns = np.sort(returns)\n",
    "    theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(returns)))\n",
    "    \n",
    "    ax.scatter(theoretical_quantiles, sorted_returns, alpha=0.5, s=10, label='Data')\n",
    "    ax.plot([theoretical_quantiles.min(), theoretical_quantiles.max()],\n",
    "            [theoretical_quantiles.min() * sig_g + mu_g, theoretical_quantiles.max() * sig_g + mu_g],\n",
    "            'r--', linewidth=2, label='Gaussian fit')\n",
    "    ax.set_xlabel('Theoretical Quantiles (Normal)', fontsize=12)\n",
    "    ax.set_ylabel('Sample Quantiles', fontsize=12)\n",
    "    ax.set_title('Q-Q Plot: Gaussian', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Tail comparison\n",
    "    ax = axes[2]\n",
    "    \n",
    "    # Empirical tail probabilities\n",
    "    tail_thresholds = np.linspace(2, 4, 20) * returns.std()\n",
    "    empirical_left = [np.mean(returns < -t) for t in tail_thresholds]\n",
    "    empirical_right = [np.mean(returns > t) for t in tail_thresholds]\n",
    "    \n",
    "    gaussian_left = [stats.norm.cdf(-t, mu_g, sig_g) for t in tail_thresholds]\n",
    "    gaussian_right = [1 - stats.norm.cdf(t, mu_g, sig_g) for t in tail_thresholds]\n",
    "    \n",
    "    student_left = [stats.t.cdf(-t, df=nu_t, loc=mu_t, scale=sig_t) for t in tail_thresholds]\n",
    "    student_right = [1 - stats.t.cdf(t, df=nu_t, loc=mu_t, scale=sig_t) for t in tail_thresholds]\n",
    "    \n",
    "    ax.semilogy(tail_thresholds / returns.std(), empirical_left, 'ko', markersize=8, label='Empirical left tail')\n",
    "    ax.semilogy(tail_thresholds / returns.std(), gaussian_left, 'b-', linewidth=2, label='Gaussian')\n",
    "    ax.semilogy(tail_thresholds / returns.std(), student_left, 'r-', linewidth=2, label='Student-t')\n",
    "    ax.set_xlabel('Standard Deviations from Mean', fontsize=12)\n",
    "    ax.set_ylabel('Tail Probability (log scale)', fontsize=12)\n",
    "    ax.set_title('Tail Behavior Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Model comparison\n",
    "    print(\"\\nüìä MODEL COMPARISON (Log-Likelihood)\")\n",
    "    print(\"-\" * 50)\n",
    "    ll_gauss = LikelihoodFunctions.gaussian_loglik(gauss_result.x, returns)\n",
    "    ll_t = LikelihoodFunctions.student_t_loglik(t_result.x, returns)\n",
    "    ll_mix = LikelihoodFunctions.mixture_loglik(mix_result.x, returns)\n",
    "    \n",
    "    n = len(returns)\n",
    "    aic_gauss = 2 * 2 - 2 * ll_gauss\n",
    "    aic_t = 2 * 3 - 2 * ll_t\n",
    "    aic_mix = 2 * 5 - 2 * ll_mix\n",
    "    \n",
    "    bic_gauss = np.log(n) * 2 - 2 * ll_gauss\n",
    "    bic_t = np.log(n) * 3 - 2 * ll_t\n",
    "    bic_mix = np.log(n) * 5 - 2 * ll_mix\n",
    "    \n",
    "    print(f\"{'Model':<20} {'Log-Lik':>12} {'AIC':>12} {'BIC':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Gaussian':<20} {ll_gauss:>12.2f} {aic_gauss:>12.2f} {bic_gauss:>12.2f}\")\n",
    "    print(f\"{'Student-t':<20} {ll_t:>12.2f} {aic_t:>12.2f} {bic_t:>12.2f}\")\n",
    "    print(f\"{'Mixture':<20} {ll_mix:>12.2f} {aic_mix:>12.2f} {bic_mix:>12.2f}\")\n",
    "    \n",
    "    print(\"\\nüéØ KEY INSIGHT:\")\n",
    "    print(\"Student-t and mixture models better capture fat tails in returns\")\n",
    "    print(\"This matters for risk management (VaR, CVaR) and option pricing\")\n",
    "\n",
    "likelihood_functions_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee17c29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Posterior Distribution Computation\n",
    "\n",
    "### üìê **Computing Posteriors**\n",
    "\n",
    "For simple models, we can compute posteriors:\n",
    "1. **Analytically** - Using conjugate priors (exact)\n",
    "2. **Grid Approximation** - Discretize parameter space\n",
    "3. **MCMC Sampling** - For complex models (Section 11)\n",
    "\n",
    "### **Grid Approximation Algorithm**\n",
    "\n",
    "```\n",
    "1. Define grid of parameter values: Œ∏‚ÇÅ, Œ∏‚ÇÇ, ..., Œ∏‚Çñ\n",
    "2. Compute prior P(Œ∏·µ¢) at each grid point\n",
    "3. Compute likelihood P(D|Œ∏·µ¢) at each grid point\n",
    "4. Multiply: P(Œ∏·µ¢|D) ‚àù P(D|Œ∏·µ¢) √ó P(Œ∏·µ¢)\n",
    "5. Normalize: P(Œ∏·µ¢|D) = P(Œ∏·µ¢|D) / Œ£‚±º P(Œ∏‚±º|D)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_grid_approximation():\n",
    "    \"\"\"\n",
    "    Demonstrate grid approximation for posterior computation.\n",
    "    Example: Estimating mean return with known variance.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GRID APPROXIMATION FOR POSTERIOR COMPUTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate some return data\n",
    "    np.random.seed(42)\n",
    "    true_mu = 0.0008  # True daily mean return (~20% annual)\n",
    "    true_sigma = 0.015  # Known daily volatility\n",
    "    n_obs = 60  # 60 days of data (limited sample)\n",
    "    \n",
    "    returns = np.random.normal(true_mu, true_sigma, n_obs)\n",
    "    sample_mean = returns.mean()\n",
    "    \n",
    "    print(f\"\\nüìà Data Generation:\")\n",
    "    print(f\"   True mean: {true_mu:.5f} ({true_mu*252*100:.1f}% annualized)\")\n",
    "    print(f\"   Known œÉ: {true_sigma:.5f}\")\n",
    "    print(f\"   Sample size: {n_obs}\")\n",
    "    print(f\"   Sample mean: {sample_mean:.5f}\")\n",
    "    \n",
    "    # Grid approximation\n",
    "    mu_grid = np.linspace(-0.002, 0.003, 1000)\n",
    "    \n",
    "    # Prior: Normal with mean 0 (skeptical) and std 0.001\n",
    "    prior_mu = 0\n",
    "    prior_sigma = 0.001\n",
    "    prior = stats.norm.pdf(mu_grid, prior_mu, prior_sigma)\n",
    "    \n",
    "    # Likelihood: Product of normals (or equivalently, normal for sample mean)\n",
    "    # The likelihood for sample mean given true mean Œº:\n",
    "    # L(Œº) ‚àù N(sample_mean | Œº, œÉ/‚àön)\n",
    "    likelihood_sigma = true_sigma / np.sqrt(n_obs)\n",
    "    likelihood = stats.norm.pdf(sample_mean, mu_grid, likelihood_sigma)\n",
    "    \n",
    "    # Posterior (unnormalized)\n",
    "    posterior_unnorm = likelihood * prior\n",
    "    \n",
    "    # Normalize\n",
    "    posterior = posterior_unnorm / np.trapz(posterior_unnorm, mu_grid)\n",
    "    \n",
    "    # Analytical posterior (Normal-Normal conjugate)\n",
    "    # Posterior precision = prior precision + data precision\n",
    "    prior_precision = 1 / prior_sigma**2\n",
    "    data_precision = n_obs / true_sigma**2\n",
    "    post_precision = prior_precision + data_precision\n",
    "    post_sigma = np.sqrt(1 / post_precision)\n",
    "    \n",
    "    # Posterior mean = weighted average\n",
    "    post_mu = (prior_precision * prior_mu + data_precision * sample_mean) / post_precision\n",
    "    \n",
    "    posterior_analytical = stats.norm.pdf(mu_grid, post_mu, post_sigma)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Panel 1: Prior, Likelihood, Posterior\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(mu_grid * 252 * 100, prior / np.max(prior), 'b-', linewidth=2, \n",
    "            label=f'Prior: N({prior_mu}, {prior_sigma})')\n",
    "    ax.plot(mu_grid * 252 * 100, likelihood / np.max(likelihood), 'g-', linewidth=2,\n",
    "            label=f'Likelihood (from {n_obs} obs)')\n",
    "    ax.fill_between(mu_grid * 252 * 100, posterior / np.max(posterior), alpha=0.3, color='purple')\n",
    "    ax.plot(mu_grid * 252 * 100, posterior / np.max(posterior), 'purple', linewidth=2,\n",
    "            label='Posterior (Grid)')\n",
    "    ax.axvline(true_mu * 252 * 100, color='red', linestyle='--', alpha=0.7, label='True Œº')\n",
    "    ax.axvline(sample_mean * 252 * 100, color='orange', linestyle=':', alpha=0.7, label='Sample mean')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Density (scaled)', fontsize=12)\n",
    "    ax.set_title('Bayesian Inference via Grid Approximation', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(-50, 80)\n",
    "    \n",
    "    # Panel 2: Grid vs Analytical\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(mu_grid * 252 * 100, posterior, 'purple', linewidth=2, label='Grid approximation')\n",
    "    ax.plot(mu_grid * 252 * 100, posterior_analytical, 'k--', linewidth=2, label='Analytical')\n",
    "    ax.fill_between(mu_grid * 252 * 100, posterior, alpha=0.3, color='purple')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "    ax.set_title('Grid vs Analytical Posterior', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Panel 3: Effect of sample size\n",
    "    ax = axes[1, 0]\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, 5))\n",
    "    \n",
    "    for i, n in enumerate([10, 30, 60, 120, 252]):\n",
    "        # Simulate data\n",
    "        returns_n = np.random.normal(true_mu, true_sigma, n)\n",
    "        sample_mean_n = returns_n.mean()\n",
    "        \n",
    "        # Posterior\n",
    "        data_prec_n = n / true_sigma**2\n",
    "        post_prec_n = prior_precision + data_prec_n\n",
    "        post_sig_n = np.sqrt(1 / post_prec_n)\n",
    "        post_mu_n = (prior_precision * prior_mu + data_prec_n * sample_mean_n) / post_prec_n\n",
    "        \n",
    "        post_n = stats.norm.pdf(mu_grid, post_mu_n, post_sig_n)\n",
    "        ax.plot(mu_grid * 252 * 100, post_n, color=colors[i], linewidth=2, label=f'n={n}')\n",
    "    \n",
    "    ax.axvline(true_mu * 252 * 100, color='red', linestyle='--', alpha=0.7, label='True Œº')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "    ax.set_title('Posterior Sharpens with More Data', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Panel 4: Effect of prior strength\n",
    "    ax = axes[1, 1]\n",
    "    prior_sigmas = [0.0003, 0.0007, 0.001, 0.002, 0.01]\n",
    "    labels = ['Very strong', 'Strong', 'Moderate', 'Weak', 'Very weak']\n",
    "    \n",
    "    for i, (ps, label) in enumerate(zip(prior_sigmas, labels)):\n",
    "        prior_prec_i = 1 / ps**2\n",
    "        post_prec_i = prior_prec_i + data_precision\n",
    "        post_sig_i = np.sqrt(1 / post_prec_i)\n",
    "        post_mu_i = (prior_prec_i * prior_mu + data_precision * sample_mean) / post_prec_i\n",
    "        \n",
    "        post_i = stats.norm.pdf(mu_grid, post_mu_i, post_sig_i)\n",
    "        ax.plot(mu_grid * 252 * 100, post_i, color=colors[i], linewidth=2, label=f'{label} (œÉ={ps})')\n",
    "    \n",
    "    ax.axvline(prior_mu * 252 * 100, color='blue', linestyle=':', alpha=0.7, label='Prior mean')\n",
    "    ax.axvline(sample_mean * 252 * 100, color='orange', linestyle=':', alpha=0.7, label='Sample mean')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "    ax.set_title('Effect of Prior Strength', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä POSTERIOR SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Prior: N(Œº‚ÇÄ={prior_mu:.5f}, œÉ‚ÇÄ={prior_sigma:.5f})\")\n",
    "    print(f\"Data: n={n_obs}, xÃÑ={sample_mean:.5f}\")\n",
    "    print(f\"\\nPosterior: N(Œº={post_mu:.5f}, œÉ={post_sigma:.5f})\")\n",
    "    print(f\"Annualized: {post_mu*252*100:.2f}% ¬± {1.96*post_sigma*252*100:.2f}%\")\n",
    "    \n",
    "    # Shrinkage factor\n",
    "    shrinkage = prior_precision / post_precision\n",
    "    print(f\"\\nShrinkage toward prior: {shrinkage*100:.1f}%\")\n",
    "    print(f\"Weight on data: {(1-shrinkage)*100:.1f}%\")\n",
    "\n",
    "posterior_grid_approximation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd20ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Conjugate Priors in Finance\n",
    "\n",
    "### üîó **What are Conjugate Priors?**\n",
    "\n",
    "A prior distribution is **conjugate** to a likelihood if the posterior belongs to the same family as the prior. This allows:\n",
    "- **Closed-form posteriors** (no numerical integration)\n",
    "- **Efficient sequential updating**\n",
    "- **Interpretable parameters**\n",
    "\n",
    "### **Key Conjugate Pairs for Finance**\n",
    "\n",
    "| Likelihood | Prior | Posterior | Application |\n",
    "|------------|-------|-----------|-------------|\n",
    "| Normal (known œÉ¬≤) | Normal | Normal | Return estimation |\n",
    "| Normal (unknown œÉ¬≤) | Normal-Inverse-Gamma | Normal-Inverse-Gamma | Mean + volatility |\n",
    "| Binomial | Beta | Beta | Win rates, hit ratios |\n",
    "| Poisson | Gamma | Gamma | Trade counts |\n",
    "| Multinomial | Dirichlet | Dirichlet | Portfolio weights |\n",
    "| Exponential | Gamma | Gamma | Time between trades |\n",
    "\n",
    "### **Mathematical Details**\n",
    "\n",
    "**Beta-Binomial (Win Rate Estimation):**\n",
    "- Prior: $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$\n",
    "- Data: $k$ successes in $n$ trials\n",
    "- Posterior: $\\theta | k, n \\sim \\text{Beta}(\\alpha + k, \\beta + n - k)$\n",
    "\n",
    "**Normal-Normal (Return Mean Estimation):**\n",
    "- Prior: $\\mu \\sim N(\\mu_0, \\sigma_0^2)$\n",
    "- Data: $\\bar{x}$ from $n$ observations with known $\\sigma^2$\n",
    "- Posterior: $\\mu | \\bar{x} \\sim N(\\mu_n, \\sigma_n^2)$ where:\n",
    "  - $\\sigma_n^2 = (1/\\sigma_0^2 + n/\\sigma^2)^{-1}$\n",
    "  - $\\mu_n = \\sigma_n^2 (\\mu_0/\\sigma_0^2 + n\\bar{x}/\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConjugatePriors:\n",
    "    \"\"\"\n",
    "    Implementation of conjugate prior-posterior pairs for finance applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def beta_binomial_update(prior_alpha, prior_beta, successes, trials):\n",
    "        \"\"\"\n",
    "        Beta-Binomial conjugate update for win rate estimation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_alpha, prior_beta : float\n",
    "            Parameters of Beta prior\n",
    "        successes : int\n",
    "            Number of successes (winning trades)\n",
    "        trials : int\n",
    "            Total number of trials (trades)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        post_alpha, post_beta : float\n",
    "            Parameters of Beta posterior\n",
    "        \"\"\"\n",
    "        post_alpha = prior_alpha + successes\n",
    "        post_beta = prior_beta + (trials - successes)\n",
    "        return post_alpha, post_beta\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_normal_update(prior_mu, prior_sigma, data_mean, data_sigma, n):\n",
    "        \"\"\"\n",
    "        Normal-Normal conjugate update for mean estimation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_mu, prior_sigma : float\n",
    "            Prior mean and std\n",
    "        data_mean : float\n",
    "            Sample mean\n",
    "        data_sigma : float\n",
    "            Known data std\n",
    "        n : int\n",
    "            Sample size\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        post_mu, post_sigma : float\n",
    "            Posterior mean and std\n",
    "        \"\"\"\n",
    "        prior_precision = 1 / prior_sigma**2\n",
    "        data_precision = n / data_sigma**2\n",
    "        \n",
    "        post_precision = prior_precision + data_precision\n",
    "        post_sigma = np.sqrt(1 / post_precision)\n",
    "        post_mu = (prior_mu * prior_precision + data_mean * data_precision) / post_precision\n",
    "        \n",
    "        return post_mu, post_sigma\n",
    "    \n",
    "    @staticmethod\n",
    "    def gamma_poisson_update(prior_alpha, prior_beta, total_counts, n_periods):\n",
    "        \"\"\"\n",
    "        Gamma-Poisson conjugate update for rate estimation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_alpha, prior_beta : float\n",
    "            Parameters of Gamma prior (shape, rate)\n",
    "        total_counts : int\n",
    "            Total observed counts\n",
    "        n_periods : int\n",
    "            Number of observation periods\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        post_alpha, post_beta : float\n",
    "            Parameters of Gamma posterior\n",
    "        \"\"\"\n",
    "        post_alpha = prior_alpha + total_counts\n",
    "        post_beta = prior_beta + n_periods\n",
    "        return post_alpha, post_beta\n",
    "    \n",
    "    @staticmethod\n",
    "    def normal_inverse_gamma_update(prior_mu0, prior_n0, prior_alpha, prior_beta, data, sample_mean, sample_var):\n",
    "        \"\"\"\n",
    "        Normal-Inverse-Gamma conjugate update for unknown mean and variance.\n",
    "        \n",
    "        This is for the case where both Œº and œÉ¬≤ are unknown.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_mu0 : float\n",
    "            Prior mean for Œº\n",
    "        prior_n0 : float\n",
    "            Prior \"sample size\" (strength of belief in mean)\n",
    "        prior_alpha, prior_beta : float\n",
    "            Prior parameters for œÉ¬≤ (Inverse-Gamma)\n",
    "        data : array\n",
    "            Observed data\n",
    "        sample_mean : float\n",
    "            Sample mean of data\n",
    "        sample_var : float\n",
    "            Sample variance of data (n-1 denominator)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        post_mu0, post_n0, post_alpha, post_beta : float\n",
    "            Posterior parameters\n",
    "        \"\"\"\n",
    "        n = len(data)\n",
    "        \n",
    "        # Update parameters\n",
    "        post_n0 = prior_n0 + n\n",
    "        post_mu0 = (prior_n0 * prior_mu0 + n * sample_mean) / post_n0\n",
    "        post_alpha = prior_alpha + n / 2\n",
    "        \n",
    "        # Sum of squares term\n",
    "        ss = np.sum((data - sample_mean)**2)\n",
    "        post_beta = prior_beta + 0.5 * ss + (prior_n0 * n * (sample_mean - prior_mu0)**2) / (2 * post_n0)\n",
    "        \n",
    "        return post_mu0, post_n0, post_alpha, post_beta\n",
    "\n",
    "\n",
    "def conjugate_priors_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate conjugate priors for trading applications.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CONJUGATE PRIORS FOR TRADING APPLICATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # =========================================\n",
    "    # 1. Beta-Binomial: Win Rate Estimation\n",
    "    # =========================================\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # Prior: Skeptical about edge, centered at 50%\n",
    "    prior_alpha, prior_beta = 5, 5\n",
    "    \n",
    "    # Simulate trading results\n",
    "    true_win_rate = 0.55\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    p_grid = np.linspace(0.01, 0.99, 200)\n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 1, 5))\n",
    "    \n",
    "    # Show prior\n",
    "    prior_dist = stats.beta.pdf(p_grid, prior_alpha, prior_beta)\n",
    "    ax.plot(p_grid, prior_dist, 'k--', linewidth=2, label='Prior')\n",
    "    \n",
    "    # Sequential updating\n",
    "    cumulative_wins = 0\n",
    "    cumulative_trades = 0\n",
    "    trade_batches = [20, 40, 60, 80, 100]\n",
    "    \n",
    "    for i, n_trades in enumerate(trade_batches):\n",
    "        new_trades = n_trades - cumulative_trades\n",
    "        wins = np.sum(np.random.random(new_trades) < true_win_rate)\n",
    "        cumulative_wins += wins\n",
    "        cumulative_trades = n_trades\n",
    "        \n",
    "        post_alpha, post_beta = ConjugatePriors.beta_binomial_update(\n",
    "            prior_alpha, prior_beta, cumulative_wins, cumulative_trades\n",
    "        )\n",
    "        \n",
    "        post_dist = stats.beta.pdf(p_grid, post_alpha, post_beta)\n",
    "        ax.plot(p_grid, post_dist, color=colors[i], linewidth=2,\n",
    "                label=f'n={n_trades} (wins={cumulative_wins})')\n",
    "    \n",
    "    ax.axvline(true_win_rate, color='red', linestyle='--', alpha=0.7, label=f'True rate = {true_win_rate}')\n",
    "    ax.set_xlabel('Win Rate', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Beta-Binomial: Win Rate Estimation', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # =========================================\n",
    "    # 2. Normal-Normal: Return Estimation\n",
    "    # =========================================\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Prior: Skeptical, centered at 0\n",
    "    prior_mu, prior_sigma = 0, 0.001  # ~25% annualized std on prior\n",
    "    \n",
    "    # True parameters\n",
    "    true_mu = 0.0006  # ~15% annual\n",
    "    data_sigma = 0.015  # Known daily volatility\n",
    "    \n",
    "    mu_grid = np.linspace(-0.002, 0.002, 200)\n",
    "    \n",
    "    # Prior distribution\n",
    "    ax.plot(mu_grid * 252 * 100, stats.norm.pdf(mu_grid, prior_mu, prior_sigma), \n",
    "            'k--', linewidth=2, label='Prior')\n",
    "    \n",
    "    # Sequential updating\n",
    "    np.random.seed(123)\n",
    "    sample_sizes = [20, 50, 100, 200, 500]\n",
    "    colors = plt.cm.Greens(np.linspace(0.3, 1, len(sample_sizes)))\n",
    "    \n",
    "    for i, n in enumerate(sample_sizes):\n",
    "        data = np.random.normal(true_mu, data_sigma, n)\n",
    "        sample_mean = data.mean()\n",
    "        \n",
    "        post_mu, post_sigma = ConjugatePriors.normal_normal_update(\n",
    "            prior_mu, prior_sigma, sample_mean, data_sigma, n\n",
    "        )\n",
    "        \n",
    "        post_dist = stats.norm.pdf(mu_grid, post_mu, post_sigma)\n",
    "        ax.plot(mu_grid * 252 * 100, post_dist, color=colors[i], linewidth=2,\n",
    "                label=f'n={n}')\n",
    "    \n",
    "    ax.axvline(true_mu * 252 * 100, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'True Œº = {true_mu*252*100:.1f}%')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Normal-Normal: Mean Return Estimation', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # =========================================\n",
    "    # 3. Gamma-Poisson: Trade Count Modeling\n",
    "    # =========================================\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Prior: Expect about 5 trades per day\n",
    "    prior_alpha, prior_beta = 5, 1  # Mean = 5, variance = 5\n",
    "    \n",
    "    # True rate\n",
    "    true_rate = 7\n",
    "    \n",
    "    lambda_grid = np.linspace(0.1, 15, 200)\n",
    "    \n",
    "    # Prior\n",
    "    ax.plot(lambda_grid, stats.gamma.pdf(lambda_grid, prior_alpha, scale=1/prior_beta),\n",
    "            'k--', linewidth=2, label='Prior')\n",
    "    \n",
    "    # Sequential updating\n",
    "    np.random.seed(456)\n",
    "    n_days_list = [5, 10, 20, 50, 100]\n",
    "    colors = plt.cm.Oranges(np.linspace(0.3, 1, len(n_days_list)))\n",
    "    \n",
    "    for i, n_days in enumerate(n_days_list):\n",
    "        daily_trades = np.random.poisson(true_rate, n_days)\n",
    "        total_trades = daily_trades.sum()\n",
    "        \n",
    "        post_alpha, post_beta = ConjugatePriors.gamma_poisson_update(\n",
    "            prior_alpha, prior_beta, total_trades, n_days\n",
    "        )\n",
    "        \n",
    "        post_dist = stats.gamma.pdf(lambda_grid, post_alpha, scale=1/post_beta)\n",
    "        ax.plot(lambda_grid, post_dist, color=colors[i], linewidth=2,\n",
    "                label=f'{n_days} days (total={total_trades})')\n",
    "    \n",
    "    ax.axvline(true_rate, color='red', linestyle='--', alpha=0.7, label=f'True Œª = {true_rate}')\n",
    "    ax.set_xlabel('Trades per Day (Œª)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Gamma-Poisson: Trade Count Rate', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # =========================================\n",
    "    # 4. Summary Statistics Comparison\n",
    "    # =========================================\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = \"\"\"\n",
    "    CONJUGATE PRIOR SUMMARY FOR FINANCE\n",
    "    ====================================\n",
    "    \n",
    "    1. BETA-BINOMIAL (Win Rate)\n",
    "       ‚Ä¢ Prior: Beta(Œ±, Œ≤) with mean = Œ±/(Œ±+Œ≤)\n",
    "       ‚Ä¢ After k wins in n trades:\n",
    "         Posterior: Beta(Œ±+k, Œ≤+n-k)\n",
    "       ‚Ä¢ Equivalent to starting with Œ±-1 \"pseudo-wins\"\n",
    "         and Œ≤-1 \"pseudo-losses\"\n",
    "    \n",
    "    2. NORMAL-NORMAL (Mean Return)\n",
    "       ‚Ä¢ Prior: N(Œº‚ÇÄ, œÉ‚ÇÄ¬≤) on mean return\n",
    "       ‚Ä¢ Data: n observations with known œÉ¬≤\n",
    "       ‚Ä¢ Posterior mean is precision-weighted average:\n",
    "         Œº‚Çô = (Œº‚ÇÄ/œÉ‚ÇÄ¬≤ + nxÃÑ/œÉ¬≤) / (1/œÉ‚ÇÄ¬≤ + n/œÉ¬≤)\n",
    "       ‚Ä¢ Posterior variance: œÉ‚Çô¬≤ = 1/(1/œÉ‚ÇÄ¬≤ + n/œÉ¬≤)\n",
    "    \n",
    "    3. GAMMA-POISSON (Trade Counts)\n",
    "       ‚Ä¢ Prior: Gamma(Œ±, Œ≤) on rate Œª\n",
    "       ‚Ä¢ After observing total T events in n periods:\n",
    "         Posterior: Gamma(Œ±+T, Œ≤+n)\n",
    "       ‚Ä¢ Expected rate: (Œ±+T)/(Œ≤+n)\n",
    "    \n",
    "    KEY INSIGHT: Conjugate priors allow efficient\n",
    "    sequential updating as new data arrives!\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "            fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "conjugate_priors_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907879a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Bayesian Parameter Estimation for Returns\n",
    "\n",
    "### üìà **Estimating Return Parameters with Uncertainty**\n",
    "\n",
    "In frequentist analysis, we get point estimates (MLE). In Bayesian analysis, we get full posterior distributions capturing parameter uncertainty.\n",
    "\n",
    "### **Why This Matters for Trading**\n",
    "\n",
    "1. **Small samples**: Limited historical data makes point estimates unreliable\n",
    "2. **Risk management**: Need uncertainty in estimates for conservative risk measures\n",
    "3. **Portfolio optimization**: Incorporate estimation error in allocation\n",
    "4. **Strategy evaluation**: Distinguish skill from luck with proper uncertainty\n",
    "\n",
    "### **Joint Estimation of Mean and Variance**\n",
    "\n",
    "When both Œº and œÉ¬≤ are unknown, we use the Normal-Inverse-Gamma prior:\n",
    "\n",
    "$$p(\\mu, \\sigma^2) = p(\\mu | \\sigma^2) \\cdot p(\\sigma^2)$$\n",
    "\n",
    "where:\n",
    "- $\\mu | \\sigma^2 \\sim N(\\mu_0, \\sigma^2/\\kappa_0)$\n",
    "- $\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dafbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_return_estimation():\n",
    "    \"\"\"\n",
    "    Bayesian estimation of return parameters (mean and variance).\n",
    "    Compare with frequentist MLE estimates.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BAYESIAN RETURN PARAMETER ESTIMATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate asset returns\n",
    "    np.random.seed(42)\n",
    "    true_mu = 0.0006  # Daily mean (~15% annual)\n",
    "    true_sigma = 0.018  # Daily vol (~28% annual)\n",
    "    n_obs = 100\n",
    "    \n",
    "    returns = np.random.normal(true_mu, true_sigma, n_obs)\n",
    "    \n",
    "    # Frequentist (MLE) estimates\n",
    "    mle_mu = returns.mean()\n",
    "    mle_sigma = returns.std(ddof=0)  # MLE uses n, not n-1\n",
    "    \n",
    "    # Standard errors (frequentist)\n",
    "    se_mu = mle_sigma / np.sqrt(n_obs)\n",
    "    se_sigma = mle_sigma / np.sqrt(2 * n_obs)\n",
    "    \n",
    "    print(f\"\\nüìà TRUE PARAMETERS:\")\n",
    "    print(f\"   Œº = {true_mu:.6f} ({true_mu*252*100:.2f}% annualized)\")\n",
    "    print(f\"   œÉ = {true_sigma:.6f} ({true_sigma*np.sqrt(252)*100:.2f}% annualized)\")\n",
    "    \n",
    "    print(f\"\\nüìä FREQUENTIST (MLE) ESTIMATES:\")\n",
    "    print(f\"   ŒºÃÇ = {mle_mu:.6f} ¬± {1.96*se_mu:.6f}\")\n",
    "    print(f\"   œÉÃÇ = {mle_sigma:.6f} ¬± {1.96*se_sigma:.6f}\")\n",
    "    \n",
    "    # =========================================\n",
    "    # Bayesian Estimation with Normal-Inverse-Gamma Prior\n",
    "    # =========================================\n",
    "    \n",
    "    # Prior parameters (weakly informative)\n",
    "    # Prior on Œº|œÉ¬≤: N(Œº‚ÇÄ, œÉ¬≤/Œ∫‚ÇÄ)\n",
    "    mu0 = 0  # Prior mean for Œº (skeptical)\n",
    "    kappa0 = 1  # Prior \"sample size\" for mean\n",
    "    \n",
    "    # Prior on œÉ¬≤: Inverse-Gamma(Œ±‚ÇÄ, Œ≤‚ÇÄ)\n",
    "    alpha0 = 3  # Shape\n",
    "    beta0 = 0.0005  # Scale (implies prior mean œÉ¬≤ = Œ≤‚ÇÄ/(Œ±‚ÇÄ-1) ‚âà 0.00025)\n",
    "    \n",
    "    # Posterior parameters (Normal-Inverse-Gamma update)\n",
    "    n = len(returns)\n",
    "    sample_mean = returns.mean()\n",
    "    sample_var = returns.var(ddof=0)\n",
    "    \n",
    "    # Update\n",
    "    kappa_n = kappa0 + n\n",
    "    mu_n = (kappa0 * mu0 + n * sample_mean) / kappa_n\n",
    "    alpha_n = alpha0 + n / 2\n",
    "    \n",
    "    ss = np.sum((returns - sample_mean)**2)\n",
    "    beta_n = beta0 + 0.5 * ss + (kappa0 * n * (sample_mean - mu0)**2) / (2 * kappa_n)\n",
    "    \n",
    "    # Marginal posterior of œÉ¬≤ is Inverse-Gamma(Œ±_n, Œ≤_n)\n",
    "    # Marginal posterior of Œº is Student-t\n",
    "    \n",
    "    print(f\"\\nüéØ BAYESIAN ESTIMATES:\")\n",
    "    print(f\"   Prior: Œº‚ÇÄ={mu0}, Œ∫‚ÇÄ={kappa0}, Œ±‚ÇÄ={alpha0}, Œ≤‚ÇÄ={beta0}\")\n",
    "    print(f\"   Posterior: Œº‚Çô={mu_n:.6f}, Œ∫‚Çô={kappa_n}, Œ±‚Çô={alpha_n:.1f}, Œ≤‚Çô={beta_n:.6f}\")\n",
    "    \n",
    "    # Posterior mean and variance of œÉ¬≤\n",
    "    post_mean_sigma2 = beta_n / (alpha_n - 1)\n",
    "    post_sigma = np.sqrt(post_mean_sigma2)\n",
    "    \n",
    "    # Posterior mean of Œº (same as Œº_n for Normal-Inverse-Gamma)\n",
    "    post_mu = mu_n\n",
    "    \n",
    "    print(f\"\\n   Posterior mean Œº = {post_mu:.6f}\")\n",
    "    print(f\"   Posterior mean œÉ = {post_sigma:.6f}\")\n",
    "    \n",
    "    # =========================================\n",
    "    # Visualization\n",
    "    # =========================================\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Panel 1: Data histogram\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(returns, bins=30, density=True, alpha=0.6, color='steelblue', label='Data')\n",
    "    \n",
    "    x_range = np.linspace(returns.min() - 0.02, returns.max() + 0.02, 200)\n",
    "    ax.plot(x_range, stats.norm.pdf(x_range, true_mu, true_sigma), 'r-', \n",
    "            linewidth=2, label=f'True N({true_mu:.4f}, {true_sigma:.4f}¬≤)')\n",
    "    ax.plot(x_range, stats.norm.pdf(x_range, mle_mu, mle_sigma), 'g--',\n",
    "            linewidth=2, label=f'MLE N({mle_mu:.4f}, {mle_sigma:.4f}¬≤)')\n",
    "    ax.set_xlabel('Daily Return', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Return Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Panel 2: Posterior of Œº (marginal is Student-t)\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    mu_grid = np.linspace(-0.003, 0.003, 500)\n",
    "    \n",
    "    # Prior on Œº (integrated over œÉ¬≤): Student-t\n",
    "    prior_mu_df = 2 * alpha0\n",
    "    prior_mu_scale = np.sqrt(beta0 / (alpha0 * kappa0))\n",
    "    prior_mu_dist = stats.t.pdf(mu_grid, df=prior_mu_df, loc=mu0, scale=prior_mu_scale)\n",
    "    \n",
    "    # Posterior on Œº: Student-t\n",
    "    post_mu_df = 2 * alpha_n\n",
    "    post_mu_scale = np.sqrt(beta_n / (alpha_n * kappa_n))\n",
    "    post_mu_dist = stats.t.pdf(mu_grid, df=post_mu_df, loc=mu_n, scale=post_mu_scale)\n",
    "    \n",
    "    # MLE distribution (frequentist)\n",
    "    mle_dist = stats.norm.pdf(mu_grid, mle_mu, se_mu)\n",
    "    \n",
    "    ax.plot(mu_grid * 252 * 100, prior_mu_dist / np.max(prior_mu_dist), 'b--', \n",
    "            linewidth=2, alpha=0.7, label='Prior (scaled)')\n",
    "    ax.fill_between(mu_grid * 252 * 100, post_mu_dist, alpha=0.3, color='purple')\n",
    "    ax.plot(mu_grid * 252 * 100, post_mu_dist, 'purple', linewidth=2, \n",
    "            label='Posterior (Bayesian)')\n",
    "    ax.plot(mu_grid * 252 * 100, mle_dist, 'g--', linewidth=2, \n",
    "            label='MLE Sampling Dist')\n",
    "    ax.axvline(true_mu * 252 * 100, color='red', linestyle=':', \n",
    "               linewidth=2, label='True Œº')\n",
    "    ax.set_xlabel('Annualized Return (%)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Posterior Distribution of Mean Return', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Panel 3: Posterior of œÉ¬≤ (Inverse-Gamma)\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    sigma2_grid = np.linspace(0.0001, 0.0008, 500)\n",
    "    sigma_grid = np.sqrt(sigma2_grid)\n",
    "    \n",
    "    # Prior (Inverse-Gamma for œÉ¬≤)\n",
    "    prior_sigma2 = stats.invgamma.pdf(sigma2_grid, alpha0, scale=beta0)\n",
    "    \n",
    "    # Posterior\n",
    "    post_sigma2 = stats.invgamma.pdf(sigma2_grid, alpha_n, scale=beta_n)\n",
    "    \n",
    "    # Convert to œÉ scale via Jacobian\n",
    "    ax.plot(sigma_grid * np.sqrt(252) * 100, prior_sigma2 * 2 * sigma_grid / np.max(prior_sigma2 * 2 * sigma_grid), \n",
    "            'b--', linewidth=2, alpha=0.7, label='Prior (scaled)')\n",
    "    ax.fill_between(sigma_grid * np.sqrt(252) * 100, post_sigma2 * 2 * sigma_grid, alpha=0.3, color='orange')\n",
    "    ax.plot(sigma_grid * np.sqrt(252) * 100, post_sigma2 * 2 * sigma_grid, 'orange', \n",
    "            linewidth=2, label='Posterior')\n",
    "    ax.axvline(true_sigma * np.sqrt(252) * 100, color='red', linestyle=':', \n",
    "               linewidth=2, label='True œÉ')\n",
    "    ax.axvline(mle_sigma * np.sqrt(252) * 100, color='green', linestyle='--',\n",
    "               linewidth=2, label='MLE œÉ')\n",
    "    ax.set_xlabel('Annualized Volatility (%)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Posterior Distribution of Volatility', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Panel 4: Comparison summary\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Credible intervals\n",
    "    ci_mu_low = stats.t.ppf(0.025, df=post_mu_df, loc=mu_n, scale=post_mu_scale)\n",
    "    ci_mu_high = stats.t.ppf(0.975, df=post_mu_df, loc=mu_n, scale=post_mu_scale)\n",
    "    \n",
    "    ci_sigma2_low = stats.invgamma.ppf(0.025, alpha_n, scale=beta_n)\n",
    "    ci_sigma2_high = stats.invgamma.ppf(0.975, alpha_n, scale=beta_n)\n",
    "    \n",
    "    # Shrinkage analysis\n",
    "    shrinkage_mu = (mle_mu - post_mu) / (mle_mu - mu0) if mle_mu != mu0 else 0\n",
    "    \n",
    "    summary_data = {\n",
    "        'Parameter': ['Œº (daily)', 'Œº (annual %)', 'œÉ (daily)', 'œÉ (annual %)'],\n",
    "        'True': [f'{true_mu:.5f}', f'{true_mu*252*100:.2f}', \n",
    "                 f'{true_sigma:.5f}', f'{true_sigma*np.sqrt(252)*100:.2f}'],\n",
    "        'MLE': [f'{mle_mu:.5f}', f'{mle_mu*252*100:.2f}',\n",
    "                f'{mle_sigma:.5f}', f'{mle_sigma*np.sqrt(252)*100:.2f}'],\n",
    "        'Bayesian': [f'{post_mu:.5f}', f'{post_mu*252*100:.2f}',\n",
    "                    f'{post_sigma:.5f}', f'{post_sigma*np.sqrt(252)*100:.2f}'],\n",
    "        '95% CI': [f'[{ci_mu_low:.5f}, {ci_mu_high:.5f}]', \n",
    "                   f'[{ci_mu_low*252*100:.1f}%, {ci_mu_high*252*100:.1f}%]',\n",
    "                   f'[{np.sqrt(ci_sigma2_low):.5f}, {np.sqrt(ci_sigma2_high):.5f}]',\n",
    "                   f'[{np.sqrt(ci_sigma2_low)*np.sqrt(252)*100:.1f}%, {np.sqrt(ci_sigma2_high)*np.sqrt(252)*100:.1f}%]']\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    table = ax.table(cellText=df_summary.values,\n",
    "                     colLabels=df_summary.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colColours=['lightblue']*5)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.8)\n",
    "    ax.set_title('Estimation Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ KEY INSIGHTS:\")\n",
    "    print(f\"   1. Bayesian estimate of Œº is shrunk toward prior ({shrinkage_mu*100:.1f}% shrinkage)\")\n",
    "    print(f\"   2. 95% credible interval captures true Œº: {ci_mu_low*252*100:.1f}% to {ci_mu_high*252*100:.1f}%\")\n",
    "    print(f\"   3. Uncertainty in œÉ affects confidence in Sharpe ratio!\")\n",
    "\n",
    "bayesian_return_estimation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941e14d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Bayesian Updating with New Market Data\n",
    "\n",
    "### üîÑ **Sequential Bayesian Learning**\n",
    "\n",
    "One of the most powerful features of Bayesian inference is **sequential updating**:\n",
    "- Today's posterior becomes tomorrow's prior\n",
    "- No need to re-analyze all historical data\n",
    "- Natural framework for online learning\n",
    "\n",
    "### **The Update Equation**\n",
    "\n",
    "$$P(\\theta | D_1, D_2) = \\frac{P(D_2 | \\theta) \\cdot P(\\theta | D_1)}{P(D_2 | D_1)}$$\n",
    "\n",
    "This is equivalent to analyzing all data at once:\n",
    "$$P(\\theta | D_1, D_2) \\propto P(D_2 | \\theta) \\cdot P(D_1 | \\theta) \\cdot P(\\theta)$$\n",
    "\n",
    "### **Applications in Trading**\n",
    "\n",
    "1. **Adaptive parameter estimation**: Update strategy parameters as market evolves\n",
    "2. **Regime detection**: Track changing market conditions\n",
    "3. **Strategy selection**: Learn which strategy performs best (Thompson Sampling)\n",
    "4. **Risk monitoring**: Update risk estimates in real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308b3be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_online_learning():\n",
    "    \"\"\"\n",
    "    Demonstrate sequential Bayesian updating for trading.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEQUENTIAL BAYESIAN UPDATING FOR TRADING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Simulate a trading strategy with regime change\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Regime 1: Lower win rate (first 100 trades)\n",
    "    # Regime 2: Higher win rate (next 100 trades)\n",
    "    true_win_rate_1 = 0.48\n",
    "    true_win_rate_2 = 0.58\n",
    "    \n",
    "    trades_regime_1 = np.random.random(100) < true_win_rate_1\n",
    "    trades_regime_2 = np.random.random(100) < true_win_rate_2\n",
    "    all_trades = np.concatenate([trades_regime_1, trades_regime_2])\n",
    "    \n",
    "    # Initialize prior: Beta(2, 2) - mild skepticism\n",
    "    alpha, beta_param = 2, 2\n",
    "    \n",
    "    # Store history for visualization\n",
    "    history = {\n",
    "        'trade_num': [0],\n",
    "        'alpha': [alpha],\n",
    "        'beta': [beta_param],\n",
    "        'posterior_mean': [alpha / (alpha + beta_param)],\n",
    "        'ci_low': [stats.beta.ppf(0.025, alpha, beta_param)],\n",
    "        'ci_high': [stats.beta.ppf(0.975, alpha, beta_param)],\n",
    "        'cumulative_win_rate': [0.5]\n",
    "    }\n",
    "    \n",
    "    cumulative_wins = 0\n",
    "    \n",
    "    # Sequential update\n",
    "    for i, win in enumerate(all_trades):\n",
    "        # Update posterior\n",
    "        if win:\n",
    "            alpha += 1\n",
    "            cumulative_wins += 1\n",
    "        else:\n",
    "            beta_param += 1\n",
    "        \n",
    "        # Store\n",
    "        history['trade_num'].append(i + 1)\n",
    "        history['alpha'].append(alpha)\n",
    "        history['beta'].append(beta_param)\n",
    "        history['posterior_mean'].append(alpha / (alpha + beta_param))\n",
    "        history['ci_low'].append(stats.beta.ppf(0.025, alpha, beta_param))\n",
    "        history['ci_high'].append(stats.beta.ppf(0.975, alpha, beta_param))\n",
    "        history['cumulative_win_rate'].append(cumulative_wins / (i + 1))\n",
    "    \n",
    "    df_history = pd.DataFrame(history)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Panel 1: Posterior mean with credible interval\n",
    "    ax = axes[0, 0]\n",
    "    ax.fill_between(df_history['trade_num'], df_history['ci_low'], df_history['ci_high'],\n",
    "                    alpha=0.3, color='blue', label='95% Credible Interval')\n",
    "    ax.plot(df_history['trade_num'], df_history['posterior_mean'], 'b-', \n",
    "            linewidth=2, label='Posterior Mean')\n",
    "    ax.plot(df_history['trade_num'], df_history['cumulative_win_rate'], 'g--',\n",
    "            linewidth=1.5, alpha=0.7, label='Cumulative Win Rate (MLE)')\n",
    "    ax.axhline(true_win_rate_1, color='red', linestyle=':', alpha=0.7, \n",
    "               label=f'True Rate Regime 1 = {true_win_rate_1}')\n",
    "    ax.axhline(true_win_rate_2, color='orange', linestyle=':', alpha=0.7,\n",
    "               label=f'True Rate Regime 2 = {true_win_rate_2}')\n",
    "    ax.axvline(100, color='black', linestyle='--', alpha=0.5, label='Regime Change')\n",
    "    ax.set_xlabel('Number of Trades', fontsize=12)\n",
    "    ax.set_ylabel('Win Rate', fontsize=12)\n",
    "    ax.set_title('Sequential Bayesian Updating of Win Rate', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='lower right')\n",
    "    ax.set_xlim(0, 200)\n",
    "    ax.set_ylim(0.3, 0.75)\n",
    "    \n",
    "    # Panel 2: Evolution of posterior distribution\n",
    "    ax = axes[0, 1]\n",
    "    p_grid = np.linspace(0.01, 0.99, 200)\n",
    "    \n",
    "    snapshots = [0, 10, 50, 100, 150, 200]\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(snapshots)))\n",
    "    \n",
    "    for i, n in enumerate(snapshots):\n",
    "        a = df_history.loc[n, 'alpha']\n",
    "        b = df_history.loc[n, 'beta']\n",
    "        posterior = stats.beta.pdf(p_grid, a, b)\n",
    "        ax.plot(p_grid, posterior, color=colors[i], linewidth=2, label=f'n={n}')\n",
    "    \n",
    "    ax.axvline(true_win_rate_1, color='red', linestyle=':', alpha=0.7)\n",
    "    ax.axvline(true_win_rate_2, color='orange', linestyle=':', alpha=0.7)\n",
    "    ax.set_xlabel('Win Rate', fontsize=12)\n",
    "    ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "    ax.set_title('Posterior Evolution Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    \n",
    "    # Panel 3: Credible interval width (uncertainty reduction)\n",
    "    ax = axes[1, 0]\n",
    "    ci_width = np.array(df_history['ci_high']) - np.array(df_history['ci_low'])\n",
    "    ax.plot(df_history['trade_num'], ci_width, 'purple', linewidth=2)\n",
    "    ax.set_xlabel('Number of Trades', fontsize=12)\n",
    "    ax.set_ylabel('95% CI Width', fontsize=12)\n",
    "    ax.set_title('Uncertainty Reduction with More Data', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(100, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlim(0, 200)\n",
    "    \n",
    "    # Panel 4: Bayesian Forgetting (Exponential weighting)\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Implement Bayesian updating with exponential forgetting\n",
    "    # This gives more weight to recent observations\n",
    "    decay_factor = 0.99  # \"Forget\" 1% per observation\n",
    "    \n",
    "    alpha_forget, beta_forget = 2, 2\n",
    "    forget_history = {'trade_num': [0], 'posterior_mean': [0.5]}\n",
    "    \n",
    "    for i, win in enumerate(all_trades):\n",
    "        # Decay toward prior before update\n",
    "        alpha_forget = 1 + decay_factor * (alpha_forget - 1)\n",
    "        beta_forget = 1 + decay_factor * (beta_forget - 1)\n",
    "        \n",
    "        # Update\n",
    "        if win:\n",
    "            alpha_forget += 1\n",
    "        else:\n",
    "            beta_forget += 1\n",
    "        \n",
    "        forget_history['trade_num'].append(i + 1)\n",
    "        forget_history['posterior_mean'].append(alpha_forget / (alpha_forget + beta_forget))\n",
    "    \n",
    "    df_forget = pd.DataFrame(forget_history)\n",
    "    \n",
    "    ax.plot(df_history['trade_num'], df_history['posterior_mean'], 'b-',\n",
    "            linewidth=2, label='Standard Bayesian', alpha=0.7)\n",
    "    ax.plot(df_forget['trade_num'], df_forget['posterior_mean'], 'r-',\n",
    "            linewidth=2, label=f'With Forgetting (Œª={decay_factor})')\n",
    "    ax.axhline(true_win_rate_1, color='blue', linestyle=':', alpha=0.5)\n",
    "    ax.axhline(true_win_rate_2, color='orange', linestyle=':', alpha=0.5)\n",
    "    ax.axvline(100, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Number of Trades', fontsize=12)\n",
    "    ax.set_ylabel('Posterior Mean', fontsize=12)\n",
    "    ax.set_title('Bayesian Forgetting for Non-Stationary Data', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_xlim(0, 200)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä KEY OBSERVATIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Standard Bayesian: Averages all data, slow to detect regime change\")\n",
    "    print(\"2. Bayesian with forgetting: Adapts faster to new regime\")\n",
    "    print(\"3. Credible intervals shrink as we observe more data\")\n",
    "    print(\"4. Posterior mean is more stable than cumulative win rate (shrinkage)\")\n",
    "\n",
    "bayesian_online_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efe00b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Credible Intervals vs Confidence Intervals\n",
    "\n",
    "### üìè **Understanding the Difference**\n",
    "\n",
    "| Aspect | Frequentist CI | Bayesian Credible Interval |\n",
    "|--------|----------------|---------------------------|\n",
    "| **Definition** | Interval procedure | Interval of posterior |\n",
    "| **Interpretation** | 95% of such intervals contain Œ∏ | 95% probability Œ∏ is in interval |\n",
    "| **Requires** | Repeated sampling concept | Prior distribution |\n",
    "| **Computation** | Sampling distribution | Posterior distribution |\n",
    "| **Fixed** | True parameter Œ∏ | Observed data D |\n",
    "| **Random** | Interval endpoints | Parameter Œ∏ |\n",
    "\n",
    "### **Types of Bayesian Intervals**\n",
    "\n",
    "1. **Equal-tailed Interval (ETI)**: 2.5% in each tail\n",
    "2. **Highest Density Interval (HDI)**: Narrowest interval containing 95%\n",
    "\n",
    "For symmetric posteriors, ETI = HDI. For skewed posteriors, HDI is often preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credible_intervals_demo():\n",
    "    \"\"\"\n",
    "    Compare credible intervals with confidence intervals.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREDIBLE INTERVALS vs CONFIDENCE INTERVALS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def compute_hdi(samples, prob=0.95):\n",
    "        \"\"\"\n",
    "        Compute Highest Density Interval from samples.\n",
    "        \"\"\"\n",
    "        samples = np.sort(samples)\n",
    "        n = len(samples)\n",
    "        interval_idx = int(np.floor(prob * n))\n",
    "        n_intervals = n - interval_idx\n",
    "        \n",
    "        interval_widths = samples[interval_idx:] - samples[:n_intervals]\n",
    "        min_idx = np.argmin(interval_widths)\n",
    "        \n",
    "        return samples[min_idx], samples[min_idx + interval_idx]\n",
    "    \n",
    "    # Scenario: Estimate Sharpe ratio from limited data\n",
    "    np.random.seed(42)\n",
    "    true_sharpe = 0.8\n",
    "    n_obs = 50\n",
    "    \n",
    "    # Generate returns consistent with Sharpe = 0.8 (annual)\n",
    "    daily_sharpe = true_sharpe / np.sqrt(252)\n",
    "    returns = np.random.normal(daily_sharpe * 0.02, 0.02, n_obs)  # 2% daily vol\n",
    "    \n",
    "    sample_mean = returns.mean()\n",
    "    sample_std = returns.std(ddof=1)\n",
    "    sample_sharpe = (sample_mean / sample_std) * np.sqrt(252)\n",
    "    \n",
    "    # =========================================\n",
    "    # Frequentist Confidence Interval\n",
    "    # =========================================\n",
    "    # Standard error of Sharpe ratio (Lo, 2002)\n",
    "    se_sharpe = np.sqrt((1 + 0.5 * sample_sharpe**2) / n_obs) * np.sqrt(252)\n",
    "    freq_ci_low = sample_sharpe - 1.96 * se_sharpe\n",
    "    freq_ci_high = sample_sharpe + 1.96 * se_sharpe\n",
    "    \n",
    "    # =========================================\n",
    "    # Bayesian Credible Interval (via sampling)\n",
    "    # =========================================\n",
    "    # Prior on Sharpe ratio: Normal(0, 1) - skeptical\n",
    "    prior_mu = 0\n",
    "    prior_sigma = 1\n",
    "    \n",
    "    # Bootstrap posterior approximation\n",
    "    n_bootstrap = 10000\n",
    "    sharpe_samples = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample from posterior (approximate via bootstrap + prior)\n",
    "        boot_idx = np.random.choice(n_obs, n_obs, replace=True)\n",
    "        boot_returns = returns[boot_idx]\n",
    "        boot_sharpe = (boot_returns.mean() / boot_returns.std()) * np.sqrt(252)\n",
    "        \n",
    "        # Add prior influence (approximate)\n",
    "        prior_weight = 1 / (1 + n_obs/10)  # Prior gets less weight with more data\n",
    "        posterior_sharpe = (1 - prior_weight) * boot_sharpe + prior_weight * np.random.normal(prior_mu, prior_sigma)\n",
    "        sharpe_samples.append(posterior_sharpe)\n",
    "    \n",
    "    sharpe_samples = np.array(sharpe_samples)\n",
    "    \n",
    "    # Equal-tailed credible interval\n",
    "    eti_low = np.percentile(sharpe_samples, 2.5)\n",
    "    eti_high = np.percentile(sharpe_samples, 97.5)\n",
    "    \n",
    "    # Highest Density Interval\n",
    "    hdi_low, hdi_high = compute_hdi(sharpe_samples, 0.95)\n",
    "    \n",
    "    # =========================================\n",
    "    # Visualization\n",
    "    # =========================================\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Panel 1: Posterior distribution with intervals\n",
    "    ax = axes[0, 0]\n",
    "    ax.hist(sharpe_samples, bins=50, density=True, alpha=0.6, color='steelblue',\n",
    "            label='Posterior samples')\n",
    "    \n",
    "    # Mark intervals\n",
    "    ax.axvline(sample_sharpe, color='green', linestyle='-', linewidth=2, label='Sample Sharpe')\n",
    "    ax.axvline(true_sharpe, color='red', linestyle='--', linewidth=2, label='True Sharpe')\n",
    "    \n",
    "    # ETI\n",
    "    ax.axvline(eti_low, color='purple', linestyle=':', linewidth=2)\n",
    "    ax.axvline(eti_high, color='purple', linestyle=':', linewidth=2, label=f'95% ETI [{eti_low:.2f}, {eti_high:.2f}]')\n",
    "    \n",
    "    # HDI\n",
    "    ax.axvline(hdi_low, color='orange', linestyle='-.', linewidth=2)\n",
    "    ax.axvline(hdi_high, color='orange', linestyle='-.', linewidth=2, label=f'95% HDI [{hdi_low:.2f}, {hdi_high:.2f}]')\n",
    "    \n",
    "    ax.set_xlabel('Sharpe Ratio (Annualized)', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Posterior Distribution of Sharpe Ratio', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Panel 2: Frequentist vs Bayesian intervals\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    y_positions = [0, 1, 2]\n",
    "    labels = ['Frequentist 95% CI', '95% ETI (Bayesian)', '95% HDI (Bayesian)']\n",
    "    \n",
    "    # Frequentist CI\n",
    "    ax.barh(y_positions[0], freq_ci_high - freq_ci_low, left=freq_ci_low, \n",
    "            height=0.4, color='green', alpha=0.6)\n",
    "    ax.plot(sample_sharpe, y_positions[0], 'g^', markersize=12)\n",
    "    \n",
    "    # ETI\n",
    "    ax.barh(y_positions[1], eti_high - eti_low, left=eti_low,\n",
    "            height=0.4, color='purple', alpha=0.6)\n",
    "    ax.plot(np.mean(sharpe_samples), y_positions[1], 'p^', markersize=12, color='purple')\n",
    "    \n",
    "    # HDI\n",
    "    ax.barh(y_positions[2], hdi_high - hdi_low, left=hdi_low,\n",
    "            height=0.4, color='orange', alpha=0.6)\n",
    "    ax.plot(np.median(sharpe_samples), y_positions[2], 'o^', markersize=12, color='orange')\n",
    "    \n",
    "    ax.axvline(true_sharpe, color='red', linestyle='--', linewidth=2, label='True Sharpe')\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel('Sharpe Ratio', fontsize=12)\n",
    "    ax.set_title('Comparison of Interval Estimates', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Panel 3: Repeated sampling demonstration\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    n_experiments = 100\n",
    "    freq_contains_true = 0\n",
    "    bayes_contains_true = 0\n",
    "    \n",
    "    freq_intervals = []\n",
    "    bayes_intervals = []\n",
    "    \n",
    "    for exp in range(n_experiments):\n",
    "        # Generate new data\n",
    "        exp_returns = np.random.normal(daily_sharpe * 0.02, 0.02, n_obs)\n",
    "        exp_sharpe = (exp_returns.mean() / exp_returns.std()) * np.sqrt(252)\n",
    "        \n",
    "        # Frequentist CI\n",
    "        exp_se = np.sqrt((1 + 0.5 * exp_sharpe**2) / n_obs) * np.sqrt(252)\n",
    "        f_low = exp_sharpe - 1.96 * exp_se\n",
    "        f_high = exp_sharpe + 1.96 * exp_se\n",
    "        \n",
    "        if f_low <= true_sharpe <= f_high:\n",
    "            freq_contains_true += 1\n",
    "        freq_intervals.append((f_low, f_high, f_low <= true_sharpe <= f_high))\n",
    "        \n",
    "        # Bayesian (simplified)\n",
    "        prior_weight = 1 / (1 + n_obs/10)\n",
    "        b_mean = (1 - prior_weight) * exp_sharpe + prior_weight * prior_mu\n",
    "        b_std = np.sqrt((1 - prior_weight)**2 * exp_se**2 + prior_weight**2 * prior_sigma**2)\n",
    "        b_low = b_mean - 1.96 * b_std\n",
    "        b_high = b_mean + 1.96 * b_std\n",
    "        \n",
    "        if b_low <= true_sharpe <= b_high:\n",
    "            bayes_contains_true += 1\n",
    "        bayes_intervals.append((b_low, b_high, b_low <= true_sharpe <= b_high))\n",
    "    \n",
    "    # Plot first 30 intervals\n",
    "    n_show = 30\n",
    "    for i in range(n_show):\n",
    "        color = 'green' if freq_intervals[i][2] else 'red'\n",
    "        ax.plot([freq_intervals[i][0], freq_intervals[i][1]], [i, i], \n",
    "                color=color, linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    ax.axvline(true_sharpe, color='red', linestyle='--', linewidth=2)\n",
    "    ax.set_xlabel('Sharpe Ratio', fontsize=12)\n",
    "    ax.set_ylabel('Experiment', fontsize=12)\n",
    "    ax.set_title(f'Frequentist CI Coverage: {freq_contains_true}% (Expected: 95%)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Panel 4: Interpretation summary\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    INTERPRETATION COMPARISON\n",
    "    ========================\n",
    "    \n",
    "    FREQUENTIST 95% CI: [{freq_ci_low:.2f}, {freq_ci_high:.2f}]\n",
    "    ---------------------------------------------------------\n",
    "    \"If we repeated this experiment many times, 95% of \n",
    "     the constructed intervals would contain the true value.\"\n",
    "    \n",
    "    ‚Ä¢ The interval is random (depends on data)\n",
    "    ‚Ä¢ The parameter is fixed (but unknown)\n",
    "    ‚Ä¢ This particular interval either contains Œ∏ or doesn't!\n",
    "    \n",
    "    \n",
    "    BAYESIAN 95% CREDIBLE INTERVAL: [{eti_low:.2f}, {eti_high:.2f}]\n",
    "    ---------------------------------------------------------\n",
    "    \"Given the observed data and our prior beliefs, there is\n",
    "     a 95% probability that the true Sharpe ratio lies in \n",
    "     this interval.\"\n",
    "    \n",
    "    ‚Ä¢ The interval is determined by the data\n",
    "    ‚Ä¢ The parameter is treated as random\n",
    "    ‚Ä¢ Direct probability statement about Œ∏!\n",
    "    \n",
    "    \n",
    "    KEY INSIGHT FOR TRADING\n",
    "    -----------------------\n",
    "    Sample Sharpe: {sample_sharpe:.2f}\n",
    "    True Sharpe: {true_sharpe:.2f}\n",
    "    \n",
    "    The Bayesian interval is often narrower due to shrinkage\n",
    "    toward the skeptical prior (0). This prevents \n",
    "    overconfidence from noisy small samples!\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, summary_text, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä COVERAGE RESULTS ({n_experiments} experiments):\")\n",
    "    print(f\"   Frequentist CI coverage: {freq_contains_true}%\")\n",
    "    print(f\"   Bayesian CI coverage: {bayes_contains_true}%\")\n",
    "\n",
    "credible_intervals_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31d350",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Bayesian Linear Regression for Price Prediction\n",
    "\n",
    "### üìà **Why Bayesian Linear Regression?**\n",
    "\n",
    "Standard OLS gives point estimates. Bayesian linear regression provides:\n",
    "- **Posterior distributions** over all coefficients\n",
    "- **Predictive distributions** with uncertainty\n",
    "- **Natural regularization** via priors\n",
    "- **Model comparison** via marginal likelihood\n",
    "\n",
    "### **Mathematical Framework**\n",
    "\n",
    "**Model:** $y = X\\beta + \\epsilon$ where $\\epsilon \\sim N(0, \\sigma^2)$\n",
    "\n",
    "**Prior on coefficients:** $\\beta \\sim N(\\beta_0, \\Sigma_0)$\n",
    "\n",
    "**Posterior:** $\\beta | y, X \\sim N(\\beta_n, \\Sigma_n)$\n",
    "\n",
    "Where:\n",
    "- $\\Sigma_n = (\\Sigma_0^{-1} + \\sigma^{-2} X^T X)^{-1}$\n",
    "- $\\beta_n = \\Sigma_n (\\Sigma_0^{-1} \\beta_0 + \\sigma^{-2} X^T y)$\n",
    "\n",
    "### **Connection to Ridge Regression**\n",
    "\n",
    "With prior $\\beta \\sim N(0, \\tau^2 I)$:\n",
    "- Posterior mean = Ridge estimate with $\\lambda = \\sigma^2/\\tau^2$\n",
    "- Bayesian interpretation: $\\lambda$ controls prior strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLinearRegression:\n",
    "    \"\"\"\n",
    "    Bayesian Linear Regression with conjugate Normal-Inverse-Gamma prior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prior_beta_mean=None, prior_beta_precision=None,\n",
    "                 prior_sigma_alpha=1, prior_sigma_beta=1):\n",
    "        \"\"\"\n",
    "        Initialize Bayesian Linear Regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prior_beta_mean : array-like\n",
    "            Prior mean for coefficients (default: zeros)\n",
    "        prior_beta_precision : array-like\n",
    "            Prior precision matrix for coefficients (default: weak)\n",
    "        prior_sigma_alpha, prior_sigma_beta : float\n",
    "            Inverse-Gamma prior parameters for noise variance\n",
    "        \"\"\"\n",
    "        self.prior_beta_mean = prior_beta_mean\n",
    "        self.prior_beta_precision = prior_beta_precision\n",
    "        self.prior_sigma_alpha = prior_sigma_alpha\n",
    "        self.prior_sigma_beta = prior_sigma_beta\n",
    "        \n",
    "        self.posterior_beta_mean = None\n",
    "        self.posterior_beta_cov = None\n",
    "        self.posterior_sigma_alpha = None\n",
    "        self.posterior_sigma_beta = None\n",
    "        self.n_features = None\n",
    "        \n",
    "    def fit(self, X, y, known_sigma=None):\n",
    "        \"\"\"\n",
    "        Fit Bayesian linear regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Feature matrix\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target vector\n",
    "        known_sigma : float, optional\n",
    "            If provided, use this as known noise std\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Set default priors if not specified\n",
    "        if self.prior_beta_mean is None:\n",
    "            self.prior_beta_mean = np.zeros(n_features)\n",
    "        if self.prior_beta_precision is None:\n",
    "            # Weak prior: precision = 0.01 * I\n",
    "            self.prior_beta_precision = 0.01 * np.eye(n_features)\n",
    "        \n",
    "        prior_beta_mean = np.asarray(self.prior_beta_mean)\n",
    "        prior_beta_precision = np.asarray(self.prior_beta_precision)\n",
    "        \n",
    "        # OLS estimate for sigma (if not known)\n",
    "        if known_sigma is None:\n",
    "            ols_beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "            ols_residuals = y - X @ ols_beta\n",
    "            sigma_squared_estimate = np.var(ols_residuals)\n",
    "        else:\n",
    "            sigma_squared_estimate = known_sigma ** 2\n",
    "        \n",
    "        # Posterior for beta (assuming known sigma for simplicity)\n",
    "        data_precision = (1 / sigma_squared_estimate) * (X.T @ X)\n",
    "        \n",
    "        self.posterior_beta_cov = np.linalg.inv(prior_beta_precision + data_precision)\n",
    "        self.posterior_beta_mean = self.posterior_beta_cov @ (\n",
    "            prior_beta_precision @ prior_beta_mean + \n",
    "            (1 / sigma_squared_estimate) * X.T @ y\n",
    "        )\n",
    "        \n",
    "        # Posterior for sigma (Inverse-Gamma update)\n",
    "        self.posterior_sigma_alpha = self.prior_sigma_alpha + n_samples / 2\n",
    "        residuals = y - X @ self.posterior_beta_mean\n",
    "        self.posterior_sigma_beta = self.prior_sigma_beta + 0.5 * np.sum(residuals**2)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_new, return_std=True):\n",
    "        \"\"\"\n",
    "        Make predictions with uncertainty.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_new : array-like\n",
    "            New feature matrix\n",
    "        return_std : bool\n",
    "            Whether to return predictive std\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        mean : array\n",
    "            Predicted mean\n",
    "        std : array (if return_std=True)\n",
    "            Predictive standard deviation\n",
    "        \"\"\"\n",
    "        X_new = np.asarray(X_new)\n",
    "        \n",
    "        # Predictive mean\n",
    "        mean = X_new @ self.posterior_beta_mean\n",
    "        \n",
    "        if return_std:\n",
    "            # Predictive variance = noise variance + coefficient uncertainty\n",
    "            sigma_squared = self.posterior_sigma_beta / (self.posterior_sigma_alpha - 1)\n",
    "            \n",
    "            # Variance from coefficient uncertainty\n",
    "            var_coef = np.sum((X_new @ self.posterior_beta_cov) * X_new, axis=1)\n",
    "            \n",
    "            # Total predictive variance\n",
    "            var_total = sigma_squared + var_coef\n",
    "            std = np.sqrt(var_total)\n",
    "            \n",
    "            return mean, std\n",
    "        \n",
    "        return mean\n",
    "    \n",
    "    def sample_coefficients(self, n_samples=1000):\n",
    "        \"\"\"\n",
    "        Sample from posterior distribution of coefficients.\n",
    "        \"\"\"\n",
    "        return np.random.multivariate_normal(\n",
    "            self.posterior_beta_mean, \n",
    "            self.posterior_beta_cov, \n",
    "            size=n_samples\n",
    "        )\n",
    "\n",
    "\n",
    "def bayesian_regression_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate Bayesian linear regression for price prediction.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BAYESIAN LINEAR REGRESSION FOR PRICE PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate synthetic data: predict returns from factors\n",
    "    np.random.seed(42)\n",
    "    n_train = 100\n",
    "    n_test = 30\n",
    "    \n",
    "    # True factor loadings\n",
    "    true_beta = np.array([0.001, 0.5, -0.3, 0.2])  # intercept, momentum, value, size\n",
    "    noise_sigma = 0.015\n",
    "    \n",
    "    # Generate factor data\n",
    "    def generate_factors(n):\n",
    "        momentum = np.random.normal(0, 0.02, n)\n",
    "        value = np.random.normal(0, 0.03, n)\n",
    "        size = np.random.normal(0, 0.01, n)\n",
    "        return np.column_stack([np.ones(n), momentum, value, size])\n",
    "    \n",
    "    X_train = generate_factors(n_train)\n",
    "    y_train = X_train @ true_beta + np.random.normal(0, noise_sigma, n_train)\n",
    "    \n",
    "    X_test = generate_factors(n_test)\n",
    "    y_test = X_test @ true_beta + np.random.normal(0, noise_sigma, n_test)\n",
    "    \n",
    "    # Fit OLS\n",
    "    ols_beta = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n",
    "    ols_pred = X_test @ ols_beta\n",
    "    \n",
    "    # Fit Bayesian regression with skeptical prior\n",
    "    # Prior: coefficients centered at 0 with moderate precision\n",
    "    prior_precision = np.diag([0.1, 10, 10, 10])  # Weaker on intercept\n",
    "    \n",
    "    bayes_reg = BayesianLinearRegression(\n",
    "        prior_beta_mean=np.zeros(4),\n",
    "        prior_beta_precision=prior_precision\n",
    "    )\n",
    "    bayes_reg.fit(X_train, y_train)\n",
    "    bayes_pred_mean, bayes_pred_std = bayes_reg.predict(X_test)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Panel 1: Coefficient comparison\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    labels = ['Intercept', 'Momentum', 'Value', 'Size']\n",
    "    x_pos = np.arange(len(labels))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x_pos - width, true_beta, width, label='True', color='green', alpha=0.7)\n",
    "    ax.bar(x_pos, ols_beta, width, label='OLS', color='blue', alpha=0.7)\n",
    "    ax.bar(x_pos + width, bayes_reg.posterior_beta_mean, width, label='Bayesian', color='red', alpha=0.7)\n",
    "    \n",
    "    # Add error bars for Bayesian\n",
    "    bayes_std = np.sqrt(np.diag(bayes_reg.posterior_beta_cov))\n",
    "    ax.errorbar(x_pos + width, bayes_reg.posterior_beta_mean, yerr=1.96*bayes_std,\n",
    "                fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "    ax.set_title('Coefficient Estimates', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Panel 2: Posterior distributions of coefficients\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    samples = bayes_reg.sample_coefficients(5000)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, 4))\n",
    "    \n",
    "    for i, (label, color) in enumerate(zip(labels[1:], colors[1:])):  # Skip intercept\n",
    "        ax.hist(samples[:, i+1], bins=50, density=True, alpha=0.4, color=color, label=label)\n",
    "        ax.axvline(true_beta[i+1], color=color, linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title('Posterior Distribution of Factor Loadings', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Panel 3: Predictions with uncertainty\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    idx = np.argsort(y_test)\n",
    "    \n",
    "    ax.scatter(range(n_test), y_test[idx], color='black', s=50, label='Actual', zorder=3)\n",
    "    ax.plot(range(n_test), ols_pred[idx], 'b-', linewidth=2, label='OLS', alpha=0.7)\n",
    "    ax.plot(range(n_test), bayes_pred_mean[idx], 'r-', linewidth=2, label='Bayesian mean')\n",
    "    ax.fill_between(range(n_test), \n",
    "                    bayes_pred_mean[idx] - 1.96*bayes_pred_std[idx],\n",
    "                    bayes_pred_mean[idx] + 1.96*bayes_pred_std[idx],\n",
    "                    alpha=0.3, color='red', label='95% Predictive Interval')\n",
    "    \n",
    "    ax.set_xlabel('Test Sample (sorted)', fontsize=12)\n",
    "    ax.set_ylabel('Return', fontsize=12)\n",
    "    ax.set_title('Predictions with Uncertainty', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Panel 4: Model evaluation\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    ols_mse = np.mean((y_test - ols_pred)**2)\n",
    "    bayes_mse = np.mean((y_test - bayes_pred_mean)**2)\n",
    "    \n",
    "    # Coverage: proportion of actual values within 95% PI\n",
    "    coverage = np.mean((y_test >= bayes_pred_mean - 1.96*bayes_pred_std) & \n",
    "                       (y_test <= bayes_pred_mean + 1.96*bayes_pred_std))\n",
    "    \n",
    "    # Shrinkage\n",
    "    shrinkage = np.mean(np.abs(ols_beta - bayes_reg.posterior_beta_mean) / \n",
    "                        (np.abs(ols_beta) + 1e-10))\n",
    "    \n",
    "    metrics_text = f\"\"\"\n",
    "    MODEL EVALUATION\n",
    "    ================\n",
    "    \n",
    "    PREDICTION ACCURACY\n",
    "    -------------------\n",
    "    OLS Test MSE:      {ols_mse:.8f}\n",
    "    Bayesian Test MSE: {bayes_mse:.8f}\n",
    "    Improvement:       {(ols_mse - bayes_mse)/ols_mse * 100:.1f}%\n",
    "    \n",
    "    UNCERTAINTY CALIBRATION\n",
    "    -----------------------\n",
    "    95% Predictive Interval Coverage: {coverage*100:.1f}%\n",
    "    (Should be ~95% if well-calibrated)\n",
    "    \n",
    "    REGULARIZATION (SHRINKAGE)\n",
    "    --------------------------\n",
    "    Average coefficient shrinkage: {shrinkage*100:.1f}%\n",
    "    \n",
    "    COEFFICIENT SUMMARY\n",
    "    -------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        metrics_text += f\"    {label}: True={true_beta[i]:.4f}, OLS={ols_beta[i]:.4f}, \"\n",
    "        metrics_text += f\"Bayes={bayes_reg.posterior_beta_mean[i]:.4f}¬±{bayes_std[i]:.4f}\\n\"\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.text(0.05, 0.95, metrics_text, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "    print(\"   1. Bayesian estimates are shrunk toward prior (0)\")\n",
    "    print(\"   2. Predictive intervals capture uncertainty in both coefficients AND noise\")\n",
    "    print(\"   3. Regularization prevents overfitting on small samples\")\n",
    "\n",
    "bayesian_regression_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aa83b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Markov Chain Monte Carlo (MCMC) Sampling\n",
    "\n",
    "### üé≤ **Why MCMC?**\n",
    "\n",
    "For complex models, we cannot compute posteriors analytically. MCMC algorithms generate samples from the posterior distribution by constructing a Markov chain whose stationary distribution is the target posterior.\n",
    "\n",
    "### **Key MCMC Algorithms**\n",
    "\n",
    "| Algorithm | Mechanism | Pros | Cons |\n",
    "|-----------|-----------|------|------|\n",
    "| **Metropolis-Hastings** | Proposal + accept/reject | Simple, general | Can be slow, tuning required |\n",
    "| **Gibbs Sampling** | Sample each parameter conditionally | No rejection, good for conjugate | Requires conditional distributions |\n",
    "| **Hamiltonian MC** | Uses gradient information | Efficient in high dimensions | Requires differentiable model |\n",
    "| **NUTS** | Auto-tuned HMC | Best general-purpose | Computationally intensive |\n",
    "\n",
    "### **Metropolis-Hastings Algorithm**\n",
    "\n",
    "```\n",
    "1. Start at Œ∏‚ÇÄ\n",
    "2. For t = 1, 2, ..., T:\n",
    "   a. Propose Œ∏* ~ q(Œ∏*|Œ∏‚Çú‚Çã‚ÇÅ)\n",
    "   b. Compute acceptance ratio:\n",
    "      Œ± = min(1, [P(Œ∏*|D) √ó q(Œ∏‚Çú‚Çã‚ÇÅ|Œ∏*)] / [P(Œ∏‚Çú‚Çã‚ÇÅ|D) √ó q(Œ∏*|Œ∏‚Çú‚Çã‚ÇÅ)])\n",
    "   c. Accept Œ∏‚Çú = Œ∏* with probability Œ±\n",
    "      Otherwise Œ∏‚Çú = Œ∏‚Çú‚Çã‚ÇÅ\n",
    "3. Discard burn-in, return samples\n",
    "```\n",
    "\n",
    "### **Convergence Diagnostics**\n",
    "\n",
    "- **Trace plots**: Visual inspection for stationarity\n",
    "- **R-hat (Gelman-Rubin)**: Compare within-chain and between-chain variance (target < 1.01)\n",
    "- **Effective Sample Size (ESS)**: Account for autocorrelation (target > 400)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
