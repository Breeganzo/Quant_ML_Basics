{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738d563c",
   "metadata": {},
   "source": [
    "# Bayesian Trading Strategy with Backtest\n",
    "\n",
    "## Week 20: Bayesian Methods in Quantitative Finance\n",
    "\n",
    "This notebook implements a complete Bayesian trading strategy framework including:\n",
    "- **Bayesian parameter estimation** for return distributions\n",
    "- **Online posterior updates** using conjugate priors\n",
    "- **Probabilistic trading signals** based on posterior beliefs\n",
    "- **Full backtesting framework** with transaction costs\n",
    "- **Performance analysis** with key metrics\n",
    "\n",
    "### Key Concepts:\n",
    "- **Prior Distribution**: Our initial beliefs about parameters before seeing data\n",
    "- **Likelihood**: Probability of observed data given parameters\n",
    "- **Posterior Distribution**: Updated beliefs after observing data via Bayes' theorem:\n",
    "\n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta) \\cdot P(\\theta)}{P(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fcec8",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76cdca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries for Bayesian inference\n",
    "from scipy import stats\n",
    "from scipy.special import gammaln\n",
    "\n",
    "# Data fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a274f4",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Market Data\n",
    "\n",
    "We'll fetch historical price data and compute returns along with technical features that will inform our Bayesian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1a4aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching SPY data from 2018-01-01 to 2024-12-31...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Adj Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Adj Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Calculate returns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mReturns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdj Close\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.pct_change()\n\u001b[32m     12\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mLog_Returns\u001b[39m\u001b[33m'\u001b[39m] = np.log(data[\u001b[33m'\u001b[39m\u001b[33mAdj Close\u001b[39m\u001b[33m'\u001b[39m] / data[\u001b[33m'\u001b[39m\u001b[33mAdj Close\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m))\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Calculate technical features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4112\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4113\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   4114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4170\u001b[39m, in \u001b[36mDataFrame._getitem_multilevel\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   4169\u001b[39m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4170\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np.ndarray)):\n\u001b[32m   4172\u001b[39m         new_columns = \u001b[38;5;28mself\u001b[39m.columns[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:3059\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[32m   3058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[32m   3062\u001b[39m keylen = \u001b[38;5;28mlen\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:3410\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3407\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[32m   3409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3410\u001b[39m     idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_loc_single_level_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m level > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lexsort_depth == \u001b[32m0\u001b[39m:\n\u001b[32m   3413\u001b[39m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[32m   3414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   3415\u001b[39m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:2999\u001b[39m, in \u001b[36mMultiIndex._get_loc_single_level_index\u001b[39m\u001b[34m(self, level_index, key)\u001b[39m\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlevel_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Adj Close'"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "TICKER = \"SPY\"  # S&P 500 ETF\n",
    "START_DATE = \"2018-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "\n",
    "# Fetch data (using auto_adjust=True for adjusted prices in 'Close' column)\n",
    "print(f\"Fetching {TICKER} data from {START_DATE} to {END_DATE}...\")\n",
    "data = yf.download(TICKER, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)\n",
    "\n",
    "# Calculate returns (using 'Close' which is now adjusted with auto_adjust=True)\n",
    "data['Returns'] = data['Close'].pct_change()\n",
    "data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "\n",
    "# Calculate technical features\n",
    "data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "data['SMA_50'] = data['Close'].rolling(window=50).mean()\n",
    "data['Volatility_20'] = data['Returns'].rolling(window=20).std() * np.sqrt(252)\n",
    "data['Momentum_10'] = data['Close'].pct_change(periods=10)\n",
    "\n",
    "# Calculate rolling mean and std of returns (for feature engineering)\n",
    "data['Rolling_Mean_20'] = data['Returns'].rolling(window=20).mean()\n",
    "data['Rolling_Std_20'] = data['Returns'].rolling(window=20).std()\n",
    "\n",
    "# Drop NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "print(f\"\\nSample statistics:\")\n",
    "print(f\"  Mean daily return: {data['Returns'].mean()*100:.4f}%\")\n",
    "print(f\"  Std daily return: {data['Returns'].std()*100:.4f}%\")\n",
    "print(f\"  Annualized return: {data['Returns'].mean()*252*100:.2f}%\")\n",
    "print(f\"  Annualized volatility: {data['Returns'].std()*np.sqrt(252)*100:.2f}%\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c947cb0",
   "metadata": {},
   "source": [
    "## 3. Define Bayesian Model for Price Prediction\n",
    "\n",
    "### Normal-Inverse-Gamma Conjugate Prior\n",
    "\n",
    "For modeling returns with unknown mean $\\mu$ and variance $\\sigma^2$, we use the **Normal-Inverse-Gamma** conjugate prior:\n",
    "\n",
    "$$\\mu | \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2 / \\kappa_0)$$\n",
    "$$\\sigma^2 \\sim \\text{Inv-Gamma}(\\alpha_0, \\beta_0)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_0$: Prior mean of returns\n",
    "- $\\kappa_0$: Strength of belief in prior mean\n",
    "- $\\alpha_0, \\beta_0$: Shape and scale of inverse-gamma for variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9842b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianReturnEstimator:\n",
    "    \"\"\"\n",
    "    Bayesian estimator for return distribution using Normal-Inverse-Gamma conjugate prior.\n",
    "    \n",
    "    This allows for online updates as new data arrives, maintaining a posterior\n",
    "    distribution over the mean and variance of returns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mu_0=0.0, kappa_0=1.0, alpha_0=2.0, beta_0=0.0001):\n",
    "        \"\"\"\n",
    "        Initialize prior parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        mu_0 : float\n",
    "            Prior mean (default: 0, uninformative about direction)\n",
    "        kappa_0 : float\n",
    "            Strength of prior on mean (lower = weaker prior)\n",
    "        alpha_0 : float\n",
    "            Shape parameter for inverse-gamma (must be > 1 for finite mean)\n",
    "        beta_0 : float\n",
    "            Scale parameter for inverse-gamma\n",
    "        \"\"\"\n",
    "        # Prior hyperparameters\n",
    "        self.mu_0 = mu_0\n",
    "        self.kappa_0 = kappa_0\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.beta_0 = beta_0\n",
    "        \n",
    "        # Current posterior parameters (start at prior)\n",
    "        self.mu_n = mu_0\n",
    "        self.kappa_n = kappa_0\n",
    "        self.alpha_n = alpha_0\n",
    "        self.beta_n = beta_0\n",
    "        self.n = 0\n",
    "        \n",
    "    def update(self, returns):\n",
    "        \"\"\"\n",
    "        Update posterior with new observations using conjugate update rules.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns : array-like\n",
    "            New return observations\n",
    "        \"\"\"\n",
    "        returns = np.atleast_1d(returns)\n",
    "        n_new = len(returns)\n",
    "        \n",
    "        if n_new == 0:\n",
    "            return\n",
    "            \n",
    "        # Sample statistics\n",
    "        x_bar = np.mean(returns)\n",
    "        \n",
    "        # Update posterior parameters\n",
    "        kappa_new = self.kappa_n + n_new\n",
    "        mu_new = (self.kappa_n * self.mu_n + n_new * x_bar) / kappa_new\n",
    "        alpha_new = self.alpha_n + n_new / 2\n",
    "        \n",
    "        # Sum of squared deviations\n",
    "        ss = np.sum((returns - x_bar) ** 2)\n",
    "        beta_new = self.beta_n + 0.5 * ss + \\\n",
    "                   (self.kappa_n * n_new * (x_bar - self.mu_n) ** 2) / (2 * kappa_new)\n",
    "        \n",
    "        # Update state\n",
    "        self.kappa_n = kappa_new\n",
    "        self.mu_n = mu_new\n",
    "        self.alpha_n = alpha_new\n",
    "        self.beta_n = beta_new\n",
    "        self.n += n_new\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to prior.\"\"\"\n",
    "        self.mu_n = self.mu_0\n",
    "        self.kappa_n = self.kappa_0\n",
    "        self.alpha_n = self.alpha_0\n",
    "        self.beta_n = self.beta_0\n",
    "        self.n = 0\n",
    "        \n",
    "    def get_posterior_mean(self):\n",
    "        \"\"\"Get posterior mean of returns.\"\"\"\n",
    "        return self.mu_n\n",
    "    \n",
    "    def get_posterior_variance(self):\n",
    "        \"\"\"Get posterior mean of variance (E[sigma^2]).\"\"\"\n",
    "        if self.alpha_n > 1:\n",
    "            return self.beta_n / (self.alpha_n - 1)\n",
    "        return np.inf\n",
    "    \n",
    "    def get_posterior_std(self):\n",
    "        \"\"\"Get posterior estimate of standard deviation.\"\"\"\n",
    "        return np.sqrt(self.get_posterior_variance())\n",
    "    \n",
    "    def get_predictive_distribution(self):\n",
    "        \"\"\"\n",
    "        Get parameters of the posterior predictive distribution.\n",
    "        The predictive distribution for a new observation is Student-t.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (df, loc, scale) for Student-t distribution\n",
    "        \"\"\"\n",
    "        df = 2 * self.alpha_n\n",
    "        loc = self.mu_n\n",
    "        scale = np.sqrt(self.beta_n * (self.kappa_n + 1) / (self.alpha_n * self.kappa_n))\n",
    "        return df, loc, scale\n",
    "    \n",
    "    def prob_positive_return(self):\n",
    "        \"\"\"\n",
    "        Calculate P(next return > 0) using posterior predictive.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float: Probability that next return will be positive\n",
    "        \"\"\"\n",
    "        df, loc, scale = self.get_predictive_distribution()\n",
    "        # P(X > 0) = 1 - CDF(0)\n",
    "        return 1 - stats.t.cdf(0, df=df, loc=loc, scale=scale)\n",
    "    \n",
    "    def prob_return_above_threshold(self, threshold):\n",
    "        \"\"\"\n",
    "        Calculate P(next return > threshold).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float\n",
    "            Return threshold\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float: Probability that next return exceeds threshold\n",
    "        \"\"\"\n",
    "        df, loc, scale = self.get_predictive_distribution()\n",
    "        return 1 - stats.t.cdf(threshold, df=df, loc=loc, scale=scale)\n",
    "    \n",
    "    def expected_return(self):\n",
    "        \"\"\"Get expected value of next return from predictive distribution.\"\"\"\n",
    "        return self.mu_n\n",
    "    \n",
    "    def var_at_risk(self, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate Value at Risk at given confidence level.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Significance level (default 5%)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float: VaR (negative number representing loss)\n",
    "        \"\"\"\n",
    "        df, loc, scale = self.get_predictive_distribution()\n",
    "        return stats.t.ppf(alpha, df=df, loc=loc, scale=scale)\n",
    "\n",
    "\n",
    "# Test the Bayesian estimator\n",
    "print(\"Testing BayesianReturnEstimator...\")\n",
    "estimator = BayesianReturnEstimator()\n",
    "\n",
    "# Update with first 100 days of returns\n",
    "test_returns = data['Returns'].values[:100]\n",
    "estimator.update(test_returns)\n",
    "\n",
    "print(f\"\\nAfter observing {estimator.n} returns:\")\n",
    "print(f\"  Posterior mean: {estimator.get_posterior_mean()*100:.4f}%\")\n",
    "print(f\"  Posterior std: {estimator.get_posterior_std()*100:.4f}%\")\n",
    "print(f\"  P(return > 0): {estimator.prob_positive_return()*100:.2f}%\")\n",
    "print(f\"  5% VaR: {estimator.var_at_risk(0.05)*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c52bc9",
   "metadata": {},
   "source": [
    "## 4. Implement Prior and Posterior Updates\n",
    "\n",
    "### Rolling Window Bayesian Update Strategy\n",
    "\n",
    "We implement a strategy that:\n",
    "1. Uses a rolling window of recent returns to continuously update our beliefs\n",
    "2. Maintains posterior distributions that adapt to changing market conditions\n",
    "3. Applies a decay factor to give more weight to recent observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBayesianEstimator:\n",
    "    \"\"\"\n",
    "    Adaptive Bayesian estimator with exponential decay for non-stationary returns.\n",
    "    \n",
    "    Uses a rolling window approach where older observations have less influence,\n",
    "    allowing the model to adapt to changing market regimes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=60, decay_factor=0.95):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        window_size : int\n",
    "            Number of recent observations to consider\n",
    "        decay_factor : float\n",
    "            Exponential decay for weighting observations (0 < decay < 1)\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.returns_buffer = []\n",
    "        \n",
    "        # Prior parameters (weakly informative)\n",
    "        self.mu_0 = 0.0\n",
    "        self.kappa_0 = 0.1  # Weak prior on mean\n",
    "        self.alpha_0 = 2.0\n",
    "        self.beta_0 = 0.0001\n",
    "        \n",
    "    def update(self, new_return):\n",
    "        \"\"\"Add new return observation.\"\"\"\n",
    "        self.returns_buffer.append(new_return)\n",
    "        if len(self.returns_buffer) > self.window_size:\n",
    "            self.returns_buffer.pop(0)\n",
    "            \n",
    "    def get_weighted_posterior(self):\n",
    "        \"\"\"\n",
    "        Compute posterior using exponentially weighted observations.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple: (mu_n, sigma_n, n_effective)\n",
    "        \"\"\"\n",
    "        if len(self.returns_buffer) < 5:\n",
    "            return self.mu_0, np.sqrt(self.beta_0 / (self.alpha_0 - 1)), 0\n",
    "        \n",
    "        returns = np.array(self.returns_buffer)\n",
    "        n = len(returns)\n",
    "        \n",
    "        # Create exponential weights\n",
    "        weights = np.array([self.decay_factor ** (n - 1 - i) for i in range(n)])\n",
    "        weights = weights / weights.sum()  # Normalize\n",
    "        \n",
    "        # Weighted statistics\n",
    "        weighted_mean = np.average(returns, weights=weights)\n",
    "        weighted_var = np.average((returns - weighted_mean) ** 2, weights=weights)\n",
    "        \n",
    "        # Effective sample size\n",
    "        n_eff = 1 / np.sum(weights ** 2)\n",
    "        \n",
    "        # Bayesian posterior with weighted data\n",
    "        kappa_n = self.kappa_0 + n_eff\n",
    "        mu_n = (self.kappa_0 * self.mu_0 + n_eff * weighted_mean) / kappa_n\n",
    "        alpha_n = self.alpha_0 + n_eff / 2\n",
    "        beta_n = self.beta_0 + 0.5 * n_eff * weighted_var + \\\n",
    "                 (self.kappa_0 * n_eff * (weighted_mean - self.mu_0) ** 2) / (2 * kappa_n)\n",
    "        \n",
    "        sigma_n = np.sqrt(beta_n / (alpha_n - 1)) if alpha_n > 1 else np.sqrt(weighted_var)\n",
    "        \n",
    "        return mu_n, sigma_n, n_eff\n",
    "    \n",
    "    def prob_positive_return(self):\n",
    "        \"\"\"Calculate P(next return > 0).\"\"\"\n",
    "        mu_n, sigma_n, n_eff = self.get_weighted_posterior()\n",
    "        \n",
    "        if n_eff < 2:\n",
    "            return 0.5  # Uncertain\n",
    "        \n",
    "        # Student-t predictive distribution\n",
    "        df = max(2 * (self.alpha_0 + n_eff / 2), 3)\n",
    "        scale = sigma_n * np.sqrt(1 + 1 / max(n_eff, 1))\n",
    "        \n",
    "        return 1 - stats.t.cdf(0, df=df, loc=mu_n, scale=scale)\n",
    "    \n",
    "    def get_sharpe_estimate(self, annualization=252):\n",
    "        \"\"\"Estimate Sharpe ratio from posterior.\"\"\"\n",
    "        mu_n, sigma_n, _ = self.get_weighted_posterior()\n",
    "        if sigma_n > 0:\n",
    "            return (mu_n * annualization) / (sigma_n * np.sqrt(annualization))\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear buffer.\"\"\"\n",
    "        self.returns_buffer = []\n",
    "\n",
    "\n",
    "# Demonstrate the adaptive estimator on our data\n",
    "print(\"Testing AdaptiveBayesianEstimator with rolling updates...\\n\")\n",
    "\n",
    "adaptive_est = AdaptiveBayesianEstimator(window_size=60, decay_factor=0.95)\n",
    "\n",
    "# Track posterior evolution\n",
    "posterior_means = []\n",
    "posterior_stds = []\n",
    "prob_positive = []\n",
    "\n",
    "for i, ret in enumerate(data['Returns'].values[:200]):\n",
    "    adaptive_est.update(ret)\n",
    "    mu, sigma, n_eff = adaptive_est.get_weighted_posterior()\n",
    "    posterior_means.append(mu)\n",
    "    posterior_stds.append(sigma)\n",
    "    prob_positive.append(adaptive_est.prob_positive_return())\n",
    "\n",
    "# Plot posterior evolution\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# Posterior mean\n",
    "axes[0].plot(posterior_means, color='blue', linewidth=1.5)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].fill_between(range(len(posterior_means)), \n",
    "                      np.array(posterior_means) - 2*np.array(posterior_stds),\n",
    "                      np.array(posterior_means) + 2*np.array(posterior_stds),\n",
    "                      alpha=0.3, color='blue', label='95% CI')\n",
    "axes[0].set_ylabel('Posterior Mean Return')\n",
    "axes[0].set_title('Evolution of Posterior Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Posterior std\n",
    "axes[1].plot(posterior_stds, color='orange', linewidth=1.5)\n",
    "axes[1].set_ylabel('Posterior Std')\n",
    "axes[1].set_title('Uncertainty in Return Estimate')\n",
    "\n",
    "# Probability of positive return\n",
    "axes[2].plot(prob_positive, color='green', linewidth=1.5)\n",
    "axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Neutral')\n",
    "axes[2].set_ylabel('P(Return > 0)')\n",
    "axes[2].set_xlabel('Trading Day')\n",
    "axes[2].set_title('Probability of Positive Return')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e5318d",
   "metadata": {},
   "source": [
    "## 5. Generate Trading Signals\n",
    "\n",
    "### Bayesian Signal Generation Rules\n",
    "\n",
    "Our trading signals are based on posterior probabilities:\n",
    "\n",
    "1. **Long Signal**: $P(r > 0 | D) > \\theta_{long}$ (default: 0.55)\n",
    "2. **Short Signal**: $P(r > 0 | D) < \\theta_{short}$ (default: 0.45)\n",
    "3. **Neutral**: Otherwise\n",
    "\n",
    "We also incorporate:\n",
    "- **Kelly Criterion** for position sizing based on posterior beliefs\n",
    "- **Volatility scaling** to manage risk\n",
    "- **Regime-based adjustments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianSignalGenerator:\n",
    "    \"\"\"\n",
    "    Generate trading signals based on Bayesian posterior beliefs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 long_threshold=0.55,\n",
    "                 short_threshold=0.45,\n",
    "                 window_size=60,\n",
    "                 decay_factor=0.95,\n",
    "                 use_kelly=True,\n",
    "                 max_leverage=1.0,\n",
    "                 vol_target=0.15):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        long_threshold : float\n",
    "            P(r>0) threshold to go long\n",
    "        short_threshold : float\n",
    "            P(r>0) threshold to go short (below this)\n",
    "        window_size : int\n",
    "            Lookback window for Bayesian estimation\n",
    "        decay_factor : float\n",
    "            Exponential decay weight\n",
    "        use_kelly : bool\n",
    "            Use Kelly criterion for position sizing\n",
    "        max_leverage : float\n",
    "            Maximum position size (1.0 = 100% invested)\n",
    "        vol_target : float\n",
    "            Target annualized volatility for position sizing\n",
    "        \"\"\"\n",
    "        self.long_threshold = long_threshold\n",
    "        self.short_threshold = short_threshold\n",
    "        self.window_size = window_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.use_kelly = use_kelly\n",
    "        self.max_leverage = max_leverage\n",
    "        self.vol_target = vol_target\n",
    "        \n",
    "        self.estimator = AdaptiveBayesianEstimator(\n",
    "            window_size=window_size, \n",
    "            decay_factor=decay_factor\n",
    "        )\n",
    "        \n",
    "    def compute_kelly_fraction(self, prob_win, win_loss_ratio=1.0):\n",
    "        \"\"\"\n",
    "        Compute Kelly criterion position size.\n",
    "        \n",
    "        Kelly Fraction = p - (1-p)/b\n",
    "        where p = probability of winning, b = win/loss ratio\n",
    "        \"\"\"\n",
    "        if prob_win <= 0 or prob_win >= 1:\n",
    "            return 0\n",
    "        kelly = prob_win - (1 - prob_win) / win_loss_ratio\n",
    "        return np.clip(kelly, -self.max_leverage, self.max_leverage)\n",
    "    \n",
    "    def compute_vol_scaled_position(self, current_vol, annualization=252):\n",
    "        \"\"\"Scale position to target volatility.\"\"\"\n",
    "        if current_vol <= 0:\n",
    "            return 1.0\n",
    "        annualized_vol = current_vol * np.sqrt(annualization)\n",
    "        return np.clip(self.vol_target / annualized_vol, 0, self.max_leverage)\n",
    "    \n",
    "    def generate_signal(self, returns_history, current_vol=None):\n",
    "        \"\"\"\n",
    "        Generate trading signal based on posterior beliefs.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns_history : array-like\n",
    "            Historical returns up to current time\n",
    "        current_vol : float, optional\n",
    "            Current realized volatility\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Signal information including position size and metadata\n",
    "        \"\"\"\n",
    "        # Reset and update estimator\n",
    "        self.estimator.reset()\n",
    "        for ret in returns_history[-self.window_size:]:\n",
    "            self.estimator.update(ret)\n",
    "        \n",
    "        # Get posterior statistics\n",
    "        prob_pos = self.estimator.prob_positive_return()\n",
    "        mu, sigma, n_eff = self.estimator.get_weighted_posterior()\n",
    "        sharpe_est = self.estimator.get_sharpe_estimate()\n",
    "        \n",
    "        # Determine direction\n",
    "        if prob_pos > self.long_threshold:\n",
    "            direction = 1  # Long\n",
    "        elif prob_pos < self.short_threshold:\n",
    "            direction = -1  # Short\n",
    "        else:\n",
    "            direction = 0  # Neutral\n",
    "        \n",
    "        # Position sizing\n",
    "        if direction != 0:\n",
    "            if self.use_kelly:\n",
    "                prob_win = prob_pos if direction == 1 else (1 - prob_pos)\n",
    "                base_size = abs(self.compute_kelly_fraction(prob_win))\n",
    "            else:\n",
    "                base_size = 1.0\n",
    "            \n",
    "            # Apply volatility scaling\n",
    "            if current_vol is not None and current_vol > 0:\n",
    "                vol_scale = self.compute_vol_scaled_position(current_vol)\n",
    "                position_size = min(base_size * vol_scale, self.max_leverage)\n",
    "            else:\n",
    "                position_size = min(base_size, self.max_leverage)\n",
    "        else:\n",
    "            position_size = 0.0\n",
    "        \n",
    "        return {\n",
    "            'direction': direction,\n",
    "            'position_size': direction * position_size,\n",
    "            'prob_positive': prob_pos,\n",
    "            'posterior_mean': mu,\n",
    "            'posterior_std': sigma,\n",
    "            'sharpe_estimate': sharpe_est,\n",
    "            'n_effective': n_eff\n",
    "        }\n",
    "\n",
    "\n",
    "# Test signal generation\n",
    "signal_gen = BayesianSignalGenerator(\n",
    "    long_threshold=0.55,\n",
    "    short_threshold=0.45,\n",
    "    window_size=60,\n",
    "    use_kelly=True,\n",
    "    max_leverage=1.0\n",
    ")\n",
    "\n",
    "# Generate signals over the dataset\n",
    "signals_list = []\n",
    "for i in range(60, len(data)):\n",
    "    returns_history = data['Returns'].values[:i]\n",
    "    current_vol = data['Volatility_20'].values[i-1] / np.sqrt(252) if i > 0 else None\n",
    "    signal = signal_gen.generate_signal(returns_history, current_vol)\n",
    "    signal['date'] = data.index[i]\n",
    "    signals_list.append(signal)\n",
    "\n",
    "signals_df = pd.DataFrame(signals_list)\n",
    "signals_df.set_index('date', inplace=True)\n",
    "\n",
    "# Display signal statistics\n",
    "print(\"Signal Distribution:\")\n",
    "print(signals_df['direction'].value_counts())\n",
    "print(f\"\\nMean P(positive): {signals_df['prob_positive'].mean():.4f}\")\n",
    "print(f\"Mean position size: {signals_df['position_size'].abs().mean():.4f}\")\n",
    "\n",
    "# Plot signal distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# P(positive) histogram\n",
    "axes[0, 0].hist(signals_df['prob_positive'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0.55, color='green', linestyle='--', label='Long threshold')\n",
    "axes[0, 0].axvline(x=0.45, color='red', linestyle='--', label='Short threshold')\n",
    "axes[0, 0].set_xlabel('P(Return > 0)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Posterior Probability')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Position sizes\n",
    "axes[0, 1].hist(signals_df['position_size'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Position Size')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Position Sizes')\n",
    "\n",
    "# Time series of positions\n",
    "axes[1, 0].plot(signals_df.index, signals_df['position_size'], linewidth=0.8)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Position Size')\n",
    "axes[1, 0].set_title('Position Size Over Time')\n",
    "\n",
    "# Sharpe estimate evolution\n",
    "axes[1, 1].plot(signals_df.index, signals_df['sharpe_estimate'], linewidth=0.8, color='purple')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('Estimated Sharpe Ratio')\n",
    "axes[1, 1].set_title('Rolling Sharpe Ratio Estimate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a395d89",
   "metadata": {},
   "source": [
    "## 6. Build Backtest Framework\n",
    "\n",
    "Our backtesting engine simulates realistic trading including:\n",
    "- **Transaction costs**: Slippage and commissions\n",
    "- **Position tracking**: Long, short, and neutral positions\n",
    "- **Portfolio value**: Mark-to-market tracking\n",
    "- **Trade logging**: Detailed record of all trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22785ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianBacktester:\n",
    "    \"\"\"\n",
    "    Comprehensive backtesting framework for Bayesian trading strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 initial_capital=100000,\n",
    "                 transaction_cost_bps=5,\n",
    "                 slippage_bps=2,\n",
    "                 risk_free_rate=0.02):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        initial_capital : float\n",
    "            Starting portfolio value\n",
    "        transaction_cost_bps : float\n",
    "            Transaction costs in basis points\n",
    "        slippage_bps : float\n",
    "            Slippage estimate in basis points\n",
    "        risk_free_rate : float\n",
    "            Annual risk-free rate for Sharpe calculation\n",
    "        \"\"\"\n",
    "        self.initial_capital = initial_capital\n",
    "        self.transaction_cost = transaction_cost_bps / 10000\n",
    "        self.slippage = slippage_bps / 10000\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        \n",
    "        # Results storage\n",
    "        self.portfolio_values = []\n",
    "        self.positions = []\n",
    "        self.trades = []\n",
    "        self.daily_returns = []\n",
    "        \n",
    "    def run_backtest(self, prices, signals_df, benchmark_returns=None):\n",
    "        \"\"\"\n",
    "        Execute backtest simulation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        prices : pd.Series\n",
    "            Asset prices indexed by date\n",
    "        signals_df : pd.DataFrame\n",
    "            DataFrame with 'position_size' column indexed by date\n",
    "        benchmark_returns : pd.Series, optional\n",
    "            Benchmark returns for comparison\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict: Backtest results including performance metrics\n",
    "        \"\"\"\n",
    "        # Align data\n",
    "        common_dates = prices.index.intersection(signals_df.index)\n",
    "        prices = prices.loc[common_dates]\n",
    "        signals_df = signals_df.loc[common_dates]\n",
    "        \n",
    "        # Initialize tracking\n",
    "        portfolio_value = self.initial_capital\n",
    "        current_position = 0\n",
    "        shares_held = 0\n",
    "        cash = self.initial_capital\n",
    "        \n",
    "        self.portfolio_values = [portfolio_value]\n",
    "        self.positions = [0]\n",
    "        self.trades = []\n",
    "        \n",
    "        for i in range(len(prices) - 1):\n",
    "            date = prices.index[i]\n",
    "            next_date = prices.index[i + 1]\n",
    "            current_price = prices.iloc[i]\n",
    "            next_price = prices.iloc[i + 1]\n",
    "            \n",
    "            target_position = signals_df['position_size'].iloc[i]\n",
    "            \n",
    "            # Calculate position change\n",
    "            current_exposure = shares_held * current_price\n",
    "            target_exposure = target_position * portfolio_value\n",
    "            exposure_change = target_exposure - current_exposure\n",
    "            \n",
    "            # Execute trade if needed\n",
    "            if abs(exposure_change) > 100:  # Minimum trade size\n",
    "                # Calculate transaction costs\n",
    "                trade_value = abs(exposure_change)\n",
    "                total_cost = trade_value * (self.transaction_cost + self.slippage)\n",
    "                \n",
    "                # Update shares\n",
    "                shares_change = exposure_change / current_price\n",
    "                shares_held += shares_change\n",
    "                cash -= exposure_change + total_cost\n",
    "                \n",
    "                self.trades.append({\n",
    "                    'date': date,\n",
    "                    'type': 'BUY' if exposure_change > 0 else 'SELL',\n",
    "                    'shares': abs(shares_change),\n",
    "                    'price': current_price,\n",
    "                    'value': abs(exposure_change),\n",
    "                    'cost': total_cost\n",
    "                })\n",
    "            \n",
    "            # Calculate new portfolio value\n",
    "            portfolio_value = cash + shares_held * next_price\n",
    "            self.portfolio_values.append(portfolio_value)\n",
    "            self.positions.append(shares_held * next_price / portfolio_value if portfolio_value > 0 else 0)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_dates = prices.index[1:]\n",
    "        self.results_df = pd.DataFrame({\n",
    "            'date': results_dates,\n",
    "            'portfolio_value': self.portfolio_values[1:],\n",
    "            'position': self.positions[1:]\n",
    "        }).set_index('date')\n",
    "        \n",
    "        self.results_df['strategy_returns'] = self.results_df['portfolio_value'].pct_change()\n",
    "        self.results_df['cumulative_returns'] = (1 + self.results_df['strategy_returns'].fillna(0)).cumprod() - 1\n",
    "        \n",
    "        # Add benchmark if provided\n",
    "        if benchmark_returns is not None:\n",
    "            aligned_benchmark = benchmark_returns.loc[self.results_df.index]\n",
    "            self.results_df['benchmark_returns'] = aligned_benchmark\n",
    "            self.results_df['benchmark_cumulative'] = (1 + aligned_benchmark.fillna(0)).cumprod() - 1\n",
    "        \n",
    "        return self.calculate_metrics()\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "        returns = self.results_df['strategy_returns'].dropna()\n",
    "        \n",
    "        if len(returns) < 2:\n",
    "            return {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        total_return = (self.results_df['portfolio_value'].iloc[-1] / self.initial_capital) - 1\n",
    "        ann_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "        ann_volatility = returns.std() * np.sqrt(252)\n",
    "        \n",
    "        # Risk-adjusted metrics\n",
    "        excess_returns = returns - self.risk_free_rate / 252\n",
    "        sharpe_ratio = excess_returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n",
    "        \n",
    "        # Sortino Ratio (downside deviation)\n",
    "        downside_returns = returns[returns < 0]\n",
    "        downside_std = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 1\n",
    "        sortino_ratio = ann_return / downside_std if downside_std > 0 else 0\n",
    "        \n",
    "        # Maximum Drawdown\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        rolling_max = cumulative.expanding().max()\n",
    "        drawdowns = cumulative / rolling_max - 1\n",
    "        max_drawdown = drawdowns.min()\n",
    "        \n",
    "        # Calmar Ratio\n",
    "        calmar_ratio = ann_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "        \n",
    "        # Win/Loss statistics\n",
    "        winning_days = (returns > 0).sum()\n",
    "        losing_days = (returns < 0).sum()\n",
    "        win_rate = winning_days / (winning_days + losing_days) if (winning_days + losing_days) > 0 else 0\n",
    "        \n",
    "        avg_win = returns[returns > 0].mean() if len(returns[returns > 0]) > 0 else 0\n",
    "        avg_loss = abs(returns[returns < 0].mean()) if len(returns[returns < 0]) > 0 else 1\n",
    "        profit_factor = (avg_win * winning_days) / (avg_loss * losing_days) if losing_days > 0 else np.inf\n",
    "        \n",
    "        # Trade statistics\n",
    "        n_trades = len(self.trades)\n",
    "        total_costs = sum(t['cost'] for t in self.trades)\n",
    "        \n",
    "        metrics = {\n",
    "            'Total Return': f\"{total_return*100:.2f}%\",\n",
    "            'Annualized Return': f\"{ann_return*100:.2f}%\",\n",
    "            'Annualized Volatility': f\"{ann_volatility*100:.2f}%\",\n",
    "            'Sharpe Ratio': f\"{sharpe_ratio:.3f}\",\n",
    "            'Sortino Ratio': f\"{sortino_ratio:.3f}\",\n",
    "            'Max Drawdown': f\"{max_drawdown*100:.2f}%\",\n",
    "            'Calmar Ratio': f\"{calmar_ratio:.3f}\",\n",
    "            'Win Rate': f\"{win_rate*100:.2f}%\",\n",
    "            'Profit Factor': f\"{profit_factor:.3f}\",\n",
    "            'Number of Trades': n_trades,\n",
    "            'Total Transaction Costs': f\"${total_costs:,.2f}\",\n",
    "            'Final Portfolio Value': f\"${self.results_df['portfolio_value'].iloc[-1]:,.2f}\"\n",
    "        }\n",
    "        \n",
    "        # Store numeric values for comparison\n",
    "        self.numeric_metrics = {\n",
    "            'total_return': total_return,\n",
    "            'ann_return': ann_return,\n",
    "            'ann_volatility': ann_volatility,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'sortino_ratio': sortino_ratio,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'calmar_ratio': calmar_ratio,\n",
    "            'win_rate': win_rate,\n",
    "            'profit_factor': profit_factor\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Run the backtest\n",
    "print(\"Running Bayesian Strategy Backtest...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "backtester = BayesianBacktester(\n",
    "    initial_capital=100000,\n",
    "    transaction_cost_bps=5,\n",
    "    slippage_bps=2\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "metrics = backtester.run_backtest(\n",
    "    prices=data['Adj Close'],\n",
    "    signals_df=signals_df,\n",
    "    benchmark_returns=data['Returns']\n",
    ")\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nBacktest Results:\")\n",
    "print(\"-\"*50)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:30s}: {value}\")\n",
    "    \n",
    "print(f\"\\nBacktest Period: {signals_df.index[0].date()} to {signals_df.index[-1].date()}\")\n",
    "print(f\"Total Trading Days: {len(signals_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22a788",
   "metadata": {},
   "source": [
    "## 7. Calculate Performance Metrics\n",
    "\n",
    "Let's compute additional metrics and compare our Bayesian strategy against a simple buy-and-hold benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_metrics(returns, window=252):\n",
    "    \"\"\"Calculate rolling performance metrics.\"\"\"\n",
    "    rolling_return = returns.rolling(window=window).mean() * 252\n",
    "    rolling_vol = returns.rolling(window=window).std() * np.sqrt(252)\n",
    "    rolling_sharpe = rolling_return / rolling_vol\n",
    "    \n",
    "    # Rolling max drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative.rolling(window=window, min_periods=1).max()\n",
    "    rolling_dd = cumulative / rolling_max - 1\n",
    "    rolling_max_dd = rolling_dd.rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    return rolling_return, rolling_vol, rolling_sharpe, rolling_max_dd\n",
    "\n",
    "\n",
    "def calculate_benchmark_metrics(returns, risk_free_rate=0.02):\n",
    "    \"\"\"Calculate buy-and-hold benchmark metrics.\"\"\"\n",
    "    total_return = (1 + returns).prod() - 1\n",
    "    ann_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "    ann_vol = returns.std() * np.sqrt(252)\n",
    "    sharpe = (ann_return - risk_free_rate) / ann_vol\n",
    "    \n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    rolling_max = cumulative.expanding().max()\n",
    "    max_dd = (cumulative / rolling_max - 1).min()\n",
    "    \n",
    "    return {\n",
    "        'Total Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate benchmark metrics\n",
    "benchmark_returns = data.loc[backtester.results_df.index, 'Returns']\n",
    "benchmark_metrics = calculate_benchmark_metrics(benchmark_returns)\n",
    "\n",
    "# Compare Strategy vs Benchmark\n",
    "print(\"=\" * 60)\n",
    "print(\"STRATEGY vs BENCHMARK COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Metric':<25} {'Strategy':>15} {'Benchmark':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "comparison_metrics = [\n",
    "    ('Total Return', backtester.numeric_metrics['total_return'], benchmark_metrics['Total Return']),\n",
    "    ('Ann. Return', backtester.numeric_metrics['ann_return'], benchmark_metrics['Annualized Return']),\n",
    "    ('Ann. Volatility', backtester.numeric_metrics['ann_volatility'], benchmark_metrics['Annualized Volatility']),\n",
    "    ('Sharpe Ratio', backtester.numeric_metrics['sharpe_ratio'], benchmark_metrics['Sharpe Ratio']),\n",
    "    ('Max Drawdown', backtester.numeric_metrics['max_drawdown'], benchmark_metrics['Max Drawdown']),\n",
    "]\n",
    "\n",
    "for name, strat_val, bench_val in comparison_metrics:\n",
    "    if 'Drawdown' in name:\n",
    "        print(f\"{name:<25} {strat_val*100:>14.2f}% {bench_val*100:>14.2f}%\")\n",
    "    elif 'Ratio' in name:\n",
    "        print(f\"{name:<25} {strat_val:>15.3f} {bench_val:>15.3f}\")\n",
    "    else:\n",
    "        print(f\"{name:<25} {strat_val*100:>14.2f}% {bench_val*100:>14.2f}%\")\n",
    "\n",
    "# Calculate information ratio\n",
    "strategy_returns = backtester.results_df['strategy_returns'].dropna()\n",
    "tracking_error = (strategy_returns - benchmark_returns.loc[strategy_returns.index]).std() * np.sqrt(252)\n",
    "excess_return = backtester.numeric_metrics['ann_return'] - benchmark_metrics['Annualized Return']\n",
    "information_ratio = excess_return / tracking_error if tracking_error > 0 else 0\n",
    "\n",
    "print(f\"\\n{'Information Ratio':<25} {information_ratio:>15.3f}\")\n",
    "print(f\"{'Tracking Error':<25} {tracking_error*100:>14.2f}%\")\n",
    "\n",
    "# Monthly returns analysis\n",
    "monthly_returns = strategy_returns.resample('M').apply(lambda x: (1+x).prod()-1)\n",
    "benchmark_monthly = benchmark_returns.resample('M').apply(lambda x: (1+x).prod()-1)\n",
    "\n",
    "print(f\"\\n{'Monthly Analysis':<25}\")\n",
    "print(f\"{'Best Month':<25} {monthly_returns.max()*100:>14.2f}% {benchmark_monthly.max()*100:>14.2f}%\")\n",
    "print(f\"{'Worst Month':<25} {monthly_returns.min()*100:>14.2f}% {benchmark_monthly.min()*100:>14.2f}%\")\n",
    "print(f\"{'% Positive Months':<25} {(monthly_returns>0).mean()*100:>14.2f}% {(benchmark_monthly>0).mean()*100:>14.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1854ec7",
   "metadata": {},
   "source": [
    "## 8. Visualize Strategy Results\n",
    "\n",
    "Comprehensive visualization of the backtest results including equity curves, drawdowns, and return distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_backtest_results(results_df, benchmark_returns, title=\"Bayesian Trading Strategy\"):\n",
    "    \"\"\"Create comprehensive visualization of backtest results.\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "    \n",
    "    # 1. Equity Curve\n",
    "    ax1 = plt.subplot(3, 2, 1)\n",
    "    strategy_equity = results_df['portfolio_value'] / results_df['portfolio_value'].iloc[0]\n",
    "    benchmark_equity = (1 + benchmark_returns.loc[results_df.index].fillna(0)).cumprod()\n",
    "    \n",
    "    ax1.plot(strategy_equity.index, strategy_equity.values, label='Bayesian Strategy', linewidth=2)\n",
    "    ax1.plot(benchmark_equity.index, benchmark_equity.values, label='Buy & Hold', linewidth=2, alpha=0.7)\n",
    "    ax1.set_ylabel('Portfolio Value (Normalized)')\n",
    "    ax1.set_title('Equity Curve Comparison')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Drawdown Chart\n",
    "    ax2 = plt.subplot(3, 2, 2)\n",
    "    strategy_returns = results_df['strategy_returns'].dropna()\n",
    "    \n",
    "    # Strategy drawdown\n",
    "    strategy_cumulative = (1 + strategy_returns).cumprod()\n",
    "    strategy_rolling_max = strategy_cumulative.expanding().max()\n",
    "    strategy_dd = strategy_cumulative / strategy_rolling_max - 1\n",
    "    \n",
    "    # Benchmark drawdown\n",
    "    benchmark_cumulative = (1 + benchmark_returns.loc[results_df.index].dropna()).cumprod()\n",
    "    benchmark_rolling_max = benchmark_cumulative.expanding().max()\n",
    "    benchmark_dd = benchmark_cumulative / benchmark_rolling_max - 1\n",
    "    \n",
    "    ax2.fill_between(strategy_dd.index, strategy_dd.values, 0, alpha=0.3, label='Strategy DD')\n",
    "    ax2.fill_between(benchmark_dd.index, benchmark_dd.values, 0, alpha=0.3, label='Benchmark DD')\n",
    "    ax2.set_ylabel('Drawdown')\n",
    "    ax2.set_title('Drawdown Comparison')\n",
    "    ax2.legend(loc='lower left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Return Distribution\n",
    "    ax3 = plt.subplot(3, 2, 3)\n",
    "    ax3.hist(strategy_returns * 100, bins=50, alpha=0.6, label='Strategy', density=True)\n",
    "    ax3.hist(benchmark_returns.loc[results_df.index] * 100, bins=50, alpha=0.6, label='Benchmark', density=True)\n",
    "    ax3.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax3.set_xlabel('Daily Return (%)')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Return Distribution')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Rolling Sharpe Ratio\n",
    "    ax4 = plt.subplot(3, 2, 4)\n",
    "    window = 63  # Quarterly\n",
    "    rolling_sharpe_strat = (strategy_returns.rolling(window).mean() / \n",
    "                            strategy_returns.rolling(window).std()) * np.sqrt(252)\n",
    "    rolling_sharpe_bench = (benchmark_returns.loc[results_df.index].rolling(window).mean() / \n",
    "                            benchmark_returns.loc[results_df.index].rolling(window).std()) * np.sqrt(252)\n",
    "    \n",
    "    ax4.plot(rolling_sharpe_strat.index, rolling_sharpe_strat.values, label='Strategy', linewidth=1.5)\n",
    "    ax4.plot(rolling_sharpe_bench.index, rolling_sharpe_bench.values, label='Benchmark', linewidth=1.5, alpha=0.7)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_ylabel('Sharpe Ratio')\n",
    "    ax4.set_title(f'Rolling Sharpe Ratio ({window}-day)')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Position Over Time\n",
    "    ax5 = plt.subplot(3, 2, 5)\n",
    "    ax5.fill_between(results_df.index, results_df['position'].values, 0, \n",
    "                     where=results_df['position'] > 0, alpha=0.5, color='green', label='Long')\n",
    "    ax5.fill_between(results_df.index, results_df['position'].values, 0,\n",
    "                     where=results_df['position'] < 0, alpha=0.5, color='red', label='Short')\n",
    "    ax5.set_ylabel('Position')\n",
    "    ax5.set_xlabel('Date')\n",
    "    ax5.set_title('Position Exposure Over Time')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Monthly Returns Heatmap\n",
    "    ax6 = plt.subplot(3, 2, 6)\n",
    "    monthly = strategy_returns.resample('M').apply(lambda x: (1+x).prod()-1) * 100\n",
    "    monthly_df = pd.DataFrame({\n",
    "        'Year': monthly.index.year,\n",
    "        'Month': monthly.index.month,\n",
    "        'Return': monthly.values\n",
    "    })\n",
    "    monthly_pivot = monthly_df.pivot_table(values='Return', index='Year', columns='Month', aggfunc='first')\n",
    "    \n",
    "    sns.heatmap(monthly_pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "                ax=ax6, cbar_kws={'label': 'Monthly Return (%)'})\n",
    "    ax6.set_title('Monthly Returns Heatmap')\n",
    "    ax6.set_xlabel('Month')\n",
    "    ax6.set_ylabel('Year')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate the comprehensive visualization\n",
    "plot_backtest_results(\n",
    "    backtester.results_df,\n",
    "    data['Returns'],\n",
    "    title=f\"Bayesian Trading Strategy - {TICKER}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408aae57",
   "metadata": {},
   "source": [
    "## 9. Sensitivity Analysis\n",
    "\n",
    "Let's analyze how the strategy performs with different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca3296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parameter_sweep(data, window_sizes, thresholds):\n",
    "    \"\"\"\n",
    "    Run parameter sweep to find optimal strategy parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Market data\n",
    "    window_sizes : list\n",
    "        List of window sizes to test\n",
    "    thresholds : list\n",
    "        List of threshold values (for long_threshold, short = 1 - threshold)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame: Results for each parameter combination\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    total_combinations = len(window_sizes) * len(thresholds)\n",
    "    print(f\"Testing {total_combinations} parameter combinations...\")\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        for thresh in thresholds:\n",
    "            # Generate signals\n",
    "            signal_gen = BayesianSignalGenerator(\n",
    "                long_threshold=thresh,\n",
    "                short_threshold=1-thresh,\n",
    "                window_size=window,\n",
    "                use_kelly=True,\n",
    "                max_leverage=1.0\n",
    "            )\n",
    "            \n",
    "            signals_list = []\n",
    "            for i in range(window, len(data)):\n",
    "                returns_history = data['Returns'].values[:i]\n",
    "                current_vol = data['Volatility_20'].values[i-1] / np.sqrt(252)\n",
    "                signal = signal_gen.generate_signal(returns_history, current_vol)\n",
    "                signal['date'] = data.index[i]\n",
    "                signals_list.append(signal)\n",
    "            \n",
    "            signals_df = pd.DataFrame(signals_list).set_index('date')\n",
    "            \n",
    "            # Run backtest\n",
    "            backtester = BayesianBacktester(initial_capital=100000)\n",
    "            metrics = backtester.run_backtest(\n",
    "                prices=data['Adj Close'],\n",
    "                signals_df=signals_df\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'window_size': window,\n",
    "                'threshold': thresh,\n",
    "                'sharpe_ratio': backtester.numeric_metrics['sharpe_ratio'],\n",
    "                'total_return': backtester.numeric_metrics['total_return'],\n",
    "                'max_drawdown': backtester.numeric_metrics['max_drawdown'],\n",
    "                'win_rate': backtester.numeric_metrics['win_rate'],\n",
    "                'calmar_ratio': backtester.numeric_metrics['calmar_ratio']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run parameter sweep\n",
    "window_sizes = [30, 45, 60, 90, 120]\n",
    "thresholds = [0.52, 0.55, 0.58, 0.60, 0.62]\n",
    "\n",
    "sweep_results = run_parameter_sweep(data, window_sizes, thresholds)\n",
    "\n",
    "# Find best parameters\n",
    "best_sharpe_idx = sweep_results['sharpe_ratio'].idxmax()\n",
    "best_params = sweep_results.loc[best_sharpe_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER SWEEP RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Parameters (by Sharpe Ratio):\")\n",
    "print(f\"  Window Size: {int(best_params['window_size'])}\")\n",
    "print(f\"  Threshold: {best_params['threshold']:.2f}\")\n",
    "print(f\"  Sharpe Ratio: {best_params['sharpe_ratio']:.3f}\")\n",
    "print(f\"  Total Return: {best_params['total_return']*100:.2f}%\")\n",
    "print(f\"  Max Drawdown: {best_params['max_drawdown']*100:.2f}%\")\n",
    "\n",
    "# Visualize parameter sensitivity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Heatmap of Sharpe Ratios\n",
    "pivot_sharpe = sweep_results.pivot_table(values='sharpe_ratio', \n",
    "                                          index='window_size', \n",
    "                                          columns='threshold')\n",
    "sns.heatmap(pivot_sharpe, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[0])\n",
    "axes[0].set_title('Sharpe Ratio')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Window Size')\n",
    "\n",
    "# Heatmap of Total Returns\n",
    "pivot_return = sweep_results.pivot_table(values='total_return', \n",
    "                                          index='window_size', \n",
    "                                          columns='threshold') * 100\n",
    "sns.heatmap(pivot_return, annot=True, fmt='.1f', cmap='RdYlGn', ax=axes[1])\n",
    "axes[1].set_title('Total Return (%)')\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Window Size')\n",
    "\n",
    "# Heatmap of Max Drawdown\n",
    "pivot_dd = sweep_results.pivot_table(values='max_drawdown', \n",
    "                                      index='window_size', \n",
    "                                      columns='threshold') * 100\n",
    "sns.heatmap(pivot_dd, annot=True, fmt='.1f', cmap='RdYlGn_r', ax=axes[2])\n",
    "axes[2].set_title('Max Drawdown (%)')\n",
    "axes[2].set_xlabel('Threshold')\n",
    "axes[2].set_ylabel('Window Size')\n",
    "\n",
    "plt.suptitle('Parameter Sensitivity Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f95de",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook demonstrated a complete **Bayesian trading strategy** framework:\n",
    "\n",
    "1. **Normal-Inverse-Gamma Conjugate Priors**: Enable efficient online updates of return distribution beliefs\n",
    "2. **Adaptive Estimation**: Exponential decay weighting adapts to changing market regimes\n",
    "3. **Probabilistic Signals**: Trading decisions based on $P(\\text{return} > 0)$ rather than point estimates\n",
    "4. **Kelly Criterion Position Sizing**: Optimal bet sizing based on posterior beliefs\n",
    "5. **Comprehensive Backtesting**: Realistic simulation with transaction costs\n",
    "\n",
    "### Key Bayesian Advantages\n",
    "\n",
    "| Aspect | Frequentist Approach | Bayesian Approach |\n",
    "|--------|---------------------|-------------------|\n",
    "| Uncertainty | Point estimates | Full posterior distributions |\n",
    "| Updates | Re-estimate from scratch | Efficient conjugate updates |\n",
    "| Small samples | Unreliable | Prior regularizes estimates |\n",
    "| Decision making | Ad-hoc thresholds | Principled probabilistic |\n",
    "\n",
    "### Extensions to Explore\n",
    "\n",
    "1. **Bayesian Model Comparison**: Use Bayes factors to select between different model structures\n",
    "2. **Hierarchical Models**: Pool information across multiple assets\n",
    "3. **MCMC Sampling**: For more complex non-conjugate models (PyMC, Stan)\n",
    "4. **Regime-Switching**: Hidden Markov Models with Bayesian inference\n",
    "5. **Bayesian Neural Networks**: Uncertainty quantification in deep learning\n",
    "\n",
    "### References\n",
    "\n",
    "- Gelman et al. \"Bayesian Data Analysis\" (3rd Edition)\n",
    "- Murphy, K. \"Machine Learning: A Probabilistic Perspective\"\n",
    "- Lpez de Prado, \"Advances in Financial Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Statistics\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BACKTEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nStrategy: Bayesian Posterior-Based Trading\")\n",
    "print(f\"Asset: {TICKER}\")\n",
    "print(f\"Period: {START_DATE} to {END_DATE}\")\n",
    "print(f\"\\nBayesian Model: Normal-Inverse-Gamma Conjugate Prior\")\n",
    "print(f\"Signal Rule: Long when P(return > 0) > {signal_gen.long_threshold}\")\n",
    "print(f\"            Short when P(return > 0) < {signal_gen.short_threshold}\")\n",
    "print(f\"Position Sizing: Kelly Criterion with volatility scaling\")\n",
    "print(f\"\\n\" + \"-\"*60)\n",
    "print(\"Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
