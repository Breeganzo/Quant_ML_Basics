{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d45aa5a",
   "metadata": {},
   "source": [
    "# Day 3: Gaussian Processes for Nonparametric Regression in Trading\n",
    "\n",
    "## Week 20: Bayesian Methods\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Understand Gaussian Process fundamentals** - distributions over functions, prior and posterior\n",
    "2. **Master kernel functions** - RBF, Matérn, periodic, and custom kernels for financial data\n",
    "3. **Implement GP regression** - from scratch and using GPyTorch/scikit-learn\n",
    "4. **Apply GPs to trading problems** - volatility modeling, price prediction, uncertainty quantification\n",
    "5. **Understand GP limitations** - computational complexity, scalability solutions\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Gaussian Processes](#1-introduction)\n",
    "2. [Mathematical Foundations](#2-mathematical-foundations)\n",
    "3. [Kernel Functions for Finance](#3-kernel-functions)\n",
    "4. [GP Regression from Scratch](#4-gp-from-scratch)\n",
    "5. [GP with GPyTorch](#5-gpytorch)\n",
    "6. [Trading Applications](#6-trading-applications)\n",
    "7. [Scalable GP Methods](#7-scalable-gp)\n",
    "8. [Practice Exercises](#8-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9c35d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Gaussian Processes <a name=\"1-introduction\"></a>\n",
    "\n",
    "### What is a Gaussian Process?\n",
    "\n",
    "A **Gaussian Process (GP)** is a collection of random variables, any finite number of which have a joint Gaussian distribution. GPs provide a **nonparametric** approach to regression that:\n",
    "\n",
    "- Places a **prior distribution over functions** rather than parameters\n",
    "- Provides **uncertainty estimates** for predictions\n",
    "- Is **flexible** and can model complex nonlinear relationships\n",
    "- Is **data-efficient** for small datasets\n",
    "\n",
    "### Why GPs for Trading?\n",
    "\n",
    "| Advantage | Trading Application |\n",
    "|-----------|--------------------|\n",
    "| **Uncertainty quantification** | Risk management, position sizing |\n",
    "| **Nonparametric flexibility** | Regime-adaptive modeling |\n",
    "| **Smooth interpolation** | Volatility surface modeling |\n",
    "| **Bayesian framework** | Sequential updating with new data |\n",
    "| **Small data effectiveness** | Limited historical scenarios |\n",
    "\n",
    "### GP vs Parametric Models\n",
    "\n",
    "```\n",
    "Parametric (e.g., Linear Regression):\n",
    "- Fixed functional form: f(x) = wx + b\n",
    "- Learn parameters w, b\n",
    "- Limited flexibility\n",
    "\n",
    "Gaussian Process:\n",
    "- No fixed functional form\n",
    "- Distribution over all possible functions\n",
    "- Flexibility controlled by kernel choice\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.linalg import cholesky, cho_solve\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47beacf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Mathematical Foundations <a name=\"2-mathematical-foundations\"></a>\n",
    "\n",
    "### Definition\n",
    "\n",
    "A Gaussian Process is fully specified by its **mean function** $m(x)$ and **covariance function** (kernel) $k(x, x')$:\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
    "\n",
    "For any finite set of points $\\{x_1, ..., x_n\\}$:\n",
    "\n",
    "$$\\begin{bmatrix} f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix} \\sim \\mathcal{N}\\left( \\begin{bmatrix} m(x_1) \\\\ \\vdots \\\\ m(x_n) \\end{bmatrix}, \\begin{bmatrix} k(x_1, x_1) & \\cdots & k(x_1, x_n) \\\\ \\vdots & \\ddots & \\vdots \\\\ k(x_n, x_1) & \\cdots & k(x_n, x_n) \\end{bmatrix} \\right)$$\n",
    "\n",
    "### GP Prior\n",
    "\n",
    "Before observing data, the GP prior encodes our beliefs about function smoothness, periodicity, etc.\n",
    "\n",
    "### GP Posterior\n",
    "\n",
    "Given training data $\\mathbf{X}, \\mathbf{y}$ and test points $\\mathbf{X}_*$:\n",
    "\n",
    "$$\\mathbf{f}_* | \\mathbf{X}, \\mathbf{y}, \\mathbf{X}_* \\sim \\mathcal{N}(\\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\mu}_* = K(\\mathbf{X}_*, \\mathbf{X})[K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I]^{-1}\\mathbf{y}$\n",
    "- $\\boldsymbol{\\Sigma}_* = K(\\mathbf{X}_*, \\mathbf{X}_*) - K(\\mathbf{X}_*, \\mathbf{X})[K(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 I]^{-1}K(\\mathbf{X}, \\mathbf{X}_*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GP Prior - Sampling functions from prior\n",
    "\n",
    "def rbf_kernel(X1, X2, length_scale=1.0, variance=1.0):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) / Squared Exponential kernel.\n",
    "    \n",
    "    k(x, x') = variance * exp(-0.5 * ||x - x'||^2 / length_scale^2)\n",
    "    \"\"\"\n",
    "    X1 = np.atleast_2d(X1)\n",
    "    X2 = np.atleast_2d(X2)\n",
    "    \n",
    "    # Compute squared Euclidean distances\n",
    "    sqdist = np.sum(X1**2, axis=1).reshape(-1, 1) + \\\n",
    "             np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n",
    "    \n",
    "    return variance * np.exp(-0.5 * sqdist / length_scale**2)\n",
    "\n",
    "# Create test points\n",
    "X_test = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "# Sample from GP prior with different length scales\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "length_scales = [0.5, 1.0, 2.0]\n",
    "\n",
    "for ax, ls in zip(axes, length_scales):\n",
    "    # Compute covariance matrix\n",
    "    K = rbf_kernel(X_test, X_test, length_scale=ls)\n",
    "    K += 1e-8 * np.eye(len(X_test))  # Numerical stability\n",
    "    \n",
    "    # Sample functions from prior\n",
    "    samples = np.random.multivariate_normal(np.zeros(len(X_test)), K, size=5)\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax.plot(X_test, sample, alpha=0.7, label=f'Sample {i+1}')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.set_title(f'GP Prior Samples (length_scale={ls})')\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle('Effect of Length Scale on GP Prior Samples', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Shorter length scale → more wiggly functions\")\n",
    "print(\"Longer length scale → smoother functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3434b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GP Posterior - Conditioning on observations\n",
    "\n",
    "def gp_posterior(X_train, y_train, X_test, length_scale=1.0, variance=1.0, noise=1e-4):\n",
    "    \"\"\"\n",
    "    Compute GP posterior mean and covariance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : training inputs\n",
    "    y_train : training targets\n",
    "    X_test : test inputs\n",
    "    length_scale : kernel length scale\n",
    "    variance : kernel variance\n",
    "    noise : observation noise variance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mu : posterior mean\n",
    "    cov : posterior covariance\n",
    "    \"\"\"\n",
    "    # Compute kernel matrices\n",
    "    K = rbf_kernel(X_train, X_train, length_scale, variance) + noise * np.eye(len(X_train))\n",
    "    K_s = rbf_kernel(X_train, X_test, length_scale, variance)\n",
    "    K_ss = rbf_kernel(X_test, X_test, length_scale, variance)\n",
    "    \n",
    "    # Cholesky decomposition for numerical stability\n",
    "    L = cholesky(K, lower=True)\n",
    "    \n",
    "    # Solve for alpha = K^{-1} y\n",
    "    alpha = cho_solve((L, True), y_train)\n",
    "    \n",
    "    # Posterior mean\n",
    "    mu = K_s.T @ alpha\n",
    "    \n",
    "    # Solve for v = L^{-1} K_s\n",
    "    v = cho_solve((L, True), K_s)\n",
    "    \n",
    "    # Posterior covariance\n",
    "    cov = K_ss - K_s.T @ v\n",
    "    \n",
    "    return mu, cov\n",
    "\n",
    "# Generate synthetic training data\n",
    "np.random.seed(42)\n",
    "X_train = np.array([-4, -3, -2, -1, 1, 2, 3, 4]).reshape(-1, 1)\n",
    "y_train = np.sin(X_train).flatten() + 0.1 * np.random.randn(len(X_train))\n",
    "\n",
    "# Test points\n",
    "X_test = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "# Compute posterior\n",
    "mu, cov = gp_posterior(X_train, y_train, X_test, length_scale=1.0, variance=1.0, noise=0.01)\n",
    "std = np.sqrt(np.diag(cov))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Prior\n",
    "ax = axes[0]\n",
    "K_prior = rbf_kernel(X_test, X_test) + 1e-8 * np.eye(len(X_test))\n",
    "prior_samples = np.random.multivariate_normal(np.zeros(len(X_test)), K_prior, size=5)\n",
    "for sample in prior_samples:\n",
    "    ax.plot(X_test, sample, alpha=0.5)\n",
    "ax.fill_between(X_test.flatten(), -2, 2, alpha=0.2, color='gray', label='Prior uncertainty')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('GP Prior (Before Seeing Data)')\n",
    "ax.set_ylim(-3, 3)\n",
    "\n",
    "# Posterior\n",
    "ax = axes[1]\n",
    "ax.fill_between(X_test.flatten(), mu - 2*std, mu + 2*std, alpha=0.3, color='blue', label='95% CI')\n",
    "ax.plot(X_test, mu, 'b-', lw=2, label='Posterior mean')\n",
    "ax.scatter(X_train, y_train, c='red', s=100, zorder=5, label='Training data')\n",
    "ax.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label='True function')\n",
    "\n",
    "# Sample from posterior\n",
    "cov_stable = cov + 1e-8 * np.eye(len(X_test))\n",
    "posterior_samples = np.random.multivariate_normal(mu.flatten(), cov_stable, size=3)\n",
    "for i, sample in enumerate(posterior_samples):\n",
    "    ax.plot(X_test, sample, alpha=0.4, linestyle='--')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('GP Posterior (After Conditioning on Data)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Uncertainty is reduced near training points\")\n",
    "print(\"- Uncertainty grows away from data\")\n",
    "print(\"- Posterior samples pass through/near training points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd864a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Kernel Functions for Finance <a name=\"3-kernel-functions\"></a>\n",
    "\n",
    "The kernel (covariance function) encodes our assumptions about the function we're modeling.\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "| Kernel | Formula | Properties | Financial Use |\n",
    "|--------|---------|------------|---------------|\n",
    "| **RBF/SE** | $k(x,x') = \\sigma^2 \\exp\\left(-\\frac{\\|x-x'\\|^2}{2l^2}\\right)$ | Infinitely differentiable, smooth | General regression |\n",
    "| **Matérn** | Complex | Adjustable smoothness | Realistic price paths |\n",
    "| **Periodic** | $k(x,x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi|x-x'|/p)}{l^2}\\right)$ | Captures periodicity | Seasonality, calendar effects |\n",
    "| **Rational Quadratic** | $k(x,x') = \\sigma^2\\left(1 + \\frac{\\|x-x'\\|^2}{2\\alpha l^2}\\right)^{-\\alpha}$ | Multi-scale | Mixed regime dynamics |\n",
    "| **Linear** | $k(x,x') = \\sigma_b^2 + \\sigma_v^2(x-c)(x'-c)$ | Linear trends | Trend modeling |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement various kernel functions\n",
    "\n",
    "class Kernels:\n",
    "    \"\"\"Collection of kernel functions for Gaussian Processes.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def rbf(X1, X2, length_scale=1.0, variance=1.0):\n",
    "        \"\"\"Radial Basis Function (Squared Exponential) kernel.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T\n",
    "        return variance * np.exp(-0.5 * sqdist / length_scale**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def matern32(X1, X2, length_scale=1.0, variance=1.0):\n",
    "        \"\"\"Matérn 3/2 kernel - once differentiable.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        dist = np.sqrt(np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T + 1e-10)\n",
    "        scaled_dist = np.sqrt(3) * dist / length_scale\n",
    "        return variance * (1 + scaled_dist) * np.exp(-scaled_dist)\n",
    "    \n",
    "    @staticmethod\n",
    "    def matern52(X1, X2, length_scale=1.0, variance=1.0):\n",
    "        \"\"\"Matérn 5/2 kernel - twice differentiable.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        dist = np.sqrt(np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T + 1e-10)\n",
    "        scaled_dist = np.sqrt(5) * dist / length_scale\n",
    "        return variance * (1 + scaled_dist + scaled_dist**2 / 3) * np.exp(-scaled_dist)\n",
    "    \n",
    "    @staticmethod\n",
    "    def periodic(X1, X2, length_scale=1.0, variance=1.0, period=1.0):\n",
    "        \"\"\"Periodic kernel for seasonal patterns.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        dist = np.abs(X1 - X2.T)\n",
    "        return variance * np.exp(-2 * np.sin(np.pi * dist / period)**2 / length_scale**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rational_quadratic(X1, X2, length_scale=1.0, variance=1.0, alpha=1.0):\n",
    "        \"\"\"Rational Quadratic kernel - mixture of RBFs with different length scales.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T\n",
    "        return variance * (1 + sqdist / (2 * alpha * length_scale**2))**(-alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def linear(X1, X2, variance=1.0, variance_bias=1.0, offset=0.0):\n",
    "        \"\"\"Linear kernel for linear trends.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        return variance_bias + variance * (X1 - offset) @ (X2 - offset).T\n",
    "\n",
    "# Visualize different kernels\n",
    "X = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "kernels_info = [\n",
    "    ('RBF', lambda X1, X2: Kernels.rbf(X1, X2, length_scale=1.0)),\n",
    "    ('Matérn 3/2', lambda X1, X2: Kernels.matern32(X1, X2, length_scale=1.0)),\n",
    "    ('Matérn 5/2', lambda X1, X2: Kernels.matern52(X1, X2, length_scale=1.0)),\n",
    "    ('Periodic', lambda X1, X2: Kernels.periodic(X1, X2, length_scale=1.0, period=2.0)),\n",
    "    ('Rational Quadratic', lambda X1, X2: Kernels.rational_quadratic(X1, X2, alpha=0.5)),\n",
    "    ('Linear', lambda X1, X2: Kernels.linear(X1, X2))\n",
    "]\n",
    "\n",
    "for ax, (name, kernel_func) in zip(axes.flatten(), kernels_info):\n",
    "    K = kernel_func(X, X) + 1e-8 * np.eye(len(X))\n",
    "    samples = np.random.multivariate_normal(np.zeros(len(X)), K, size=5)\n",
    "    \n",
    "    for sample in samples:\n",
    "        ax.plot(X, sample, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{name} Kernel')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "\n",
    "plt.suptitle('GP Prior Samples with Different Kernels', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d932ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel combinations - sum and product\n",
    "\n",
    "def combined_kernel(X1, X2, config):\n",
    "    \"\"\"\n",
    "    Combine multiple kernels for complex patterns.\n",
    "    \n",
    "    Example: Trend + Seasonality + Noise\n",
    "    k(x,x') = k_linear(x,x') + k_periodic(x,x') + k_rbf(x,x')\n",
    "    \"\"\"\n",
    "    K = np.zeros((len(X1), len(X2)))\n",
    "    \n",
    "    if 'linear' in config:\n",
    "        K += config['linear']['weight'] * Kernels.linear(X1, X2, **config['linear']['params'])\n",
    "    \n",
    "    if 'periodic' in config:\n",
    "        K += config['periodic']['weight'] * Kernels.periodic(X1, X2, **config['periodic']['params'])\n",
    "    \n",
    "    if 'rbf' in config:\n",
    "        K += config['rbf']['weight'] * Kernels.rbf(X1, X2, **config['rbf']['params'])\n",
    "    \n",
    "    return K\n",
    "\n",
    "# Financial time series kernel: Trend + Seasonality\n",
    "X = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "\n",
    "config = {\n",
    "    'linear': {'weight': 0.3, 'params': {'variance': 0.5, 'variance_bias': 0.1}},\n",
    "    'periodic': {'weight': 0.5, 'params': {'length_scale': 1.0, 'period': 2.5}},\n",
    "    'rbf': {'weight': 0.2, 'params': {'length_scale': 0.5, 'variance': 0.5}}\n",
    "}\n",
    "\n",
    "K_combined = combined_kernel(X, X, config) + 1e-8 * np.eye(len(X))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Samples from combined kernel\n",
    "ax = axes[0]\n",
    "samples = np.random.multivariate_normal(np.zeros(len(X)), K_combined, size=5)\n",
    "for i, sample in enumerate(samples):\n",
    "    ax.plot(X, sample, alpha=0.7, label=f'Sample {i+1}')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('GP Samples: Linear + Periodic + RBF Kernel')\n",
    "ax.legend()\n",
    "\n",
    "# Kernel matrix visualization\n",
    "ax = axes[1]\n",
    "im = ax.imshow(K_combined, cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel('Time index j')\n",
    "ax.set_ylabel('Time index i')\n",
    "ax.set_title('Covariance Matrix K(i,j)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Combined kernels capture multiple patterns:\")\n",
    "print(\"- Linear: Overall trend\")\n",
    "print(\"- Periodic: Seasonality/cycles\")\n",
    "print(\"- RBF: Local variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41642e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. GP Regression from Scratch <a name=\"4-gp-from-scratch\"></a>\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Kernel computation** - Build covariance matrices\n",
    "2. **Posterior inference** - Condition on training data\n",
    "3. **Hyperparameter optimization** - Maximize marginal likelihood\n",
    "4. **Prediction** - Mean and uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessRegressor:\n",
    "    \"\"\"\n",
    "    Gaussian Process Regressor from scratch.\n",
    "    \n",
    "    Implements:\n",
    "    - RBF kernel\n",
    "    - Marginal likelihood optimization\n",
    "    - Posterior prediction with uncertainty\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel='rbf', length_scale=1.0, variance=1.0, noise=1e-4):\n",
    "        self.kernel = kernel\n",
    "        self.length_scale = length_scale\n",
    "        self.variance = variance\n",
    "        self.noise = noise\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.L = None  # Cholesky decomposition\n",
    "        self.alpha = None  # K^{-1} y\n",
    "        \n",
    "    def _kernel(self, X1, X2):\n",
    "        \"\"\"Compute kernel matrix.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T\n",
    "        return self.variance * np.exp(-0.5 * sqdist / self.length_scale**2)\n",
    "    \n",
    "    def _negative_log_marginal_likelihood(self, theta, X, y):\n",
    "        \"\"\"\n",
    "        Compute negative log marginal likelihood.\n",
    "        \n",
    "        log p(y|X,theta) = -0.5 * y^T K^{-1} y - 0.5 * log|K| - n/2 * log(2π)\n",
    "        \"\"\"\n",
    "        self.length_scale = np.exp(theta[0])\n",
    "        self.variance = np.exp(theta[1])\n",
    "        self.noise = np.exp(theta[2])\n",
    "        \n",
    "        K = self._kernel(X, X) + self.noise * np.eye(len(X))\n",
    "        \n",
    "        try:\n",
    "            L = cholesky(K, lower=True)\n",
    "        except:\n",
    "            return 1e10  # Return large value if not positive definite\n",
    "        \n",
    "        alpha = cho_solve((L, True), y)\n",
    "        \n",
    "        # Log marginal likelihood\n",
    "        log_likelihood = -0.5 * y @ alpha - np.sum(np.log(np.diag(L))) - 0.5 * len(X) * np.log(2 * np.pi)\n",
    "        \n",
    "        return -log_likelihood\n",
    "    \n",
    "    def fit(self, X, y, optimize=True):\n",
    "        \"\"\"\n",
    "        Fit the GP model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,)\n",
    "        optimize : bool, whether to optimize hyperparameters\n",
    "        \"\"\"\n",
    "        self.X_train = np.atleast_2d(X)\n",
    "        self.y_train = np.atleast_1d(y)\n",
    "        \n",
    "        if optimize:\n",
    "            # Optimize hyperparameters\n",
    "            theta0 = np.log([self.length_scale, self.variance, self.noise])\n",
    "            \n",
    "            result = minimize(\n",
    "                self._negative_log_marginal_likelihood,\n",
    "                theta0,\n",
    "                args=(self.X_train, self.y_train),\n",
    "                method='L-BFGS-B',\n",
    "                bounds=[(-5, 5), (-5, 5), (-10, 1)]\n",
    "            )\n",
    "            \n",
    "            self.length_scale = np.exp(result.x[0])\n",
    "            self.variance = np.exp(result.x[1])\n",
    "            self.noise = np.exp(result.x[2])\n",
    "        \n",
    "        # Precompute for predictions\n",
    "        K = self._kernel(self.X_train, self.X_train) + self.noise * np.eye(len(self.X_train))\n",
    "        self.L = cholesky(K, lower=True)\n",
    "        self.alpha = cho_solve((self.L, True), self.y_train)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, return_std=True, return_cov=False):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        mu : posterior mean\n",
    "        std : posterior standard deviation (if return_std=True)\n",
    "        cov : posterior covariance (if return_cov=True)\n",
    "        \"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "        \n",
    "        K_s = self._kernel(self.X_train, X)\n",
    "        K_ss = self._kernel(X, X)\n",
    "        \n",
    "        # Posterior mean\n",
    "        mu = K_s.T @ self.alpha\n",
    "        \n",
    "        # Posterior covariance\n",
    "        v = cho_solve((self.L, True), K_s)\n",
    "        cov = K_ss - K_s.T @ v\n",
    "        \n",
    "        if return_cov:\n",
    "            return mu, cov\n",
    "        elif return_std:\n",
    "            std = np.sqrt(np.maximum(np.diag(cov), 1e-10))\n",
    "            return mu, std\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def sample_posterior(self, X, n_samples=5):\n",
    "        \"\"\"Sample functions from the posterior.\"\"\"\n",
    "        mu, cov = self.predict(X, return_cov=True)\n",
    "        cov += 1e-8 * np.eye(len(X))  # Numerical stability\n",
    "        return np.random.multivariate_normal(mu, cov, size=n_samples)\n",
    "\n",
    "print(\"GaussianProcessRegressor class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the GP regressor on synthetic data\n",
    "\n",
    "# Generate data from a nonlinear function\n",
    "np.random.seed(42)\n",
    "n_train = 20\n",
    "\n",
    "def true_function(x):\n",
    "    return np.sin(x) + 0.3 * np.sin(3*x)\n",
    "\n",
    "X_train = np.sort(np.random.uniform(-4, 4, n_train)).reshape(-1, 1)\n",
    "y_train = true_function(X_train).flatten() + 0.1 * np.random.randn(n_train)\n",
    "\n",
    "X_test = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "# Fit GP\n",
    "gp = GaussianProcessRegressor(length_scale=1.0, variance=1.0, noise=0.01)\n",
    "gp.fit(X_train, y_train, optimize=True)\n",
    "\n",
    "print(f\"Optimized hyperparameters:\")\n",
    "print(f\"  Length scale: {gp.length_scale:.4f}\")\n",
    "print(f\"  Variance: {gp.variance:.4f}\")\n",
    "print(f\"  Noise: {gp.noise:.6f}\")\n",
    "\n",
    "# Predict\n",
    "mu, std = gp.predict(X_test)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(X_test.flatten(), mu - 2*std, mu + 2*std, alpha=0.2, color='blue', label='95% CI')\n",
    "plt.fill_between(X_test.flatten(), mu - std, mu + std, alpha=0.3, color='blue', label='68% CI')\n",
    "\n",
    "# Mean prediction\n",
    "plt.plot(X_test, mu, 'b-', lw=2, label='GP Mean')\n",
    "\n",
    "# True function\n",
    "plt.plot(X_test, true_function(X_test), 'k--', lw=1.5, alpha=0.7, label='True function')\n",
    "\n",
    "# Training data\n",
    "plt.scatter(X_train, y_train, c='red', s=80, zorder=5, edgecolors='k', label='Training data')\n",
    "\n",
    "# Posterior samples\n",
    "samples = gp.sample_posterior(X_test, n_samples=3)\n",
    "for i, sample in enumerate(samples):\n",
    "    plt.plot(X_test, sample, '--', alpha=0.5, lw=1)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gaussian Process Regression (from scratch)', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adae475",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. GP with GPyTorch <a name=\"5-gpytorch\"></a>\n",
    "\n",
    "GPyTorch provides GPU-accelerated GP models with:\n",
    "- Scalable inference (inducing points, variational methods)\n",
    "- Deep kernel learning\n",
    "- Integration with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ae14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GPyTorch if needed\n",
    "try:\n",
    "    import gpytorch\n",
    "    import torch\n",
    "    print(f\"GPyTorch version: {gpytorch.__version__}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing GPyTorch...\")\n",
    "    !pip install gpytorch\n",
    "    import gpytorch\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b042f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel, PeriodicKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "class ExactGPModel(ExactGP):\n",
    "    \"\"\"\n",
    "    Standard GP model using GPyTorch.\n",
    "    \n",
    "    Components:\n",
    "    - Mean function: constant or linear\n",
    "    - Kernel: RBF with learnable length scale\n",
    "    - Likelihood: Gaussian noise\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_x, train_y, likelihood, kernel_type='rbf'):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Mean function\n",
    "        self.mean_module = ConstantMean()\n",
    "        \n",
    "        # Covariance function\n",
    "        if kernel_type == 'rbf':\n",
    "            self.covar_module = ScaleKernel(RBFKernel())\n",
    "        elif kernel_type == 'matern':\n",
    "            self.covar_module = ScaleKernel(MaternKernel(nu=2.5))\n",
    "        elif kernel_type == 'periodic':\n",
    "            self.covar_module = ScaleKernel(PeriodicKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "def train_gp_model(model, likelihood, train_x, train_y, n_iterations=100, lr=0.1):\n",
    "    \"\"\"\n",
    "    Train GP model by maximizing marginal likelihood.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f'Iteration {i+1}/{n_iterations} - Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "print(\"GPyTorch model and training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae942bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GPyTorch model on synthetic data\n",
    "\n",
    "# Convert to tensors\n",
    "train_x = torch.tensor(X_train.flatten(), dtype=torch.float32)\n",
    "train_y = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_x = torch.tensor(X_test.flatten(), dtype=torch.float32)\n",
    "\n",
    "# Initialize model\n",
    "likelihood = GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood, kernel_type='rbf')\n",
    "\n",
    "# Train\n",
    "print(\"Training GP model...\")\n",
    "losses = train_gp_model(model, likelihood, train_x, train_y, n_iterations=100)\n",
    "\n",
    "# Print learned hyperparameters\n",
    "print(f\"\\nLearned hyperparameters:\")\n",
    "print(f\"  Noise: {likelihood.noise.item():.6f}\")\n",
    "print(f\"  Output scale: {model.covar_module.outputscale.item():.4f}\")\n",
    "print(f\"  Length scale: {model.covar_module.base_kernel.lengthscale.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Get predictive distribution\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    # Mean and confidence intervals\n",
    "    mean = observed_pred.mean.numpy()\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    lower, upper = lower.numpy(), upper.numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Predictions\n",
    "ax = axes[0]\n",
    "ax.fill_between(test_x.numpy(), lower, upper, alpha=0.3, color='blue', label='95% CI')\n",
    "ax.plot(test_x.numpy(), mean, 'b-', lw=2, label='GP Mean')\n",
    "ax.plot(X_test.flatten(), true_function(X_test), 'k--', alpha=0.7, label='True function')\n",
    "ax.scatter(train_x.numpy(), train_y.numpy(), c='red', s=80, zorder=5, edgecolors='k', label='Training data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('GPyTorch Predictions')\n",
    "ax.legend()\n",
    "\n",
    "# Training loss\n",
    "ax = axes[1]\n",
    "ax.plot(losses, 'b-', lw=2)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Negative Log Marginal Likelihood')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd36b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Trading Applications <a name=\"6-trading-applications\"></a>\n",
    "\n",
    "### Application 1: Volatility Surface Modeling\n",
    "\n",
    "GPs can model the implied volatility surface as a smooth function of strike and maturity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce58fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility Surface Modeling with GP\n",
    "\n",
    "# Generate synthetic implied volatility data\n",
    "np.random.seed(42)\n",
    "\n",
    "def true_vol_surface(moneyness, maturity):\n",
    "    \"\"\"\n",
    "    Synthetic volatility surface with smile and term structure.\n",
    "    \n",
    "    Vol = base_vol + smile_effect + term_effect\n",
    "    \"\"\"\n",
    "    base_vol = 0.20\n",
    "    smile = 0.05 * (moneyness - 1)**2  # Smile around ATM\n",
    "    term = 0.02 * np.sqrt(maturity)  # Term structure\n",
    "    skew = -0.03 * (moneyness - 1)  # Skew\n",
    "    return base_vol + smile + term + skew\n",
    "\n",
    "# Generate training data (market observations)\n",
    "n_samples = 50\n",
    "moneyness_train = np.random.uniform(0.8, 1.2, n_samples)\n",
    "maturity_train = np.random.uniform(0.1, 2.0, n_samples)\n",
    "X_vol_train = np.column_stack([moneyness_train, maturity_train])\n",
    "y_vol_train = true_vol_surface(moneyness_train, maturity_train) + 0.005 * np.random.randn(n_samples)\n",
    "\n",
    "# Create grid for prediction\n",
    "moneyness_grid = np.linspace(0.8, 1.2, 30)\n",
    "maturity_grid = np.linspace(0.1, 2.0, 30)\n",
    "M, T = np.meshgrid(moneyness_grid, maturity_grid)\n",
    "X_vol_test = np.column_stack([M.ravel(), T.ravel()])\n",
    "\n",
    "# Fit GP\n",
    "gp_vol = GaussianProcessRegressor(length_scale=0.3, variance=0.01, noise=0.0001)\n",
    "gp_vol.fit(X_vol_train, y_vol_train, optimize=True)\n",
    "\n",
    "# Predict\n",
    "vol_mean, vol_std = gp_vol.predict(X_vol_test)\n",
    "vol_mean = vol_mean.reshape(M.shape)\n",
    "vol_std = vol_std.reshape(M.shape)\n",
    "vol_true = true_vol_surface(M, T)\n",
    "\n",
    "print(f\"GP Vol Surface - Optimized parameters:\")\n",
    "print(f\"  Length scale: {gp_vol.length_scale:.4f}\")\n",
    "print(f\"  Variance: {gp_vol.variance:.6f}\")\n",
    "print(f\"  Noise: {gp_vol.noise:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volatility surface\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# True surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(M, T, vol_true, cmap='viridis', alpha=0.7)\n",
    "ax1.scatter(moneyness_train, maturity_train, y_vol_train, c='red', s=30)\n",
    "ax1.set_xlabel('Moneyness')\n",
    "ax1.set_ylabel('Maturity')\n",
    "ax1.set_zlabel('IV')\n",
    "ax1.set_title('True Vol Surface')\n",
    "\n",
    "# GP prediction\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.plot_surface(M, T, vol_mean, cmap='viridis', alpha=0.7)\n",
    "ax2.scatter(moneyness_train, maturity_train, y_vol_train, c='red', s=30)\n",
    "ax2.set_xlabel('Moneyness')\n",
    "ax2.set_ylabel('Maturity')\n",
    "ax2.set_zlabel('IV')\n",
    "ax2.set_title('GP Predicted Surface')\n",
    "\n",
    "# Uncertainty\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.plot_surface(M, T, vol_std, cmap='Reds', alpha=0.7)\n",
    "ax3.scatter(moneyness_train, maturity_train, np.zeros_like(y_vol_train), c='black', s=30)\n",
    "ax3.set_xlabel('Moneyness')\n",
    "ax3.set_ylabel('Maturity')\n",
    "ax3.set_zlabel('Std Dev')\n",
    "ax3.set_title('Prediction Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "rmse = np.sqrt(np.mean((vol_mean - vol_true)**2))\n",
    "print(f\"\\nRMSE: {rmse*100:.2f} vol points\")\n",
    "print(f\"Mean uncertainty: {vol_std.mean()*100:.2f} vol points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54727bbd",
   "metadata": {},
   "source": [
    "### Application 2: Return Prediction with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc87aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic price/return data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate a stock with mean-reverting features\n",
    "n_days = 500\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_days, freq='B')\n",
    "\n",
    "# Features: RSI-like, MA deviation, volatility\n",
    "def generate_features_and_returns(n_days):\n",
    "    # Generate correlated features\n",
    "    rsi = 50 + 20 * np.sin(np.linspace(0, 8*np.pi, n_days)) + 10 * np.random.randn(n_days)\n",
    "    rsi = np.clip(rsi, 0, 100)\n",
    "    \n",
    "    ma_dev = 0.02 * np.sin(np.linspace(0, 6*np.pi, n_days)) + 0.01 * np.random.randn(n_days)\n",
    "    \n",
    "    volatility = 0.15 + 0.05 * np.abs(np.sin(np.linspace(0, 4*np.pi, n_days))) + 0.02 * np.random.randn(n_days)\n",
    "    volatility = np.clip(volatility, 0.05, 0.4)\n",
    "    \n",
    "    # Generate returns with feature dependency\n",
    "    base_return = 0.0005  # Daily drift\n",
    "    rsi_effect = -0.0002 * (rsi - 50)  # Mean reversion\n",
    "    ma_effect = -0.01 * ma_dev  # MA reversion\n",
    "    \n",
    "    returns = base_return + rsi_effect + ma_effect + volatility * np.random.randn(n_days) / 16\n",
    "    \n",
    "    features = np.column_stack([rsi, ma_dev, volatility])\n",
    "    return features, returns\n",
    "\n",
    "features, returns = generate_features_and_returns(n_days)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'RSI': features[:, 0],\n",
    "    'MA_Dev': features[:, 1],\n",
    "    'Volatility': features[:, 2],\n",
    "    'Return': returns\n",
    "})\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GP for return prediction\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = ['RSI', 'MA_Dev', 'Volatility']\n",
    "X = df[feature_cols].values\n",
    "y = df['Return'].values\n",
    "\n",
    "# Normalize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "train_size = 400\n",
    "X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Fit GP\n",
    "gp_returns = GaussianProcessRegressor(length_scale=1.0, variance=0.001, noise=0.0001)\n",
    "gp_returns.fit(X_train, y_train, optimize=True)\n",
    "\n",
    "print(f\"Optimized hyperparameters:\")\n",
    "print(f\"  Length scale: {gp_returns.length_scale:.4f}\")\n",
    "print(f\"  Variance: {gp_returns.variance:.6f}\")\n",
    "print(f\"  Noise: {gp_returns.noise:.8f}\")\n",
    "\n",
    "# Predict\n",
    "y_pred, y_std = gp_returns.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ed4bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "test_dates = df.index[train_size:]\n",
    "\n",
    "# Predictions with uncertainty\n",
    "ax = axes[0, 0]\n",
    "ax.fill_between(range(len(y_test)), y_pred - 2*y_std, y_pred + 2*y_std, \n",
    "                alpha=0.3, color='blue', label='95% CI')\n",
    "ax.plot(range(len(y_test)), y_pred, 'b-', lw=1.5, label='GP Prediction')\n",
    "ax.plot(range(len(y_test)), y_test, 'r-', lw=1, alpha=0.7, label='Actual')\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Return')\n",
    "ax.set_title('GP Return Predictions with Uncertainty')\n",
    "ax.legend()\n",
    "ax.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Calibration plot\n",
    "ax = axes[0, 1]\n",
    "z_scores = (y_test - y_pred) / y_std\n",
    "ax.hist(z_scores, bins=30, density=True, alpha=0.7, color='blue')\n",
    "x_norm = np.linspace(-4, 4, 100)\n",
    "ax.plot(x_norm, stats.norm.pdf(x_norm), 'r-', lw=2, label='N(0,1)')\n",
    "ax.set_xlabel('Z-score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Uncertainty Calibration')\n",
    "ax.legend()\n",
    "\n",
    "# Prediction vs Actual\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(y_test, y_pred, alpha=0.5, c=y_std, cmap='viridis')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual Return')\n",
    "ax.set_ylabel('Predicted Return')\n",
    "ax.set_title('Predicted vs Actual (color = uncertainty)')\n",
    "plt.colorbar(ax.collections[0], ax=ax, label='Std Dev')\n",
    "\n",
    "# Uncertainty over time\n",
    "ax = axes[1, 1]\n",
    "ax.plot(range(len(y_test)), y_std, 'g-', lw=1.5)\n",
    "ax.set_xlabel('Day')\n",
    "ax.set_ylabel('Prediction Std Dev')\n",
    "ax.set_title('Prediction Uncertainty Over Time')\n",
    "ax.fill_between(range(len(y_test)), 0, y_std, alpha=0.3, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "corr = np.corrcoef(y_test, y_pred)[0, 1]\n",
    "coverage = np.mean(np.abs(z_scores) < 2)  # Should be ~95%\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  RMSE: {rmse:.6f}\")\n",
    "print(f\"  Correlation: {corr:.4f}\")\n",
    "print(f\"  95% CI Coverage: {coverage:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26c4cc",
   "metadata": {},
   "source": [
    "### Application 3: Position Sizing with GP Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17f741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GP uncertainty for position sizing\n",
    "\n",
    "def kelly_position_size(predicted_return, std, risk_free=0):\n",
    "    \"\"\"\n",
    "    Kelly criterion position sizing using GP predictions.\n",
    "    \n",
    "    f* = (mu - r) / sigma^2\n",
    "    \n",
    "    With uncertainty:\n",
    "    - Conservative: use lower bound of return\n",
    "    - Scale by confidence\n",
    "    \"\"\"\n",
    "    # Conservative Kelly: use prediction - 1 std\n",
    "    conservative_return = predicted_return - std\n",
    "    \n",
    "    # Variance estimate (include prediction uncertainty)\n",
    "    variance = std ** 2\n",
    "    \n",
    "    # Kelly fraction (capped)\n",
    "    kelly = np.clip(conservative_return / variance, -2, 2)\n",
    "    \n",
    "    # Further scale by confidence (inverse of uncertainty)\n",
    "    confidence = 1 / (1 + std * 100)  # Scale std to [0, 1]\n",
    "    position = kelly * confidence\n",
    "    \n",
    "    return position\n",
    "\n",
    "# Calculate positions\n",
    "positions = kelly_position_size(y_pred, y_std)\n",
    "\n",
    "# Simple strategy: trade based on GP signal\n",
    "df_test = df.iloc[train_size:].copy()\n",
    "df_test['Prediction'] = y_pred\n",
    "df_test['Uncertainty'] = y_std\n",
    "df_test['Position'] = positions\n",
    "df_test['Strategy_Return'] = df_test['Position'].shift(1) * df_test['Return']\n",
    "df_test['Cumulative_Return'] = (1 + df_test['Strategy_Return']).cumprod()\n",
    "df_test['BuyHold_Return'] = (1 + df_test['Return']).cumprod()\n",
    "\n",
    "# Compare strategies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cumulative returns\n",
    "ax = axes[0, 0]\n",
    "ax.plot(df_test.index, df_test['Cumulative_Return'], 'b-', lw=2, label='GP Strategy')\n",
    "ax.plot(df_test.index, df_test['BuyHold_Return'], 'r--', lw=2, label='Buy & Hold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Strategy Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Position sizes\n",
    "ax = axes[0, 1]\n",
    "ax.plot(df_test.index, df_test['Position'], 'g-', lw=1, alpha=0.7)\n",
    "ax.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Position Size')\n",
    "ax.set_title('GP-Based Position Sizes')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Position vs Uncertainty\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(df_test['Uncertainty'], np.abs(df_test['Position']), \n",
    "                     c=df_test['Prediction'], cmap='RdYlGn', alpha=0.6)\n",
    "ax.set_xlabel('Prediction Uncertainty')\n",
    "ax.set_ylabel('Absolute Position Size')\n",
    "ax.set_title('Position Size vs Uncertainty')\n",
    "plt.colorbar(scatter, ax=ax, label='Predicted Return')\n",
    "\n",
    "# Return distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(df_test['Strategy_Return'].dropna(), bins=40, alpha=0.6, color='blue', \n",
    "        label=f'GP Strategy (Sharpe: {df_test[\"Strategy_Return\"].mean()/df_test[\"Strategy_Return\"].std()*np.sqrt(252):.2f})')\n",
    "ax.hist(df_test['Return'], bins=40, alpha=0.6, color='red', \n",
    "        label=f'Buy & Hold (Sharpe: {df_test[\"Return\"].mean()/df_test[\"Return\"].std()*np.sqrt(252):.2f})')\n",
    "ax.set_xlabel('Return')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Return Distribution')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Strategy stats\n",
    "gp_sharpe = df_test['Strategy_Return'].mean() / df_test['Strategy_Return'].std() * np.sqrt(252)\n",
    "bh_sharpe = df_test['Return'].mean() / df_test['Return'].std() * np.sqrt(252)\n",
    "gp_final = df_test['Cumulative_Return'].iloc[-1]\n",
    "bh_final = df_test['BuyHold_Return'].iloc[-1]\n",
    "\n",
    "print(f\"\\nStrategy Comparison:\")\n",
    "print(f\"  GP Strategy Sharpe: {gp_sharpe:.2f}\")\n",
    "print(f\"  Buy & Hold Sharpe: {bh_sharpe:.2f}\")\n",
    "print(f\"  GP Final Return: {(gp_final-1)*100:.1f}%\")\n",
    "print(f\"  Buy & Hold Return: {(bh_final-1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b651ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Scalable GP Methods <a name=\"7-scalable-gp\"></a>\n",
    "\n",
    "### Challenge: Computational Complexity\n",
    "\n",
    "Standard GP has $O(n^3)$ complexity due to matrix inversion. For large datasets:\n",
    "\n",
    "| Method | Complexity | Description |\n",
    "|--------|------------|-------------|\n",
    "| **Sparse GP (Inducing Points)** | $O(nm^2)$ | Use $m << n$ inducing points |\n",
    "| **KISS-GP** | $O(n)$ | Kernel interpolation |\n",
    "| **Variational GP** | $O(nm^2)$ | Variational inference |\n",
    "| **Local GP** | $O(k^3)$ | Fit on local neighborhoods |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95672d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse GP Implementation using Inducing Points\n",
    "\n",
    "class SparseGPRegressor:\n",
    "    \"\"\"\n",
    "    Sparse GP using inducing points (FITC approximation).\n",
    "    \n",
    "    Approximates the full GP by using m inducing points:\n",
    "    K ≈ Q + diag(K - Q) where Q = K_nm @ K_mm^{-1} @ K_mn\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inducing=50, length_scale=1.0, variance=1.0, noise=1e-4):\n",
    "        self.n_inducing = n_inducing\n",
    "        self.length_scale = length_scale\n",
    "        self.variance = variance\n",
    "        self.noise = noise\n",
    "        self.inducing_points = None\n",
    "        \n",
    "    def _kernel(self, X1, X2):\n",
    "        \"\"\"RBF kernel.\"\"\"\n",
    "        X1, X2 = np.atleast_2d(X1), np.atleast_2d(X2)\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * X1 @ X2.T\n",
    "        return self.variance * np.exp(-0.5 * sqdist / self.length_scale**2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit sparse GP.\n",
    "        \n",
    "        Select inducing points via k-means clustering.\n",
    "        \"\"\"\n",
    "        self.X_train = np.atleast_2d(X)\n",
    "        self.y_train = np.atleast_1d(y)\n",
    "        \n",
    "        # Select inducing points via k-means\n",
    "        from sklearn.cluster import KMeans\n",
    "        n_inducing = min(self.n_inducing, len(X))\n",
    "        kmeans = KMeans(n_clusters=n_inducing, random_state=42, n_init=10)\n",
    "        kmeans.fit(self.X_train)\n",
    "        self.inducing_points = kmeans.cluster_centers_\n",
    "        \n",
    "        # Compute kernel matrices\n",
    "        K_mm = self._kernel(self.inducing_points, self.inducing_points)\n",
    "        K_nm = self._kernel(self.X_train, self.inducing_points)\n",
    "        K_nn_diag = np.full(len(self.X_train), self.variance)  # Diagonal of K_nn\n",
    "        \n",
    "        # FITC approximation\n",
    "        L_m = cholesky(K_mm + 1e-8 * np.eye(n_inducing), lower=True)\n",
    "        V = cho_solve((L_m, True), K_nm.T)  # K_mm^{-1} @ K_mn\n",
    "        Q_nn_diag = np.sum(K_nm * V.T, axis=1)  # Diagonal of Q = K_nm @ K_mm^{-1} @ K_mn\n",
    "        \n",
    "        # Lambda = diag(K_nn - Q_nn) + noise\n",
    "        Lambda = np.maximum(K_nn_diag - Q_nn_diag, 1e-8) + self.noise\n",
    "        \n",
    "        # Woodbury identity for efficient computation\n",
    "        Lambda_inv = 1.0 / Lambda\n",
    "        B = np.eye(n_inducing) + V @ np.diag(Lambda_inv) @ V.T\n",
    "        L_B = cholesky(B, lower=True)\n",
    "        \n",
    "        self.L_m = L_m\n",
    "        self.L_B = L_B\n",
    "        self.Lambda_inv = Lambda_inv\n",
    "        self.K_nm = K_nm\n",
    "        \n",
    "        # Precompute for predictions\n",
    "        self.alpha = cho_solve((L_B, True), V @ (Lambda_inv * self.y_train))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, return_std=True):\n",
    "        \"\"\"Make predictions using sparse GP.\"\"\"\n",
    "        X = np.atleast_2d(X)\n",
    "        \n",
    "        K_sm = self._kernel(X, self.inducing_points)\n",
    "        V_s = cho_solve((self.L_m, True), K_sm.T)\n",
    "        \n",
    "        # Posterior mean\n",
    "        mu = K_sm @ cho_solve((self.L_m, True), self.alpha)\n",
    "        \n",
    "        if return_std:\n",
    "            # Posterior variance (approximate)\n",
    "            V_s_B = cho_solve((self.L_B, True), V_s)\n",
    "            var = self.variance - np.sum(V_s * V_s, axis=0) + np.sum(V_s * V_s_B, axis=0)\n",
    "            var = np.maximum(var, 1e-10)\n",
    "            return mu.flatten(), np.sqrt(var)\n",
    "        \n",
    "        return mu.flatten()\n",
    "\n",
    "print(\"SparseGPRegressor defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5283f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Full GP vs Sparse GP\n",
    "import time\n",
    "\n",
    "# Generate larger dataset\n",
    "np.random.seed(42)\n",
    "n_large = 1000\n",
    "\n",
    "X_large = np.sort(np.random.uniform(-5, 5, n_large)).reshape(-1, 1)\n",
    "y_large = np.sin(X_large).flatten() + 0.1 * np.random.randn(n_large)\n",
    "\n",
    "X_test_large = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "\n",
    "# Full GP\n",
    "print(\"Training Full GP...\")\n",
    "start = time.time()\n",
    "gp_full = GaussianProcessRegressor(length_scale=1.0, variance=1.0, noise=0.01)\n",
    "gp_full.fit(X_large, y_large, optimize=False)  # Skip optimization for fair comparison\n",
    "mu_full, std_full = gp_full.predict(X_test_large)\n",
    "time_full = time.time() - start\n",
    "print(f\"  Time: {time_full:.3f}s\")\n",
    "\n",
    "# Sparse GP with different numbers of inducing points\n",
    "inducing_counts = [20, 50, 100]\n",
    "results = []\n",
    "\n",
    "for n_ind in inducing_counts:\n",
    "    print(f\"\\nTraining Sparse GP (m={n_ind})...\")\n",
    "    start = time.time()\n",
    "    gp_sparse = SparseGPRegressor(n_inducing=n_ind, length_scale=1.0, variance=1.0, noise=0.01)\n",
    "    gp_sparse.fit(X_large, y_large)\n",
    "    mu_sparse, std_sparse = gp_sparse.predict(X_test_large)\n",
    "    time_sparse = time.time() - start\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((mu_full - mu_sparse)**2))\n",
    "    results.append((n_ind, time_sparse, rmse))\n",
    "    print(f\"  Time: {time_sparse:.3f}s, RMSE vs Full: {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Full vs Sparse comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Full GP\n",
    "ax = axes[0, 0]\n",
    "ax.fill_between(X_test_large.flatten(), mu_full - 2*std_full, mu_full + 2*std_full, alpha=0.3)\n",
    "ax.plot(X_test_large, mu_full, 'b-', lw=2, label='Full GP')\n",
    "ax.scatter(X_large[::20], y_large[::20], c='red', s=20, alpha=0.5, label='Data (subset)')\n",
    "ax.set_title(f'Full GP (n={n_large})')\n",
    "ax.legend()\n",
    "\n",
    "# Sparse GP with 50 inducing points\n",
    "ax = axes[0, 1]\n",
    "gp_sparse = SparseGPRegressor(n_inducing=50, length_scale=1.0, variance=1.0, noise=0.01)\n",
    "gp_sparse.fit(X_large, y_large)\n",
    "mu_sparse, std_sparse = gp_sparse.predict(X_test_large)\n",
    "\n",
    "ax.fill_between(X_test_large.flatten(), mu_sparse - 2*std_sparse, mu_sparse + 2*std_sparse, alpha=0.3)\n",
    "ax.plot(X_test_large, mu_sparse, 'b-', lw=2, label='Sparse GP (m=50)')\n",
    "ax.scatter(gp_sparse.inducing_points, \n",
    "           gp_sparse.predict(gp_sparse.inducing_points, return_std=False),\n",
    "           c='green', s=100, marker='^', label='Inducing points')\n",
    "ax.set_title('Sparse GP (m=50)')\n",
    "ax.legend()\n",
    "\n",
    "# Time comparison\n",
    "ax = axes[1, 0]\n",
    "methods = ['Full'] + [f'Sparse (m={r[0]})' for r in results]\n",
    "times = [time_full] + [r[1] for r in results]\n",
    "bars = ax.bar(methods, times, color=['red'] + ['blue']*len(results))\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Computational Time Comparison')\n",
    "for bar, t in zip(bars, times):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{t:.3f}s', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[1, 1]\n",
    "ax.plot([r[0] for r in results], [r[2] for r in results], 'bo-', markersize=10, lw=2)\n",
    "ax.set_xlabel('Number of Inducing Points')\n",
    "ax.set_ylabel('RMSE vs Full GP')\n",
    "ax.set_title('Approximation Accuracy')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Sparse GP achieves similar accuracy with much less computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd74a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practice Exercises <a name=\"8-exercises\"></a>\n",
    "\n",
    "### Exercise 1: Custom Financial Kernel\n",
    "\n",
    "Implement a kernel that captures both **short-term mean reversion** and **long-term momentum**:\n",
    "\n",
    "$$k(x, x') = k_{\\text{short}}(x, x') + k_{\\text{long}}(x, x')$$\n",
    "\n",
    "where short uses small length scale (mean reversion) and long uses large length scale (momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d00db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement dual-scale kernel\n",
    "\n",
    "def dual_scale_kernel(X1, X2, \n",
    "                      short_length=0.5, short_variance=0.3,\n",
    "                      long_length=5.0, long_variance=0.7):\n",
    "    \"\"\"\n",
    "    Kernel combining short-term and long-term dynamics.\n",
    "    \n",
    "    TODO: Implement this kernel\n",
    "    \n",
    "    Hint: Sum two RBF kernels with different length scales\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your kernel\n",
    "# X = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "# K = dual_scale_kernel(X, X)\n",
    "# samples = np.random.multivariate_normal(np.zeros(len(X)), K + 1e-8*np.eye(len(X)), size=5)\n",
    "# plt.plot(X, samples.T)\n",
    "# plt.title('Dual-Scale Kernel Samples')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f441595",
   "metadata": {},
   "source": [
    "### Exercise 2: GP for Realized Volatility Prediction\n",
    "\n",
    "Build a GP model to predict next-day realized volatility using:\n",
    "- Past realized volatility (HAR-like features)\n",
    "- VIX level\n",
    "- Return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f78195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: GP for Volatility Prediction\n",
    "\n",
    "# Generate synthetic volatility data\n",
    "np.random.seed(42)\n",
    "n_days = 500\n",
    "\n",
    "# HAR-like volatility dynamics\n",
    "rv = np.zeros(n_days)\n",
    "rv[0] = 0.15\n",
    "\n",
    "for t in range(1, n_days):\n",
    "    # HAR model: RV_t = c + b_d*RV_{t-1} + b_w*RV_weekly + b_m*RV_monthly + eps\n",
    "    rv_d = rv[t-1]\n",
    "    rv_w = np.mean(rv[max(0,t-5):t]) if t > 0 else rv[0]\n",
    "    rv_m = np.mean(rv[max(0,t-22):t]) if t > 0 else rv[0]\n",
    "    \n",
    "    rv[t] = 0.02 + 0.4*rv_d + 0.3*rv_w + 0.2*rv_m + 0.02*np.random.randn()\n",
    "    rv[t] = np.clip(rv[t], 0.05, 0.5)\n",
    "\n",
    "# Create features\n",
    "# TODO: Build HAR-like features and fit GP to predict next-day volatility\n",
    "\n",
    "# Your code here\n",
    "print(\"Implement GP for volatility prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43682e",
   "metadata": {},
   "source": [
    "### Exercise 3: Bayesian Optimization for Hyperparameter Tuning\n",
    "\n",
    "Use GP to optimize trading strategy hyperparameters (e.g., lookback window, threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Bayesian Optimization\n",
    "\n",
    "def strategy_sharpe(lookback, threshold, returns):\n",
    "    \"\"\"\n",
    "    Simple momentum strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    - lookback: rolling window for momentum signal\n",
    "    - threshold: entry threshold\n",
    "    - returns: asset returns\n",
    "    \n",
    "    Returns: Sharpe ratio\n",
    "    \"\"\"\n",
    "    lookback = int(lookback)\n",
    "    momentum = pd.Series(returns).rolling(lookback).mean()\n",
    "    signal = np.where(momentum > threshold, 1, np.where(momentum < -threshold, -1, 0))\n",
    "    strat_returns = np.array(signal[:-1]) * np.array(returns[1:])\n",
    "    \n",
    "    if np.std(strat_returns) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return np.mean(strat_returns) / np.std(strat_returns) * np.sqrt(252)\n",
    "\n",
    "# Generate returns\n",
    "np.random.seed(42)\n",
    "returns = 0.0005 + 0.02 * np.random.randn(500)\n",
    "\n",
    "# TODO: Use GP to find optimal (lookback, threshold) that maximizes Sharpe\n",
    "# Hint: Sample some initial points, fit GP, use acquisition function\n",
    "\n",
    "# Your code here\n",
    "print(\"Implement Bayesian optimization for strategy tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d984a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Gaussian Processes** provide nonparametric regression with built-in uncertainty quantification\n",
    "\n",
    "2. **Kernel choice** determines GP properties:\n",
    "   - RBF: Smooth functions\n",
    "   - Matérn: Adjustable roughness\n",
    "   - Periodic: Seasonal patterns\n",
    "   - Combinations: Complex dynamics\n",
    "\n",
    "3. **Trading applications**:\n",
    "   - Volatility surface modeling\n",
    "   - Return prediction with uncertainty\n",
    "   - Position sizing via Kelly criterion\n",
    "   - Bayesian optimization for strategy tuning\n",
    "\n",
    "4. **Scalability solutions**:\n",
    "   - Sparse GP with inducing points\n",
    "   - Variational inference\n",
    "   - Local GP methods\n",
    "\n",
    "### Financial Use Cases\n",
    "\n",
    "| Application | Why GP? |\n",
    "|-------------|----------|\n",
    "| **Volatility surfaces** | Smooth interpolation, uncertainty |\n",
    "| **Return prediction** | Uncertainty for risk management |\n",
    "| **Strategy optimization** | Efficient hyperparameter search |\n",
    "| **Regime detection** | Nonparametric flexibility |\n",
    "| **Factor modeling** | Capturing nonlinear relationships |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Day 4: **Bayesian Neural Networks** - Combining deep learning with uncertainty\n",
    "- Explore **Deep Kernel Learning** (GPyTorch)\n",
    "- Apply to real market data with proper backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a06087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Day 3 Complete: Gaussian Processes for Trading!\")\n",
    "print(\"\\nKey concepts covered:\")\n",
    "print(\"  ✓ GP mathematical foundations\")\n",
    "print(\"  ✓ Kernel functions for finance\")\n",
    "print(\"  ✓ GP regression from scratch\")\n",
    "print(\"  ✓ GPyTorch implementation\")\n",
    "print(\"  ✓ Trading applications\")\n",
    "print(\"  ✓ Scalable GP methods\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
