{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c104f2f",
   "metadata": {},
   "source": [
    "# Day 7: Bayesian Methods - Interview Questions & Week Summary\n",
    "\n",
    "## Week 20 | Quant ML Learning Path\n",
    "\n",
    "**Objective:** Master 10 essential Bayesian interview questions and consolidate week's learning\n",
    "\n",
    "---\n",
    "\n",
    "### Interview Topics Covered:\n",
    "1. **Bayes' Theorem** - Foundation of Bayesian inference\n",
    "2. **Prior vs Posterior** - Belief updating mechanism\n",
    "3. **Conjugate Priors** - Analytical tractability\n",
    "4. **MAP Estimation** - Point estimation with priors\n",
    "5. **Bayesian vs Frequentist** - Philosophical and practical differences\n",
    "6. **MCMC Methods** - Sampling from complex posteriors\n",
    "7. **Bayesian Linear Regression** - Uncertainty quantification\n",
    "8. **Naive Bayes Classifier** - Probabilistic classification\n",
    "9. **Credible vs Confidence Intervals** - Interval interpretation\n",
    "10. **Bayesian Model Selection** - Model comparison techniques\n",
    "\n",
    "---\n",
    "\n",
    "**Finance Applications:** Portfolio optimization under uncertainty, risk modeling, strategy parameter estimation, regime detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5816a",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.special import gamma, beta as beta_func\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries loaded successfully\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Day 7: Bayesian Interview Questions & Week Summary\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee64d05",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Explain Bayes' Theorem and Implement It\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain Bayes' theorem and provide a practical example. How would you implement it to calculate the probability of a trading strategy being profitable given a positive signal?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Bayes' Theorem:** $P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
    "- **Components:** Prior P(A), Likelihood P(B|A), Evidence P(B), Posterior P(A|B)\n",
    "- **Interpretation:** Update beliefs with new evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c51d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_theorem(prior, likelihood, evidence):\n",
    "    \"\"\"\n",
    "    Calculate posterior probability using Bayes' theorem.\n",
    "    \n",
    "    P(A|B) = P(B|A) * P(A) / P(B)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prior : float - P(A) - Prior probability of hypothesis\n",
    "    likelihood : float - P(B|A) - Probability of evidence given hypothesis\n",
    "    evidence : float - P(B) - Total probability of evidence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float - Posterior probability P(A|B)\n",
    "    \"\"\"\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    return posterior\n",
    "\n",
    "\n",
    "# Example: Trading Strategy Signal Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYES' THEOREM: Trading Strategy Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Signal predicts profitable trade\n",
    "# Historical data:\n",
    "# - P(Profitable) = 0.55 (55% of trades are profitable - prior)\n",
    "# - P(Signal | Profitable) = 0.80 (signal appears 80% of time when profitable - likelihood)\n",
    "# - P(Signal | Not Profitable) = 0.30 (false positive rate)\n",
    "\n",
    "P_profitable = 0.55  # Prior\n",
    "P_signal_given_profitable = 0.80  # Likelihood\n",
    "P_signal_given_not_profitable = 0.30  # False positive rate\n",
    "\n",
    "# Calculate P(Signal) using law of total probability\n",
    "P_signal = (P_signal_given_profitable * P_profitable + \n",
    "            P_signal_given_not_profitable * (1 - P_profitable))\n",
    "\n",
    "# Apply Bayes' theorem\n",
    "P_profitable_given_signal = bayes_theorem(\n",
    "    prior=P_profitable,\n",
    "    likelihood=P_signal_given_profitable,\n",
    "    evidence=P_signal\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Given Information:\")\n",
    "print(f\"   P(Profitable) = {P_profitable:.2%} [Prior]\")\n",
    "print(f\"   P(Signal | Profitable) = {P_signal_given_profitable:.2%} [Likelihood]\")\n",
    "print(f\"   P(Signal | Not Profitable) = {P_signal_given_not_profitable:.2%} [False Positive]\")\n",
    "print(f\"\\nğŸ“ Calculated:\")\n",
    "print(f\"   P(Signal) = {P_signal:.2%} [Evidence]\")\n",
    "print(f\"\\nâœ… P(Profitable | Signal) = {P_profitable_given_signal:.2%} [Posterior]\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Components of Bayes' Theorem\n",
    "components = ['Prior\\nP(A)', 'Likelihood\\nP(B|A)', 'Evidence\\nP(B)', 'Posterior\\nP(A|B)']\n",
    "values = [P_profitable, P_signal_given_profitable, P_signal, P_profitable_given_signal]\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
    "\n",
    "bars = axes[0].bar(components, values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel('Probability', fontsize=12)\n",
    "axes[0].set_title(\"Bayes' Theorem Components\", fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, values):\n",
    "    axes[0].annotate(f'{val:.1%}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                     ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Right: Posterior update visualization\n",
    "categories = ['Before Signal\\n(Prior)', 'After Signal\\n(Posterior)']\n",
    "prob_values = [P_profitable, P_profitable_given_signal]\n",
    "\n",
    "bars2 = axes[1].bar(categories, prob_values, color=['#3498db', '#9b59b6'], \n",
    "                     edgecolor='black', linewidth=1.5, width=0.5)\n",
    "axes[1].axhline(y=0.5, color='red', linestyle='--', label='Break-even', alpha=0.7)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel('P(Profitable)', fontsize=12)\n",
    "axes[1].set_title('Belief Update with Evidence', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "for bar, val in zip(bars2, prob_values):\n",
    "    axes[1].annotate(f'{val:.1%}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                     ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Bayes' theorem relates conditional probabilities\")\n",
    "print(\"   2. Allows updating prior beliefs with new evidence\")\n",
    "print(\"   3. Essential for sequential learning and signal processing\")\n",
    "print(\"   4. In finance: refine probability estimates as data arrives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21573717",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Prior vs Posterior Distribution\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain the difference between prior and posterior distributions. Demonstrate how observing data updates our beliefs using a coin flip example.\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Prior:** Initial belief before seeing data\n",
    "- **Posterior:** Updated belief after seeing data\n",
    "- **Beta-Binomial Model:** Conjugate pair for binary outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PRIOR VS POSTERIOR: Beta-Binomial Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Beta-Binomial conjugate pair for coin flips\n",
    "# Prior: Beta(Î±, Î²) â†’ models belief about probability Î¸\n",
    "# Likelihood: Binomial(n, k | Î¸)\n",
    "# Posterior: Beta(Î± + k, Î² + n - k)\n",
    "\n",
    "class BetaBinomialModel:\n",
    "    \"\"\"Bayesian model for binary outcomes using Beta-Binomial conjugacy.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha_prior=1, beta_prior=1):\n",
    "        \"\"\"\n",
    "        Initialize with prior parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha_prior : float - Prior successes + 1 (pseudo-counts)\n",
    "        beta_prior : float - Prior failures + 1 (pseudo-counts)\n",
    "        \n",
    "        Common priors:\n",
    "        - Beta(1, 1): Uniform - no prior knowledge\n",
    "        - Beta(0.5, 0.5): Jeffreys - non-informative\n",
    "        - Beta(2, 2): Weakly informative - slight preference for Î¸=0.5\n",
    "        \"\"\"\n",
    "        self.alpha_prior = alpha_prior\n",
    "        self.beta_prior = beta_prior\n",
    "        self.alpha_post = alpha_prior\n",
    "        self.beta_post = beta_prior\n",
    "        self.n_observations = 0\n",
    "        self.n_successes = 0\n",
    "        \n",
    "    def update(self, successes, failures):\n",
    "        \"\"\"Update posterior with new observations.\"\"\"\n",
    "        self.n_successes += successes\n",
    "        self.n_observations += successes + failures\n",
    "        self.alpha_post = self.alpha_prior + self.n_successes\n",
    "        self.beta_post = self.beta_prior + (self.n_observations - self.n_successes)\n",
    "        \n",
    "    def get_prior(self):\n",
    "        \"\"\"Return prior distribution.\"\"\"\n",
    "        return stats.beta(self.alpha_prior, self.beta_prior)\n",
    "    \n",
    "    def get_posterior(self):\n",
    "        \"\"\"Return posterior distribution.\"\"\"\n",
    "        return stats.beta(self.alpha_post, self.beta_post)\n",
    "    \n",
    "    def posterior_stats(self):\n",
    "        \"\"\"Calculate posterior summary statistics.\"\"\"\n",
    "        posterior = self.get_posterior()\n",
    "        return {\n",
    "            'mean': posterior.mean(),\n",
    "            'std': posterior.std(),\n",
    "            'mode': (self.alpha_post - 1) / (self.alpha_post + self.beta_post - 2) \n",
    "                    if self.alpha_post > 1 and self.beta_post > 1 else None,\n",
    "            '95% CI': posterior.interval(0.95)\n",
    "        }\n",
    "\n",
    "# Simulate: Strategy win rate estimation\n",
    "# True win rate = 0.65 (unknown to us)\n",
    "true_win_rate = 0.65\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate trade outcomes\n",
    "n_trades = 100\n",
    "trade_outcomes = np.random.binomial(1, true_win_rate, n_trades)\n",
    "\n",
    "# Start with uniform prior (no prior knowledge)\n",
    "model = BetaBinomialModel(alpha_prior=1, beta_prior=1)\n",
    "\n",
    "# Track posterior evolution\n",
    "observation_points = [0, 10, 30, 100]\n",
    "posteriors = {}\n",
    "x = np.linspace(0, 1, 200)\n",
    "\n",
    "# Store initial prior\n",
    "posteriors[0] = {'dist': model.get_prior(), 'alpha': 1, 'beta': 1}\n",
    "\n",
    "# Update sequentially\n",
    "for i, outcome in enumerate(trade_outcomes):\n",
    "    model.update(successes=outcome, failures=1-outcome)\n",
    "    if i + 1 in observation_points:\n",
    "        posteriors[i + 1] = {\n",
    "            'dist': model.get_posterior(),\n",
    "            'alpha': model.alpha_post,\n",
    "            'beta': model.beta_post\n",
    "        }\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "for ax, (n_obs, color) in zip(axes.flat, zip(observation_points, colors)):\n",
    "    post = posteriors[n_obs]\n",
    "    \n",
    "    if n_obs == 0:\n",
    "        label = f'Prior: Beta({post[\"alpha\"]}, {post[\"beta\"]})'\n",
    "        title = 'Prior Distribution (n=0)'\n",
    "    else:\n",
    "        wins = sum(trade_outcomes[:n_obs])\n",
    "        label = f'Posterior: Beta({post[\"alpha\"]}, {post[\"beta\"]})'\n",
    "        title = f'After {n_obs} trades ({wins} wins)'\n",
    "    \n",
    "    y = post['dist'].pdf(x)\n",
    "    ax.fill_between(x, y, alpha=0.3, color=color)\n",
    "    ax.plot(x, y, color=color, linewidth=2, label=label)\n",
    "    ax.axvline(x=true_win_rate, color='red', linestyle='--', \n",
    "               linewidth=2, label=f'True rate: {true_win_rate}')\n",
    "    ax.axvline(x=post['dist'].mean(), color=color, linestyle=':',\n",
    "               linewidth=2, label=f'Posterior mean: {post[\"dist\"].mean():.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Win Rate (Î¸)', fontsize=11)\n",
    "    ax.set_ylabel('Density', fontsize=11)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('Bayesian Belief Update: Prior â†’ Posterior', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final statistics\n",
    "print(f\"\\nğŸ“Š Final Posterior Analysis (n={n_trades}):\")\n",
    "final_stats = model.posterior_stats()\n",
    "print(f\"   Posterior Mean: {final_stats['mean']:.4f}\")\n",
    "print(f\"   Posterior Std: {final_stats['std']:.4f}\")\n",
    "print(f\"   Posterior Mode: {final_stats['mode']:.4f}\")\n",
    "print(f\"   95% Credible Interval: [{final_stats['95% CI'][0]:.4f}, {final_stats['95% CI'][1]:.4f}]\")\n",
    "print(f\"   True Win Rate: {true_win_rate}\")\n",
    "print(f\"   Sample Win Rate (MLE): {sum(trade_outcomes)/len(trade_outcomes):.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Prior encodes beliefs BEFORE seeing data\")\n",
    "print(\"   2. Posterior is updated belief AFTER seeing data\")\n",
    "print(\"   3. More data â†’ posterior concentrates around true value\")\n",
    "print(\"   4. Prior influence diminishes with more observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faf7e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Conjugate Priors\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"What are conjugate priors and why are they useful? Provide examples of common conjugate pairs.\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Conjugate Prior:** Prior and posterior belong to same family\n",
    "- **Analytical Tractability:** Closed-form posterior updates\n",
    "- **Common Pairs:** Beta-Binomial, Normal-Normal, Gamma-Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52c32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CONJUGATE PRIORS: Common Pairs in Finance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "x = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "# ===== 1. Beta-Binomial (Binary outcomes) =====\n",
    "# Use case: Win rate estimation, success probability\n",
    "print(\"\\n1ï¸âƒ£ BETA-BINOMIAL CONJUGATE PAIR\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Likelihood: Binomial(n, k | Î¸)\")\n",
    "print(\"   Prior: Beta(Î±, Î²)\")\n",
    "print(\"   Posterior: Beta(Î± + k, Î² + n - k)\")\n",
    "print(\"   Use: Win rate, default probability, conversion rates\")\n",
    "\n",
    "alpha_prior, beta_prior = 2, 2  # Weakly informative prior\n",
    "n_trials, successes = 50, 35\n",
    "\n",
    "# Prior and Posterior\n",
    "prior_beta = stats.beta(alpha_prior, beta_prior)\n",
    "post_beta = stats.beta(alpha_prior + successes, beta_prior + n_trials - successes)\n",
    "\n",
    "axes[0].fill_between(x, prior_beta.pdf(x), alpha=0.3, color='blue', label=f'Prior: Beta({alpha_prior},{beta_prior})')\n",
    "axes[0].fill_between(x, post_beta.pdf(x), alpha=0.3, color='red', \n",
    "                      label=f'Posterior: Beta({alpha_prior+successes},{beta_prior+n_trials-successes})')\n",
    "axes[0].plot(x, prior_beta.pdf(x), 'b-', linewidth=2)\n",
    "axes[0].plot(x, post_beta.pdf(x), 'r-', linewidth=2)\n",
    "axes[0].axvline(successes/n_trials, color='green', linestyle='--', label=f'MLE: {successes/n_trials:.2f}')\n",
    "axes[0].set_xlabel('Î¸ (Success Probability)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Beta-Binomial\\n(Binary Outcomes)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# ===== 2. Normal-Normal (Mean estimation with known variance) =====\n",
    "# Use case: Return estimation, parameter updates\n",
    "print(\"\\n2ï¸âƒ£ NORMAL-NORMAL CONJUGATE PAIR\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Likelihood: Normal(Î¼, ÏƒÂ²) [Ïƒ known]\")\n",
    "print(\"   Prior: Normal(Î¼â‚€, Ïƒâ‚€Â²)\")\n",
    "print(\"   Posterior: Normal(Î¼â‚™, Ïƒâ‚™Â²)\")\n",
    "print(\"   Use: Return estimation, value estimation\")\n",
    "\n",
    "# Prior parameters\n",
    "mu_prior, sigma_prior = 0.05, 0.10  # Prior: mean=5%, std=10%\n",
    "sigma_known = 0.20  # Known observation noise\n",
    "\n",
    "# Observed data\n",
    "data = np.array([0.08, 0.12, -0.02, 0.15, 0.06, 0.10, 0.03])\n",
    "n = len(data)\n",
    "x_bar = np.mean(data)\n",
    "\n",
    "# Posterior calculation\n",
    "precision_prior = 1 / sigma_prior**2\n",
    "precision_data = n / sigma_known**2\n",
    "precision_post = precision_prior + precision_data\n",
    "\n",
    "mu_post = (precision_prior * mu_prior + precision_data * x_bar) / precision_post\n",
    "sigma_post = np.sqrt(1 / precision_post)\n",
    "\n",
    "x_norm = np.linspace(-0.3, 0.4, 200)\n",
    "prior_norm = stats.norm(mu_prior, sigma_prior)\n",
    "post_norm = stats.norm(mu_post, sigma_post)\n",
    "\n",
    "axes[1].fill_between(x_norm, prior_norm.pdf(x_norm), alpha=0.3, color='blue', \n",
    "                      label=f'Prior: N({mu_prior:.2f}, {sigma_prior:.2f}Â²)')\n",
    "axes[1].fill_between(x_norm, post_norm.pdf(x_norm), alpha=0.3, color='red',\n",
    "                      label=f'Posterior: N({mu_post:.2f}, {sigma_post:.2f}Â²)')\n",
    "axes[1].plot(x_norm, prior_norm.pdf(x_norm), 'b-', linewidth=2)\n",
    "axes[1].plot(x_norm, post_norm.pdf(x_norm), 'r-', linewidth=2)\n",
    "axes[1].axvline(x_bar, color='green', linestyle='--', label=f'Sample Mean: {x_bar:.2f}')\n",
    "axes[1].set_xlabel('Î¼ (Mean Return)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title('Normal-Normal\\n(Mean Estimation)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "# ===== 3. Gamma-Poisson (Count data) =====\n",
    "# Use case: Trade arrival rate, event frequency\n",
    "print(\"\\n3ï¸âƒ£ GAMMA-POISSON CONJUGATE PAIR\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Likelihood: Poisson(k | Î»)\")\n",
    "print(\"   Prior: Gamma(Î±, Î²)\")\n",
    "print(\"   Posterior: Gamma(Î± + Î£k, Î² + n)\")\n",
    "print(\"   Use: Trade frequency, event counts, order arrivals\")\n",
    "\n",
    "alpha_gamma, beta_gamma = 3, 1  # Prior\n",
    "observed_counts = np.array([5, 8, 6, 7, 9, 4, 6])  # Daily trade counts\n",
    "n_obs = len(observed_counts)\n",
    "sum_counts = np.sum(observed_counts)\n",
    "\n",
    "# Posterior\n",
    "alpha_post = alpha_gamma + sum_counts\n",
    "beta_post = beta_gamma + n_obs\n",
    "\n",
    "x_gamma = np.linspace(0.01, 15, 200)\n",
    "prior_gamma = stats.gamma(alpha_gamma, scale=1/beta_gamma)\n",
    "post_gamma = stats.gamma(alpha_post, scale=1/beta_post)\n",
    "\n",
    "axes[2].fill_between(x_gamma, prior_gamma.pdf(x_gamma), alpha=0.3, color='blue',\n",
    "                      label=f'Prior: Gamma({alpha_gamma}, {beta_gamma})')\n",
    "axes[2].fill_between(x_gamma, post_gamma.pdf(x_gamma), alpha=0.3, color='red',\n",
    "                      label=f'Posterior: Gamma({alpha_post}, {beta_post})')\n",
    "axes[2].plot(x_gamma, prior_gamma.pdf(x_gamma), 'b-', linewidth=2)\n",
    "axes[2].plot(x_gamma, post_gamma.pdf(x_gamma), 'r-', linewidth=2)\n",
    "axes[2].axvline(np.mean(observed_counts), color='green', linestyle='--', \n",
    "                label=f'MLE: {np.mean(observed_counts):.2f}')\n",
    "axes[2].set_xlabel('Î» (Rate Parameter)', fontsize=11)\n",
    "axes[2].set_ylabel('Density', fontsize=11)\n",
    "axes[2].set_title('Gamma-Poisson\\n(Count Data)', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle('Conjugate Prior Pairs: Prior (Blue) â†’ Posterior (Red)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š CONJUGATE PRIOR REFERENCE TABLE\")\n",
    "print(\"=\" * 60)\n",
    "conjugate_table = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Likelihood      â”‚ Prior           â”‚ Posterior       â”‚ Finance Application  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Binomial        â”‚ Beta            â”‚ Beta            â”‚ Win rate, hit ratio  â”‚\n",
    "â”‚ Normal (Î¼)      â”‚ Normal          â”‚ Normal          â”‚ Expected returns     â”‚\n",
    "â”‚ Normal (ÏƒÂ²)     â”‚ Inv-Gamma       â”‚ Inv-Gamma       â”‚ Volatility           â”‚\n",
    "â”‚ Poisson         â”‚ Gamma           â”‚ Gamma           â”‚ Trade frequency      â”‚\n",
    "â”‚ Exponential     â”‚ Gamma           â”‚ Gamma           â”‚ Time between trades  â”‚\n",
    "â”‚ Multinomial     â”‚ Dirichlet       â”‚ Dirichlet       â”‚ Regime probabilities â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(conjugate_table)\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Conjugacy = prior & posterior same distribution family\")\n",
    "print(\"   2. Enables closed-form posterior (no MCMC needed)\")\n",
    "print(\"   3. Computationally efficient for real-time updates\")\n",
    "print(\"   4. Prior hyperparameters = pseudo-observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baa120",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"What is MAP estimation and how does it differ from MLE? When would you prefer one over the other?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **MLE:** $\\hat{\\theta}_{MLE} = \\arg\\max_\\theta P(D|\\theta)$\n",
    "- **MAP:** $\\hat{\\theta}_{MAP} = \\arg\\max_\\theta P(\\theta|D) = \\arg\\max_\\theta P(D|\\theta)P(\\theta)$\n",
    "- **Regularization:** MAP with Gaussian prior = L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d60c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MAP VS MLE: Point Estimation Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate data with small sample size to show MAP benefits\n",
    "np.random.seed(42)\n",
    "true_theta = 0.3  # True success probability (rare event)\n",
    "n_samples = 10\n",
    "data = np.random.binomial(1, true_theta, n_samples)\n",
    "successes = sum(data)\n",
    "failures = n_samples - successes\n",
    "\n",
    "print(f\"\\nğŸ“Š Scenario: Estimating rare event probability\")\n",
    "print(f\"   True probability: {true_theta}\")\n",
    "print(f\"   Observed: {successes} successes out of {n_samples} trials\")\n",
    "\n",
    "# MLE Estimation\n",
    "theta_mle = successes / n_samples\n",
    "print(f\"\\n1ï¸âƒ£ MLE Estimate: {theta_mle:.4f}\")\n",
    "print(f\"   Formula: k/n = {successes}/{n_samples}\")\n",
    "\n",
    "# MAP Estimation with Beta prior\n",
    "def neg_log_posterior(theta, alpha, beta, k, n):\n",
    "    \"\"\"Negative log posterior for Beta-Binomial model.\"\"\"\n",
    "    if theta <= 0 or theta >= 1:\n",
    "        return np.inf\n",
    "    log_likelihood = k * np.log(theta) + (n - k) * np.log(1 - theta)\n",
    "    log_prior = (alpha - 1) * np.log(theta) + (beta - 1) * np.log(1 - theta)\n",
    "    return -(log_likelihood + log_prior)\n",
    "\n",
    "# Try different priors\n",
    "priors = {\n",
    "    'Uniform Beta(1,1)': (1, 1),\n",
    "    'Informative Beta(3,7)': (3, 7),  # Prior belief: ~30% success\n",
    "    'Skeptical Beta(2,8)': (2, 8),    # Prior belief: ~20% success\n",
    "}\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ MAP Estimates with Different Priors:\")\n",
    "map_estimates = {}\n",
    "\n",
    "for name, (alpha, beta) in priors.items():\n",
    "    # Analytical MAP for Beta-Binomial\n",
    "    theta_map = (alpha + successes - 1) / (alpha + beta + n_samples - 2)\n",
    "    map_estimates[name] = theta_map\n",
    "    print(f\"   {name}: Î¸_MAP = {theta_map:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Different estimators comparison\n",
    "theta_range = np.linspace(0.001, 0.999, 200)\n",
    "\n",
    "# Likelihood function\n",
    "log_likelihood = successes * np.log(theta_range) + failures * np.log(1 - theta_range)\n",
    "likelihood = np.exp(log_likelihood - np.max(log_likelihood))\n",
    "likelihood /= np.max(likelihood)\n",
    "\n",
    "axes[0].plot(theta_range, likelihood, 'k-', linewidth=2, label='Likelihood')\n",
    "axes[0].axvline(theta_mle, color='blue', linestyle='--', linewidth=2, label=f'MLE: {theta_mle:.3f}')\n",
    "axes[0].axvline(true_theta, color='green', linestyle='-', linewidth=2, label=f'True: {true_theta:.3f}')\n",
    "\n",
    "colors = ['red', 'orange', 'purple']\n",
    "for (name, theta_map), color in zip(map_estimates.items(), colors):\n",
    "    axes[0].axvline(theta_map, color=color, linestyle=':', linewidth=2, label=f'MAP {name.split()[0]}: {theta_map:.3f}')\n",
    "\n",
    "axes[0].set_xlabel('Î¸', fontsize=12)\n",
    "axes[0].set_ylabel('Normalized Likelihood', fontsize=12)\n",
    "axes[0].set_title('MLE vs MAP Estimates', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=8, loc='upper right')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Right: MAP as regularization in regression\n",
    "# Ridge regression = MAP with Gaussian prior\n",
    "np.random.seed(42)\n",
    "n_points = 20\n",
    "X = np.random.randn(n_points, 5)  # 5 features, small dataset\n",
    "true_weights = np.array([1.0, -0.5, 0.2, 0, 0])  # Sparse true weights\n",
    "noise = np.random.randn(n_points) * 0.5\n",
    "y = X @ true_weights + noise\n",
    "\n",
    "# OLS (MLE)\n",
    "w_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "\n",
    "# Ridge (MAP with Gaussian prior)\n",
    "from sklearn.linear_model import Ridge\n",
    "lambdas = [0.01, 0.1, 1.0, 10.0]\n",
    "ridge_weights = {}\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam, fit_intercept=False)\n",
    "    ridge.fit(X, y)\n",
    "    ridge_weights[lam] = ridge.coef_\n",
    "\n",
    "# Plot weight comparison\n",
    "x_pos = np.arange(5)\n",
    "width = 0.15\n",
    "\n",
    "axes[1].bar(x_pos - 2*width, true_weights, width, label='True Weights', color='green', alpha=0.8)\n",
    "axes[1].bar(x_pos - width, w_ols, width, label='OLS (MLE)', color='blue', alpha=0.8)\n",
    "for i, lam in enumerate([0.1, 1.0]):\n",
    "    axes[1].bar(x_pos + i*width, ridge_weights[lam], width, \n",
    "                label=f'Ridge Î»={lam} (MAP)', alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Feature Index', fontsize=12)\n",
    "axes[1].set_ylabel('Weight Value', fontsize=12)\n",
    "axes[1].set_title('MAP = Regularization in Regression', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].axhline(0, color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š MAP VS MLE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "comparison_table = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect             â”‚ MLE                     â”‚ MAP                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Formula            â”‚ argmax P(D|Î¸)           â”‚ argmax P(Î¸|D)           â”‚\n",
    "â”‚ Prior              â”‚ None (implicit uniform) â”‚ Explicit prior P(Î¸)     â”‚\n",
    "â”‚ Small samples      â”‚ Overfits                â”‚ Regularized estimate    â”‚\n",
    "â”‚ Large samples      â”‚ Consistent              â”‚ Converges to MLE        â”‚\n",
    "â”‚ Equivalent to      â”‚ No regularization       â”‚ L2 regularization       â”‚\n",
    "â”‚ Use case           â”‚ Abundant data           â”‚ Limited data + beliefs  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(comparison_table)\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. MLE maximizes likelihood; MAP maximizes posterior\")\n",
    "print(\"   2. MAP includes prior beliefs, acts as regularization\")\n",
    "print(\"   3. Gaussian prior on weights â†’ L2 (Ridge) regularization\")\n",
    "print(\"   4. Laplace prior on weights â†’ L1 (Lasso) regularization\")\n",
    "print(\"   5. With infinite data, MAP â†’ MLE (prior becomes irrelevant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684fb60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Bayesian vs Frequentist Approach\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Compare and contrast Bayesian and Frequentist approaches to statistics. When would you use each in quantitative finance?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Philosophical Difference:** Probability as degree of belief vs long-run frequency\n",
    "- **Parameters:** Random (Bayesian) vs fixed but unknown (Frequentist)\n",
    "- **Inference:** Posterior distribution vs point estimates + confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630af21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN VS FREQUENTIST: Side-by-Side Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Estimate expected return of a strategy\n",
    "np.random.seed(42)\n",
    "true_mu = 0.08  # 8% annual return\n",
    "true_sigma = 0.20  # 20% volatility\n",
    "n_observations = 30  # Monthly returns for 2.5 years\n",
    "\n",
    "# Generate monthly returns\n",
    "monthly_returns = np.random.normal(true_mu/12, true_sigma/np.sqrt(12), n_observations)\n",
    "annual_return_est = np.mean(monthly_returns) * 12\n",
    "annual_vol_est = np.std(monthly_returns) * np.sqrt(12)\n",
    "\n",
    "print(f\"\\nğŸ“Š Scenario: Estimate Strategy's Expected Annual Return\")\n",
    "print(f\"   True Î¼ = {true_mu:.1%}, True Ïƒ = {true_sigma:.1%}\")\n",
    "print(f\"   Sample: {n_observations} monthly returns\")\n",
    "print(f\"   Sample mean (annualized): {annual_return_est:.2%}\")\n",
    "\n",
    "# ============== FREQUENTIST APPROACH ==============\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”µ FREQUENTIST APPROACH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Point estimate\n",
    "mu_freq = np.mean(monthly_returns) * 12\n",
    "\n",
    "# Standard error\n",
    "se_freq = (np.std(monthly_returns, ddof=1) / np.sqrt(n_observations)) * 12\n",
    "\n",
    "# 95% Confidence Interval\n",
    "from scipy.stats import t as t_dist\n",
    "t_critical = t_dist.ppf(0.975, df=n_observations-1)\n",
    "ci_freq = (mu_freq - t_critical * se_freq, mu_freq + t_critical * se_freq)\n",
    "\n",
    "print(f\"   Point Estimate: Î¼ = {mu_freq:.2%}\")\n",
    "print(f\"   Standard Error: {se_freq:.2%}\")\n",
    "print(f\"   95% Confidence Interval: [{ci_freq[0]:.2%}, {ci_freq[1]:.2%}]\")\n",
    "print(f\"\\n   âš ï¸ Interpretation: If we repeated this experiment many times,\")\n",
    "print(f\"      95% of intervals would contain the true parameter.\")\n",
    "print(f\"      We CANNOT say there's a 95% probability Î¼ is in this interval.\")\n",
    "\n",
    "# ============== BAYESIAN APPROACH ==============\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸŸ¢ BAYESIAN APPROACH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prior: Informed by historical market data\n",
    "# Assume historical equity premium ~6-8% with uncertainty\n",
    "mu_prior = 0.07  # 7% prior expected return\n",
    "sigma_prior = 0.05  # Prior uncertainty on mean\n",
    "\n",
    "# Known volatility (simplification)\n",
    "sigma_known = true_sigma\n",
    "\n",
    "# Bayesian update (Normal-Normal conjugate)\n",
    "precision_prior = 1 / sigma_prior**2\n",
    "precision_data = n_observations / (sigma_known/np.sqrt(12))**2  # Monthly data precision\n",
    "\n",
    "# Posterior parameters\n",
    "precision_post = precision_prior + precision_data\n",
    "mu_post = (precision_prior * mu_prior + precision_data * annual_return_est) / precision_post\n",
    "sigma_post = np.sqrt(1 / precision_post)\n",
    "\n",
    "# 95% Credible Interval\n",
    "ci_bayes = stats.norm(mu_post, sigma_post).interval(0.95)\n",
    "\n",
    "print(f\"   Prior: Î¼ ~ N({mu_prior:.1%}, {sigma_prior:.1%}Â²)\")\n",
    "print(f\"   Posterior: Î¼ ~ N({mu_post:.2%}, {sigma_post:.2%}Â²)\")\n",
    "print(f\"   95% Credible Interval: [{ci_bayes[0]:.2%}, {ci_bayes[1]:.2%}]\")\n",
    "print(f\"\\n   âœ… Interpretation: Given the data and prior,\")\n",
    "print(f\"      there is a 95% probability Î¼ lies in this interval.\")\n",
    "\n",
    "# Additional Bayesian insights\n",
    "prob_positive = 1 - stats.norm(mu_post, sigma_post).cdf(0)\n",
    "prob_beat_rf = 1 - stats.norm(mu_post, sigma_post).cdf(0.04)  # Beat 4% risk-free\n",
    "print(f\"\\n   P(Î¼ > 0) = {prob_positive:.1%}\")\n",
    "print(f\"   P(Î¼ > 4% risk-free) = {prob_beat_rf:.1%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Frequentist CI interpretation\n",
    "ax = axes[0]\n",
    "np.random.seed(123)\n",
    "n_simulations = 20\n",
    "cis = []\n",
    "contains_true = []\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    sample = np.random.normal(true_mu/12, true_sigma/np.sqrt(12), n_observations)\n",
    "    mean_i = np.mean(sample) * 12\n",
    "    se_i = (np.std(sample, ddof=1) / np.sqrt(n_observations)) * 12\n",
    "    ci_i = (mean_i - t_critical * se_i, mean_i + t_critical * se_i)\n",
    "    cis.append((mean_i, ci_i))\n",
    "    contains_true.append(ci_i[0] <= true_mu <= ci_i[1])\n",
    "\n",
    "for i, (mean_i, ci_i) in enumerate(cis):\n",
    "    color = 'green' if contains_true[i] else 'red'\n",
    "    ax.plot([ci_i[0], ci_i[1]], [i, i], color=color, linewidth=2)\n",
    "    ax.scatter([mean_i], [i], color=color, s=30, zorder=5)\n",
    "\n",
    "ax.axvline(true_mu, color='blue', linestyle='--', linewidth=2, label=f'True Î¼ = {true_mu:.1%}')\n",
    "ax.set_xlabel('Estimated Return', fontsize=11)\n",
    "ax.set_ylabel('Simulation', fontsize=11)\n",
    "ax.set_title(f'Frequentist: 95% CI Coverage\\n{sum(contains_true)}/{n_simulations} contain true Î¼', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Bayesian posterior with prior\n",
    "ax = axes[1]\n",
    "x = np.linspace(-0.1, 0.25, 200)\n",
    "\n",
    "prior_dist = stats.norm(mu_prior, sigma_prior)\n",
    "post_dist = stats.norm(mu_post, sigma_post)\n",
    "\n",
    "ax.fill_between(x, prior_dist.pdf(x), alpha=0.3, color='blue', label=f'Prior N({mu_prior:.0%}, {sigma_prior:.0%}Â²)')\n",
    "ax.fill_between(x, post_dist.pdf(x), alpha=0.3, color='green', label=f'Posterior N({mu_post:.1%}, {sigma_post:.1%}Â²)')\n",
    "ax.plot(x, prior_dist.pdf(x), 'b-', linewidth=2)\n",
    "ax.plot(x, post_dist.pdf(x), 'g-', linewidth=2)\n",
    "ax.axvline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True Î¼ = {true_mu:.0%}')\n",
    "ax.axvline(mu_post, color='green', linestyle=':', linewidth=2, label=f'Posterior mean')\n",
    "ax.axvspan(ci_bayes[0], ci_bayes[1], alpha=0.2, color='green', label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Expected Return (Î¼)', fontsize=11)\n",
    "ax.set_ylabel('Density', fontsize=11)\n",
    "ax.set_title('Bayesian: Prior â†’ Posterior Update', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 3: Comparison summary\n",
    "ax = axes[2]\n",
    "comparison_data = {\n",
    "    'Parameters': ['Point Est.', '95% Lower', '95% Upper', 'Width'],\n",
    "    'Frequentist': [mu_freq, ci_freq[0], ci_freq[1], ci_freq[1]-ci_freq[0]],\n",
    "    'Bayesian': [mu_post, ci_bayes[0], ci_bayes[1], ci_bayes[1]-ci_bayes[0]],\n",
    "}\n",
    "\n",
    "x_pos = np.arange(4)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, comparison_data['Frequentist'], width, label='Frequentist', color='blue', alpha=0.7)\n",
    "bars2 = ax.bar(x_pos + width/2, comparison_data['Bayesian'], width, label='Bayesian', color='green', alpha=0.7)\n",
    "\n",
    "ax.axhline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True Î¼ = {true_mu:.1%}')\n",
    "ax.set_ylabel('Value', fontsize=11)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(comparison_data['Parameters'])\n",
    "ax.set_title('Method Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison Table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š BAYESIAN VS FREQUENTIST COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect              â”‚ Frequentist             â”‚ Bayesian                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Parameters          â”‚ Fixed, unknown          â”‚ Random variables        â”‚\n",
    "â”‚ Probability         â”‚ Long-run frequency      â”‚ Degree of belief        â”‚\n",
    "â”‚ Prior knowledge     â”‚ Not used                â”‚ Incorporated via prior  â”‚\n",
    "â”‚ Uncertainty         â”‚ Confidence intervals    â”‚ Posterior distribution  â”‚\n",
    "â”‚ Interpretation      â”‚ Repeated sampling       â”‚ Direct probability      â”‚\n",
    "â”‚ Computation         â”‚ Often analytical        â”‚ May need MCMC           â”‚\n",
    "â”‚ Small samples       â”‚ Unreliable              â”‚ Prior provides info     â”‚\n",
    "â”‚ Large samples       â”‚ Both converge           â”‚ Both converge           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Frequentist: parameters fixed; probability = long-run frequency\")\n",
    "print(\"   2. Bayesian: parameters random; probability = degree of belief\")  \n",
    "print(\"   3. Bayesian naturally incorporates prior knowledge\")\n",
    "print(\"   4. Credible intervals have direct probabilistic interpretation\")\n",
    "print(\"   5. Finance: Bayesian better for rare events, regime changes, small samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdf42a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6: Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain MCMC and the Metropolis-Hastings algorithm. Why is it used in Bayesian inference?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Purpose:** Sample from complex posterior distributions\n",
    "- **Markov Chain:** Sequence where next state depends only on current state\n",
    "- **Metropolis-Hastings:** Accept/reject proposals based on posterior ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249cf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MCMC: Metropolis-Hastings Algorithm from Scratch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def metropolis_hastings(log_target, initial_state, proposal_std, n_samples, burn_in=1000):\n",
    "    \"\"\"\n",
    "    Metropolis-Hastings MCMC sampler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    log_target : callable - Log of target distribution (unnormalized posterior)\n",
    "    initial_state : float or array - Starting point\n",
    "    proposal_std : float - Standard deviation of Gaussian proposal\n",
    "    n_samples : int - Number of samples to draw\n",
    "    burn_in : int - Samples to discard at beginning\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    samples : array - MCMC samples from target distribution\n",
    "    acceptance_rate : float - Proportion of accepted proposals\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    current = initial_state\n",
    "    current_log_prob = log_target(current)\n",
    "    n_accepted = 0\n",
    "    \n",
    "    for i in range(n_samples + burn_in):\n",
    "        # Propose new state (symmetric Gaussian proposal)\n",
    "        proposal = current + np.random.normal(0, proposal_std)\n",
    "        proposal_log_prob = log_target(proposal)\n",
    "        \n",
    "        # Calculate acceptance probability\n",
    "        log_alpha = proposal_log_prob - current_log_prob\n",
    "        \n",
    "        # Accept or reject\n",
    "        if np.log(np.random.random()) < log_alpha:\n",
    "            current = proposal\n",
    "            current_log_prob = proposal_log_prob\n",
    "            if i >= burn_in:\n",
    "                n_accepted += 1\n",
    "        \n",
    "        if i >= burn_in:\n",
    "            samples.append(current)\n",
    "    \n",
    "    acceptance_rate = n_accepted / n_samples\n",
    "    return np.array(samples), acceptance_rate\n",
    "\n",
    "\n",
    "# Example: Estimate mean of returns with unknown mean AND variance\n",
    "# Target posterior: p(Î¼, Ïƒ | data) âˆ p(data | Î¼, Ïƒ) * p(Î¼) * p(Ïƒ)\n",
    "\n",
    "np.random.seed(42)\n",
    "true_mu = 0.10\n",
    "true_sigma = 0.25\n",
    "data = np.random.normal(true_mu, true_sigma, 50)\n",
    "\n",
    "print(f\"\\nğŸ“Š Problem: Estimate Î¼ and Ïƒ from 50 return observations\")\n",
    "print(f\"   True Î¼ = {true_mu}, True Ïƒ = {true_sigma}\")\n",
    "print(f\"   Sample mean = {np.mean(data):.4f}, Sample std = {np.std(data):.4f}\")\n",
    "\n",
    "# Define log posterior (unnormalized)\n",
    "def log_posterior(params):\n",
    "    \"\"\"Log posterior for Normal model: p(Î¼, Ïƒ | data)\"\"\"\n",
    "    mu, log_sigma = params\n",
    "    sigma = np.exp(log_sigma)  # Transform to ensure Ïƒ > 0\n",
    "    \n",
    "    if sigma <= 0:\n",
    "        return -np.inf\n",
    "    \n",
    "    # Log likelihood: Normal(data | Î¼, Ïƒ)\n",
    "    log_likelihood = np.sum(stats.norm.logpdf(data, mu, sigma))\n",
    "    \n",
    "    # Priors: Î¼ ~ N(0, 1), Ïƒ ~ Exp(1) [via log_Ïƒ ~ Logistic]\n",
    "    log_prior_mu = stats.norm.logpdf(mu, 0, 1)\n",
    "    log_prior_sigma = stats.expon.logpdf(sigma, scale=1) + log_sigma  # Jacobian\n",
    "    \n",
    "    return log_likelihood + log_prior_mu + log_prior_sigma\n",
    "\n",
    "# Run MCMC\n",
    "n_samples = 10000\n",
    "burn_in = 2000\n",
    "initial = np.array([0.0, np.log(0.3)])\n",
    "proposal_std = 0.05\n",
    "\n",
    "print(f\"\\nğŸ”„ Running Metropolis-Hastings...\")\n",
    "print(f\"   Samples: {n_samples}, Burn-in: {burn_in}\")\n",
    "\n",
    "# Run for both parameters together\n",
    "samples_list = []\n",
    "current = initial.copy()\n",
    "current_log_prob = log_posterior(current)\n",
    "n_accepted = 0\n",
    "\n",
    "for i in range(n_samples + burn_in):\n",
    "    proposal = current + np.random.normal(0, proposal_std, size=2)\n",
    "    proposal_log_prob = log_posterior(proposal)\n",
    "    \n",
    "    log_alpha = proposal_log_prob - current_log_prob\n",
    "    \n",
    "    if np.log(np.random.random()) < log_alpha:\n",
    "        current = proposal\n",
    "        current_log_prob = proposal_log_prob\n",
    "        if i >= burn_in:\n",
    "            n_accepted += 1\n",
    "    \n",
    "    if i >= burn_in:\n",
    "        samples_list.append(current.copy())\n",
    "\n",
    "samples = np.array(samples_list)\n",
    "mu_samples = samples[:, 0]\n",
    "sigma_samples = np.exp(samples[:, 1])\n",
    "acceptance_rate = n_accepted / n_samples\n",
    "\n",
    "print(f\"   Acceptance rate: {acceptance_rate:.1%}\")\n",
    "\n",
    "# Results\n",
    "print(f\"\\nğŸ“ˆ Posterior Estimates:\")\n",
    "print(f\"   Î¼: mean = {np.mean(mu_samples):.4f}, std = {np.std(mu_samples):.4f}\")\n",
    "print(f\"      95% CI: [{np.percentile(mu_samples, 2.5):.4f}, {np.percentile(mu_samples, 97.5):.4f}]\")\n",
    "print(f\"   Ïƒ: mean = {np.mean(sigma_samples):.4f}, std = {np.std(sigma_samples):.4f}\")\n",
    "print(f\"      95% CI: [{np.percentile(sigma_samples, 2.5):.4f}, {np.percentile(sigma_samples, 97.5):.4f}]\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Row 1: Î¼ analysis\n",
    "# Trace plot\n",
    "axes[0, 0].plot(mu_samples, alpha=0.7, linewidth=0.5)\n",
    "axes[0, 0].axhline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True Î¼ = {true_mu}')\n",
    "axes[0, 0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Î¼', fontsize=11)\n",
    "axes[0, 0].set_title('Trace Plot (Î¼)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Histogram\n",
    "axes[0, 1].hist(mu_samples, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 1].axvline(true_mu, color='red', linestyle='--', linewidth=2, label=f'True Î¼ = {true_mu}')\n",
    "axes[0, 1].axvline(np.mean(mu_samples), color='green', linestyle=':', linewidth=2, \n",
    "                   label=f'Post. mean = {np.mean(mu_samples):.3f}')\n",
    "axes[0, 1].set_xlabel('Î¼', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Density', fontsize=11)\n",
    "axes[0, 1].set_title('Posterior Distribution (Î¼)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Autocorrelation\n",
    "max_lag = 100\n",
    "autocorr = [np.corrcoef(mu_samples[:-lag], mu_samples[lag:])[0, 1] for lag in range(1, max_lag+1)]\n",
    "axes[0, 2].bar(range(1, max_lag+1), autocorr, alpha=0.7)\n",
    "axes[0, 2].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0, 2].set_xlabel('Lag', fontsize=11)\n",
    "axes[0, 2].set_ylabel('Autocorrelation', fontsize=11)\n",
    "axes[0, 2].set_title('Autocorrelation (Î¼)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Row 2: Ïƒ analysis\n",
    "axes[1, 0].plot(sigma_samples, alpha=0.7, linewidth=0.5, color='orange')\n",
    "axes[1, 0].axhline(true_sigma, color='red', linestyle='--', linewidth=2, label=f'True Ïƒ = {true_sigma}')\n",
    "axes[1, 0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Ïƒ', fontsize=11)\n",
    "axes[1, 0].set_title('Trace Plot (Ïƒ)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].hist(sigma_samples, bins=50, density=True, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 1].axvline(true_sigma, color='red', linestyle='--', linewidth=2, label=f'True Ïƒ = {true_sigma}')\n",
    "axes[1, 1].axvline(np.mean(sigma_samples), color='green', linestyle=':', linewidth=2,\n",
    "                   label=f'Post. mean = {np.mean(sigma_samples):.3f}')\n",
    "axes[1, 1].set_xlabel('Ïƒ', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Density', fontsize=11)\n",
    "axes[1, 1].set_title('Posterior Distribution (Ïƒ)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Joint posterior\n",
    "axes[1, 2].scatter(mu_samples[::10], sigma_samples[::10], alpha=0.3, s=10)\n",
    "axes[1, 2].scatter([true_mu], [true_sigma], color='red', s=100, marker='*', \n",
    "                   label=f'True (Î¼={true_mu}, Ïƒ={true_sigma})', zorder=5)\n",
    "axes[1, 2].scatter([np.mean(mu_samples)], [np.mean(sigma_samples)], color='green', s=100, \n",
    "                   marker='x', linewidths=3, label='Posterior mean', zorder=5)\n",
    "axes[1, 2].set_xlabel('Î¼', fontsize=11)\n",
    "axes[1, 2].set_ylabel('Ïƒ', fontsize=11)\n",
    "axes[1, 2].set_title('Joint Posterior', fontsize=12, fontweight='bold')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. MCMC samples from complex posteriors that can't be solved analytically\")\n",
    "print(\"   2. Metropolis-Hastings: propose â†’ accept/reject based on posterior ratio\")\n",
    "print(\"   3. Burn-in discards initial samples before chain converges\")\n",
    "print(\"   4. Check convergence: trace plots, autocorrelation, R-hat statistic\")\n",
    "print(\"   5. Modern alternatives: Hamiltonian MC (HMC), No-U-Turn Sampler (NUTS)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc818de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7: Bayesian Linear Regression\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain Bayesian Linear Regression. How does it differ from OLS, and what are its advantages?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Weights as Distributions:** $P(w|D)$ instead of point estimates\n",
    "- **Predictive Uncertainty:** Quantify confidence in predictions\n",
    "- **Natural Regularization:** Prior acts as regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e402a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN LINEAR REGRESSION: From Scratch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class BayesianLinearRegression:\n",
    "    \"\"\"\n",
    "    Bayesian Linear Regression with conjugate Normal-InverseGamma prior.\n",
    "    \n",
    "    Model: y = Xw + Îµ, where Îµ ~ N(0, ÏƒÂ²)\n",
    "    Prior: w ~ N(mâ‚€, Sâ‚€), 1/ÏƒÂ² ~ Gamma(aâ‚€, bâ‚€)\n",
    "    \n",
    "    For known ÏƒÂ²:\n",
    "    Posterior: w | D ~ N(mâ‚™, Sâ‚™)\n",
    "    - Sâ‚™ = (Sâ‚€â»Â¹ + Ïƒâ»Â²X'X)â»Â¹\n",
    "    - mâ‚™ = Sâ‚™(Sâ‚€â»Â¹mâ‚€ + Ïƒâ»Â²X'y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, beta=1.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float - Precision of prior on weights (1/ÏƒÂ²_w)\n",
    "        beta : float - Precision of noise (1/ÏƒÂ²_n)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha  # Prior precision\n",
    "        self.beta = beta    # Noise precision\n",
    "        self.m_n = None     # Posterior mean\n",
    "        self.S_n = None     # Posterior covariance\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit Bayesian linear regression.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Prior: w ~ N(0, Î±â»Â¹I)\n",
    "        S_0_inv = self.alpha * np.eye(n_features)\n",
    "        m_0 = np.zeros(n_features)\n",
    "        \n",
    "        # Posterior covariance\n",
    "        self.S_n = np.linalg.inv(S_0_inv + self.beta * X.T @ X)\n",
    "        \n",
    "        # Posterior mean\n",
    "        self.m_n = self.S_n @ (S_0_inv @ m_0 + self.beta * X.T @ y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, return_std=False):\n",
    "        \"\"\"\n",
    "        Make predictions with uncertainty.\n",
    "        \n",
    "        Predictive distribution: p(y*|x*, D) = N(m_n'x*, ÏƒÂ²_pred)\n",
    "        ÏƒÂ²_pred = 1/Î² + x*'Sâ‚™x*\n",
    "        \"\"\"\n",
    "        y_mean = X @ self.m_n\n",
    "        \n",
    "        if return_std:\n",
    "            # Predictive variance = noise variance + weight uncertainty\n",
    "            y_var = 1/self.beta + np.sum(X @ self.S_n * X, axis=1)\n",
    "            return y_mean, np.sqrt(y_var)\n",
    "        \n",
    "        return y_mean\n",
    "    \n",
    "    def sample_weights(self, n_samples=100):\n",
    "        \"\"\"Sample weights from posterior.\"\"\"\n",
    "        return np.random.multivariate_normal(self.m_n, self.S_n, n_samples)\n",
    "\n",
    "\n",
    "# Generate synthetic factor return prediction data\n",
    "np.random.seed(42)\n",
    "n_points = 50\n",
    "\n",
    "# Features: factor exposures\n",
    "X_raw = np.random.randn(n_points, 1)\n",
    "X = np.hstack([np.ones((n_points, 1)), X_raw])  # Add intercept\n",
    "\n",
    "# True relationship\n",
    "true_w = np.array([0.02, 0.5])  # alpha=2%, beta=0.5\n",
    "noise_std = 0.15\n",
    "\n",
    "y = X @ true_w + np.random.randn(n_points) * noise_std\n",
    "\n",
    "print(f\"\\nğŸ“Š Factor Return Prediction Problem\")\n",
    "print(f\"   True weights: intercept={true_w[0]:.2f}, slope={true_w[1]:.2f}\")\n",
    "print(f\"   Noise std: {noise_std}\")\n",
    "print(f\"   Training samples: {n_points}\")\n",
    "\n",
    "# Fit models\n",
    "# 1. OLS\n",
    "ols = LinearRegression(fit_intercept=False)\n",
    "ols.fit(X, y)\n",
    "\n",
    "# 2. Bayesian Linear Regression\n",
    "blr = BayesianLinearRegression(alpha=1.0, beta=1/noise_std**2)\n",
    "blr.fit(X, y)\n",
    "\n",
    "# 3. Sklearn BayesianRidge\n",
    "sklearn_blr = BayesianRidge()\n",
    "sklearn_blr.fit(X_raw, y)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Weight Estimates:\")\n",
    "print(f\"   OLS:      intercept={ols.coef_[0]:.4f}, slope={ols.coef_[1]:.4f}\")\n",
    "print(f\"   Bayesian: intercept={blr.m_n[0]:.4f}, slope={blr.m_n[1]:.4f}\")\n",
    "print(f\"   Weight uncertainty (posterior std):\")\n",
    "print(f\"            intercept_std={np.sqrt(blr.S_n[0,0]):.4f}, slope_std={np.sqrt(blr.S_n[1,1]):.4f}\")\n",
    "\n",
    "# Predictions with uncertainty\n",
    "X_test = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "X_test_full = np.hstack([np.ones((100, 1)), X_test])\n",
    "\n",
    "y_ols = X_test_full @ ols.coef_\n",
    "y_bayes, y_std = blr.predict(X_test_full, return_std=True)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Data and predictions\n",
    "ax = axes[0]\n",
    "ax.scatter(X_raw, y, c='blue', alpha=0.6, label='Training data', s=40)\n",
    "ax.plot(X_test, y_ols, 'g-', linewidth=2, label='OLS fit')\n",
    "ax.plot(X_test, y_bayes, 'r-', linewidth=2, label='Bayesian mean')\n",
    "ax.fill_between(X_test.ravel(), \n",
    "                y_bayes - 2*y_std, \n",
    "                y_bayes + 2*y_std, \n",
    "                color='red', alpha=0.2, label='95% predictive interval')\n",
    "ax.plot(X_test, X_test_full @ true_w, 'k--', linewidth=2, label='True function')\n",
    "ax.set_xlabel('Factor Exposure', fontsize=11)\n",
    "ax.set_ylabel('Return', fontsize=11)\n",
    "ax.set_title('OLS vs Bayesian Regression', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 2: Posterior over weights (sample regression lines)\n",
    "ax = axes[1]\n",
    "ax.scatter(X_raw, y, c='blue', alpha=0.6, s=40)\n",
    "\n",
    "# Sample from weight posterior\n",
    "weight_samples = blr.sample_weights(n_samples=50)\n",
    "for w_sample in weight_samples:\n",
    "    y_sample = X_test_full @ w_sample\n",
    "    ax.plot(X_test, y_sample, 'r-', alpha=0.1, linewidth=1)\n",
    "\n",
    "ax.plot(X_test, X_test_full @ blr.m_n, 'r-', linewidth=2, label='Posterior mean')\n",
    "ax.plot(X_test, X_test_full @ true_w, 'k--', linewidth=2, label='True function')\n",
    "ax.set_xlabel('Factor Exposure', fontsize=11)\n",
    "ax.set_ylabel('Return', fontsize=11)\n",
    "ax.set_title('Posterior Sample Lines\\n(Uncertainty in Weights)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 3: Posterior distribution of weights\n",
    "ax = axes[2]\n",
    "\n",
    "# Create grid for 2D posterior visualization\n",
    "w0_range = np.linspace(blr.m_n[0] - 3*np.sqrt(blr.S_n[0,0]), \n",
    "                        blr.m_n[0] + 3*np.sqrt(blr.S_n[0,0]), 100)\n",
    "w1_range = np.linspace(blr.m_n[1] - 3*np.sqrt(blr.S_n[1,1]), \n",
    "                        blr.m_n[1] + 3*np.sqrt(blr.S_n[1,1]), 100)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "pos = np.dstack((W0, W1))\n",
    "\n",
    "rv = stats.multivariate_normal(blr.m_n, blr.S_n)\n",
    "Z = rv.pdf(pos)\n",
    "\n",
    "ax.contourf(W0, W1, Z, levels=20, cmap='Reds', alpha=0.8)\n",
    "ax.contour(W0, W1, Z, levels=5, colors='darkred', linewidths=0.5)\n",
    "ax.scatter([true_w[0]], [true_w[1]], c='blue', s=100, marker='*', \n",
    "           label=f'True: ({true_w[0]:.2f}, {true_w[1]:.2f})', zorder=5)\n",
    "ax.scatter([blr.m_n[0]], [blr.m_n[1]], c='red', s=100, marker='x', \n",
    "           linewidths=3, label=f'MAP: ({blr.m_n[0]:.2f}, {blr.m_n[1]:.2f})', zorder=5)\n",
    "ax.scatter([ols.coef_[0]], [ols.coef_[1]], c='green', s=100, marker='+', \n",
    "           linewidths=3, label=f'OLS: ({ols.coef_[0]:.2f}, {ols.coef_[1]:.2f})', zorder=5)\n",
    "ax.set_xlabel('wâ‚€ (Intercept)', fontsize=11)\n",
    "ax.set_ylabel('wâ‚ (Slope)', fontsize=11)\n",
    "ax.set_title('Posterior Distribution of Weights', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š OLS VS BAYESIAN LINEAR REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect              â”‚ OLS                     â”‚ Bayesian LR             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Weights             â”‚ Point estimates         â”‚ Probability distributionâ”‚\n",
    "â”‚ Uncertainty         â”‚ SE via residuals        â”‚ Full posterior          â”‚\n",
    "â”‚ Predictions         â”‚ Point predictions       â”‚ Predictive distribution â”‚\n",
    "â”‚ Regularization      â”‚ None (add manually)     â”‚ Built-in via prior      â”‚\n",
    "â”‚ Small data          â”‚ Overfits                â”‚ Prior regularizes       â”‚\n",
    "â”‚ Out-of-sample       â”‚ No uncertainty growth   â”‚ Uncertainty increases   â”‚\n",
    "â”‚ Computation         â”‚ Closed form             â”‚ Closed form (conjugate) â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Bayesian LR gives distribution over weights, not just point estimates\")\n",
    "print(\"   2. Predictive uncertainty grows away from training data\")\n",
    "print(\"   3. Prior precision Î± acts like L2 regularization strength\")\n",
    "print(\"   4. Enables principled uncertainty quantification for risk management\")\n",
    "print(\"   5. Finance use: factor models, return prediction with confidence bands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd6d97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 8: Naive Bayes Classifier\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain the Naive Bayes classifier. What is the 'naive' assumption, and when does it work well?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Naive Assumption:** Features are conditionally independent given class\n",
    "- **Formula:** $P(y|x_1,...,x_n) \\propto P(y) \\prod_i P(x_i|y)$\n",
    "- **Variants:** Gaussian, Multinomial, Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05179c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NAIVE BAYES: From Scratch Implementation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class GaussianNaiveBayes:\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes classifier from scratch.\n",
    "    \n",
    "    Assumes: P(x_i | y) ~ Normal(Î¼_yi, Ïƒ_yiÂ²)\n",
    "    \n",
    "    Classification: argmax_y P(y) * âˆ_i P(x_i | y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = {}\n",
    "        self.means = {}\n",
    "        self.stds = {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Estimate class priors and class-conditional feature distributions.\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Class prior: P(y=c)\n",
    "            self.class_priors[c] = len(X_c) / n_samples\n",
    "            \n",
    "            # Class-conditional means and stds: P(x_i | y=c) ~ N(Î¼_ci, Ïƒ_ci)\n",
    "            self.means[c] = np.mean(X_c, axis=0)\n",
    "            self.stds[c] = np.std(X_c, axis=0) + 1e-9  # Add small constant for stability\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _log_likelihood(self, X, c):\n",
    "        \"\"\"Calculate log P(X | y=c) assuming independence.\"\"\"\n",
    "        # Log of product = sum of logs\n",
    "        # log P(x | y=c) = Î£_i log N(x_i | Î¼_ci, Ïƒ_ci)\n",
    "        log_probs = -0.5 * np.sum(((X - self.means[c]) / self.stds[c])**2, axis=1)\n",
    "        log_probs -= np.sum(np.log(self.stds[c]))\n",
    "        return log_probs\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Calculate posterior probabilities for each class.\"\"\"\n",
    "        log_posteriors = []\n",
    "        \n",
    "        for c in self.classes:\n",
    "            log_prior = np.log(self.class_priors[c])\n",
    "            log_likelihood = self._log_likelihood(X, c)\n",
    "            log_posteriors.append(log_prior + log_likelihood)\n",
    "        \n",
    "        log_posteriors = np.array(log_posteriors).T\n",
    "        \n",
    "        # Normalize (softmax)\n",
    "        max_log = np.max(log_posteriors, axis=1, keepdims=True)\n",
    "        log_posteriors -= max_log\n",
    "        posteriors = np.exp(log_posteriors)\n",
    "        posteriors /= np.sum(posteriors, axis=1, keepdims=True)\n",
    "        \n",
    "        return posteriors\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        posteriors = self.predict_proba(X)\n",
    "        return self.classes[np.argmax(posteriors, axis=1)]\n",
    "\n",
    "\n",
    "# Application: Market Regime Classification\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic market data\n",
    "# Features: momentum, volatility, volume\n",
    "n_per_class = 200\n",
    "\n",
    "# Class 0: Bear market (low momentum, high vol)\n",
    "X_bear = np.random.multivariate_normal(\n",
    "    mean=[-0.5, 0.3, -0.2],\n",
    "    cov=[[0.1, 0.02, 0], [0.02, 0.1, 0], [0, 0, 0.1]],\n",
    "    size=n_per_class\n",
    ")\n",
    "\n",
    "# Class 1: Bull market (high momentum, low vol)\n",
    "X_bull = np.random.multivariate_normal(\n",
    "    mean=[0.5, -0.2, 0.3],\n",
    "    cov=[[0.1, -0.02, 0], [-0.02, 0.1, 0], [0, 0, 0.1]],\n",
    "    size=n_per_class\n",
    ")\n",
    "\n",
    "# Class 2: Sideways (neutral)\n",
    "X_sideways = np.random.multivariate_normal(\n",
    "    mean=[0, 0, 0],\n",
    "    cov=[[0.08, 0, 0], [0, 0.08, 0], [0, 0, 0.08]],\n",
    "    size=n_per_class\n",
    ")\n",
    "\n",
    "X = np.vstack([X_bear, X_bull, X_sideways])\n",
    "y = np.array([0]*n_per_class + [1]*n_per_class + [2]*n_per_class)\n",
    "feature_names = ['Momentum', 'Volatility', 'Volume']\n",
    "class_names = ['Bear', 'Bull', 'Sideways']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nğŸ“Š Market Regime Classification Problem\")\n",
    "print(f\"   Features: {feature_names}\")\n",
    "print(f\"   Classes: {class_names}\")\n",
    "print(f\"   Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "# Fit models\n",
    "# 1. Our implementation\n",
    "gnb_scratch = GaussianNaiveBayes()\n",
    "gnb_scratch.fit(X_train, y_train)\n",
    "y_pred_scratch = gnb_scratch.predict(X_test)\n",
    "\n",
    "# 2. Sklearn implementation\n",
    "gnb_sklearn = GaussianNB()\n",
    "gnb_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = gnb_sklearn.predict(X_test)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Results:\")\n",
    "print(f\"   Our implementation accuracy: {accuracy_score(y_test, y_pred_scratch):.2%}\")\n",
    "print(f\"   Sklearn accuracy: {accuracy_score(y_test, y_pred_sklearn):.2%}\")\n",
    "\n",
    "# Show learned parameters\n",
    "print(f\"\\nğŸ“ Learned Parameters (Class-Conditional Distributions):\")\n",
    "for i, c in enumerate(gnb_scratch.classes):\n",
    "    print(f\"\\n   Class {c} ({class_names[c]}):\")\n",
    "    print(f\"      Prior P(y={c}): {gnb_scratch.class_priors[c]:.3f}\")\n",
    "    print(f\"      Feature means: {gnb_scratch.means[c]}\")\n",
    "    print(f\"      Feature stds:  {gnb_scratch.stds[c]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Feature distributions by class\n",
    "ax = axes[0]\n",
    "for i, c in enumerate(gnb_scratch.classes):\n",
    "    X_c = X_train[y_train == c]\n",
    "    for j, feat in enumerate(feature_names):\n",
    "        x_range = np.linspace(X_c[:, j].min() - 0.5, X_c[:, j].max() + 0.5, 100)\n",
    "        y_pdf = stats.norm.pdf(x_range, gnb_scratch.means[c][j], gnb_scratch.stds[c][j])\n",
    "        ax.plot(x_range, y_pdf + j*2, label=f'{class_names[c]}-{feat}' if i==0 else '', \n",
    "                alpha=0.7, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Feature Value', fontsize=11)\n",
    "ax.set_ylabel('Density (stacked)', fontsize=11)\n",
    "ax.set_title('Class-Conditional Feature Distributions', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Plot 2: Decision boundaries (2D projection)\n",
    "ax = axes[1]\n",
    "h = 0.05\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Use mean of third feature for prediction\n",
    "X_grid = np.c_[xx.ravel(), yy.ravel(), np.full(xx.ravel().shape, X[:, 2].mean())]\n",
    "Z = gnb_scratch.predict(X_grid)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "ax.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "colors = ['red', 'blue', 'gray']\n",
    "for i, c in enumerate(gnb_scratch.classes):\n",
    "    ax.scatter(X[y==c, 0], X[y==c, 1], c=colors[i], label=class_names[c], alpha=0.6, s=30)\n",
    "\n",
    "ax.set_xlabel(f'{feature_names[0]}', fontsize=11)\n",
    "ax.set_ylabel(f'{feature_names[1]}', fontsize=11)\n",
    "ax.set_title('Decision Boundaries\\n(Momentum vs Volatility)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Posterior probabilities for test samples\n",
    "ax = axes[2]\n",
    "proba = gnb_scratch.predict_proba(X_test[:30])\n",
    "x_pos = np.arange(30)\n",
    "width = 0.25\n",
    "\n",
    "for i, c in enumerate(gnb_scratch.classes):\n",
    "    ax.bar(x_pos + i*width, proba[:, i], width, label=class_names[c], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Test Sample', fontsize=11)\n",
    "ax.set_ylabel('Posterior Probability', fontsize=11)\n",
    "ax.set_title('Posterior Probabilities\\n(First 30 Test Samples)', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xticks(x_pos[::5] + width)\n",
    "ax.set_xticklabels(x_pos[::5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Explain the naive assumption\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š THE NAIVE ASSUMPTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Full Bayes:     P(y|xâ‚,xâ‚‚,...,xâ‚™) âˆ P(y) Ã— P(xâ‚,xâ‚‚,...,xâ‚™|y)\n",
    "\n",
    "Naive Bayes:    P(y|xâ‚,xâ‚‚,...,xâ‚™) âˆ P(y) Ã— âˆáµ¢ P(xáµ¢|y)\n",
    "\n",
    "The \"naive\" assumption: Features are conditionally independent given class.\n",
    "This means: P(xâ‚,xâ‚‚|y) = P(xâ‚|y) Ã— P(xâ‚‚|y)\n",
    "\n",
    "WHY IT WORKS DESPITE BEING \"WRONG\":\n",
    "1. Classification only needs correct ranking, not exact probabilities\n",
    "2. Feature dependencies often don't change which class has highest posterior\n",
    "3. High-dimensional problems benefit from reduced parameter count\n",
    "4. Works well when class-conditional correlations are similar across classes\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Naive = conditional independence assumption P(X|y) = âˆP(xáµ¢|y)\")\n",
    "print(\"   2. Reduces parameters from O(dÂ²) to O(d) per class\")\n",
    "print(\"   3. Works well for high-dimensional data (text, spam filtering)\")\n",
    "print(\"   4. Fast training and prediction - good for real-time classification\")\n",
    "print(\"   5. Finance: sentiment classification, regime detection, alert systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8fe76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 9: Credible Intervals vs Confidence Intervals\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"Explain the difference between credible intervals and confidence intervals. Why does the distinction matter in practice?\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Confidence Interval:** Frequentist - covers true parameter in X% of repeated experiments\n",
    "- **Credible Interval:** Bayesian - X% probability that parameter lies in interval\n",
    "- **Interpretation:** Direct probability vs long-run frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8890d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREDIBLE INTERVALS VS CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Estimate Sharpe ratio of a strategy\n",
    "np.random.seed(42)\n",
    "\n",
    "# True parameters\n",
    "true_sharpe = 1.2\n",
    "true_mu = 0.15  # 15% annual return\n",
    "true_sigma = true_mu / true_sharpe  # Volatility to achieve target Sharpe\n",
    "\n",
    "# Generate sample returns (monthly)\n",
    "n_months = 36  # 3 years of data\n",
    "monthly_mu = true_mu / 12\n",
    "monthly_sigma = true_sigma / np.sqrt(12)\n",
    "returns = np.random.normal(monthly_mu, monthly_sigma, n_months)\n",
    "\n",
    "# Sample statistics\n",
    "sample_mu = np.mean(returns) * 12  # Annualized\n",
    "sample_sigma = np.std(returns, ddof=1) * np.sqrt(12)\n",
    "sample_sharpe = sample_mu / sample_sigma\n",
    "\n",
    "print(f\"\\nğŸ“Š Scenario: Estimate Strategy Sharpe Ratio\")\n",
    "print(f\"   True Sharpe: {true_sharpe:.2f}\")\n",
    "print(f\"   Sample Sharpe: {sample_sharpe:.2f}\")\n",
    "print(f\"   Sample size: {n_months} months\")\n",
    "\n",
    "# ============== FREQUENTIST: Confidence Interval ==============\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ”µ FREQUENTIST: 95% Confidence Interval\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sharpe ratio SE (Lo, 2002 approximation)\n",
    "# SE(SR) â‰ˆ sqrt((1 + SRÂ²/2) / n)\n",
    "se_sharpe = np.sqrt((1 + sample_sharpe**2 / 2) / n_months)\n",
    "\n",
    "# Confidence interval using t-distribution\n",
    "t_crit = stats.t.ppf(0.975, df=n_months-1)\n",
    "ci_freq = (sample_sharpe - t_crit * se_sharpe, sample_sharpe + t_crit * se_sharpe)\n",
    "\n",
    "print(f\"   Sample Sharpe: {sample_sharpe:.3f}\")\n",
    "print(f\"   Standard Error: {se_sharpe:.3f}\")\n",
    "print(f\"   95% CI: [{ci_freq[0]:.3f}, {ci_freq[1]:.3f}]\")\n",
    "print(f\"\\n   âš ï¸ INTERPRETATION (Frequentist):\")\n",
    "print(f\"   'If we repeated this experiment many times,\")\n",
    "print(f\"    95% of calculated CIs would contain the true Sharpe.'\")\n",
    "print(f\"   We CANNOT say: 'There's 95% probability true Sharpe is in this interval.'\")\n",
    "\n",
    "# ============== BAYESIAN: Credible Interval ==============\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸŸ¢ BAYESIAN: 95% Credible Interval\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use MCMC to get posterior of Sharpe ratio\n",
    "def sharpe_posterior_samples(returns, n_samples=10000):\n",
    "    \"\"\"Sample from posterior of Sharpe ratio using bootstrap-like approach.\"\"\"\n",
    "    # Simple approach: use conjugate Normal-InverseGamma posterior\n",
    "    # Then compute Sharpe = mu / sigma for each posterior sample\n",
    "    \n",
    "    n = len(returns)\n",
    "    x_bar = np.mean(returns)\n",
    "    s2 = np.var(returns, ddof=1)\n",
    "    \n",
    "    # Prior: non-informative\n",
    "    # Posterior for sigma^2: Inv-Chi-squared(n-1, s^2)\n",
    "    # Posterior for mu | sigma^2: Normal(x_bar, sigma^2/n)\n",
    "    \n",
    "    sigma2_samples = (n-1) * s2 / np.random.chisquare(n-1, n_samples)\n",
    "    sigma_samples = np.sqrt(sigma2_samples)\n",
    "    mu_samples = np.random.normal(x_bar, sigma_samples / np.sqrt(n))\n",
    "    \n",
    "    # Annualize\n",
    "    mu_annual = mu_samples * 12\n",
    "    sigma_annual = sigma_samples * np.sqrt(12)\n",
    "    sharpe_samples = mu_annual / sigma_annual\n",
    "    \n",
    "    return sharpe_samples\n",
    "\n",
    "sharpe_samples = sharpe_posterior_samples(returns, n_samples=50000)\n",
    "\n",
    "# Credible intervals\n",
    "hdi_95 = np.percentile(sharpe_samples, [2.5, 97.5])  # Equal-tailed\n",
    "posterior_mean = np.mean(sharpe_samples)\n",
    "posterior_median = np.median(sharpe_samples)\n",
    "\n",
    "print(f\"   Posterior Mean: {posterior_mean:.3f}\")\n",
    "print(f\"   Posterior Median: {posterior_median:.3f}\")\n",
    "print(f\"   95% Equal-Tailed CI: [{hdi_95[0]:.3f}, {hdi_95[1]:.3f}]\")\n",
    "print(f\"\\n   âœ… INTERPRETATION (Bayesian):\")\n",
    "print(f\"   'Given the data and prior, there is 95% probability\")\n",
    "print(f\"    that the true Sharpe ratio lies in this interval.'\")\n",
    "\n",
    "# Additional Bayesian insights\n",
    "prob_positive = np.mean(sharpe_samples > 0)\n",
    "prob_above_1 = np.mean(sharpe_samples > 1.0)\n",
    "prob_above_2 = np.mean(sharpe_samples > 2.0)\n",
    "\n",
    "print(f\"\\n   ğŸ“ˆ Probabilistic Statements:\")\n",
    "print(f\"   P(Sharpe > 0) = {prob_positive:.1%}\")\n",
    "print(f\"   P(Sharpe > 1.0) = {prob_above_1:.1%}\")\n",
    "print(f\"   P(Sharpe > 2.0) = {prob_above_2:.1%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Confidence Interval interpretation (simulation)\n",
    "ax = axes[0]\n",
    "np.random.seed(123)\n",
    "n_experiments = 50\n",
    "cis_list = []\n",
    "contains_true = []\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    sample = np.random.normal(monthly_mu, monthly_sigma, n_months)\n",
    "    sr = np.mean(sample) * 12 / (np.std(sample, ddof=1) * np.sqrt(12))\n",
    "    se = np.sqrt((1 + sr**2 / 2) / n_months)\n",
    "    ci = (sr - t_crit * se, sr + t_crit * se)\n",
    "    cis_list.append((sr, ci))\n",
    "    contains_true.append(ci[0] <= true_sharpe <= ci[1])\n",
    "\n",
    "for i, (sr, ci) in enumerate(cis_list):\n",
    "    color = 'green' if contains_true[i] else 'red'\n",
    "    ax.plot([ci[0], ci[1]], [i, i], color=color, linewidth=1.5)\n",
    "    ax.scatter([sr], [i], color=color, s=20, zorder=5)\n",
    "\n",
    "ax.axvline(true_sharpe, color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'True SR = {true_sharpe:.1f}')\n",
    "coverage = sum(contains_true) / n_experiments\n",
    "ax.set_xlabel('Sharpe Ratio', fontsize=11)\n",
    "ax.set_ylabel('Experiment', fontsize=11)\n",
    "ax.set_title(f'Frequentist CI Coverage\\n{sum(contains_true)}/{n_experiments} = {coverage:.0%} contain true value', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Posterior distribution with credible interval\n",
    "ax = axes[1]\n",
    "ax.hist(sharpe_samples, bins=100, density=True, alpha=0.7, color='green', edgecolor='none')\n",
    "ax.axvline(true_sharpe, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'True SR = {true_sharpe:.2f}')\n",
    "ax.axvline(posterior_mean, color='green', linestyle=':', linewidth=2,\n",
    "           label=f'Posterior mean = {posterior_mean:.2f}')\n",
    "ax.axvspan(hdi_95[0], hdi_95[1], alpha=0.2, color='blue', \n",
    "           label=f'95% Credible Interval')\n",
    "\n",
    "# Add vertical lines at CI bounds\n",
    "ax.axvline(hdi_95[0], color='blue', linestyle='-', linewidth=1.5, alpha=0.7)\n",
    "ax.axvline(hdi_95[1], color='blue', linestyle='-', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Sharpe Ratio', fontsize=11)\n",
    "ax.set_ylabel('Posterior Density', fontsize=11)\n",
    "ax.set_title('Bayesian Posterior Distribution\\nwith 95% Credible Interval', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Plot 3: Comparison of intervals\n",
    "ax = axes[2]\n",
    "methods = ['Frequentist\\nConfidence', 'Bayesian\\nCredible']\n",
    "point_est = [sample_sharpe, posterior_mean]\n",
    "lower = [ci_freq[0], hdi_95[0]]\n",
    "upper = [ci_freq[1], hdi_95[1]]\n",
    "colors = ['blue', 'green']\n",
    "\n",
    "x_pos = np.arange(2)\n",
    "for i in range(2):\n",
    "    ax.errorbar(x_pos[i], point_est[i], \n",
    "                yerr=[[point_est[i] - lower[i]], [upper[i] - point_est[i]]],\n",
    "                fmt='o', markersize=10, color=colors[i], capsize=10, capthick=2,\n",
    "                elinewidth=2)\n",
    "\n",
    "ax.axhline(true_sharpe, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'True Sharpe = {true_sharpe:.1f}')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(methods, fontsize=11)\n",
    "ax.set_ylabel('Sharpe Ratio', fontsize=11)\n",
    "ax.set_title('Interval Comparison', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Add interval width annotations\n",
    "for i in range(2):\n",
    "    width = upper[i] - lower[i]\n",
    "    ax.annotate(f'Width: {width:.2f}', xy=(x_pos[i], lower[i] - 0.1), \n",
    "                ha='center', fontsize=10, color=colors[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š CREDIBLE VS CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect           â”‚ Confidence Interval        â”‚ Credible Interval          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Framework        â”‚ Frequentist                â”‚ Bayesian                   â”‚\n",
    "â”‚ Parameter        â”‚ Fixed, unknown             â”‚ Random variable            â”‚\n",
    "â”‚ Interpretation   â”‚ Long-run coverage          â”‚ Direct probability         â”‚\n",
    "â”‚ Can say          â”‚ \"95% of CIs cover Î¸\"       â”‚ \"95% prob Î¸ in interval\"   â”‚\n",
    "â”‚ Cannot say       â”‚ \"95% prob Î¸ is here\"       â”‚ N/A                        â”‚\n",
    "â”‚ Requires         â”‚ Sampling distribution      â”‚ Prior + Posterior          â”‚\n",
    "â”‚ Width depends on â”‚ Sample size, variance      â”‚ Sample + Prior precision   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Confidence: procedure property; Credible: probability statement\")\n",
    "print(\"   2. CI: 'If repeated, 95% of intervals contain true value'\")\n",
    "print(\"   3. Credible: 'Given data, 95% probability parameter is in interval'\")\n",
    "print(\"   4. Bayesian allows direct probability questions: P(Î¸ > threshold)\")\n",
    "print(\"   5. For risk management, credible intervals are more intuitive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf33cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 10: Bayesian Model Selection\n",
    "\n",
    "### ğŸ“‹ Interview Question:\n",
    "*\"How do you compare models in a Bayesian framework? Explain Bayes factors and information criteria.\"*\n",
    "\n",
    "### ğŸ¯ Key Concepts:\n",
    "- **Bayes Factor:** Ratio of marginal likelihoods $BF_{12} = P(D|M_1) / P(D|M_2)$\n",
    "- **Marginal Likelihood:** Evidence = $\\int P(D|\\theta, M) P(\\theta|M) d\\theta$\n",
    "- **Information Criteria:** BIC approximates log Bayes factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN MODEL SELECTION: Bayes Factors & Information Criteria\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate data from a quadratic relationship\n",
    "np.random.seed(42)\n",
    "n_points = 50\n",
    "x = np.linspace(-3, 3, n_points)\n",
    "noise_std = 1.5\n",
    "\n",
    "# True model: y = 1 + 0.5x + 0.3xÂ²\n",
    "true_a, true_b, true_c = 1.0, 0.5, 0.3\n",
    "y_true = true_a + true_b * x + true_c * x**2\n",
    "y = y_true + np.random.randn(n_points) * noise_std\n",
    "\n",
    "print(f\"\\nğŸ“Š Problem: Which polynomial degree fits best?\")\n",
    "print(f\"   True model: y = {true_a} + {true_b}x + {true_c}xÂ²\")\n",
    "print(f\"   Testing models: Linear (deg=1), Quadratic (deg=2), Cubic (deg=3)\")\n",
    "\n",
    "def fit_polynomial(x, y, degree):\n",
    "    \"\"\"Fit polynomial and return metrics.\"\"\"\n",
    "    X = np.vstack([x**i for i in range(degree + 1)]).T\n",
    "    \n",
    "    # OLS fit\n",
    "    coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    y_pred = X @ coeffs\n",
    "    residuals = y - y_pred\n",
    "    \n",
    "    # Metrics\n",
    "    n = len(y)\n",
    "    k = degree + 1  # Number of parameters\n",
    "    rss = np.sum(residuals**2)\n",
    "    sigma2_mle = rss / n\n",
    "    \n",
    "    # Log-likelihood\n",
    "    log_likelihood = -n/2 * np.log(2 * np.pi * sigma2_mle) - rss / (2 * sigma2_mle)\n",
    "    \n",
    "    # Information criteria\n",
    "    aic = 2 * k - 2 * log_likelihood\n",
    "    bic = k * np.log(n) - 2 * log_likelihood\n",
    "    \n",
    "    # Adjusted RÂ²\n",
    "    tss = np.sum((y - np.mean(y))**2)\n",
    "    r2 = 1 - rss / tss\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)\n",
    "    \n",
    "    return {\n",
    "        'coeffs': coeffs,\n",
    "        'y_pred': y_pred,\n",
    "        'rss': rss,\n",
    "        'log_likelihood': log_likelihood,\n",
    "        'aic': aic,\n",
    "        'bic': bic,\n",
    "        'r2': r2,\n",
    "        'adj_r2': adj_r2,\n",
    "        'n_params': k\n",
    "    }\n",
    "\n",
    "\n",
    "# Fit models of different degrees\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "results = {d: fit_polynomial(x, y, d) for d in degrees}\n",
    "\n",
    "print(\"\\nğŸ“ˆ Model Comparison Results:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Degree':<10} {'Params':<10} {'RÂ²':<10} {'Adj RÂ²':<10} {'AIC':<12} {'BIC':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for d in degrees:\n",
    "    r = results[d]\n",
    "    print(f\"{d:<10} {r['n_params']:<10} {r['r2']:.4f}    {r['adj_r2']:.4f}    {r['aic']:.2f}      {r['bic']:.2f}\")\n",
    "\n",
    "# Find best model by each criterion\n",
    "best_aic = min(degrees, key=lambda d: results[d]['aic'])\n",
    "best_bic = min(degrees, key=lambda d: results[d]['bic'])\n",
    "\n",
    "print(f\"\\nâœ… Best Model Selection:\")\n",
    "print(f\"   By AIC: Degree {best_aic}\")\n",
    "print(f\"   By BIC: Degree {best_bic}\")\n",
    "\n",
    "# Bayes Factor approximation via BIC\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BAYES FACTORS (Approximated via BIC)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reference_model = 2  # Compare against quadratic\n",
    "print(f\"\\nComparing models against Model {reference_model} (Quadratic):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bic_ref = results[reference_model]['bic']\n",
    "for d in degrees:\n",
    "    if d != reference_model:\n",
    "        delta_bic = results[d]['bic'] - bic_ref\n",
    "        # BF â‰ˆ exp(-0.5 * Î”BIC)\n",
    "        log_bf = -0.5 * delta_bic\n",
    "        \n",
    "        # Interpretation\n",
    "        if delta_bic > 10:\n",
    "            evidence = \"Very strong against\"\n",
    "        elif delta_bic > 6:\n",
    "            evidence = \"Strong against\"\n",
    "        elif delta_bic > 2:\n",
    "            evidence = \"Positive against\"\n",
    "        elif delta_bic > -2:\n",
    "            evidence = \"Weak/inconclusive\"\n",
    "        elif delta_bic > -6:\n",
    "            evidence = \"Positive for\"\n",
    "        elif delta_bic > -10:\n",
    "            evidence = \"Strong for\"\n",
    "        else:\n",
    "            evidence = \"Very strong for\"\n",
    "            \n",
    "        print(f\"   Model {d} vs Model {reference_model}: Î”BIC = {delta_bic:.2f}, log(BF) â‰ˆ {log_bf:.2f}\")\n",
    "        print(f\"      â†’ {evidence} model {d}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Data and fitted models\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(x, y, c='black', alpha=0.6, s=40, label='Data')\n",
    "ax.plot(x, y_true, 'k--', linewidth=2, label='True function')\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "x_smooth = np.linspace(x.min(), x.max(), 200)\n",
    "for d, color in zip([1, 2, 3], colors[:3]):\n",
    "    X_smooth = np.vstack([x_smooth**i for i in range(d + 1)]).T\n",
    "    y_smooth = X_smooth @ results[d]['coeffs']\n",
    "    ax.plot(x_smooth, y_smooth, color=color, linewidth=2, label=f'Degree {d}')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Polynomial Fits', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Information criteria comparison\n",
    "ax = axes[0, 1]\n",
    "x_pos = np.arange(len(degrees))\n",
    "width = 0.35\n",
    "\n",
    "aic_values = [results[d]['aic'] for d in degrees]\n",
    "bic_values = [results[d]['bic'] for d in degrees]\n",
    "\n",
    "bars1 = ax.bar(x_pos - width/2, aic_values, width, label='AIC', color='blue', alpha=0.7)\n",
    "bars2 = ax.bar(x_pos + width/2, bic_values, width, label='BIC', color='green', alpha=0.7)\n",
    "\n",
    "# Highlight best models\n",
    "ax.bar(best_aic - 1 - width/2, aic_values[best_aic-1], width, color='blue', edgecolor='red', linewidth=3)\n",
    "ax.bar(best_bic - 1 + width/2, bic_values[best_bic-1], width, color='green', edgecolor='red', linewidth=3)\n",
    "\n",
    "ax.set_xlabel('Polynomial Degree', fontsize=11)\n",
    "ax.set_ylabel('Information Criterion', fontsize=11)\n",
    "ax.set_title('AIC vs BIC Model Selection', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(degrees)\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Bayes Factor interpretation scale\n",
    "ax = axes[1, 0]\n",
    "categories = ['Î”BIC < -10', '-10 to -6', '-6 to -2', '-2 to 2', '2 to 6', '6 to 10', 'Î”BIC > 10']\n",
    "interpretations = ['Very Strong\\nFor', 'Strong\\nFor', 'Positive\\nFor', \n",
    "                   'Weak/\\nInconclusive', 'Positive\\nAgainst', 'Strong\\nAgainst', 'Very Strong\\nAgainst']\n",
    "colors_scale = ['darkgreen', 'green', 'lightgreen', 'gray', 'lightsalmon', 'red', 'darkred']\n",
    "\n",
    "bars = ax.barh(range(len(categories)), [1]*len(categories), color=colors_scale, edgecolor='black')\n",
    "ax.set_yticks(range(len(categories)))\n",
    "ax.set_yticklabels([f'{cat}\\n{interp}' for cat, interp in zip(categories, interpretations)], fontsize=9)\n",
    "ax.set_xlabel('Scale', fontsize=11)\n",
    "ax.set_title('Bayes Factor Evidence Scale\\n(Kass & Raftery Guidelines)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 1.2)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# Plot 4: Out-of-sample prediction\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Generate test data\n",
    "x_test = np.linspace(-4, 4, 100)\n",
    "y_test_true = true_a + true_b * x_test + true_c * x_test**2\n",
    "\n",
    "for d, color in zip([1, 2, 5], ['blue', 'green', 'purple']):\n",
    "    X_test = np.vstack([x_test**i for i in range(d + 1)]).T\n",
    "    y_test_pred = X_test @ results[d]['coeffs']\n",
    "    ax.plot(x_test, y_test_pred, color=color, linewidth=2, \n",
    "            label=f'Degree {d} (BIC={results[d][\"bic\"]:.1f})')\n",
    "\n",
    "ax.plot(x_test, y_test_true, 'k--', linewidth=2, label='True function')\n",
    "ax.scatter(x, y, c='black', alpha=0.4, s=30)\n",
    "ax.axvspan(x.min(), x.max(), alpha=0.1, color='gray', label='Training range')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Extrapolation: Why Model Selection Matters', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylim(-20, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“š BAYESIAN MODEL SELECTION METHODS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. BAYES FACTOR (BF):\n",
    "   - BFâ‚â‚‚ = P(D|Mâ‚) / P(D|Mâ‚‚) = Evidence ratio\n",
    "   - BF > 1: Favor Mâ‚; BF < 1: Favor Mâ‚‚\n",
    "   - Interpretation: Kass & Raftery scale (2log(BF))\n",
    "\n",
    "2. MARGINAL LIKELIHOOD (Evidence):\n",
    "   - P(D|M) = âˆ« P(D|Î¸,M) P(Î¸|M) dÎ¸\n",
    "   - Automatically penalizes complexity\n",
    "   - Hard to compute for complex models\n",
    "\n",
    "3. INFORMATION CRITERIA:\n",
    "   - AIC = 2k - 2log(L) â†’ Prediction focused\n",
    "   - BIC = klog(n) - 2log(L) â†’ Model selection, approximates log(BF)\n",
    "   - BIC penalizes complexity more heavily for large n\n",
    "\n",
    "4. CROSS-VALIDATION (Bayesian LOO):\n",
    "   - WAIC (Widely Applicable IC)\n",
    "   - LOO-CV via Pareto Smoothed Importance Sampling\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Interview Answer Key Points:\")\n",
    "print(\"   1. Bayes factor = ratio of marginal likelihoods\")\n",
    "print(\"   2. Naturally penalizes complexity via Occam's razor\")\n",
    "print(\"   3. BIC â‰ˆ -2 Ã— log(Bayes Factor) for large n\")\n",
    "print(\"   4. AIC: prediction; BIC: model selection (more conservative)\")\n",
    "print(\"   5. Finance: compare factor models, regime-switching specifications\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
