{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67382129",
   "metadata": {},
   "source": [
    "# Day 3: Position Sizing\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement Kelly Criterion\n",
    "- Volatility-based position sizing\n",
    "- Risk budgeting approaches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a60cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ğŸ“š Day 3: Position Sizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ddd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREPARE DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_days = 1500\n",
    "\n",
    "returns = np.random.normal(0.0003, 0.015, n_days)\n",
    "for i in range(1, len(returns)):\n",
    "    returns[i] += 0.05 * returns[i-1]\n",
    "\n",
    "prices = 100 * np.cumprod(1 + returns)\n",
    "df = pd.DataFrame({'price': prices, 'returns': returns})\n",
    "\n",
    "df['ret_5d'] = df['price'].pct_change(5)\n",
    "df['vol_5d'] = df['returns'].rolling(5).std()\n",
    "df['mom_5d'] = df['returns'].rolling(5).sum()\n",
    "df['target'] = (df['returns'].shift(-1) > 0).astype(int)\n",
    "df = df.dropna()\n",
    "\n",
    "feature_cols = ['ret_5d', 'vol_5d', 'mom_5d']\n",
    "X = df[feature_cols].values\n",
    "y = df['target'].values\n",
    "\n",
    "split = int(len(X) * 0.7)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X[:split])\n",
    "X_test = scaler.transform(X[split:])\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_returns = returns[split + 5:split + 5 + len(predictions)]\n",
    "min_len = min(len(test_returns), len(predictions))\n",
    "test_returns = test_returns[:min_len]\n",
    "predictions = predictions[:min_len]\n",
    "probabilities = probabilities[:min_len]\n",
    "\n",
    "print(f\"Test accuracy: {(predictions == y_test[:min_len]).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe317473",
   "metadata": {},
   "source": [
    "## Part 1: Kelly Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd1ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KELLY CRITERION\n",
    "# ============================================================\n",
    "\n",
    "print(\"KELLY CRITERION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def kelly_fraction(win_prob, win_return, loss_return):\n",
    "    \"\"\"\n",
    "    Calculate Kelly fraction.\n",
    "    \n",
    "    f* = (p * b - q) / b\n",
    "    \n",
    "    where:\n",
    "        p = win probability\n",
    "        q = 1 - p\n",
    "        b = win/loss ratio (odds)\n",
    "    \"\"\"\n",
    "    if loss_return == 0:\n",
    "        return 0\n",
    "    \n",
    "    b = abs(win_return / loss_return)\n",
    "    q = 1 - win_prob\n",
    "    \n",
    "    kelly = (win_prob * b - q) / b\n",
    "    \n",
    "    return max(0, kelly)  # Don't go negative (no shorts in basic Kelly)\n",
    "\n",
    "# Estimate from training data\n",
    "train_predictions = model.predict(X_train)\n",
    "train_returns = returns[5:split + 5]\n",
    "\n",
    "# When prediction is UP (1)\n",
    "up_mask = train_predictions == 1\n",
    "up_actual = train_returns[up_mask]\n",
    "up_win_prob = (up_actual > 0).mean()\n",
    "up_win_return = up_actual[up_actual > 0].mean() if (up_actual > 0).any() else 0\n",
    "up_loss_return = up_actual[up_actual <= 0].mean() if (up_actual <= 0).any() else 0\n",
    "\n",
    "kelly_up = kelly_fraction(up_win_prob, up_win_return, up_loss_return)\n",
    "\n",
    "print(f\"When predicting UP:\")\n",
    "print(f\"  Win probability: {up_win_prob:.2%}\")\n",
    "print(f\"  Avg win return: {up_win_return:.4%}\")\n",
    "print(f\"  Avg loss return: {up_loss_return:.4%}\")\n",
    "print(f\"  Kelly fraction: {kelly_up:.2%}\")\n",
    "print(f\"  Half-Kelly: {kelly_up/2:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4256f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROBABILITY-WEIGHTED KELLY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nPROBABILITY-WEIGHTED POSITION SIZING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def probability_kelly_size(prob, base_kelly=1.0, max_position=1.0):\n",
    "    \"\"\"\n",
    "    Scale position by prediction confidence.\n",
    "    \n",
    "    Position = base_kelly * (2 * prob - 1)  # Maps [0,1] to [-1,1]\n",
    "    \"\"\"\n",
    "    # Edge from 0.5\n",
    "    edge = 2 * prob - 1\n",
    "    position = base_kelly * edge\n",
    "    \n",
    "    # Clip to max\n",
    "    position = np.clip(position, -max_position, max_position)\n",
    "    \n",
    "    return position\n",
    "\n",
    "# Apply to test predictions\n",
    "half_kelly = kelly_up / 2\n",
    "positions_kelly = np.array([probability_kelly_size(p, base_kelly=half_kelly) for p in probabilities])\n",
    "\n",
    "print(f\"Position statistics:\")\n",
    "print(f\"  Mean: {positions_kelly.mean():.4f}\")\n",
    "print(f\"  Std: {positions_kelly.std():.4f}\")\n",
    "print(f\"  Min: {positions_kelly.min():.4f}\")\n",
    "print(f\"  Max: {positions_kelly.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6959e3",
   "metadata": {},
   "source": [
    "## Part 2: Volatility Targeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VOLATILITY TARGETING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nVOLATILITY TARGETING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def volatility_target_position(returns_history, target_vol=0.15, lookback=20):\n",
    "    \"\"\"\n",
    "    Scale position to target a specific volatility.\n",
    "    \n",
    "    Position = target_vol / realized_vol\n",
    "    \"\"\"\n",
    "    # Realized volatility (annualized)\n",
    "    realized_vol = returns_history[-lookback:].std() * np.sqrt(252)\n",
    "    \n",
    "    if realized_vol > 0:\n",
    "        position_scale = target_vol / realized_vol\n",
    "    else:\n",
    "        position_scale = 1.0\n",
    "    \n",
    "    # Cap leverage\n",
    "    position_scale = min(position_scale, 2.0)\n",
    "    \n",
    "    return position_scale, realized_vol\n",
    "\n",
    "# Apply volatility targeting\n",
    "target_vol = 0.15  # 15% annualized target\n",
    "vol_scales = []\n",
    "realized_vols = []\n",
    "\n",
    "for i in range(len(test_returns)):\n",
    "    # Use rolling history\n",
    "    history_start = max(0, split + i - 60)\n",
    "    history = returns[history_start:split + i + 1]\n",
    "    \n",
    "    scale, real_vol = volatility_target_position(history, target_vol=target_vol)\n",
    "    vol_scales.append(scale)\n",
    "    realized_vols.append(real_vol)\n",
    "\n",
    "vol_scales = np.array(vol_scales)\n",
    "\n",
    "print(f\"Target volatility: {target_vol:.0%}\")\n",
    "print(f\"Average position scale: {vol_scales.mean():.2f}\")\n",
    "print(f\"Average realized vol: {np.mean(realized_vols):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332cbe9",
   "metadata": {},
   "source": [
    "## Part 3: Compare Sizing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f38312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE POSITION SIZING METHODS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nCOMPARING POSITION SIZING METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Binary positions (baseline)\n",
    "positions_binary = 2 * predictions - 1\n",
    "\n",
    "# Kelly-weighted\n",
    "positions_kelly = np.array([probability_kelly_size(p, base_kelly=0.5) for p in probabilities])\n",
    "\n",
    "# Volatility-targeted\n",
    "positions_vol = positions_binary * vol_scales\n",
    "\n",
    "# Combined\n",
    "positions_combined = positions_kelly * vol_scales\n",
    "\n",
    "# Calculate returns\n",
    "returns_binary = test_returns * positions_binary\n",
    "returns_kelly = test_returns * positions_kelly\n",
    "returns_vol = test_returns * positions_vol\n",
    "returns_combined = test_returns * positions_combined\n",
    "\n",
    "# Performance\n",
    "def calc_metrics(rets, name):\n",
    "    cum = np.cumprod(1 + rets)[-1]\n",
    "    sharpe = np.sqrt(252) * rets.mean() / rets.std() if rets.std() > 0 else 0\n",
    "    vol = rets.std() * np.sqrt(252)\n",
    "    max_dd = (np.maximum.accumulate(np.cumprod(1+rets)) - np.cumprod(1+rets)).max()\n",
    "    \n",
    "    print(f\"{name:<15} Return: {(cum-1)*100:>6.1f}%  Sharpe: {sharpe:>5.2f}  Vol: {vol*100:>5.1f}%  MaxDD: {max_dd*100:>5.1f}%\")\n",
    "    return cum, sharpe, vol, max_dd\n",
    "\n",
    "calc_metrics(returns_binary, 'Binary')\n",
    "calc_metrics(returns_kelly, 'Kelly')\n",
    "calc_metrics(returns_vol, 'Vol-Target')\n",
    "calc_metrics(returns_combined, 'Combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad945675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cumulative returns\n",
    "axes[0, 0].plot(np.cumprod(1 + returns_binary), label='Binary', linewidth=1.5)\n",
    "axes[0, 0].plot(np.cumprod(1 + returns_kelly), label='Kelly', linewidth=1.5)\n",
    "axes[0, 0].plot(np.cumprod(1 + returns_vol), label='Vol-Target', linewidth=1.5)\n",
    "axes[0, 0].plot(np.cumprod(1 + returns_combined), label='Combined', linewidth=1.5)\n",
    "axes[0, 0].set_xlabel('Days')\n",
    "axes[0, 0].set_ylabel('Cumulative Return')\n",
    "axes[0, 0].set_title('Position Sizing Comparison', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Position distributions\n",
    "axes[0, 1].hist(positions_binary, bins=30, alpha=0.5, label='Binary')\n",
    "axes[0, 1].hist(positions_kelly, bins=30, alpha=0.5, label='Kelly')\n",
    "axes[0, 1].hist(positions_combined, bins=30, alpha=0.5, label='Combined')\n",
    "axes[0, 1].set_xlabel('Position Size')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Position Size Distribution', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Position over time\n",
    "axes[1, 0].plot(positions_kelly, label='Kelly', alpha=0.7)\n",
    "axes[1, 0].plot(vol_scales, label='Vol Scale', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Days')\n",
    "axes[1, 0].set_ylabel('Position/Scale')\n",
    "axes[1, 0].set_title('Position Sizing Over Time', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling Sharpe\n",
    "window = 60\n",
    "rolling_sharpe_binary = pd.Series(returns_binary).rolling(window).apply(lambda x: np.sqrt(252) * x.mean() / x.std())\n",
    "rolling_sharpe_combined = pd.Series(returns_combined).rolling(window).apply(lambda x: np.sqrt(252) * x.mean() / x.std())\n",
    "axes[1, 1].plot(rolling_sharpe_binary, label='Binary', alpha=0.7)\n",
    "axes[1, 1].plot(rolling_sharpe_combined, label='Combined', alpha=0.7)\n",
    "axes[1, 1].axhline(0, color='gray', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Days')\n",
    "axes[1, 1].set_ylabel('Rolling Sharpe (60d)')\n",
    "axes[1, 1].set_title('Rolling Sharpe Ratio', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘              DAY 3 COMPLETE: POSITION SIZING                     â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  âœ“ Kelly Criterion (optimal bet size)                           â•‘\n",
    "â•‘  âœ“ Probability-weighted sizing                                  â•‘\n",
    "â•‘  âœ“ Volatility targeting                                         â•‘\n",
    "â•‘  âœ“ Combined sizing approach                                     â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Tomorrow: Day 4 - Risk Management\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
