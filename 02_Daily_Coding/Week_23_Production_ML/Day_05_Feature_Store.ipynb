{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1974f87",
   "metadata": {},
   "source": [
    "# Day 05: Feature Stores for ML Systems\n",
    "\n",
    "## Production ML - Week 23\n",
    "\n",
    "Feature stores are a critical component of production ML systems, providing a centralized repository for storing, managing, and serving features. In quantitative finance, feature stores help ensure consistency between training and inference while managing the complexity of financial time-series data.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand feature store architecture and components\n",
    "2. Implement offline and online feature stores\n",
    "3. Handle point-in-time correctness for financial data\n",
    "4. Build feature pipelines with versioning and lineage\n",
    "5. Implement feature serving for real-time trading systems\n",
    "\n",
    "### Why Feature Stores Matter in Finance\n",
    "- **Training-Serving Skew**: Ensure features computed during training match production\n",
    "- **Point-in-Time Correctness**: Prevent look-ahead bias in backtesting\n",
    "- **Feature Reuse**: Share features across multiple models and teams\n",
    "- **Data Quality**: Centralized validation and monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Tuple, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Feature Store Development Environment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2e4ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Feature Store Architecture\n",
    "\n",
    "A feature store typically consists of:\n",
    "1. **Feature Registry**: Metadata about features (schema, lineage, ownership)\n",
    "2. **Offline Store**: Historical features for training (data warehouse)\n",
    "3. **Online Store**: Low-latency features for inference (key-value store)\n",
    "4. **Feature Pipelines**: Transform raw data into features\n",
    "5. **Feature Serving**: APIs for retrieving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120e3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureDefinition:\n",
    "    \"\"\"Metadata definition for a feature.\"\"\"\n",
    "    name: str\n",
    "    dtype: str\n",
    "    description: str\n",
    "    entity: str  # e.g., 'ticker', 'portfolio'\n",
    "    owner: str\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    version: int = 1\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    updated_at: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    # Feature computation metadata\n",
    "    computation_fn: Optional[str] = None  # Serialized function name\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    window_size: Optional[str] = None  # e.g., '20D', '1H'\n",
    "    \n",
    "    # Data quality constraints\n",
    "    min_value: Optional[float] = None\n",
    "    max_value: Optional[float] = None\n",
    "    nullable: bool = False\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        d = asdict(self)\n",
    "        d['created_at'] = self.created_at.isoformat()\n",
    "        d['updated_at'] = self.updated_at.isoformat()\n",
    "        return d\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict) -> 'FeatureDefinition':\n",
    "        d['created_at'] = datetime.fromisoformat(d['created_at'])\n",
    "        d['updated_at'] = datetime.fromisoformat(d['updated_at'])\n",
    "        return cls(**d)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FeatureGroup:\n",
    "    \"\"\"Collection of related features.\"\"\"\n",
    "    name: str\n",
    "    entity: str\n",
    "    description: str\n",
    "    features: List[FeatureDefinition] = field(default_factory=list)\n",
    "    event_timestamp_column: str = 'timestamp'\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def add_feature(self, feature: FeatureDefinition):\n",
    "        if feature.entity != self.entity:\n",
    "            raise ValueError(f\"Feature entity {feature.entity} doesn't match group entity {self.entity}\")\n",
    "        self.features.append(feature)\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        return [f.name for f in self.features]\n",
    "\n",
    "\n",
    "# Example: Define financial features\n",
    "price_momentum_20d = FeatureDefinition(\n",
    "    name='price_momentum_20d',\n",
    "    dtype='float64',\n",
    "    description='20-day price momentum (return)',\n",
    "    entity='ticker',\n",
    "    owner='quant_team',\n",
    "    tags=['momentum', 'technical', 'alpha'],\n",
    "    window_size='20D',\n",
    "    min_value=-1.0,\n",
    "    max_value=5.0\n",
    ")\n",
    "\n",
    "volatility_20d = FeatureDefinition(\n",
    "    name='volatility_20d',\n",
    "    dtype='float64',\n",
    "    description='20-day realized volatility (annualized)',\n",
    "    entity='ticker',\n",
    "    owner='quant_team',\n",
    "    tags=['volatility', 'risk', 'technical'],\n",
    "    window_size='20D',\n",
    "    min_value=0.0,\n",
    "    max_value=5.0\n",
    ")\n",
    "\n",
    "print(f\"Feature: {price_momentum_20d.name}\")\n",
    "print(f\"Description: {price_momentum_20d.description}\")\n",
    "print(f\"Tags: {price_momentum_20d.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureRegistry:\n",
    "    \"\"\"Central registry for feature metadata.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = ':memory:'):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "        self._init_tables()\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def _init_tables(self):\n",
    "        \"\"\"Initialize registry tables.\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        # Feature groups table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS feature_groups (\n",
    "                name TEXT PRIMARY KEY,\n",
    "                entity TEXT NOT NULL,\n",
    "                description TEXT,\n",
    "                event_timestamp_column TEXT DEFAULT 'timestamp',\n",
    "                created_at TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Features table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS features (\n",
    "                name TEXT,\n",
    "                group_name TEXT,\n",
    "                dtype TEXT,\n",
    "                description TEXT,\n",
    "                entity TEXT,\n",
    "                owner TEXT,\n",
    "                tags TEXT,\n",
    "                version INTEGER DEFAULT 1,\n",
    "                computation_fn TEXT,\n",
    "                dependencies TEXT,\n",
    "                window_size TEXT,\n",
    "                min_value REAL,\n",
    "                max_value REAL,\n",
    "                nullable INTEGER DEFAULT 0,\n",
    "                created_at TEXT,\n",
    "                updated_at TEXT,\n",
    "                PRIMARY KEY (name, group_name, version)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Feature lineage table\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS feature_lineage (\n",
    "                feature_name TEXT,\n",
    "                source_feature TEXT,\n",
    "                transformation TEXT,\n",
    "                created_at TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def register_feature_group(self, group: FeatureGroup):\n",
    "        \"\"\"Register a feature group.\"\"\"\n",
    "        with self._lock:\n",
    "            cursor = self.conn.cursor()\n",
    "            \n",
    "            # Insert group\n",
    "            cursor.execute('''\n",
    "                INSERT OR REPLACE INTO feature_groups \n",
    "                (name, entity, description, event_timestamp_column, created_at)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "            ''', (group.name, group.entity, group.description, \n",
    "                  group.event_timestamp_column, group.created_at.isoformat()))\n",
    "            \n",
    "            # Insert features\n",
    "            for feature in group.features:\n",
    "                cursor.execute('''\n",
    "                    INSERT OR REPLACE INTO features\n",
    "                    (name, group_name, dtype, description, entity, owner, tags, version,\n",
    "                     computation_fn, dependencies, window_size, min_value, max_value,\n",
    "                     nullable, created_at, updated_at)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', (feature.name, group.name, feature.dtype, feature.description,\n",
    "                      feature.entity, feature.owner, json.dumps(feature.tags),\n",
    "                      feature.version, feature.computation_fn,\n",
    "                      json.dumps(feature.dependencies), feature.window_size,\n",
    "                      feature.min_value, feature.max_value, int(feature.nullable),\n",
    "                      feature.created_at.isoformat(), feature.updated_at.isoformat()))\n",
    "            \n",
    "            self.conn.commit()\n",
    "    \n",
    "    def get_feature(self, name: str, group_name: Optional[str] = None) -> Optional[FeatureDefinition]:\n",
    "        \"\"\"Get feature definition.\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        if group_name:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM features WHERE name = ? AND group_name = ?\n",
    "                ORDER BY version DESC LIMIT 1\n",
    "            ''', (name, group_name))\n",
    "        else:\n",
    "            cursor.execute('''\n",
    "                SELECT * FROM features WHERE name = ?\n",
    "                ORDER BY version DESC LIMIT 1\n",
    "            ''', (name,))\n",
    "        \n",
    "        row = cursor.fetchone()\n",
    "        if row:\n",
    "            return self._row_to_feature(row)\n",
    "        return None\n",
    "    \n",
    "    def _row_to_feature(self, row) -> FeatureDefinition:\n",
    "        \"\"\"Convert database row to FeatureDefinition.\"\"\"\n",
    "        return FeatureDefinition(\n",
    "            name=row[0],\n",
    "            dtype=row[2],\n",
    "            description=row[3],\n",
    "            entity=row[4],\n",
    "            owner=row[5],\n",
    "            tags=json.loads(row[6]) if row[6] else [],\n",
    "            version=row[7],\n",
    "            computation_fn=row[8],\n",
    "            dependencies=json.loads(row[9]) if row[9] else [],\n",
    "            window_size=row[10],\n",
    "            min_value=row[11],\n",
    "            max_value=row[12],\n",
    "            nullable=bool(row[13]),\n",
    "            created_at=datetime.fromisoformat(row[14]),\n",
    "            updated_at=datetime.fromisoformat(row[15])\n",
    "        )\n",
    "    \n",
    "    def search_features(self, tags: Optional[List[str]] = None, \n",
    "                       entity: Optional[str] = None) -> List[FeatureDefinition]:\n",
    "        \"\"\"Search features by tags or entity.\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        query = \"SELECT * FROM features WHERE 1=1\"\n",
    "        params = []\n",
    "        \n",
    "        if entity:\n",
    "            query += \" AND entity = ?\"\n",
    "            params.append(entity)\n",
    "        \n",
    "        cursor.execute(query, params)\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        features = [self._row_to_feature(row) for row in rows]\n",
    "        \n",
    "        if tags:\n",
    "            features = [f for f in features if any(t in f.tags for t in tags)]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def list_feature_groups(self) -> List[str]:\n",
    "        \"\"\"List all feature groups.\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM feature_groups\")\n",
    "        return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "\n",
    "# Create registry and register features\n",
    "registry = FeatureRegistry()\n",
    "\n",
    "# Create technical features group\n",
    "technical_group = FeatureGroup(\n",
    "    name='technical_indicators',\n",
    "    entity='ticker',\n",
    "    description='Technical analysis features for equities'\n",
    ")\n",
    "technical_group.add_feature(price_momentum_20d)\n",
    "technical_group.add_feature(volatility_20d)\n",
    "\n",
    "# Register\n",
    "registry.register_feature_group(technical_group)\n",
    "\n",
    "# Search features\n",
    "momentum_features = registry.search_features(tags=['momentum'])\n",
    "print(f\"Features with 'momentum' tag: {[f.name for f in momentum_features]}\")\n",
    "\n",
    "# Get specific feature\n",
    "feature = registry.get_feature('volatility_20d')\n",
    "print(f\"\\nFeature details: {feature.name} - {feature.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68960d52",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Offline Feature Store\n",
    "\n",
    "The offline store holds historical feature values for training. Key considerations:\n",
    "- **Point-in-time correctness**: Features must be computed as they would have been at each historical point\n",
    "- **Efficient storage**: Handle large time-series datasets\n",
    "- **Versioning**: Track feature computation changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineFeatureStore:\n",
    "    \"\"\"Offline store for historical feature storage and retrieval.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = ':memory:'):\n",
    "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "        self._feature_tables: Dict[str, bool] = {}\n",
    "        self._lock = threading.Lock()\n",
    "    \n",
    "    def _create_feature_table(self, group_name: str, entity_col: str = 'entity_id'):\n",
    "        \"\"\"Create table for a feature group.\"\"\"\n",
    "        table_name = f\"features_{group_name}\"\n",
    "        \n",
    "        if table_name in self._feature_tables:\n",
    "            return\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                {entity_col} TEXT NOT NULL,\n",
    "                event_timestamp TEXT NOT NULL,\n",
    "                created_timestamp TEXT NOT NULL,\n",
    "                feature_data TEXT NOT NULL,\n",
    "                PRIMARY KEY ({entity_col}, event_timestamp)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Index for efficient point-in-time lookups\n",
    "        cursor.execute(f'''\n",
    "            CREATE INDEX IF NOT EXISTS idx_{table_name}_time \n",
    "            ON {table_name} (event_timestamp)\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "        self._feature_tables[table_name] = True\n",
    "    \n",
    "    def write_features(self, group_name: str, df: pd.DataFrame,\n",
    "                       entity_col: str = 'entity_id',\n",
    "                       timestamp_col: str = 'timestamp'):\n",
    "        \"\"\"Write features to offline store.\"\"\"\n",
    "        self._create_feature_table(group_name, entity_col)\n",
    "        table_name = f\"features_{group_name}\"\n",
    "        \n",
    "        # Identify feature columns (exclude entity and timestamp)\n",
    "        feature_cols = [c for c in df.columns if c not in [entity_col, timestamp_col]]\n",
    "        \n",
    "        with self._lock:\n",
    "            cursor = self.conn.cursor()\n",
    "            created_ts = datetime.now().isoformat()\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                feature_data = {col: row[col] for col in feature_cols}\n",
    "                \n",
    "                # Handle NaN values\n",
    "                feature_data = {\n",
    "                    k: (None if pd.isna(v) else v) \n",
    "                    for k, v in feature_data.items()\n",
    "                }\n",
    "                \n",
    "                cursor.execute(f'''\n",
    "                    INSERT OR REPLACE INTO {table_name}\n",
    "                    ({entity_col}, event_timestamp, created_timestamp, feature_data)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                ''', (str(row[entity_col]), \n",
    "                      str(row[timestamp_col]),\n",
    "                      created_ts,\n",
    "                      json.dumps(feature_data)))\n",
    "            \n",
    "            self.conn.commit()\n",
    "    \n",
    "    def read_features(self, group_name: str, \n",
    "                      entity_ids: List[str],\n",
    "                      start_time: datetime,\n",
    "                      end_time: datetime,\n",
    "                      feature_names: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Read historical features.\"\"\"\n",
    "        table_name = f\"features_{group_name}\"\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        placeholders = ','.join(['?' for _ in entity_ids])\n",
    "        \n",
    "        cursor.execute(f'''\n",
    "            SELECT entity_id, event_timestamp, feature_data\n",
    "            FROM {table_name}\n",
    "            WHERE entity_id IN ({placeholders})\n",
    "            AND event_timestamp >= ?\n",
    "            AND event_timestamp <= ?\n",
    "            ORDER BY event_timestamp\n",
    "        ''', (*entity_ids, start_time.isoformat(), end_time.isoformat()))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        if not rows:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Parse results\n",
    "        data = []\n",
    "        for entity_id, event_ts, feature_json in rows:\n",
    "            features = json.loads(feature_json)\n",
    "            features['entity_id'] = entity_id\n",
    "            features['timestamp'] = pd.to_datetime(event_ts)\n",
    "            data.append(features)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if feature_names:\n",
    "            cols = ['entity_id', 'timestamp'] + feature_names\n",
    "            df = df[[c for c in cols if c in df.columns]]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_point_in_time_features(self, group_name: str,\n",
    "                                    entity_timestamps: pd.DataFrame,\n",
    "                                    entity_col: str = 'entity_id',\n",
    "                                    timestamp_col: str = 'timestamp',\n",
    "                                    feature_names: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get features as of specific timestamps (point-in-time join).\n",
    "        \n",
    "        This is crucial for preventing look-ahead bias in backtesting.\n",
    "        For each (entity, timestamp) pair, return features that were\n",
    "        available at that exact moment.\n",
    "        \"\"\"\n",
    "        table_name = f\"features_{group_name}\"\n",
    "        results = []\n",
    "        \n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        for _, row in entity_timestamps.iterrows():\n",
    "            entity_id = str(row[entity_col])\n",
    "            as_of_time = row[timestamp_col]\n",
    "            \n",
    "            # Get most recent feature before or at as_of_time\n",
    "            cursor.execute(f'''\n",
    "                SELECT entity_id, event_timestamp, feature_data\n",
    "                FROM {table_name}\n",
    "                WHERE entity_id = ?\n",
    "                AND event_timestamp <= ?\n",
    "                ORDER BY event_timestamp DESC\n",
    "                LIMIT 1\n",
    "            ''', (entity_id, as_of_time.isoformat()))\n",
    "            \n",
    "            result_row = cursor.fetchone()\n",
    "            \n",
    "            if result_row:\n",
    "                features = json.loads(result_row[2])\n",
    "                features['entity_id'] = entity_id\n",
    "                features['request_timestamp'] = as_of_time\n",
    "                features['feature_timestamp'] = pd.to_datetime(result_row[1])\n",
    "                results.append(features)\n",
    "            else:\n",
    "                # No features available at this time\n",
    "                results.append({\n",
    "                    'entity_id': entity_id,\n",
    "                    'request_timestamp': as_of_time,\n",
    "                    'feature_timestamp': None\n",
    "                })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        if feature_names and not df.empty:\n",
    "            cols = ['entity_id', 'request_timestamp', 'feature_timestamp'] + feature_names\n",
    "            df = df[[c for c in cols if c in df.columns]]\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "# Create offline store\n",
    "offline_store = OfflineFeatureStore()\n",
    "print(\"Offline Feature Store initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa28446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample financial data and features\n",
    "def generate_sample_features(tickers: List[str], \n",
    "                             start_date: str, \n",
    "                             end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Generate sample feature data for demonstration.\"\"\"\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "    \n",
    "    all_data = []\n",
    "    for ticker in tickers:\n",
    "        # Simulate prices\n",
    "        np.random.seed(hash(ticker) % 2**32)\n",
    "        returns = np.random.normal(0.0005, 0.02, len(dates))\n",
    "        prices = 100 * np.exp(np.cumsum(returns))\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Compute features\n",
    "            if i >= 20:\n",
    "                momentum_20d = prices[i] / prices[i-20] - 1\n",
    "                volatility_20d = np.std(returns[i-20:i]) * np.sqrt(252)\n",
    "                volume_ma_ratio = np.random.uniform(0.8, 1.2)\n",
    "                rsi = 50 + np.random.normal(0, 15)\n",
    "                rsi = np.clip(rsi, 0, 100)\n",
    "            else:\n",
    "                momentum_20d = np.nan\n",
    "                volatility_20d = np.nan\n",
    "                volume_ma_ratio = np.nan\n",
    "                rsi = np.nan\n",
    "            \n",
    "            all_data.append({\n",
    "                'entity_id': ticker,\n",
    "                'timestamp': date,\n",
    "                'price': prices[i],\n",
    "                'price_momentum_20d': momentum_20d,\n",
    "                'volatility_20d': volatility_20d,\n",
    "                'volume_ma_ratio': volume_ma_ratio,\n",
    "                'rsi': rsi\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "# Generate and store features\n",
    "tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META']\n",
    "features_df = generate_sample_features(tickers, '2024-01-01', '2024-12-31')\n",
    "\n",
    "print(f\"Generated {len(features_df)} feature records\")\n",
    "print(f\"\\nSample data:\")\n",
    "features_df.head(25).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f16f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write features to offline store\n",
    "offline_store.write_features(\n",
    "    group_name='technical_indicators',\n",
    "    df=features_df\n",
    ")\n",
    "\n",
    "# Read features for specific entities and time range\n",
    "read_df = offline_store.read_features(\n",
    "    group_name='technical_indicators',\n",
    "    entity_ids=['AAPL', 'GOOGL'],\n",
    "    start_time=datetime(2024, 6, 1),\n",
    "    end_time=datetime(2024, 6, 30),\n",
    "    feature_names=['price_momentum_20d', 'volatility_20d']\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(read_df)} records\")\n",
    "read_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate point-in-time join (critical for backtesting)\n",
    "# Create sample entity-timestamp requests (e.g., trade signals)\n",
    "trade_signals = pd.DataFrame({\n",
    "    'entity_id': ['AAPL', 'GOOGL', 'AAPL', 'MSFT', 'AAPL'],\n",
    "    'timestamp': pd.to_datetime([\n",
    "        '2024-06-15 10:30:00',  # Mid-day - should get features from 2024-06-14\n",
    "        '2024-06-15 14:00:00',\n",
    "        '2024-06-20 09:30:00',  # Early morning - should get features from 2024-06-19\n",
    "        '2024-06-25 16:00:00',  # End of day\n",
    "        '2024-02-01 12:00:00',  # Early in year\n",
    "    ])\n",
    "})\n",
    "\n",
    "# Get point-in-time features\n",
    "pit_features = offline_store.get_point_in_time_features(\n",
    "    group_name='technical_indicators',\n",
    "    entity_timestamps=trade_signals,\n",
    "    feature_names=['price_momentum_20d', 'volatility_20d', 'rsi']\n",
    ")\n",
    "\n",
    "print(\"Point-in-Time Feature Join Result:\")\n",
    "print(\"(Note: feature_timestamp shows when features were actually computed)\")\n",
    "pit_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35927f74",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Online Feature Store\n",
    "\n",
    "The online store provides low-latency feature retrieval for real-time inference. Key characteristics:\n",
    "- Sub-millisecond read latency\n",
    "- Key-value based access\n",
    "- Stores only the latest feature values\n",
    "- Synchronized from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineFeatureStore:\n",
    "    \"\"\"Low-latency online store for real-time feature serving.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In production: Redis, DynamoDB, or similar\n",
    "        # Here: in-memory dict with TTL support\n",
    "        self._store: Dict[str, Dict[str, Any]] = defaultdict(dict)\n",
    "        self._timestamps: Dict[str, datetime] = {}\n",
    "        self._ttl_seconds: Dict[str, int] = {}\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Metrics\n",
    "        self._metrics = {\n",
    "            'reads': 0,\n",
    "            'writes': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0,\n",
    "            'latencies': []\n",
    "        }\n",
    "    \n",
    "    def _make_key(self, group_name: str, entity_id: str) -> str:\n",
    "        \"\"\"Create composite key.\"\"\"\n",
    "        return f\"{group_name}:{entity_id}\"\n",
    "    \n",
    "    def write_feature(self, group_name: str, entity_id: str,\n",
    "                      features: Dict[str, Any], ttl_seconds: int = 86400):\n",
    "        \"\"\"Write features for an entity.\"\"\"\n",
    "        key = self._make_key(group_name, entity_id)\n",
    "        \n",
    "        with self._lock:\n",
    "            self._store[key] = features.copy()\n",
    "            self._timestamps[key] = datetime.now()\n",
    "            self._ttl_seconds[key] = ttl_seconds\n",
    "            self._metrics['writes'] += 1\n",
    "    \n",
    "    def write_batch(self, group_name: str, \n",
    "                    entity_features: Dict[str, Dict[str, Any]],\n",
    "                    ttl_seconds: int = 86400):\n",
    "        \"\"\"Batch write features for multiple entities.\"\"\"\n",
    "        for entity_id, features in entity_features.items():\n",
    "            self.write_feature(group_name, entity_id, features, ttl_seconds)\n",
    "    \n",
    "    def read_feature(self, group_name: str, entity_id: str,\n",
    "                     feature_names: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Read features for an entity with latency tracking.\"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        key = self._make_key(group_name, entity_id)\n",
    "        \n",
    "        with self._lock:\n",
    "            self._metrics['reads'] += 1\n",
    "            \n",
    "            if key not in self._store:\n",
    "                self._metrics['cache_misses'] += 1\n",
    "                return None\n",
    "            \n",
    "            # Check TTL\n",
    "            if self._is_expired(key):\n",
    "                del self._store[key]\n",
    "                self._metrics['cache_misses'] += 1\n",
    "                return None\n",
    "            \n",
    "            self._metrics['cache_hits'] += 1\n",
    "            features = self._store[key].copy()\n",
    "        \n",
    "        # Track latency\n",
    "        latency = (time.perf_counter() - start_time) * 1000  # ms\n",
    "        self._metrics['latencies'].append(latency)\n",
    "        \n",
    "        if feature_names:\n",
    "            features = {k: v for k, v in features.items() if k in feature_names}\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def read_batch(self, group_name: str, entity_ids: List[str],\n",
    "                   feature_names: Optional[List[str]] = None) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Batch read features for multiple entities.\"\"\"\n",
    "        results = {}\n",
    "        for entity_id in entity_ids:\n",
    "            features = self.read_feature(group_name, entity_id, feature_names)\n",
    "            if features:\n",
    "                results[entity_id] = features\n",
    "        return results\n",
    "    \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a key has expired.\"\"\"\n",
    "        if key not in self._timestamps:\n",
    "            return True\n",
    "        age = (datetime.now() - self._timestamps[key]).total_seconds()\n",
    "        return age > self._ttl_seconds.get(key, 86400)\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get store metrics.\"\"\"\n",
    "        latencies = self._metrics['latencies']\n",
    "        return {\n",
    "            'total_reads': self._metrics['reads'],\n",
    "            'total_writes': self._metrics['writes'],\n",
    "            'cache_hit_rate': (\n",
    "                self._metrics['cache_hits'] / max(1, self._metrics['reads'])\n",
    "            ),\n",
    "            'avg_latency_ms': np.mean(latencies) if latencies else 0,\n",
    "            'p50_latency_ms': np.percentile(latencies, 50) if latencies else 0,\n",
    "            'p99_latency_ms': np.percentile(latencies, 99) if latencies else 0,\n",
    "            'stored_keys': len(self._store)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create online store\n",
    "online_store = OnlineFeatureStore()\n",
    "\n",
    "# Populate with latest features for each ticker\n",
    "latest_features = features_df.groupby('entity_id').last().reset_index()\n",
    "\n",
    "for _, row in latest_features.iterrows():\n",
    "    online_store.write_feature(\n",
    "        group_name='technical_indicators',\n",
    "        entity_id=row['entity_id'],\n",
    "        features={\n",
    "            'price': row['price'],\n",
    "            'price_momentum_20d': row['price_momentum_20d'],\n",
    "            'volatility_20d': row['volatility_20d'],\n",
    "            'rsi': row['rsi']\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Online store populated with latest features\")\n",
    "print(f\"Stored keys: {len(online_store._store)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time feature serving\n",
    "def simulate_realtime_inference(online_store: OnlineFeatureStore,\n",
    "                                 num_requests: int = 1000):\n",
    "    \"\"\"Simulate real-time model inference with feature fetching.\"\"\"\n",
    "    tickers = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META']\n",
    "    feature_names = ['price_momentum_20d', 'volatility_20d']\n",
    "    \n",
    "    for _ in range(num_requests):\n",
    "        ticker = np.random.choice(tickers)\n",
    "        features = online_store.read_feature(\n",
    "            'technical_indicators', ticker, feature_names\n",
    "        )\n",
    "    \n",
    "    return online_store.get_metrics()\n",
    "\n",
    "\n",
    "# Run simulation\n",
    "metrics = simulate_realtime_inference(online_store, num_requests=5000)\n",
    "\n",
    "print(\"Online Store Performance Metrics:\")\n",
    "print(f\"  Total reads: {metrics['total_reads']}\")\n",
    "print(f\"  Cache hit rate: {metrics['cache_hit_rate']:.2%}\")\n",
    "print(f\"  Avg latency: {metrics['avg_latency_ms']:.4f} ms\")\n",
    "print(f\"  P50 latency: {metrics['p50_latency_ms']:.4f} ms\")\n",
    "print(f\"  P99 latency: {metrics['p99_latency_ms']:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec542bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch feature retrieval for portfolio\n",
    "portfolio_tickers = ['AAPL', 'GOOGL', 'MSFT']\n",
    "\n",
    "portfolio_features = online_store.read_batch(\n",
    "    group_name='technical_indicators',\n",
    "    entity_ids=portfolio_tickers,\n",
    "    feature_names=['price_momentum_20d', 'volatility_20d', 'rsi']\n",
    ")\n",
    "\n",
    "print(\"Portfolio Features (Real-time):\")\n",
    "for ticker, features in portfolio_features.items():\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    for name, value in features.items():\n",
    "        print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5fde90",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Feature Pipelines\n",
    "\n",
    "Feature pipelines transform raw data into features. Key components:\n",
    "- **Transformation logic**: Reusable feature computation functions\n",
    "- **Scheduling**: Batch and streaming pipelines\n",
    "- **Validation**: Ensure data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ab7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformation(ABC):\n",
    "    \"\"\"Base class for feature transformations.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply transformation to dataframe.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_required_columns(self) -> List[str]:\n",
    "        \"\"\"Get columns required for transformation.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_output_columns(self) -> List[str]:\n",
    "        \"\"\"Get columns produced by transformation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MomentumFeatures(FeatureTransformation):\n",
    "    \"\"\"Compute momentum-based features.\"\"\"\n",
    "    \n",
    "    def __init__(self, windows: List[int] = [5, 10, 20, 60]):\n",
    "        self.windows = windows\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        \n",
    "        for window in self.windows:\n",
    "            # Price momentum\n",
    "            result[f'momentum_{window}d'] = (\n",
    "                result.groupby('entity_id')['close']\n",
    "                .pct_change(window)\n",
    "            )\n",
    "            \n",
    "            # Normalized momentum (z-score)\n",
    "            result[f'momentum_{window}d_zscore'] = (\n",
    "                result.groupby('entity_id')[f'momentum_{window}d']\n",
    "                .transform(lambda x: (x - x.rolling(60).mean()) / x.rolling(60).std())\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_required_columns(self) -> List[str]:\n",
    "        return ['entity_id', 'close']\n",
    "    \n",
    "    def get_output_columns(self) -> List[str]:\n",
    "        cols = []\n",
    "        for w in self.windows:\n",
    "            cols.extend([f'momentum_{w}d', f'momentum_{w}d_zscore'])\n",
    "        return cols\n",
    "\n",
    "\n",
    "class VolatilityFeatures(FeatureTransformation):\n",
    "    \"\"\"Compute volatility-based features.\"\"\"\n",
    "    \n",
    "    def __init__(self, windows: List[int] = [10, 20, 60]):\n",
    "        self.windows = windows\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Daily returns\n",
    "        result['daily_return'] = result.groupby('entity_id')['close'].pct_change()\n",
    "        \n",
    "        for window in self.windows:\n",
    "            # Realized volatility (annualized)\n",
    "            result[f'volatility_{window}d'] = (\n",
    "                result.groupby('entity_id')['daily_return']\n",
    "                .transform(lambda x: x.rolling(window).std() * np.sqrt(252))\n",
    "            )\n",
    "            \n",
    "            # Volatility ratio (short/long term)\n",
    "            if window > 10:\n",
    "                result[f'vol_ratio_10_{window}'] = (\n",
    "                    result['volatility_10d'] / result[f'volatility_{window}d']\n",
    "                )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_required_columns(self) -> List[str]:\n",
    "        return ['entity_id', 'close']\n",
    "    \n",
    "    def get_output_columns(self) -> List[str]:\n",
    "        cols = ['daily_return']\n",
    "        for w in self.windows:\n",
    "            cols.append(f'volatility_{w}d')\n",
    "            if w > 10:\n",
    "                cols.append(f'vol_ratio_10_{w}')\n",
    "        return cols\n",
    "\n",
    "\n",
    "class TechnicalIndicators(FeatureTransformation):\n",
    "    \"\"\"Compute technical indicators.\"\"\"\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        result = df.copy()\n",
    "        \n",
    "        # RSI\n",
    "        delta = result.groupby('entity_id')['close'].diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = (-delta).where(delta < 0, 0)\n",
    "        \n",
    "        avg_gain = gain.groupby(result['entity_id']).transform(\n",
    "            lambda x: x.rolling(14).mean()\n",
    "        )\n",
    "        avg_loss = loss.groupby(result['entity_id']).transform(\n",
    "            lambda x: x.rolling(14).mean()\n",
    "        )\n",
    "        \n",
    "        rs = avg_gain / avg_loss.replace(0, np.nan)\n",
    "        result['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        ema_12 = result.groupby('entity_id')['close'].transform(\n",
    "            lambda x: x.ewm(span=12).mean()\n",
    "        )\n",
    "        ema_26 = result.groupby('entity_id')['close'].transform(\n",
    "            lambda x: x.ewm(span=26).mean()\n",
    "        )\n",
    "        result['macd'] = ema_12 - ema_26\n",
    "        result['macd_signal'] = result.groupby('entity_id')['macd'].transform(\n",
    "            lambda x: x.ewm(span=9).mean()\n",
    "        )\n",
    "        result['macd_hist'] = result['macd'] - result['macd_signal']\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        sma_20 = result.groupby('entity_id')['close'].transform(\n",
    "            lambda x: x.rolling(20).mean()\n",
    "        )\n",
    "        std_20 = result.groupby('entity_id')['close'].transform(\n",
    "            lambda x: x.rolling(20).std()\n",
    "        )\n",
    "        result['bb_upper'] = sma_20 + 2 * std_20\n",
    "        result['bb_lower'] = sma_20 - 2 * std_20\n",
    "        result['bb_position'] = (\n",
    "            (result['close'] - result['bb_lower']) / \n",
    "            (result['bb_upper'] - result['bb_lower'])\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_required_columns(self) -> List[str]:\n",
    "        return ['entity_id', 'close']\n",
    "    \n",
    "    def get_output_columns(self) -> List[str]:\n",
    "        return ['rsi_14', 'macd', 'macd_signal', 'macd_hist', \n",
    "                'bb_upper', 'bb_lower', 'bb_position']\n",
    "\n",
    "\n",
    "print(\"Feature transformations defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePipeline:\n",
    "    \"\"\"Pipeline for computing and validating features.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.transformations: List[FeatureTransformation] = []\n",
    "        self.validators: List['FeatureValidator'] = []\n",
    "    \n",
    "    def add_transformation(self, transformation: FeatureTransformation):\n",
    "        \"\"\"Add a transformation to the pipeline.\"\"\"\n",
    "        self.transformations.append(transformation)\n",
    "        return self\n",
    "    \n",
    "    def add_validator(self, validator: 'FeatureValidator'):\n",
    "        \"\"\"Add a validator to the pipeline.\"\"\"\n",
    "        self.validators.append(validator)\n",
    "        return self\n",
    "    \n",
    "    def run(self, df: pd.DataFrame, validate: bool = True) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"Run the pipeline on input data.\"\"\"\n",
    "        result = df.copy()\n",
    "        metrics = {\n",
    "            'input_rows': len(df),\n",
    "            'transformations': [],\n",
    "            'validation_results': []\n",
    "        }\n",
    "        \n",
    "        # Apply transformations\n",
    "        for transform in self.transformations:\n",
    "            start_time = time.time()\n",
    "            result = transform.transform(result)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            metrics['transformations'].append({\n",
    "                'name': transform.__class__.__name__,\n",
    "                'output_columns': transform.get_output_columns(),\n",
    "                'elapsed_seconds': elapsed\n",
    "            })\n",
    "        \n",
    "        # Validate results\n",
    "        if validate:\n",
    "            for validator in self.validators:\n",
    "                validation_result = validator.validate(result)\n",
    "                metrics['validation_results'].append(validation_result)\n",
    "        \n",
    "        metrics['output_rows'] = len(result)\n",
    "        metrics['output_columns'] = list(result.columns)\n",
    "        \n",
    "        return result, metrics\n",
    "\n",
    "\n",
    "class FeatureValidator:\n",
    "    \"\"\"Validate feature quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.rules: List[Dict] = []\n",
    "    \n",
    "    def add_null_check(self, columns: List[str], max_null_pct: float = 0.1):\n",
    "        \"\"\"Add null value check.\"\"\"\n",
    "        self.rules.append({\n",
    "            'type': 'null_check',\n",
    "            'columns': columns,\n",
    "            'max_null_pct': max_null_pct\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def add_range_check(self, column: str, min_val: float, max_val: float):\n",
    "        \"\"\"Add value range check.\"\"\"\n",
    "        self.rules.append({\n",
    "            'type': 'range_check',\n",
    "            'column': column,\n",
    "            'min': min_val,\n",
    "            'max': max_val\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def add_distribution_check(self, column: str, \n",
    "                                expected_mean: float, \n",
    "                                tolerance: float = 0.5):\n",
    "        \"\"\"Add distribution check.\"\"\"\n",
    "        self.rules.append({\n",
    "            'type': 'distribution_check',\n",
    "            'column': column,\n",
    "            'expected_mean': expected_mean,\n",
    "            'tolerance': tolerance\n",
    "        })\n",
    "        return self\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Run all validation rules.\"\"\"\n",
    "        results = {\n",
    "            'validator': self.name,\n",
    "            'passed': True,\n",
    "            'checks': []\n",
    "        }\n",
    "        \n",
    "        for rule in self.rules:\n",
    "            check_result = self._run_check(df, rule)\n",
    "            results['checks'].append(check_result)\n",
    "            if not check_result['passed']:\n",
    "                results['passed'] = False\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _run_check(self, df: pd.DataFrame, rule: Dict) -> Dict:\n",
    "        \"\"\"Run a single validation check.\"\"\"\n",
    "        check_type = rule['type']\n",
    "        \n",
    "        if check_type == 'null_check':\n",
    "            null_pcts = {}\n",
    "            passed = True\n",
    "            for col in rule['columns']:\n",
    "                if col in df.columns:\n",
    "                    null_pct = df[col].isna().mean()\n",
    "                    null_pcts[col] = null_pct\n",
    "                    if null_pct > rule['max_null_pct']:\n",
    "                        passed = False\n",
    "            return {\n",
    "                'type': 'null_check',\n",
    "                'passed': passed,\n",
    "                'details': null_pcts\n",
    "            }\n",
    "        \n",
    "        elif check_type == 'range_check':\n",
    "            col = rule['column']\n",
    "            if col not in df.columns:\n",
    "                return {'type': 'range_check', 'passed': False, 'error': 'column not found'}\n",
    "            \n",
    "            out_of_range = (\n",
    "                (df[col] < rule['min']) | (df[col] > rule['max'])\n",
    "            ).sum()\n",
    "            total = df[col].notna().sum()\n",
    "            \n",
    "            return {\n",
    "                'type': 'range_check',\n",
    "                'column': col,\n",
    "                'passed': out_of_range == 0,\n",
    "                'out_of_range_count': int(out_of_range),\n",
    "                'out_of_range_pct': out_of_range / max(1, total)\n",
    "            }\n",
    "        \n",
    "        elif check_type == 'distribution_check':\n",
    "            col = rule['column']\n",
    "            if col not in df.columns:\n",
    "                return {'type': 'distribution_check', 'passed': False, 'error': 'column not found'}\n",
    "            \n",
    "            actual_mean = df[col].mean()\n",
    "            diff = abs(actual_mean - rule['expected_mean'])\n",
    "            passed = diff <= rule['tolerance']\n",
    "            \n",
    "            return {\n",
    "                'type': 'distribution_check',\n",
    "                'column': col,\n",
    "                'passed': passed,\n",
    "                'expected_mean': rule['expected_mean'],\n",
    "                'actual_mean': actual_mean,\n",
    "                'difference': diff\n",
    "            }\n",
    "        \n",
    "        return {'type': check_type, 'passed': False, 'error': 'unknown check type'}\n",
    "\n",
    "\n",
    "print(\"Feature Pipeline classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bac723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate raw OHLCV data for pipeline\n",
    "def generate_ohlcv_data(tickers: List[str], num_days: int = 500) -> pd.DataFrame:\n",
    "    \"\"\"Generate sample OHLCV data.\"\"\"\n",
    "    dates = pd.date_range(end=datetime.now(), periods=num_days, freq='B')\n",
    "    \n",
    "    all_data = []\n",
    "    for ticker in tickers:\n",
    "        np.random.seed(hash(ticker) % 2**32)\n",
    "        \n",
    "        # Generate price series\n",
    "        returns = np.random.normal(0.0005, 0.02, num_days)\n",
    "        close = 100 * np.exp(np.cumsum(returns))\n",
    "        \n",
    "        # Generate OHLCV\n",
    "        for i, date in enumerate(dates):\n",
    "            daily_vol = np.random.uniform(0.005, 0.02)\n",
    "            open_price = close[i] * (1 + np.random.uniform(-daily_vol, daily_vol))\n",
    "            high = max(open_price, close[i]) * (1 + np.random.uniform(0, daily_vol))\n",
    "            low = min(open_price, close[i]) * (1 - np.random.uniform(0, daily_vol))\n",
    "            volume = np.random.randint(1000000, 10000000)\n",
    "            \n",
    "            all_data.append({\n",
    "                'entity_id': ticker,\n",
    "                'timestamp': date,\n",
    "                'open': open_price,\n",
    "                'high': high,\n",
    "                'low': low,\n",
    "                'close': close[i],\n",
    "                'volume': volume\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "# Create raw data\n",
    "raw_data = generate_ohlcv_data(['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'META'])\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build feature pipeline\n",
    "pipeline = FeaturePipeline('equity_features')\n",
    "\n",
    "# Add transformations\n",
    "pipeline.add_transformation(MomentumFeatures(windows=[5, 10, 20]))\n",
    "pipeline.add_transformation(VolatilityFeatures(windows=[10, 20]))\n",
    "pipeline.add_transformation(TechnicalIndicators())\n",
    "\n",
    "# Add validators\n",
    "validator = FeatureValidator('quality_checks')\n",
    "validator.add_null_check(['momentum_20d', 'volatility_20d'], max_null_pct=0.15)\n",
    "validator.add_range_check('rsi_14', min_val=0, max_val=100)\n",
    "validator.add_distribution_check('rsi_14', expected_mean=50, tolerance=15)\n",
    "\n",
    "pipeline.add_validator(validator)\n",
    "\n",
    "# Run pipeline\n",
    "features_result, pipeline_metrics = pipeline.run(raw_data)\n",
    "\n",
    "print(\"Pipeline Execution Results:\")\n",
    "print(f\"  Input rows: {pipeline_metrics['input_rows']}\")\n",
    "print(f\"  Output rows: {pipeline_metrics['output_rows']}\")\n",
    "print(f\"  Output columns: {len(pipeline_metrics['output_columns'])}\")\n",
    "\n",
    "print(\"\\nTransformations:\")\n",
    "for t in pipeline_metrics['transformations']:\n",
    "    print(f\"  {t['name']}: {t['elapsed_seconds']:.3f}s, {len(t['output_columns'])} features\")\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for v in pipeline_metrics['validation_results']:\n",
    "    status = '' if v['passed'] else ''\n",
    "    print(f\"  {status} {v['validator']}\")\n",
    "    for check in v['checks']:\n",
    "        check_status = '' if check['passed'] else ''\n",
    "        print(f\"    {check_status} {check['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbe85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of computed features\n",
    "feature_cols = ['entity_id', 'timestamp', 'close', 'momentum_20d', 'momentum_20d_zscore',\n",
    "                'volatility_20d', 'rsi_14', 'macd', 'bb_position']\n",
    "\n",
    "features_result[feature_cols].dropna().tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7f9e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Complete Feature Store System\n",
    "\n",
    "Integrating all components into a cohesive feature store system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166eafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Complete Feature Store system integrating:\n",
    "    - Feature Registry\n",
    "    - Offline Store\n",
    "    - Online Store\n",
    "    - Feature Pipelines\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = 'default'):\n",
    "        self.name = name\n",
    "        self.registry = FeatureRegistry()\n",
    "        self.offline_store = OfflineFeatureStore()\n",
    "        self.online_store = OnlineFeatureStore()\n",
    "        self.pipelines: Dict[str, FeaturePipeline] = {}\n",
    "        self._sync_lock = threading.Lock()\n",
    "    \n",
    "    def register_feature_group(self, group: FeatureGroup):\n",
    "        \"\"\"Register a feature group with the store.\"\"\"\n",
    "        self.registry.register_feature_group(group)\n",
    "        print(f\"Registered feature group: {group.name} with {len(group.features)} features\")\n",
    "    \n",
    "    def register_pipeline(self, name: str, pipeline: FeaturePipeline):\n",
    "        \"\"\"Register a feature pipeline.\"\"\"\n",
    "        self.pipelines[name] = pipeline\n",
    "        print(f\"Registered pipeline: {name}\")\n",
    "    \n",
    "    def materialize_features(self, pipeline_name: str, \n",
    "                             raw_data: pd.DataFrame,\n",
    "                             group_name: str,\n",
    "                             entity_col: str = 'entity_id',\n",
    "                             timestamp_col: str = 'timestamp') -> Dict:\n",
    "        \"\"\"\n",
    "        Run pipeline and write results to both offline and online stores.\n",
    "        \"\"\"\n",
    "        if pipeline_name not in self.pipelines:\n",
    "            raise ValueError(f\"Pipeline {pipeline_name} not found\")\n",
    "        \n",
    "        pipeline = self.pipelines[pipeline_name]\n",
    "        \n",
    "        # Run pipeline\n",
    "        features_df, metrics = pipeline.run(raw_data)\n",
    "        \n",
    "        # Check validation\n",
    "        if metrics['validation_results']:\n",
    "            all_passed = all(v['passed'] for v in metrics['validation_results'])\n",
    "            if not all_passed:\n",
    "                print(\"Warning: Some validation checks failed\")\n",
    "        \n",
    "        # Write to offline store\n",
    "        self.offline_store.write_features(\n",
    "            group_name=group_name,\n",
    "            df=features_df,\n",
    "            entity_col=entity_col,\n",
    "            timestamp_col=timestamp_col\n",
    "        )\n",
    "        \n",
    "        # Update online store with latest values\n",
    "        self._sync_to_online(features_df, group_name, entity_col)\n",
    "        \n",
    "        metrics['materialization'] = {\n",
    "            'offline_rows': len(features_df),\n",
    "            'online_entities': features_df[entity_col].nunique()\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _sync_to_online(self, df: pd.DataFrame, group_name: str, entity_col: str):\n",
    "        \"\"\"Sync latest features to online store.\"\"\"\n",
    "        # Get latest row for each entity\n",
    "        latest = df.groupby(entity_col).last().reset_index()\n",
    "        \n",
    "        # Get feature columns (exclude entity, timestamp, and price data)\n",
    "        exclude_cols = {entity_col, 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n",
    "        feature_cols = [c for c in latest.columns if c not in exclude_cols]\n",
    "        \n",
    "        with self._sync_lock:\n",
    "            for _, row in latest.iterrows():\n",
    "                features = {col: row[col] for col in feature_cols \n",
    "                           if pd.notna(row[col])}\n",
    "                self.online_store.write_feature(\n",
    "                    group_name=group_name,\n",
    "                    entity_id=str(row[entity_col]),\n",
    "                    features=features\n",
    "                )\n",
    "    \n",
    "    def get_training_data(self, group_name: str,\n",
    "                          entity_ids: List[str],\n",
    "                          start_time: datetime,\n",
    "                          end_time: datetime,\n",
    "                          feature_names: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Get historical features for training.\"\"\"\n",
    "        return self.offline_store.read_features(\n",
    "            group_name=group_name,\n",
    "            entity_ids=entity_ids,\n",
    "            start_time=start_time,\n",
    "            end_time=end_time,\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "    \n",
    "    def get_online_features(self, group_name: str,\n",
    "                            entity_ids: List[str],\n",
    "                            feature_names: Optional[List[str]] = None) -> Dict[str, Dict]:\n",
    "        \"\"\"Get latest features for inference.\"\"\"\n",
    "        return self.online_store.read_batch(\n",
    "            group_name=group_name,\n",
    "            entity_ids=entity_ids,\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "    \n",
    "    def get_point_in_time_features(self, group_name: str,\n",
    "                                    entity_timestamps: pd.DataFrame,\n",
    "                                    feature_names: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Get point-in-time features for backtesting.\"\"\"\n",
    "        return self.offline_store.get_point_in_time_features(\n",
    "            group_name=group_name,\n",
    "            entity_timestamps=entity_timestamps,\n",
    "            feature_names=feature_names\n",
    "        )\n",
    "    \n",
    "    def search_features(self, tags: Optional[List[str]] = None,\n",
    "                        entity: Optional[str] = None) -> List[FeatureDefinition]:\n",
    "        \"\"\"Search features in registry.\"\"\"\n",
    "        return self.registry.search_features(tags=tags, entity=entity)\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get store metrics.\"\"\"\n",
    "        return {\n",
    "            'online_store': self.online_store.get_metrics(),\n",
    "            'registered_pipelines': list(self.pipelines.keys()),\n",
    "            'feature_groups': self.registry.list_feature_groups()\n",
    "        }\n",
    "\n",
    "\n",
    "# Create feature store instance\n",
    "feature_store = FeatureStore('quant_features')\n",
    "print(f\"Feature Store '{feature_store.name}' initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register feature group\n",
    "equity_group = FeatureGroup(\n",
    "    name='equity_technical',\n",
    "    entity='ticker',\n",
    "    description='Technical analysis features for equities'\n",
    ")\n",
    "\n",
    "# Define features\n",
    "features_to_register = [\n",
    "    FeatureDefinition('momentum_5d', 'float64', '5-day momentum', 'ticker', 'quant_team', ['momentum']),\n",
    "    FeatureDefinition('momentum_10d', 'float64', '10-day momentum', 'ticker', 'quant_team', ['momentum']),\n",
    "    FeatureDefinition('momentum_20d', 'float64', '20-day momentum', 'ticker', 'quant_team', ['momentum']),\n",
    "    FeatureDefinition('volatility_10d', 'float64', '10-day volatility', 'ticker', 'quant_team', ['volatility']),\n",
    "    FeatureDefinition('volatility_20d', 'float64', '20-day volatility', 'ticker', 'quant_team', ['volatility']),\n",
    "    FeatureDefinition('rsi_14', 'float64', '14-period RSI', 'ticker', 'quant_team', ['technical'], \n",
    "                      min_value=0, max_value=100),\n",
    "    FeatureDefinition('macd', 'float64', 'MACD indicator', 'ticker', 'quant_team', ['technical']),\n",
    "    FeatureDefinition('bb_position', 'float64', 'Bollinger Band position', 'ticker', 'quant_team', ['technical']),\n",
    "]\n",
    "\n",
    "for feature in features_to_register:\n",
    "    equity_group.add_feature(feature)\n",
    "\n",
    "feature_store.register_feature_group(equity_group)\n",
    "\n",
    "# Register pipeline\n",
    "feature_store.register_pipeline('equity_pipeline', pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df95d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize features\n",
    "materialize_metrics = feature_store.materialize_features(\n",
    "    pipeline_name='equity_pipeline',\n",
    "    raw_data=raw_data,\n",
    "    group_name='equity_technical'\n",
    ")\n",
    "\n",
    "print(\"Materialization Results:\")\n",
    "print(f\"  Offline rows: {materialize_metrics['materialization']['offline_rows']}\")\n",
    "print(f\"  Online entities: {materialize_metrics['materialization']['online_entities']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aef083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature store for training\n",
    "training_data = feature_store.get_training_data(\n",
    "    group_name='equity_technical',\n",
    "    entity_ids=['AAPL', 'GOOGL', 'MSFT'],\n",
    "    start_time=datetime(2025, 1, 1),\n",
    "    end_time=datetime(2025, 12, 31),\n",
    "    feature_names=['momentum_20d', 'volatility_20d', 'rsi_14']\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use feature store for real-time inference\n",
    "inference_features = feature_store.get_online_features(\n",
    "    group_name='equity_technical',\n",
    "    entity_ids=['AAPL', 'GOOGL'],\n",
    "    feature_names=['momentum_20d', 'volatility_20d', 'rsi_14']\n",
    ")\n",
    "\n",
    "print(\"Real-time Inference Features:\")\n",
    "for ticker, features in inference_features.items():\n",
    "    print(f\"\\n{ticker}:\")\n",
    "    for name, value in features.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e474a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for features\n",
    "momentum_features = feature_store.search_features(tags=['momentum'])\n",
    "print(\"Features with 'momentum' tag:\")\n",
    "for f in momentum_features:\n",
    "    print(f\"  - {f.name}: {f.description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Get store metrics\n",
    "metrics = feature_store.get_metrics()\n",
    "print(\"\\nFeature Store Metrics:\")\n",
    "print(f\"  Feature groups: {metrics['feature_groups']}\")\n",
    "print(f\"  Pipelines: {metrics['registered_pipelines']}\")\n",
    "print(f\"  Online store cache hit rate: {metrics['online_store']['cache_hit_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0291b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Feature Store Best Practices\n",
    "\n",
    "### Production Considerations for Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136736c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMonitor:\n",
    "    \"\"\"\n",
    "    Monitor feature quality and drift in production.\n",
    "    \n",
    "    Critical for financial ML systems where data drift\n",
    "    can significantly impact model performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.baseline_stats: Dict[str, Dict] = {}\n",
    "        self.alerts: List[Dict] = []\n",
    "    \n",
    "    def set_baseline(self, feature_name: str, df: pd.DataFrame, column: str):\n",
    "        \"\"\"Set baseline statistics for a feature.\"\"\"\n",
    "        values = df[column].dropna()\n",
    "        self.baseline_stats[feature_name] = {\n",
    "            'mean': values.mean(),\n",
    "            'std': values.std(),\n",
    "            'min': values.min(),\n",
    "            'max': values.max(),\n",
    "            'median': values.median(),\n",
    "            'q25': values.quantile(0.25),\n",
    "            'q75': values.quantile(0.75),\n",
    "            'null_rate': df[column].isna().mean(),\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "    \n",
    "    def check_drift(self, feature_name: str, current_values: pd.Series,\n",
    "                    threshold_std: float = 2.0) -> Dict:\n",
    "        \"\"\"Check for feature drift compared to baseline.\"\"\"\n",
    "        if feature_name not in self.baseline_stats:\n",
    "            return {'error': 'No baseline set for feature'}\n",
    "        \n",
    "        baseline = self.baseline_stats[feature_name]\n",
    "        current_mean = current_values.mean()\n",
    "        current_std = current_values.std()\n",
    "        current_null_rate = current_values.isna().mean()\n",
    "        \n",
    "        # Calculate drift metrics\n",
    "        mean_drift = (current_mean - baseline['mean']) / baseline['std']\n",
    "        std_ratio = current_std / baseline['std']\n",
    "        null_rate_change = current_null_rate - baseline['null_rate']\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Check mean drift\n",
    "        if abs(mean_drift) > threshold_std:\n",
    "            alerts.append({\n",
    "                'type': 'mean_drift',\n",
    "                'feature': feature_name,\n",
    "                'severity': 'high' if abs(mean_drift) > 3 else 'medium',\n",
    "                'value': mean_drift,\n",
    "                'threshold': threshold_std\n",
    "            })\n",
    "        \n",
    "        # Check volatility change\n",
    "        if std_ratio > 1.5 or std_ratio < 0.5:\n",
    "            alerts.append({\n",
    "                'type': 'volatility_change',\n",
    "                'feature': feature_name,\n",
    "                'severity': 'high' if std_ratio > 2 or std_ratio < 0.25 else 'medium',\n",
    "                'value': std_ratio\n",
    "            })\n",
    "        \n",
    "        # Check null rate spike\n",
    "        if null_rate_change > 0.1:\n",
    "            alerts.append({\n",
    "                'type': 'null_rate_spike',\n",
    "                'feature': feature_name,\n",
    "                'severity': 'high',\n",
    "                'value': null_rate_change\n",
    "            })\n",
    "        \n",
    "        self.alerts.extend(alerts)\n",
    "        \n",
    "        return {\n",
    "            'feature': feature_name,\n",
    "            'mean_drift_std': mean_drift,\n",
    "            'std_ratio': std_ratio,\n",
    "            'null_rate_change': null_rate_change,\n",
    "            'alerts': alerts,\n",
    "            'is_healthy': len(alerts) == 0\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate monitoring report.\"\"\"\n",
    "        high_severity = [a for a in self.alerts if a['severity'] == 'high']\n",
    "        medium_severity = [a for a in self.alerts if a['severity'] == 'medium']\n",
    "        \n",
    "        return {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_alerts': len(self.alerts),\n",
    "            'high_severity': len(high_severity),\n",
    "            'medium_severity': len(medium_severity),\n",
    "            'alerts_by_feature': self._group_alerts_by_feature(),\n",
    "            'recent_alerts': self.alerts[-10:]\n",
    "        }\n",
    "    \n",
    "    def _group_alerts_by_feature(self) -> Dict[str, int]:\n",
    "        \"\"\"Group alerts by feature.\"\"\"\n",
    "        grouped = defaultdict(int)\n",
    "        for alert in self.alerts:\n",
    "            grouped[alert['feature']] += 1\n",
    "        return dict(grouped)\n",
    "\n",
    "\n",
    "# Demonstrate monitoring\n",
    "monitor = FeatureMonitor()\n",
    "\n",
    "# Set baselines from training period\n",
    "baseline_data = features_result[features_result['timestamp'] < '2025-10-01']\n",
    "monitor.set_baseline('momentum_20d', baseline_data, 'momentum_20d')\n",
    "monitor.set_baseline('volatility_20d', baseline_data, 'volatility_20d')\n",
    "monitor.set_baseline('rsi_14', baseline_data, 'rsi_14')\n",
    "\n",
    "# Check current data\n",
    "current_data = features_result[features_result['timestamp'] >= '2025-10-01']\n",
    "\n",
    "print(\"Feature Drift Analysis:\")\n",
    "for feature in ['momentum_20d', 'volatility_20d', 'rsi_14']:\n",
    "    result = monitor.check_drift(feature, current_data[feature])\n",
    "    status = ' Healthy' if result['is_healthy'] else ' Drift Detected'\n",
    "    print(f\"\\n{feature}: {status}\")\n",
    "    print(f\"  Mean drift (std): {result['mean_drift_std']:.2f}\")\n",
    "    print(f\"  Std ratio: {result['std_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature versioning example\n",
    "class FeatureVersion:\n",
    "    \"\"\"Track feature computation versions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.versions: Dict[str, List[Dict]] = defaultdict(list)\n",
    "    \n",
    "    def register_version(self, feature_name: str, version: int,\n",
    "                         computation_code: str, description: str):\n",
    "        \"\"\"Register a new version of a feature.\"\"\"\n",
    "        # Create hash of computation code\n",
    "        code_hash = hashlib.md5(computation_code.encode()).hexdigest()[:8]\n",
    "        \n",
    "        self.versions[feature_name].append({\n",
    "            'version': version,\n",
    "            'code_hash': code_hash,\n",
    "            'description': description,\n",
    "            'computation_code': computation_code,\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def get_version(self, feature_name: str, \n",
    "                    version: Optional[int] = None) -> Optional[Dict]:\n",
    "        \"\"\"Get specific version or latest.\"\"\"\n",
    "        if feature_name not in self.versions:\n",
    "            return None\n",
    "        \n",
    "        versions = self.versions[feature_name]\n",
    "        if version is None:\n",
    "            return versions[-1] if versions else None\n",
    "        \n",
    "        for v in versions:\n",
    "            if v['version'] == version:\n",
    "                return v\n",
    "        return None\n",
    "    \n",
    "    def list_versions(self, feature_name: str) -> List[Dict]:\n",
    "        \"\"\"List all versions of a feature.\"\"\"\n",
    "        return self.versions.get(feature_name, [])\n",
    "\n",
    "\n",
    "# Register feature versions\n",
    "versioner = FeatureVersion()\n",
    "\n",
    "versioner.register_version(\n",
    "    'momentum_20d', 1,\n",
    "    'price_t / price_t_minus_20 - 1',\n",
    "    'Simple 20-day price momentum'\n",
    ")\n",
    "\n",
    "versioner.register_version(\n",
    "    'momentum_20d', 2,\n",
    "    'log(price_t / price_t_minus_20)',\n",
    "    'Log 20-day momentum (more normally distributed)'\n",
    ")\n",
    "\n",
    "versioner.register_version(\n",
    "    'momentum_20d', 3,\n",
    "    '(price_t / price_t_minus_20 - 1) / volatility_20d',\n",
    "    'Risk-adjusted 20-day momentum'\n",
    ")\n",
    "\n",
    "print(\"Feature Versions for 'momentum_20d':\")\n",
    "for v in versioner.list_versions('momentum_20d'):\n",
    "    print(f\"  v{v['version']} ({v['code_hash']}): {v['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43baf8",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Feature Store Architecture\n",
    "\n",
    "### Key Components Implemented\n",
    "\n",
    "1. **Feature Registry**\n",
    "   - Centralized metadata management\n",
    "   - Feature search and discovery\n",
    "   - Lineage tracking\n",
    "\n",
    "2. **Offline Store**\n",
    "   - Historical feature storage\n",
    "   - Point-in-time correctness\n",
    "   - Efficient time-range queries\n",
    "\n",
    "3. **Online Store**\n",
    "   - Low-latency serving\n",
    "   - TTL support\n",
    "   - Performance metrics\n",
    "\n",
    "4. **Feature Pipelines**\n",
    "   - Reusable transformations\n",
    "   - Data validation\n",
    "   - Batch and incremental processing\n",
    "\n",
    "5. **Monitoring**\n",
    "   - Feature drift detection\n",
    "   - Quality alerts\n",
    "   - Version tracking\n",
    "\n",
    "### Production Considerations for Finance\n",
    "\n",
    "| Aspect | Consideration |\n",
    "|--------|---------------|\n",
    "| **Look-ahead Bias** | Point-in-time joins critical for backtesting |\n",
    "| **Data Latency** | Features must be computed before trading decisions |\n",
    "| **Versioning** | Track feature computation changes for reproducibility |\n",
    "| **Monitoring** | Detect drift that could impact model performance |\n",
    "| **Consistency** | Same features for training and production |\n",
    "\n",
    "### Popular Feature Store Solutions\n",
    "\n",
    "- **Feast**: Open-source, Kubernetes-native\n",
    "- **Tecton**: Enterprise, real-time focus\n",
    "- **Databricks Feature Store**: Integrated with MLflow\n",
    "- **AWS SageMaker Feature Store**: AWS-native\n",
    "- **Hopsworks**: Open-source, Python-centric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*60)\n",
    "print(\"Feature Store Implementation Complete\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Components Built:\n",
    "   FeatureRegistry - Metadata and discovery\n",
    "   OfflineFeatureStore - Historical storage with PIT joins\n",
    "   OnlineFeatureStore - Real-time serving\n",
    "   FeaturePipeline - Transformation and validation\n",
    "   FeatureStore - Integrated system\n",
    "   FeatureMonitor - Drift detection\n",
    "   FeatureVersion - Computation versioning\n",
    "\n",
    "Key Concepts:\n",
    "   Point-in-time correctness prevents look-ahead bias\n",
    "   Online/offline separation optimizes for different use cases\n",
    "   Feature pipelines ensure consistency\n",
    "   Monitoring detects production issues early\n",
    "   Versioning enables reproducibility\n",
    "\n",
    "Next Steps:\n",
    "   Integrate with production databases (Redis, PostgreSQL)\n",
    "   Add streaming feature computation (Kafka, Spark)\n",
    "   Implement feature sharing across teams\n",
    "   Set up automated monitoring dashboards\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
