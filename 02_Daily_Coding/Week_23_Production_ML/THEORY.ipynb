{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7e2320",
   "metadata": {},
   "source": [
    "# Week 23: Production ML - Theory\n",
    "\n",
    "## MLOps & Production Systems for Quantitative Trading\n",
    "\n",
    "This notebook covers the theoretical foundations and practical implementations of production ML systems in quantitative finance.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand MLOps fundamentals and lifecycle management\n",
    "2. Implement model versioning and experiment tracking\n",
    "3. Build feature stores for production ML\n",
    "4. Detect model drift and data drift\n",
    "5. Design A/B testing frameworks for trading models\n",
    "6. Create CI/CD pipelines for ML systems\n",
    "\n",
    "### Prerequisites\n",
    "- Understanding of ML fundamentals\n",
    "- Python programming proficiency\n",
    "- Basic knowledge of trading systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for Production ML\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n",
    "print(f\"ðŸ“… Notebook executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a2c45",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Production ML System Architecture\n",
    "\n",
    "### 1.1 Overview of Production ML Systems\n",
    "\n",
    "A production ML system in quantitative trading consists of several interconnected components:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PRODUCTION ML TRADING SYSTEM                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚ Data Sources â”‚â”€â”€â”€â–¶â”‚ Data Ingestionâ”‚â”€â”€â”€â–¶â”‚Feature Store â”‚                   â”‚\n",
    "â”‚  â”‚ - Market Dataâ”‚    â”‚ - Validation  â”‚    â”‚ - Compute    â”‚                   â”‚\n",
    "â”‚  â”‚ - Alt Data   â”‚    â”‚ - Transform   â”‚    â”‚ - Store      â”‚                   â”‚\n",
    "â”‚  â”‚ - News/Socialâ”‚    â”‚ - Quality     â”‚    â”‚ - Serve      â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                  â”‚                           â”‚\n",
    "â”‚                                                  â–¼                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚Model Registryâ”‚â—€â”€â”€â”€â”‚Model Trainingâ”‚â—€â”€â”€â”€â”‚ Training     â”‚                   â”‚\n",
    "â”‚  â”‚ - Versioning â”‚    â”‚ - Hyperparams â”‚    â”‚ Pipeline     â”‚                   â”‚\n",
    "â”‚  â”‚ - Metadata   â”‚    â”‚ - Validation  â”‚    â”‚              â”‚                   â”‚\n",
    "â”‚  â”‚ - Artifacts  â”‚    â”‚ - Metrics     â”‚    â”‚              â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚         â”‚                                                                    â”‚\n",
    "â”‚         â–¼                                                                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚Model Serving â”‚â”€â”€â”€â–¶â”‚  Prediction  â”‚â”€â”€â”€â–¶â”‚  Execution   â”‚                   â”‚\n",
    "â”‚  â”‚ - Real-time  â”‚    â”‚  Service     â”‚    â”‚  Engine      â”‚                   â”‚\n",
    "â”‚  â”‚ - Batch      â”‚    â”‚              â”‚    â”‚              â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                  â”‚                           â”‚\n",
    "â”‚                                                  â–¼                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚  â”‚  Monitoring  â”‚â—€â”€â”€â”€â”‚   Logging    â”‚â—€â”€â”€â”€â”‚   Orders     â”‚                   â”‚\n",
    "â”‚  â”‚ - Drift      â”‚    â”‚ - Audit Trailâ”‚    â”‚ - Portfolio  â”‚                   â”‚\n",
    "â”‚  â”‚ - Perf       â”‚    â”‚ - Metrics    â”‚    â”‚ - Risk       â”‚                   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "1. **Data Ingestion Pipeline**: Collects, validates, and transforms raw data\n",
    "2. **Feature Store**: Computes, stores, and serves features\n",
    "3. **Training Pipeline**: Orchestrates model training and validation\n",
    "4. **Model Registry**: Tracks model versions and metadata\n",
    "5. **Serving Infrastructure**: Delivers predictions in real-time or batch\n",
    "6. **Monitoring System**: Tracks model performance and data drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86774669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.2 Production ML Pipeline Architecture Implementation\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for production ML pipeline\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    data_source: str\n",
    "    feature_config: Dict[str, Any]\n",
    "    model_config: Dict[str, Any]\n",
    "    serving_config: Dict[str, Any]\n",
    "    monitoring_config: Dict[str, Any]\n",
    "\n",
    "\n",
    "class DataIngestionComponent:\n",
    "    \"\"\"\n",
    "    Data Ingestion Component - First stage of production pipeline\n",
    "    Responsible for collecting, validating, and transforming raw data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.validation_rules = config.get('validation_rules', {})\n",
    "        self.transform_pipeline = config.get('transforms', [])\n",
    "        \n",
    "    def validate_data(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate incoming data against predefined rules\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = self.validation_rules.get('required_columns', [])\n",
    "        missing_cols = set(required_cols) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            errors.append(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        # Check for null values in critical columns\n",
    "        critical_cols = self.validation_rules.get('no_null_columns', [])\n",
    "        for col in critical_cols:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                null_count = df[col].isnull().sum()\n",
    "                errors.append(f\"Column {col} has {null_count} null values\")\n",
    "        \n",
    "        # Check data types\n",
    "        expected_types = self.validation_rules.get('column_types', {})\n",
    "        for col, expected_type in expected_types.items():\n",
    "            if col in df.columns:\n",
    "                if expected_type == 'numeric' and not np.issubdtype(df[col].dtype, np.number):\n",
    "                    errors.append(f\"Column {col} should be numeric\")\n",
    "                elif expected_type == 'datetime' and not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                    errors.append(f\"Column {col} should be datetime\")\n",
    "        \n",
    "        # Check value ranges\n",
    "        value_ranges = self.validation_rules.get('value_ranges', {})\n",
    "        for col, (min_val, max_val) in value_ranges.items():\n",
    "            if col in df.columns:\n",
    "                if df[col].min() < min_val or df[col].max() > max_val:\n",
    "                    errors.append(f\"Column {col} values outside range [{min_val}, {max_val}]\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def ingest(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Ingest and transform data\"\"\"\n",
    "        # Validate\n",
    "        is_valid, errors = self.validate_data(df)\n",
    "        if not is_valid:\n",
    "            print(f\"âš ï¸ Data validation warnings: {errors}\")\n",
    "        \n",
    "        # Apply transforms\n",
    "        for transform in self.transform_pipeline:\n",
    "            df = transform(df)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class FeatureEngineeringComponent:\n",
    "    \"\"\"\n",
    "    Feature Engineering Component\n",
    "    Computes features from raw data for model training and inference\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_definitions: Dict[str, callable]):\n",
    "        self.feature_definitions = feature_definitions\n",
    "        self.feature_cache = {}\n",
    "        \n",
    "    def compute_features(self, df: pd.DataFrame, cache_key: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Compute all defined features\"\"\"\n",
    "        features = df.copy()\n",
    "        \n",
    "        for feature_name, feature_func in self.feature_definitions.items():\n",
    "            try:\n",
    "                features[feature_name] = feature_func(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing {feature_name}: {e}\")\n",
    "                features[feature_name] = np.nan\n",
    "        \n",
    "        # Cache if key provided\n",
    "        if cache_key:\n",
    "            self.feature_cache[cache_key] = features\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_cached_features(self, cache_key: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Retrieve cached features\"\"\"\n",
    "        return self.feature_cache.get(cache_key)\n",
    "\n",
    "\n",
    "class ModelTrainingComponent:\n",
    "    \"\"\"\n",
    "    Model Training Component\n",
    "    Handles model training, validation, and hyperparameter tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, hyperparameters: Dict[str, Any]):\n",
    "        self.model_class = model_class\n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.model = None\n",
    "        self.training_metrics = {}\n",
    "        \n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray, \n",
    "              X_val: Optional[np.ndarray] = None, \n",
    "              y_val: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
    "        \"\"\"Train model and compute metrics\"\"\"\n",
    "        self.model = self.model_class(**self.hyperparameters)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Training metrics\n",
    "        train_pred = self.model.predict(X_train)\n",
    "        self.training_metrics['train_accuracy'] = accuracy_score(y_train, train_pred)\n",
    "        \n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            train_proba = self.model.predict_proba(X_train)[:, 1]\n",
    "            self.training_metrics['train_auc'] = roc_auc_score(y_train, train_proba)\n",
    "        \n",
    "        # Validation metrics\n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_pred = self.model.predict(X_val)\n",
    "            self.training_metrics['val_accuracy'] = accuracy_score(y_val, val_pred)\n",
    "            \n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                val_proba = self.model.predict_proba(X_val)[:, 1]\n",
    "                self.training_metrics['val_auc'] = roc_auc_score(y_val, val_proba)\n",
    "        \n",
    "        return self.training_metrics\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get probability predictions\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        return self.model.predict_proba(X)\n",
    "\n",
    "\n",
    "class ProductionMLPipeline:\n",
    "    \"\"\"\n",
    "    Complete Production ML Pipeline\n",
    "    Orchestrates all components of the ML system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.data_ingestion = None\n",
    "        self.feature_engineering = None\n",
    "        self.model_training = None\n",
    "        self.is_initialized = False\n",
    "        \n",
    "    def initialize(self, ingestion_config: Dict, \n",
    "                   feature_definitions: Dict[str, callable],\n",
    "                   model_class, model_hyperparameters: Dict):\n",
    "        \"\"\"Initialize all pipeline components\"\"\"\n",
    "        self.data_ingestion = DataIngestionComponent(ingestion_config)\n",
    "        self.feature_engineering = FeatureEngineeringComponent(feature_definitions)\n",
    "        self.model_training = ModelTrainingComponent(model_class, model_hyperparameters)\n",
    "        self.is_initialized = True\n",
    "        print(f\"âœ… Pipeline '{self.config.name}' v{self.config.version} initialized\")\n",
    "        \n",
    "    def run_training_pipeline(self, raw_data: pd.DataFrame, \n",
    "                               target_col: str,\n",
    "                               feature_cols: List[str],\n",
    "                               test_size: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"Execute full training pipeline\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise ValueError(\"Pipeline not initialized\")\n",
    "        \n",
    "        # Step 1: Data Ingestion\n",
    "        print(\"ðŸ“¥ Step 1: Data Ingestion...\")\n",
    "        processed_data = self.data_ingestion.ingest(raw_data)\n",
    "        \n",
    "        # Step 2: Feature Engineering\n",
    "        print(\"ðŸ”§ Step 2: Feature Engineering...\")\n",
    "        features = self.feature_engineering.compute_features(processed_data)\n",
    "        \n",
    "        # Step 3: Prepare training data\n",
    "        print(\"ðŸ“Š Step 3: Preparing Training Data...\")\n",
    "        X = features[feature_cols].dropna()\n",
    "        y = features.loc[X.index, target_col]\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Step 4: Model Training\n",
    "        print(\"ðŸŽ¯ Step 4: Model Training...\")\n",
    "        metrics = self.model_training.train(X_train.values, y_train.values,\n",
    "                                            X_val.values, y_val.values)\n",
    "        \n",
    "        print(f\"âœ… Training Complete!\")\n",
    "        print(f\"   Training Accuracy: {metrics.get('train_accuracy', 0):.4f}\")\n",
    "        print(f\"   Validation Accuracy: {metrics.get('val_accuracy', 0):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'metrics': metrics,\n",
    "            'model': self.model_training.model,\n",
    "            'feature_cols': feature_cols,\n",
    "            'target_col': target_col\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "print(\"Production ML Pipeline Architecture defined!\")\n",
    "print(\"Components: DataIngestion â†’ FeatureEngineering â†’ ModelTraining â†’ Serving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66dfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.3 Demo: Running the Production Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "# Generate synthetic trading data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')\n",
    "synthetic_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'open': 100 + np.cumsum(np.random.randn(n_samples) * 0.5),\n",
    "    'high': 100 + np.cumsum(np.random.randn(n_samples) * 0.5) + np.abs(np.random.randn(n_samples)),\n",
    "    'low': 100 + np.cumsum(np.random.randn(n_samples) * 0.5) - np.abs(np.random.randn(n_samples)),\n",
    "    'close': 100 + np.cumsum(np.random.randn(n_samples) * 0.5),\n",
    "    'volume': np.random.randint(1000000, 10000000, n_samples)\n",
    "})\n",
    "\n",
    "# Ensure high >= close >= low\n",
    "synthetic_data['high'] = synthetic_data[['open', 'high', 'close']].max(axis=1)\n",
    "synthetic_data['low'] = synthetic_data[['open', 'low', 'close']].min(axis=1)\n",
    "\n",
    "# Create target: 1 if next day return > 0\n",
    "synthetic_data['returns'] = synthetic_data['close'].pct_change()\n",
    "synthetic_data['target'] = (synthetic_data['returns'].shift(-1) > 0).astype(int)\n",
    "\n",
    "print(\"Synthetic Trading Data:\")\n",
    "print(synthetic_data.head(10))\n",
    "print(f\"\\nShape: {synthetic_data.shape}\")\n",
    "print(f\"\\nTarget Distribution:\\n{synthetic_data['target'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature engineering functions\n",
    "def compute_sma_5(df):\n",
    "    return df['close'].rolling(window=5).mean()\n",
    "\n",
    "def compute_sma_20(df):\n",
    "    return df['close'].rolling(window=20).mean()\n",
    "\n",
    "def compute_rsi(df, period=14):\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def compute_volatility(df, period=20):\n",
    "    return df['returns'].rolling(window=period).std()\n",
    "\n",
    "def compute_volume_ma(df, period=10):\n",
    "    return df['volume'].rolling(window=period).mean()\n",
    "\n",
    "# Feature definitions\n",
    "feature_definitions = {\n",
    "    'sma_5': compute_sma_5,\n",
    "    'sma_20': compute_sma_20,\n",
    "    'rsi': compute_rsi,\n",
    "    'volatility': compute_volatility,\n",
    "    'volume_ma': compute_volume_ma,\n",
    "}\n",
    "\n",
    "# Ingestion config with validation rules\n",
    "ingestion_config = {\n",
    "    'validation_rules': {\n",
    "        'required_columns': ['open', 'high', 'low', 'close', 'volume'],\n",
    "        'no_null_columns': ['close', 'volume'],\n",
    "        'column_types': {\n",
    "            'close': 'numeric',\n",
    "            'volume': 'numeric'\n",
    "        },\n",
    "        'value_ranges': {\n",
    "            'close': (0, 10000),\n",
    "            'volume': (0, 1e12)\n",
    "        }\n",
    "    },\n",
    "    'transforms': []\n",
    "}\n",
    "\n",
    "# Initialize and run pipeline\n",
    "config = PipelineConfig(\n",
    "    name=\"Trading Signal Predictor\",\n",
    "    version=\"1.0.0\",\n",
    "    data_source=\"synthetic\",\n",
    "    feature_config={},\n",
    "    model_config={},\n",
    "    serving_config={},\n",
    "    monitoring_config={}\n",
    ")\n",
    "\n",
    "pipeline = ProductionMLPipeline(config)\n",
    "pipeline.initialize(\n",
    "    ingestion_config=ingestion_config,\n",
    "    feature_definitions=feature_definitions,\n",
    "    model_class=RandomForestClassifier,\n",
    "    model_hyperparameters={'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "# Run training\n",
    "feature_cols = ['sma_5', 'sma_20', 'rsi', 'volatility', 'volume_ma']\n",
    "results = pipeline.run_training_pipeline(\n",
    "    raw_data=synthetic_data,\n",
    "    target_col='target',\n",
    "    feature_cols=feature_cols,\n",
    "    test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cd718",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Serving Patterns\n",
    "\n",
    "### 2.1 Overview of Serving Patterns\n",
    "\n",
    "In production trading systems, there are three main model serving patterns:\n",
    "\n",
    "| Pattern | Latency | Use Case | Example |\n",
    "|---------|---------|----------|---------|\n",
    "| **Batch Prediction** | Minutes-Hours | End-of-day signals, portfolio rebalancing | Daily alpha generation |\n",
    "| **Real-time Inference** | Milliseconds | Live trading decisions, order routing | HFT signal generation |\n",
    "| **Streaming Prediction** | Sub-second | Continuous monitoring, event-driven trading | News-based trading |\n",
    "\n",
    "### Key Considerations for Trading:\n",
    "- **Latency Requirements**: HFT needs microseconds, swing trading can tolerate minutes\n",
    "- **Throughput**: How many predictions per second?\n",
    "- **Consistency**: Ensuring same features produce same predictions\n",
    "- **Fault Tolerance**: What happens when the model service fails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089db554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.2 Model Serving Patterns Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class BaseModelServer(ABC):\n",
    "    \"\"\"Abstract base class for model servers\"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_cols: List[str]):\n",
    "        self.model = model\n",
    "        self.feature_cols = feature_cols\n",
    "        self.prediction_count = 0\n",
    "        self.total_latency = 0\n",
    "        \n",
    "    @abstractmethod\n",
    "    def predict(self, features: Dict[str, float]) -> Dict[str, Any]:\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, float]:\n",
    "        avg_latency = self.total_latency / self.prediction_count if self.prediction_count > 0 else 0\n",
    "        return {\n",
    "            'prediction_count': self.prediction_count,\n",
    "            'avg_latency_ms': avg_latency * 1000\n",
    "        }\n",
    "\n",
    "\n",
    "class BatchPredictionServer(BaseModelServer):\n",
    "    \"\"\"\n",
    "    Batch Prediction Server\n",
    "    - Processes large volumes of data at once\n",
    "    - Optimized for throughput over latency\n",
    "    - Used for end-of-day predictions, portfolio construction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_cols: List[str], batch_size: int = 1000):\n",
    "        super().__init__(model, feature_cols)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def predict(self, features_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Batch predict on DataFrame\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ensure all required features present\n",
    "        missing_cols = set(self.feature_cols) - set(features_df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing features: {missing_cols}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        for i in range(0, len(features_df), self.batch_size):\n",
    "            batch = features_df[self.feature_cols].iloc[i:i+self.batch_size]\n",
    "            preds = self.model.predict(batch)\n",
    "            probas = self.model.predict_proba(batch)[:, 1]\n",
    "            all_predictions.extend(preds)\n",
    "            all_probabilities.extend(probas)\n",
    "        \n",
    "        # Record stats\n",
    "        end_time = time.time()\n",
    "        self.prediction_count += len(features_df)\n",
    "        self.total_latency += (end_time - start_time)\n",
    "        \n",
    "        result = features_df.copy()\n",
    "        result['prediction'] = all_predictions\n",
    "        result['probability'] = all_probabilities\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_for_date(self, features_df: pd.DataFrame, date: str) -> pd.DataFrame:\n",
    "        \"\"\"Predict for specific date (typical batch use case)\"\"\"\n",
    "        daily_features = features_df[features_df['date'] == date]\n",
    "        return self.predict(daily_features)\n",
    "\n",
    "\n",
    "class RealTimeInferenceServer(BaseModelServer):\n",
    "    \"\"\"\n",
    "    Real-Time Inference Server\n",
    "    - Single prediction at a time\n",
    "    - Optimized for low latency\n",
    "    - Used for live trading decisions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_cols: List[str], cache_size: int = 100):\n",
    "        super().__init__(model, feature_cols)\n",
    "        self.prediction_cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        \n",
    "    def _get_cache_key(self, features: Dict[str, float]) -> str:\n",
    "        \"\"\"Generate cache key from features\"\"\"\n",
    "        sorted_items = sorted(features.items())\n",
    "        return hashlib.md5(str(sorted_items).encode()).hexdigest()\n",
    "    \n",
    "    def predict(self, features: Dict[str, float], use_cache: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Real-time single prediction\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache\n",
    "        if use_cache:\n",
    "            cache_key = self._get_cache_key(features)\n",
    "            if cache_key in self.prediction_cache:\n",
    "                return self.prediction_cache[cache_key]\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_array = np.array([[features.get(col, np.nan) for col in self.feature_cols]])\n",
    "        \n",
    "        # Check for missing features\n",
    "        if np.any(np.isnan(feature_array)):\n",
    "            return {\n",
    "                'prediction': None,\n",
    "                'probability': None,\n",
    "                'error': 'Missing or invalid features',\n",
    "                'latency_ms': (time.time() - start_time) * 1000\n",
    "            }\n",
    "        \n",
    "        # Predict\n",
    "        prediction = int(self.model.predict(feature_array)[0])\n",
    "        probability = float(self.model.predict_proba(feature_array)[0, 1])\n",
    "        \n",
    "        # Record stats\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        self.prediction_count += 1\n",
    "        self.total_latency += latency\n",
    "        \n",
    "        result = {\n",
    "            'prediction': prediction,\n",
    "            'probability': probability,\n",
    "            'signal': 'BUY' if prediction == 1 else 'SELL',\n",
    "            'confidence': abs(probability - 0.5) * 2,\n",
    "            'latency_ms': latency * 1000,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        if use_cache and len(self.prediction_cache) < self.cache_size:\n",
    "            self.prediction_cache[cache_key] = result\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class StreamingPredictionServer(BaseModelServer):\n",
    "    \"\"\"\n",
    "    Streaming Prediction Server\n",
    "    - Processes continuous stream of data\n",
    "    - Maintains state across predictions\n",
    "    - Used for event-driven trading, real-time monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_cols: List[str], window_size: int = 100):\n",
    "        super().__init__(model, feature_cols)\n",
    "        self.window_size = window_size\n",
    "        self.feature_buffer = []\n",
    "        self.prediction_history = []\n",
    "        self.callbacks = []\n",
    "        \n",
    "    def add_callback(self, callback: callable):\n",
    "        \"\"\"Add callback for prediction events\"\"\"\n",
    "        self.callbacks.append(callback)\n",
    "    \n",
    "    def _notify_callbacks(self, prediction_result: Dict[str, Any]):\n",
    "        \"\"\"Notify all registered callbacks\"\"\"\n",
    "        for callback in self.callbacks:\n",
    "            try:\n",
    "                callback(prediction_result)\n",
    "            except Exception as e:\n",
    "                print(f\"Callback error: {e}\")\n",
    "    \n",
    "    def process_event(self, features: Dict[str, float]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Process incoming feature event\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add to buffer\n",
    "        features['event_time'] = datetime.now()\n",
    "        self.feature_buffer.append(features)\n",
    "        \n",
    "        # Keep buffer size manageable\n",
    "        if len(self.feature_buffer) > self.window_size:\n",
    "            self.feature_buffer = self.feature_buffer[-self.window_size:]\n",
    "        \n",
    "        # Make prediction\n",
    "        feature_array = np.array([[features.get(col, np.nan) for col in self.feature_cols]])\n",
    "        \n",
    "        if np.any(np.isnan(feature_array)):\n",
    "            return None\n",
    "        \n",
    "        prediction = int(self.model.predict(feature_array)[0])\n",
    "        probability = float(self.model.predict_proba(feature_array)[0, 1])\n",
    "        \n",
    "        # Compute prediction momentum (trend in predictions)\n",
    "        recent_preds = [p.get('prediction', 0) for p in self.prediction_history[-10:]]\n",
    "        momentum = sum(recent_preds) / len(recent_preds) if recent_preds else 0.5\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.prediction_count += 1\n",
    "        self.total_latency += (end_time - start_time)\n",
    "        \n",
    "        result = {\n",
    "            'prediction': prediction,\n",
    "            'probability': probability,\n",
    "            'signal': 'BUY' if prediction == 1 else 'SELL',\n",
    "            'momentum': momentum,\n",
    "            'buffer_size': len(self.feature_buffer),\n",
    "            'latency_ms': (end_time - start_time) * 1000,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.prediction_history.append(result)\n",
    "        \n",
    "        # Notify callbacks\n",
    "        self._notify_callbacks(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"Model Serving Patterns defined:\")\n",
    "print(\"  â€¢ BatchPredictionServer - For end-of-day processing\")\n",
    "print(\"  â€¢ RealTimeInferenceServer - For live trading\")\n",
    "print(\"  â€¢ StreamingPredictionServer - For event-driven trading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed133f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.3 Demo: Model Serving Patterns in Action\n",
    "# ============================================================================\n",
    "\n",
    "# Prepare features for serving demo\n",
    "features_with_target = pipeline.feature_engineering.compute_features(synthetic_data)\n",
    "features_with_target = features_with_target.dropna()\n",
    "\n",
    "# Initialize servers with trained model\n",
    "batch_server = BatchPredictionServer(results['model'], feature_cols, batch_size=100)\n",
    "realtime_server = RealTimeInferenceServer(results['model'], feature_cols)\n",
    "streaming_server = StreamingPredictionServer(results['model'], feature_cols)\n",
    "\n",
    "# Demo 1: Batch Prediction\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH PREDICTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "batch_results = batch_server.predict(features_with_target)\n",
    "print(f\"Processed {len(batch_results)} predictions\")\n",
    "print(f\"Stats: {batch_server.get_stats()}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "print(batch_results[['close', 'sma_5', 'rsi', 'prediction', 'probability']].head())\n",
    "\n",
    "# Demo 2: Real-time Prediction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REAL-TIME INFERENCE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "sample_features = features_with_target[feature_cols].iloc[100].to_dict()\n",
    "print(f\"Input features: {sample_features}\")\n",
    "rt_result = realtime_server.predict(sample_features)\n",
    "print(f\"Prediction: {rt_result}\")\n",
    "print(f\"Stats: {realtime_server.get_stats()}\")\n",
    "\n",
    "# Demo 3: Streaming Prediction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STREAMING PREDICTION DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Add callback for streaming\n",
    "def trading_callback(result):\n",
    "    if result['probability'] > 0.7:\n",
    "        print(f\"  ðŸŸ¢ Strong BUY signal: {result['probability']:.2%}\")\n",
    "    elif result['probability'] < 0.3:\n",
    "        print(f\"  ðŸ”´ Strong SELL signal: {result['probability']:.2%}\")\n",
    "\n",
    "streaming_server.add_callback(trading_callback)\n",
    "\n",
    "# Simulate stream of events\n",
    "print(\"Processing stream of events...\")\n",
    "for i in range(5):\n",
    "    event_features = features_with_target[feature_cols].iloc[100+i].to_dict()\n",
    "    stream_result = streaming_server.process_event(event_features)\n",
    "    print(f\"Event {i+1}: Signal={stream_result['signal']}, Prob={stream_result['probability']:.2%}\")\n",
    "\n",
    "print(f\"\\nStreaming Stats: {streaming_server.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f8908",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Store Concepts\n",
    "\n",
    "### 3.1 What is a Feature Store?\n",
    "\n",
    "A **Feature Store** is a centralized repository for storing, managing, and serving machine learning features. In quantitative trading, it serves as the bridge between raw market data and model-ready features.\n",
    "\n",
    "### Key Benefits:\n",
    "1. **Feature Reuse**: Compute once, use across multiple models\n",
    "2. **Consistency**: Same features for training and inference\n",
    "3. **Point-in-Time Correctness**: Prevent look-ahead bias\n",
    "4. **Versioning**: Track feature definitions and values over time\n",
    "5. **Low Latency Serving**: Pre-computed features for real-time inference\n",
    "\n",
    "### Feature Store Architecture:\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      FEATURE STORE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Feature Registryâ”‚          â”‚ Feature Catalog  â”‚          â”‚\n",
    "â”‚  â”‚ - Definitions   â”‚          â”‚ - Search        â”‚          â”‚\n",
    "â”‚  â”‚ - Metadata      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ - Documentation â”‚          â”‚\n",
    "â”‚  â”‚ - Versions      â”‚          â”‚ - Lineage       â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Offline Store   â”‚          â”‚  Online Store   â”‚          â”‚\n",
    "â”‚  â”‚ - Historical    â”‚          â”‚ - Low Latency   â”‚          â”‚\n",
    "â”‚  â”‚ - Training Data â”‚          â”‚ - Real-time     â”‚          â”‚\n",
    "â”‚  â”‚ - Batch Jobs    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ - Serving       â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3.2 Feature Store Implementation\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FeatureDefinition:\n",
    "    \"\"\"Definition of a single feature\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    dtype: str\n",
    "    computation_func: callable\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    version: str = \"1.0.0\"\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'description': self.description,\n",
    "            'dtype': self.dtype,\n",
    "            'dependencies': self.dependencies,\n",
    "            'version': self.version,\n",
    "            'created_at': self.created_at.isoformat(),\n",
    "            'tags': self.tags\n",
    "        }\n",
    "\n",
    "\n",
    "class FeatureRegistry:\n",
    "    \"\"\"Registry for feature definitions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.features: Dict[str, FeatureDefinition] = {}\n",
    "        self.feature_groups: Dict[str, List[str]] = {}\n",
    "        \n",
    "    def register(self, feature: FeatureDefinition, group: Optional[str] = None):\n",
    "        \"\"\"Register a feature definition\"\"\"\n",
    "        self.features[feature.name] = feature\n",
    "        \n",
    "        if group:\n",
    "            if group not in self.feature_groups:\n",
    "                self.feature_groups[group] = []\n",
    "            self.feature_groups[group].append(feature.name)\n",
    "        \n",
    "        print(f\"âœ… Registered feature: {feature.name} (v{feature.version})\")\n",
    "    \n",
    "    def get(self, name: str) -> Optional[FeatureDefinition]:\n",
    "        \"\"\"Get feature definition by name\"\"\"\n",
    "        return self.features.get(name)\n",
    "    \n",
    "    def list_features(self, group: Optional[str] = None) -> List[str]:\n",
    "        \"\"\"List all features or features in a group\"\"\"\n",
    "        if group:\n",
    "            return self.feature_groups.get(group, [])\n",
    "        return list(self.features.keys())\n",
    "    \n",
    "    def search(self, query: str) -> List[FeatureDefinition]:\n",
    "        \"\"\"Search features by name or description\"\"\"\n",
    "        results = []\n",
    "        for feature in self.features.values():\n",
    "            if query.lower() in feature.name.lower() or query.lower() in feature.description.lower():\n",
    "                results.append(feature)\n",
    "        return results\n",
    "\n",
    "\n",
    "class OfflineFeatureStore:\n",
    "    \"\"\"\n",
    "    Offline Feature Store for historical data\n",
    "    - Used for training data preparation\n",
    "    - Supports point-in-time queries (critical for avoiding look-ahead bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry: FeatureRegistry):\n",
    "        self.registry = registry\n",
    "        self.feature_data: Dict[str, pd.DataFrame] = {}\n",
    "        self.computation_log = []\n",
    "        \n",
    "    def compute_and_store(self, feature_name: str, \n",
    "                          source_data: pd.DataFrame,\n",
    "                          timestamp_col: str = 'date') -> pd.DataFrame:\n",
    "        \"\"\"Compute feature values and store them\"\"\"\n",
    "        feature_def = self.registry.get(feature_name)\n",
    "        if not feature_def:\n",
    "            raise ValueError(f\"Feature {feature_name} not registered\")\n",
    "        \n",
    "        # Compute feature\n",
    "        start_time = datetime.now()\n",
    "        feature_values = feature_def.computation_func(source_data)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        # Create feature dataframe with timestamp\n",
    "        feature_df = pd.DataFrame({\n",
    "            timestamp_col: source_data[timestamp_col],\n",
    "            feature_name: feature_values,\n",
    "            '_computed_at': datetime.now(),\n",
    "            '_version': feature_def.version\n",
    "        })\n",
    "        \n",
    "        self.feature_data[feature_name] = feature_df\n",
    "        \n",
    "        # Log computation\n",
    "        self.computation_log.append({\n",
    "            'feature': feature_name,\n",
    "            'computed_at': end_time,\n",
    "            'duration_seconds': (end_time - start_time).total_seconds(),\n",
    "            'row_count': len(feature_df)\n",
    "        })\n",
    "        \n",
    "        return feature_df\n",
    "    \n",
    "    def get_features_point_in_time(self, feature_names: List[str],\n",
    "                                    entity_df: pd.DataFrame,\n",
    "                                    timestamp_col: str = 'date') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get features at specific points in time\n",
    "        Critical for avoiding look-ahead bias in training data!\n",
    "        \"\"\"\n",
    "        result = entity_df.copy()\n",
    "        \n",
    "        for feature_name in feature_names:\n",
    "            if feature_name not in self.feature_data:\n",
    "                print(f\"âš ï¸ Feature {feature_name} not computed\")\n",
    "                result[feature_name] = np.nan\n",
    "                continue\n",
    "            \n",
    "            feature_df = self.feature_data[feature_name]\n",
    "            \n",
    "            # Merge on timestamp (point-in-time join)\n",
    "            result = result.merge(\n",
    "                feature_df[[timestamp_col, feature_name]],\n",
    "                on=timestamp_col,\n",
    "                how='left'\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_training_data(self, feature_names: List[str],\n",
    "                          start_date: str, end_date: str,\n",
    "                          timestamp_col: str = 'date') -> pd.DataFrame:\n",
    "        \"\"\"Get training data for date range\"\"\"\n",
    "        # Create entity dataframe for date range\n",
    "        dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        entity_df = pd.DataFrame({timestamp_col: dates})\n",
    "        \n",
    "        return self.get_features_point_in_time(feature_names, entity_df, timestamp_col)\n",
    "\n",
    "\n",
    "class OnlineFeatureStore:\n",
    "    \"\"\"\n",
    "    Online Feature Store for real-time serving\n",
    "    - Low latency feature retrieval\n",
    "    - Caching for frequently accessed features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry: FeatureRegistry, cache_ttl_seconds: int = 60):\n",
    "        self.registry = registry\n",
    "        self.feature_cache: Dict[str, Dict[str, Any]] = {}\n",
    "        self.cache_ttl = cache_ttl_seconds\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        \n",
    "    def set_feature(self, entity_id: str, feature_name: str, value: float):\n",
    "        \"\"\"Set feature value for an entity (e.g., a symbol)\"\"\"\n",
    "        cache_key = f\"{entity_id}:{feature_name}\"\n",
    "        self.feature_cache[cache_key] = {\n",
    "            'value': value,\n",
    "            'timestamp': datetime.now(),\n",
    "            'ttl': self.cache_ttl\n",
    "        }\n",
    "    \n",
    "    def get_feature(self, entity_id: str, feature_name: str) -> Optional[float]:\n",
    "        \"\"\"Get feature value for an entity\"\"\"\n",
    "        cache_key = f\"{entity_id}:{feature_name}\"\n",
    "        \n",
    "        if cache_key in self.feature_cache:\n",
    "            cached = self.feature_cache[cache_key]\n",
    "            age = (datetime.now() - cached['timestamp']).total_seconds()\n",
    "            \n",
    "            if age < cached['ttl']:\n",
    "                self.cache_hits += 1\n",
    "                return cached['value']\n",
    "        \n",
    "        self.cache_misses += 1\n",
    "        return None\n",
    "    \n",
    "    def get_features(self, entity_id: str, feature_names: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Get multiple features for an entity\"\"\"\n",
    "        return {\n",
    "            name: self.get_feature(entity_id, name)\n",
    "            for name in feature_names\n",
    "        }\n",
    "    \n",
    "    def bulk_update(self, entity_id: str, features: Dict[str, float]):\n",
    "        \"\"\"Bulk update features for an entity\"\"\"\n",
    "        for name, value in features.items():\n",
    "            self.set_feature(entity_id, name, value)\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = self.cache_hits + self.cache_misses\n",
    "        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'hit_rate': hit_rate,\n",
    "            'cache_size': len(self.feature_cache)\n",
    "        }\n",
    "\n",
    "\n",
    "class TradingFeatureStore:\n",
    "    \"\"\"\n",
    "    Unified Feature Store for Trading\n",
    "    Combines offline and online stores with the registry\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.registry = FeatureRegistry()\n",
    "        self.offline_store = OfflineFeatureStore(self.registry)\n",
    "        self.online_store = OnlineFeatureStore(self.registry)\n",
    "        \n",
    "    def register_feature(self, name: str, description: str, \n",
    "                         computation_func: callable,\n",
    "                         dtype: str = 'float64',\n",
    "                         group: Optional[str] = None,\n",
    "                         tags: Optional[List[str]] = None):\n",
    "        \"\"\"Register a new feature\"\"\"\n",
    "        feature = FeatureDefinition(\n",
    "            name=name,\n",
    "            description=description,\n",
    "            dtype=dtype,\n",
    "            computation_func=computation_func,\n",
    "            tags=tags or []\n",
    "        )\n",
    "        self.registry.register(feature, group)\n",
    "        \n",
    "    def materialize_offline(self, feature_names: List[str],\n",
    "                            source_data: pd.DataFrame,\n",
    "                            timestamp_col: str = 'date'):\n",
    "        \"\"\"Compute and store features in offline store\"\"\"\n",
    "        for name in feature_names:\n",
    "            self.offline_store.compute_and_store(name, source_data, timestamp_col)\n",
    "    \n",
    "    def sync_to_online(self, entity_id: str, features: Dict[str, float]):\n",
    "        \"\"\"Sync features to online store\"\"\"\n",
    "        self.online_store.bulk_update(entity_id, features)\n",
    "    \n",
    "    def get_training_features(self, feature_names: List[str],\n",
    "                              start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Get features for model training\"\"\"\n",
    "        return self.offline_store.get_training_data(feature_names, start_date, end_date)\n",
    "    \n",
    "    def get_serving_features(self, entity_id: str, \n",
    "                             feature_names: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Get features for real-time serving\"\"\"\n",
    "        return self.online_store.get_features(entity_id, feature_names)\n",
    "\n",
    "print(\"Feature Store implementation complete!\")\n",
    "print(\"Components: FeatureRegistry, OfflineStore, OnlineStore, TradingFeatureStore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2308e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3.3 Demo: Feature Store in Action\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize feature store\n",
    "feature_store = TradingFeatureStore()\n",
    "\n",
    "# Register trading features\n",
    "feature_store.register_feature(\n",
    "    name='sma_5',\n",
    "    description='5-day Simple Moving Average of close price',\n",
    "    computation_func=lambda df: df['close'].rolling(window=5).mean(),\n",
    "    group='technical_indicators',\n",
    "    tags=['price', 'trend', 'momentum']\n",
    ")\n",
    "\n",
    "feature_store.register_feature(\n",
    "    name='sma_20',\n",
    "    description='20-day Simple Moving Average of close price',\n",
    "    computation_func=lambda df: df['close'].rolling(window=20).mean(),\n",
    "    group='technical_indicators',\n",
    "    tags=['price', 'trend', 'momentum']\n",
    ")\n",
    "\n",
    "feature_store.register_feature(\n",
    "    name='rsi',\n",
    "    description='14-day Relative Strength Index',\n",
    "    computation_func=compute_rsi,\n",
    "    group='technical_indicators',\n",
    "    tags=['momentum', 'oscillator']\n",
    ")\n",
    "\n",
    "feature_store.register_feature(\n",
    "    name='volatility',\n",
    "    description='20-day rolling volatility of returns',\n",
    "    computation_func=compute_volatility,\n",
    "    group='risk_metrics',\n",
    "    tags=['risk', 'volatility']\n",
    ")\n",
    "\n",
    "feature_store.register_feature(\n",
    "    name='volume_ma',\n",
    "    description='10-day moving average of volume',\n",
    "    computation_func=compute_volume_ma,\n",
    "    group='volume_indicators',\n",
    "    tags=['volume', 'liquidity']\n",
    ")\n",
    "\n",
    "# Materialize features to offline store\n",
    "print(\"\\nðŸ“¦ Materializing features to offline store...\")\n",
    "feature_store.materialize_offline(\n",
    "    feature_names=['sma_5', 'sma_20', 'rsi', 'volatility', 'volume_ma'],\n",
    "    source_data=synthetic_data,\n",
    "    timestamp_col='date'\n",
    ")\n",
    "\n",
    "# Get training data\n",
    "print(\"\\nðŸ“Š Getting training data...\")\n",
    "training_data = feature_store.get_training_features(\n",
    "    feature_names=['sma_5', 'sma_20', 'rsi'],\n",
    "    start_date='2020-06-01',\n",
    "    end_date='2020-12-31'\n",
    ")\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(training_data.head())\n",
    "\n",
    "# Sync latest features to online store for a symbol\n",
    "print(\"\\nðŸ”„ Syncing to online store...\")\n",
    "latest_features = {\n",
    "    'sma_5': 105.23,\n",
    "    'sma_20': 102.15,\n",
    "    'rsi': 65.5,\n",
    "    'volatility': 0.023,\n",
    "    'volume_ma': 5000000\n",
    "}\n",
    "feature_store.sync_to_online('AAPL', latest_features)\n",
    "\n",
    "# Get features for real-time serving\n",
    "serving_features = feature_store.get_serving_features(\n",
    "    entity_id='AAPL',\n",
    "    feature_names=['sma_5', 'sma_20', 'rsi']\n",
    ")\n",
    "print(f\"\\nServing features for AAPL: {serving_features}\")\n",
    "\n",
    "# Search features\n",
    "print(\"\\nðŸ” Searching for 'momentum' features:\")\n",
    "momentum_features = feature_store.registry.search('momentum')\n",
    "for f in momentum_features:\n",
    "    print(f\"  - {f.name}: {f.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdc1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Monitoring and Drift Detection\n",
    "\n",
    "### 4.1 Types of Drift in Production ML\n",
    "\n",
    "In production trading systems, **drift** refers to changes in the statistical properties of data over time that can degrade model performance.\n",
    "\n",
    "#### Types of Drift:\n",
    "\n",
    "| Type | Description | Example in Trading |\n",
    "|------|-------------|-------------------|\n",
    "| **Data Drift** | Change in feature distributions | Volume patterns shift due to market structure changes |\n",
    "| **Concept Drift** | Change in relationship between features and target | Market regime change (bull to bear) |\n",
    "| **Prediction Drift** | Change in model output distribution | Model becomes overly bullish/bearish |\n",
    "| **Label Drift** | Change in target variable distribution | Volatility regime change |\n",
    "\n",
    "### 4.2 Statistical Tests for Drift Detection\n",
    "\n",
    "Common statistical tests used for drift detection:\n",
    "\n",
    "1. **Kolmogorov-Smirnov (KS) Test**: Compares two distributions\n",
    "2. **Population Stability Index (PSI)**: Measures distribution shift\n",
    "3. **Chi-Square Test**: For categorical variables\n",
    "4. **Wasserstein Distance**: Earth mover's distance between distributions\n",
    "5. **Jensen-Shannon Divergence**: Symmetric divergence measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede58368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4.3 Drift Detection Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"\n",
    "    Comprehensive drift detection for production ML\n",
    "    Implements multiple statistical tests for monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data: pd.DataFrame, feature_cols: List[str]):\n",
    "        self.reference_data = reference_data\n",
    "        self.feature_cols = feature_cols\n",
    "        self.thresholds = {\n",
    "            'ks_statistic': 0.1,      # KS test threshold\n",
    "            'psi': 0.2,               # PSI threshold (0.1 = slight, 0.2 = moderate)\n",
    "            'js_divergence': 0.1      # Jensen-Shannon divergence threshold\n",
    "        }\n",
    "        self.drift_history = []\n",
    "        \n",
    "    def compute_psi(self, expected: np.ndarray, actual: np.ndarray, \n",
    "                    n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Population Stability Index (PSI)\n",
    "        PSI < 0.1: No significant shift\n",
    "        PSI 0.1-0.2: Moderate shift, monitor closely\n",
    "        PSI > 0.2: Significant shift, investigate\n",
    "        \"\"\"\n",
    "        # Create bins based on expected distribution\n",
    "        breakpoints = np.percentile(expected, np.linspace(0, 100, n_bins + 1))\n",
    "        breakpoints[0] = -np.inf\n",
    "        breakpoints[-1] = np.inf\n",
    "        \n",
    "        # Calculate proportions in each bin\n",
    "        expected_counts = np.histogram(expected, bins=breakpoints)[0]\n",
    "        actual_counts = np.histogram(actual, bins=breakpoints)[0]\n",
    "        \n",
    "        # Normalize to proportions\n",
    "        expected_props = expected_counts / len(expected)\n",
    "        actual_props = actual_counts / len(actual)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        expected_props = np.where(expected_props == 0, 0.0001, expected_props)\n",
    "        actual_props = np.where(actual_props == 0, 0.0001, actual_props)\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi = np.sum((actual_props - expected_props) * np.log(actual_props / expected_props))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def compute_ks_statistic(self, expected: np.ndarray, actual: np.ndarray) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Kolmogorov-Smirnov test\n",
    "        Returns (statistic, p-value)\n",
    "        \"\"\"\n",
    "        statistic, p_value = ks_2samp(expected, actual)\n",
    "        return statistic, p_value\n",
    "    \n",
    "    def compute_js_divergence(self, expected: np.ndarray, actual: np.ndarray,\n",
    "                               n_bins: int = 50) -> float:\n",
    "        \"\"\"\n",
    "        Jensen-Shannon Divergence\n",
    "        Symmetric measure of distribution similarity\n",
    "        \"\"\"\n",
    "        # Create histogram bins\n",
    "        all_data = np.concatenate([expected, actual])\n",
    "        bins = np.histogram_bin_edges(all_data, bins=n_bins)\n",
    "        \n",
    "        # Get distributions\n",
    "        p = np.histogram(expected, bins=bins, density=True)[0]\n",
    "        q = np.histogram(actual, bins=bins, density=True)[0]\n",
    "        \n",
    "        # Normalize\n",
    "        p = p / p.sum() if p.sum() > 0 else p\n",
    "        q = q / q.sum() if q.sum() > 0 else q\n",
    "        \n",
    "        # Add small epsilon to avoid log(0)\n",
    "        epsilon = 1e-10\n",
    "        p = p + epsilon\n",
    "        q = q + epsilon\n",
    "        \n",
    "        # Compute JS divergence\n",
    "        m = 0.5 * (p + q)\n",
    "        js_div = 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
    "        \n",
    "        return js_div\n",
    "    \n",
    "    def detect_feature_drift(self, current_data: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Detect drift for all features\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for col in self.feature_cols:\n",
    "            if col not in current_data.columns or col not in self.reference_data.columns:\n",
    "                continue\n",
    "            \n",
    "            reference = self.reference_data[col].dropna().values\n",
    "            current = current_data[col].dropna().values\n",
    "            \n",
    "            if len(reference) < 10 or len(current) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Compute metrics\n",
    "            psi = self.compute_psi(reference, current)\n",
    "            ks_stat, ks_pvalue = self.compute_ks_statistic(reference, current)\n",
    "            js_div = self.compute_js_divergence(reference, current)\n",
    "            \n",
    "            # Determine drift status\n",
    "            drift_detected = (\n",
    "                psi > self.thresholds['psi'] or \n",
    "                ks_stat > self.thresholds['ks_statistic'] or\n",
    "                js_div > self.thresholds['js_divergence']\n",
    "            )\n",
    "            \n",
    "            results[col] = {\n",
    "                'psi': psi,\n",
    "                'ks_statistic': ks_stat,\n",
    "                'ks_pvalue': ks_pvalue,\n",
    "                'js_divergence': js_div,\n",
    "                'drift_detected': drift_detected,\n",
    "                'reference_mean': np.mean(reference),\n",
    "                'current_mean': np.mean(current),\n",
    "                'reference_std': np.std(reference),\n",
    "                'current_std': np.std(current)\n",
    "            }\n",
    "        \n",
    "        # Log results\n",
    "        self.drift_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'results': results\n",
    "        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_drift_summary(self, results: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"Create summary DataFrame of drift results\"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for feature, metrics in results.items():\n",
    "            summary_data.append({\n",
    "                'feature': feature,\n",
    "                'psi': metrics['psi'],\n",
    "                'ks_statistic': metrics['ks_statistic'],\n",
    "                'ks_pvalue': metrics['ks_pvalue'],\n",
    "                'js_divergence': metrics['js_divergence'],\n",
    "                'drift_detected': metrics['drift_detected'],\n",
    "                'mean_shift': metrics['current_mean'] - metrics['reference_mean'],\n",
    "                'std_ratio': metrics['current_std'] / metrics['reference_std'] if metrics['reference_std'] > 0 else 1\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "\n",
    "class ModelPerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor model performance metrics over time\n",
    "    Detect performance degradation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_metrics: Dict[str, float]):\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.performance_history = []\n",
    "        self.alert_thresholds = {\n",
    "            'accuracy_drop': 0.05,    # Alert if accuracy drops by 5%\n",
    "            'auc_drop': 0.05,         # Alert if AUC drops by 5%\n",
    "            'precision_drop': 0.1,    # Alert if precision drops by 10%\n",
    "            'recall_drop': 0.1        # Alert if recall drops by 10%\n",
    "        }\n",
    "        \n",
    "    def compute_metrics(self, y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                        y_proba: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
    "        \"\"\"Compute performance metrics\"\"\"\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                metrics['auc'] = roc_auc_score(y_true, y_proba)\n",
    "            except:\n",
    "                metrics['auc'] = 0.5\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def log_performance(self, y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                        y_proba: Optional[np.ndarray] = None,\n",
    "                        timestamp: Optional[datetime] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Log and analyze performance\"\"\"\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        metrics = self.compute_metrics(y_true, y_pred, y_proba)\n",
    "        \n",
    "        # Check for degradation\n",
    "        alerts = []\n",
    "        for metric_name, current_value in metrics.items():\n",
    "            baseline_value = self.baseline_metrics.get(metric_name, current_value)\n",
    "            drop = baseline_value - current_value\n",
    "            \n",
    "            threshold_key = f\"{metric_name}_drop\"\n",
    "            threshold = self.alert_thresholds.get(threshold_key, 0.1)\n",
    "            \n",
    "            if drop > threshold:\n",
    "                alerts.append({\n",
    "                    'metric': metric_name,\n",
    "                    'baseline': baseline_value,\n",
    "                    'current': current_value,\n",
    "                    'drop': drop,\n",
    "                    'threshold': threshold\n",
    "                })\n",
    "        \n",
    "        result = {\n",
    "            'timestamp': timestamp,\n",
    "            'metrics': metrics,\n",
    "            'alerts': alerts,\n",
    "            'degraded': len(alerts) > 0\n",
    "        }\n",
    "        \n",
    "        self.performance_history.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_performance_trend(self, metric_name: str = 'accuracy') -> pd.DataFrame:\n",
    "        \"\"\"Get historical trend for a metric\"\"\"\n",
    "        data = []\n",
    "        for record in self.performance_history:\n",
    "            data.append({\n",
    "                'timestamp': record['timestamp'],\n",
    "                'value': record['metrics'].get(metric_name, np.nan),\n",
    "                'baseline': self.baseline_metrics.get(metric_name, np.nan)\n",
    "            })\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "print(\"Drift Detection and Performance Monitoring implemented!\")\n",
    "print(\"Classes: DriftDetector, ModelPerformanceMonitor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4.4 Demo: Drift Detection in Action\n",
    "# ============================================================================\n",
    "\n",
    "# Split data into reference (training) and current (production) periods\n",
    "reference_period = features_with_target[features_with_target['date'] < '2020-07-01']\n",
    "current_period = features_with_target[features_with_target['date'] >= '2020-07-01']\n",
    "\n",
    "# Simulate drift by modifying current data\n",
    "drifted_current = current_period.copy()\n",
    "drifted_current['sma_5'] = drifted_current['sma_5'] * 1.2  # 20% increase\n",
    "drifted_current['volatility'] = drifted_current['volatility'] * 1.5  # 50% increase\n",
    "drifted_current['rsi'] = drifted_current['rsi'] + np.random.randn(len(drifted_current)) * 10\n",
    "\n",
    "# Initialize drift detector\n",
    "detector = DriftDetector(\n",
    "    reference_data=reference_period,\n",
    "    feature_cols=feature_cols\n",
    ")\n",
    "\n",
    "# Detect drift\n",
    "print(\"=\" * 60)\n",
    "print(\"DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with non-drifted data\n",
    "print(\"\\n1. Testing with non-drifted production data:\")\n",
    "results_no_drift = detector.detect_feature_drift(current_period)\n",
    "summary_no_drift = detector.get_drift_summary(results_no_drift)\n",
    "print(summary_no_drift.to_string(index=False))\n",
    "\n",
    "# Test with drifted data\n",
    "print(\"\\n2. Testing with artificially drifted data:\")\n",
    "results_drift = detector.detect_feature_drift(drifted_current)\n",
    "summary_drift = detector.get_drift_summary(results_drift)\n",
    "print(summary_drift.to_string(index=False))\n",
    "\n",
    "# Visualize drift\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: PSI comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = range(len(feature_cols))\n",
    "psi_no_drift = [results_no_drift[col]['psi'] for col in feature_cols]\n",
    "psi_drift = [results_drift[col]['psi'] for col in feature_cols]\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], psi_no_drift, width, label='No Drift', alpha=0.8)\n",
    "ax1.bar([i + width/2 for i in x], psi_drift, width, label='With Drift', alpha=0.8)\n",
    "ax1.axhline(y=0.2, color='r', linestyle='--', label='PSI Threshold')\n",
    "ax1.set_xlabel('Feature')\n",
    "ax1.set_ylabel('PSI Score')\n",
    "ax1.set_title('Population Stability Index (PSI)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(feature_cols, rotation=45)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: KS Statistics\n",
    "ax2 = axes[0, 1]\n",
    "ks_no_drift = [results_no_drift[col]['ks_statistic'] for col in feature_cols]\n",
    "ks_drift = [results_drift[col]['ks_statistic'] for col in feature_cols]\n",
    "ax2.bar([i - width/2 for i in x], ks_no_drift, width, label='No Drift', alpha=0.8)\n",
    "ax2.bar([i + width/2 for i in x], ks_drift, width, label='With Drift', alpha=0.8)\n",
    "ax2.axhline(y=0.1, color='r', linestyle='--', label='KS Threshold')\n",
    "ax2.set_xlabel('Feature')\n",
    "ax2.set_ylabel('KS Statistic')\n",
    "ax2.set_title('Kolmogorov-Smirnov Test')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(feature_cols, rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Distribution comparison for sma_5\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(reference_period['sma_5'].dropna(), bins=30, alpha=0.5, label='Reference', density=True)\n",
    "ax3.hist(current_period['sma_5'].dropna(), bins=30, alpha=0.5, label='Current (No Drift)', density=True)\n",
    "ax3.hist(drifted_current['sma_5'].dropna(), bins=30, alpha=0.5, label='Current (With Drift)', density=True)\n",
    "ax3.set_xlabel('SMA-5 Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('SMA-5 Distribution Comparison')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Distribution comparison for volatility\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(reference_period['volatility'].dropna(), bins=30, alpha=0.5, label='Reference', density=True)\n",
    "ax4.hist(current_period['volatility'].dropna(), bins=30, alpha=0.5, label='Current (No Drift)', density=True)\n",
    "ax4.hist(drifted_current['volatility'].dropna(), bins=30, alpha=0.5, label='Current (With Drift)', density=True)\n",
    "ax4.set_xlabel('Volatility Value')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Volatility Distribution Comparison')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alert summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DRIFT ALERTS\")\n",
    "print(\"=\" * 60)\n",
    "for col in feature_cols:\n",
    "    if results_drift[col]['drift_detected']:\n",
    "        print(f\"âš ï¸  DRIFT DETECTED in '{col}':\")\n",
    "        print(f\"    PSI: {results_drift[col]['psi']:.4f}\")\n",
    "        print(f\"    KS Statistic: {results_drift[col]['ks_statistic']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa2bba",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. A/B Testing Framework\n",
    "\n",
    "### 5.1 A/B Testing in Trading Systems\n",
    "\n",
    "A/B testing in quantitative trading allows us to compare different models or strategies in a controlled manner. Unlike typical web A/B tests, trading A/B tests have unique challenges:\n",
    "\n",
    "#### Key Considerations:\n",
    "1. **Non-IID Data**: Market data is autocorrelated\n",
    "2. **Market Impact**: Running multiple strategies may affect outcomes\n",
    "3. **Capital Allocation**: How to split capital between strategies\n",
    "4. **Statistical Power**: Need sufficient trades for significance\n",
    "5. **Time-Based Effects**: Markets change over time\n",
    "\n",
    "### A/B Testing Approaches for Trading:\n",
    "| Approach | Description | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Paper Trading** | Run new model in simulation | No real risk | May not capture market impact |\n",
    "| **Shadow Mode** | Run alongside production, don't execute | Real data | No execution feedback |\n",
    "| **Canary Deployment** | Small % of capital | Real validation | Lower statistical power |\n",
    "| **Time-Split** | Alternate between models | Simple | Time confounds |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c32e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5.2 A/B Testing Framework Implementation\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ABTestConfig:\n",
    "    \"\"\"Configuration for A/B test\"\"\"\n",
    "    test_name: str\n",
    "    control_model_name: str\n",
    "    treatment_model_name: str\n",
    "    traffic_split: float = 0.5  # Fraction to treatment\n",
    "    min_samples: int = 100\n",
    "    confidence_level: float = 0.95\n",
    "    primary_metric: str = 'sharpe_ratio'\n",
    "    secondary_metrics: List[str] = field(default_factory=lambda: ['accuracy', 'returns'])\n",
    "\n",
    "\n",
    "@dataclass  \n",
    "class ABTestResult:\n",
    "    \"\"\"Results from an A/B test\"\"\"\n",
    "    test_name: str\n",
    "    control_metrics: Dict[str, float]\n",
    "    treatment_metrics: Dict[str, float]\n",
    "    statistical_significance: Dict[str, bool]\n",
    "    p_values: Dict[str, float]\n",
    "    effect_sizes: Dict[str, float]\n",
    "    recommendation: str\n",
    "    sample_sizes: Dict[str, int]\n",
    "\n",
    "\n",
    "class TradingABTestFramework:\n",
    "    \"\"\"\n",
    "    A/B Testing Framework for Trading Strategies\n",
    "    Supports statistical comparison of models with multiple metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: ABTestConfig):\n",
    "        self.config = config\n",
    "        self.control_results = []\n",
    "        self.treatment_results = []\n",
    "        self.is_active = False\n",
    "        self.start_time = None\n",
    "        \n",
    "    def start_test(self):\n",
    "        \"\"\"Start the A/B test\"\"\"\n",
    "        self.is_active = True\n",
    "        self.start_time = datetime.now()\n",
    "        self.control_results = []\n",
    "        self.treatment_results = []\n",
    "        print(f\"ðŸš€ A/B Test '{self.config.test_name}' started\")\n",
    "        print(f\"   Control: {self.config.control_model_name}\")\n",
    "        print(f\"   Treatment: {self.config.treatment_model_name}\")\n",
    "        print(f\"   Traffic split: {(1-self.config.traffic_split)*100:.0f}% / {self.config.traffic_split*100:.0f}%\")\n",
    "        \n",
    "    def assign_variant(self) -> str:\n",
    "        \"\"\"Randomly assign to control or treatment\"\"\"\n",
    "        if np.random.random() < self.config.traffic_split:\n",
    "            return 'treatment'\n",
    "        return 'control'\n",
    "    \n",
    "    def log_result(self, variant: str, result: Dict[str, float]):\n",
    "        \"\"\"Log result for a variant\"\"\"\n",
    "        if not self.is_active:\n",
    "            print(\"âš ï¸ Test not active\")\n",
    "            return\n",
    "        \n",
    "        result['timestamp'] = datetime.now()\n",
    "        if variant == 'control':\n",
    "            self.control_results.append(result)\n",
    "        else:\n",
    "            self.treatment_results.append(result)\n",
    "    \n",
    "    def compute_trading_metrics(self, results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Compute trading-specific metrics\"\"\"\n",
    "        if not results:\n",
    "            return {}\n",
    "        \n",
    "        returns = [r.get('return', 0) for r in results]\n",
    "        predictions_correct = [r.get('correct', 0) for r in results]\n",
    "        \n",
    "        metrics = {\n",
    "            'n_samples': len(results),\n",
    "            'mean_return': np.mean(returns),\n",
    "            'total_return': np.sum(returns),\n",
    "            'std_return': np.std(returns),\n",
    "            'sharpe_ratio': np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0,\n",
    "            'win_rate': np.mean(predictions_correct) if predictions_correct else 0,\n",
    "            'max_drawdown': self._compute_max_drawdown(returns),\n",
    "            'sortino_ratio': self._compute_sortino_ratio(returns)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_max_drawdown(self, returns: List[float]) -> float:\n",
    "        \"\"\"Compute maximum drawdown\"\"\"\n",
    "        if not returns:\n",
    "            return 0\n",
    "        cumulative = np.cumprod(1 + np.array(returns))\n",
    "        running_max = np.maximum.accumulate(cumulative)\n",
    "        drawdowns = (cumulative - running_max) / running_max\n",
    "        return abs(min(drawdowns)) if len(drawdowns) > 0 else 0\n",
    "    \n",
    "    def _compute_sortino_ratio(self, returns: List[float], target: float = 0) -> float:\n",
    "        \"\"\"Compute Sortino ratio (downside deviation)\"\"\"\n",
    "        returns = np.array(returns)\n",
    "        downside_returns = returns[returns < target]\n",
    "        downside_std = np.std(downside_returns) if len(downside_returns) > 0 else 1\n",
    "        return (np.mean(returns) - target) / downside_std * np.sqrt(252) if downside_std > 0 else 0\n",
    "    \n",
    "    def run_statistical_tests(self, metric: str) -> Tuple[float, bool, float]:\n",
    "        \"\"\"\n",
    "        Run statistical test for a specific metric\n",
    "        Returns: (p_value, is_significant, effect_size)\n",
    "        \"\"\"\n",
    "        control_values = [r.get(metric, 0) for r in self.control_results if metric in r]\n",
    "        treatment_values = [r.get(metric, 0) for r in self.treatment_results if metric in r]\n",
    "        \n",
    "        if len(control_values) < 5 or len(treatment_values) < 5:\n",
    "            return 1.0, False, 0.0\n",
    "        \n",
    "        # Two-sample t-test\n",
    "        t_stat, p_value = stats.ttest_ind(treatment_values, control_values)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt((np.var(control_values) + np.var(treatment_values)) / 2)\n",
    "        effect_size = (np.mean(treatment_values) - np.mean(control_values)) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Significance\n",
    "        alpha = 1 - self.config.confidence_level\n",
    "        is_significant = p_value < alpha\n",
    "        \n",
    "        return p_value, is_significant, effect_size\n",
    "    \n",
    "    def analyze_results(self) -> ABTestResult:\n",
    "        \"\"\"Comprehensive analysis of A/B test results\"\"\"\n",
    "        # Compute metrics for both variants\n",
    "        control_metrics = self.compute_trading_metrics(self.control_results)\n",
    "        treatment_metrics = self.compute_trading_metrics(self.treatment_results)\n",
    "        \n",
    "        # Statistical tests for all metrics\n",
    "        p_values = {}\n",
    "        significance = {}\n",
    "        effect_sizes = {}\n",
    "        \n",
    "        metrics_to_test = ['return', 'correct']\n",
    "        for metric in metrics_to_test:\n",
    "            p_val, is_sig, effect = self.run_statistical_tests(metric)\n",
    "            p_values[metric] = p_val\n",
    "            significance[metric] = is_sig\n",
    "            effect_sizes[metric] = effect\n",
    "        \n",
    "        # Generate recommendation\n",
    "        recommendation = self._generate_recommendation(\n",
    "            control_metrics, treatment_metrics, significance\n",
    "        )\n",
    "        \n",
    "        return ABTestResult(\n",
    "            test_name=self.config.test_name,\n",
    "            control_metrics=control_metrics,\n",
    "            treatment_metrics=treatment_metrics,\n",
    "            statistical_significance=significance,\n",
    "            p_values=p_values,\n",
    "            effect_sizes=effect_sizes,\n",
    "            recommendation=recommendation,\n",
    "            sample_sizes={\n",
    "                'control': len(self.control_results),\n",
    "                'treatment': len(self.treatment_results)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def _generate_recommendation(self, control: Dict, treatment: Dict, \n",
    "                                  significance: Dict) -> str:\n",
    "        \"\"\"Generate deployment recommendation\"\"\"\n",
    "        primary_metric = self.config.primary_metric\n",
    "        \n",
    "        if control.get('n_samples', 0) < self.config.min_samples:\n",
    "            return \"INSUFFICIENT_DATA: Need more samples for reliable conclusion\"\n",
    "        \n",
    "        control_value = control.get(primary_metric, 0)\n",
    "        treatment_value = treatment.get(primary_metric, 0)\n",
    "        \n",
    "        improvement = (treatment_value - control_value) / abs(control_value) if control_value != 0 else 0\n",
    "        \n",
    "        if significance.get('return', False) and improvement > 0.05:\n",
    "            return f\"DEPLOY_TREATMENT: {improvement*100:.1f}% improvement in {primary_metric} (statistically significant)\"\n",
    "        elif significance.get('return', False) and improvement < -0.05:\n",
    "            return f\"KEEP_CONTROL: Treatment shows {improvement*100:.1f}% degradation (statistically significant)\"\n",
    "        elif not any(significance.values()):\n",
    "            return \"NO_SIGNIFICANT_DIFFERENCE: Continue testing or consider other factors\"\n",
    "        else:\n",
    "            return f\"MIXED_RESULTS: Further analysis needed (improvement: {improvement*100:.1f}%)\"\n",
    "    \n",
    "    def stop_test(self) -> ABTestResult:\n",
    "        \"\"\"Stop test and return final results\"\"\"\n",
    "        self.is_active = False\n",
    "        duration = datetime.now() - self.start_time if self.start_time else timedelta(0)\n",
    "        print(f\"\\nðŸ›‘ A/B Test '{self.config.test_name}' stopped after {duration}\")\n",
    "        return self.analyze_results()\n",
    "\n",
    "print(\"A/B Testing Framework implemented!\")\n",
    "print(\"Classes: ABTestConfig, ABTestResult, TradingABTestFramework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca21e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5.3 Demo: A/B Testing Trading Models\n",
    "# ============================================================================\n",
    "\n",
    "# Create two models with different characteristics\n",
    "# Control: Random Forest (baseline)\n",
    "# Treatment: Gradient Boosting (challenger)\n",
    "\n",
    "# Prepare data\n",
    "X = features_with_target[feature_cols].dropna()\n",
    "y = features_with_target.loc[X.index, 'target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train models\n",
    "control_model = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "treatment_model = GradientBoostingClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "\n",
    "control_model.fit(X_train, y_train)\n",
    "treatment_model.fit(X_train, y_train)\n",
    "\n",
    "# Configure A/B test\n",
    "ab_config = ABTestConfig(\n",
    "    test_name=\"RF_vs_GBM_Trading_Model\",\n",
    "    control_model_name=\"RandomForest_v1\",\n",
    "    treatment_model_name=\"GradientBoosting_v1\",\n",
    "    traffic_split=0.5,\n",
    "    min_samples=50,\n",
    "    confidence_level=0.95,\n",
    "    primary_metric='sharpe_ratio'\n",
    ")\n",
    "\n",
    "# Initialize and start test\n",
    "ab_framework = TradingABTestFramework(ab_config)\n",
    "ab_framework.start_test()\n",
    "\n",
    "# Simulate trading decisions\n",
    "print(\"\\nðŸ“Š Simulating trading decisions...\")\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    features = X_test.iloc[i:i+1]\n",
    "    actual_direction = y_test.iloc[i]\n",
    "    \n",
    "    # Assign to variant\n",
    "    variant = ab_framework.assign_variant()\n",
    "    \n",
    "    # Get prediction based on variant\n",
    "    if variant == 'control':\n",
    "        prediction = control_model.predict(features)[0]\n",
    "        prob = control_model.predict_proba(features)[0, 1]\n",
    "    else:\n",
    "        prediction = treatment_model.predict(features)[0]\n",
    "        prob = treatment_model.predict_proba(features)[0, 1]\n",
    "    \n",
    "    # Simulate return based on prediction correctness\n",
    "    # If prediction correct, positive return; otherwise negative\n",
    "    base_return = np.random.randn() * 0.02  # 2% daily volatility\n",
    "    if prediction == actual_direction:\n",
    "        trade_return = abs(base_return) * 0.5 + 0.002  # Small positive edge\n",
    "        correct = 1\n",
    "    else:\n",
    "        trade_return = -abs(base_return) * 0.3 - 0.001  # Loss\n",
    "        correct = 0\n",
    "    \n",
    "    # Log result\n",
    "    ab_framework.log_result(variant, {\n",
    "        'return': trade_return,\n",
    "        'correct': correct,\n",
    "        'probability': prob,\n",
    "        'prediction': prediction,\n",
    "        'actual': actual_direction\n",
    "    })\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"A/B TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = ab_framework.stop_test()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Control Model ({ab_config.control_model_name}):\")\n",
    "for metric, value in results.control_metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Treatment Model ({ab_config.treatment_model_name}):\")\n",
    "for metric, value in results.treatment_metrics.items():\n",
    "    print(f\"   {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Statistical Significance:\")\n",
    "for metric, is_sig in results.statistical_significance.items():\n",
    "    p_val = results.p_values[metric]\n",
    "    effect = results.effect_sizes[metric]\n",
    "    sig_str = \"âœ… Significant\" if is_sig else \"âŒ Not significant\"\n",
    "    print(f\"   {metric}: {sig_str} (p={p_val:.4f}, effect size={effect:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Recommendation: {results.recommendation}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Sample sizes\n",
    "ax1 = axes[0]\n",
    "variants = ['Control', 'Treatment']\n",
    "sizes = [results.sample_sizes['control'], results.sample_sizes['treatment']]\n",
    "ax1.bar(variants, sizes, color=['blue', 'orange'], alpha=0.7)\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.set_title('Sample Sizes by Variant')\n",
    "for i, v in enumerate(sizes):\n",
    "    ax1.text(i, v + 1, str(v), ha='center')\n",
    "\n",
    "# Plot 2: Key metrics comparison\n",
    "ax2 = axes[1]\n",
    "metrics = ['mean_return', 'sharpe_ratio', 'win_rate']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "control_vals = [results.control_metrics.get(m, 0) for m in metrics]\n",
    "treatment_vals = [results.treatment_metrics.get(m, 0) for m in metrics]\n",
    "ax2.bar(x - width/2, control_vals, width, label='Control', alpha=0.7)\n",
    "ax2.bar(x + width/2, treatment_vals, width, label='Treatment', alpha=0.7)\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Metrics Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(metrics, rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: P-values\n",
    "ax3 = axes[2]\n",
    "p_metrics = list(results.p_values.keys())\n",
    "p_vals = list(results.p_values.values())\n",
    "colors = ['green' if p < 0.05 else 'red' for p in p_vals]\n",
    "ax3.barh(p_metrics, p_vals, color=colors, alpha=0.7)\n",
    "ax3.axvline(x=0.05, color='black', linestyle='--', label='Î± = 0.05')\n",
    "ax3.set_xlabel('P-Value')\n",
    "ax3.set_title('Statistical Significance')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be118c3",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Versioning and Registry\n",
    "\n",
    "### 6.1 Importance of Model Versioning\n",
    "\n",
    "In production trading systems, proper model versioning is critical for:\n",
    "\n",
    "1. **Reproducibility**: Recreate exact model states\n",
    "2. **Auditability**: Track what model made which decisions\n",
    "3. **Rollback**: Quickly revert to previous versions\n",
    "4. **Compliance**: Regulatory requirements for model governance\n",
    "5. **Experimentation**: Track experiments and compare results\n",
    "\n",
    "### Model Registry Components:\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     MODEL REGISTRY                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚  Model Metadata â”‚          â”‚  Model Artifacts â”‚          â”‚\n",
    "â”‚  â”‚  - Name         â”‚          â”‚  - Weights       â”‚          â”‚\n",
    "â”‚  â”‚  - Version      â”‚          â”‚  - Preprocessors â”‚          â”‚\n",
    "â”‚  â”‚  - Author       â”‚          â”‚  - Configs       â”‚          â”‚\n",
    "â”‚  â”‚  - Created At   â”‚          â”‚  - Dependencies  â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Training Info   â”‚          â”‚   Performance   â”‚          â”‚\n",
    "â”‚  â”‚ - Parameters    â”‚          â”‚   - Metrics     â”‚          â”‚\n",
    "â”‚  â”‚ - Data Version  â”‚          â”‚   - Validation  â”‚          â”‚\n",
    "â”‚  â”‚ - Git Commit    â”‚          â”‚   - Monitoring  â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\n",
    "â”‚  â”‚                    Lifecycle Stage                      â”‚â”‚\n",
    "â”‚  â”‚  [Development] â†’ [Staging] â†’ [Production] â†’ [Archived] â”‚â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ac85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6.2 Model Registry Implementation\n",
    "# ============================================================================\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class ModelStage(Enum):\n",
    "    \"\"\"Model lifecycle stages\"\"\"\n",
    "    DEVELOPMENT = \"development\"\n",
    "    STAGING = \"staging\"\n",
    "    PRODUCTION = \"production\"\n",
    "    ARCHIVED = \"archived\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelVersion:\n",
    "    \"\"\"Represents a specific version of a model\"\"\"\n",
    "    model_name: str\n",
    "    version: str\n",
    "    stage: ModelStage\n",
    "    created_at: datetime\n",
    "    author: str\n",
    "    description: str\n",
    "    \n",
    "    # Training info\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    training_data_version: str\n",
    "    training_data_hash: str\n",
    "    git_commit: Optional[str] = None\n",
    "    \n",
    "    # Performance metrics\n",
    "    metrics: Dict[str, float] = field(default_factory=dict)\n",
    "    \n",
    "    # Artifacts\n",
    "    model_artifact_path: Optional[str] = None\n",
    "    preprocessor_path: Optional[str] = None\n",
    "    \n",
    "    # Tags and metadata\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'model_name': self.model_name,\n",
    "            'version': self.version,\n",
    "            'stage': self.stage.value,\n",
    "            'created_at': self.created_at.isoformat(),\n",
    "            'author': self.author,\n",
    "            'description': self.description,\n",
    "            'hyperparameters': self.hyperparameters,\n",
    "            'training_data_version': self.training_data_version,\n",
    "            'training_data_hash': self.training_data_hash,\n",
    "            'git_commit': self.git_commit,\n",
    "            'metrics': self.metrics,\n",
    "            'tags': self.tags,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Comprehensive Model Registry\n",
    "    Tracks model versions, artifacts, and lifecycle\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"./model_registry\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.models: Dict[str, Dict[str, ModelVersion]] = {}  # name -> version -> ModelVersion\n",
    "        self.production_models: Dict[str, str] = {}  # name -> production version\n",
    "        self.registry_log = []\n",
    "        \n",
    "    def _generate_version(self, model_name: str) -> str:\n",
    "        \"\"\"Generate next version number\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            return \"1.0.0\"\n",
    "        \n",
    "        versions = list(self.models[model_name].keys())\n",
    "        if not versions:\n",
    "            return \"1.0.0\"\n",
    "        \n",
    "        # Parse latest version and increment\n",
    "        latest = max(versions, key=lambda v: [int(x) for x in v.split('.')])\n",
    "        parts = [int(x) for x in latest.split('.')]\n",
    "        parts[2] += 1  # Increment patch version\n",
    "        return '.'.join(map(str, parts))\n",
    "    \n",
    "    def _compute_data_hash(self, data: pd.DataFrame) -> str:\n",
    "        \"\"\"Compute hash of training data for versioning\"\"\"\n",
    "        data_str = data.to_json()\n",
    "        return hashlib.md5(data_str.encode()).hexdigest()[:12]\n",
    "    \n",
    "    def register_model(self, model_name: str, model: Any,\n",
    "                       hyperparameters: Dict[str, Any],\n",
    "                       metrics: Dict[str, float],\n",
    "                       training_data: pd.DataFrame,\n",
    "                       author: str = \"system\",\n",
    "                       description: str = \"\",\n",
    "                       tags: Optional[List[str]] = None,\n",
    "                       preprocessor: Any = None) -> ModelVersion:\n",
    "        \"\"\"Register a new model version\"\"\"\n",
    "        \n",
    "        # Generate version\n",
    "        version = self._generate_version(model_name)\n",
    "        \n",
    "        # Compute data hash\n",
    "        data_hash = self._compute_data_hash(training_data)\n",
    "        \n",
    "        # Create version entry\n",
    "        model_version = ModelVersion(\n",
    "            model_name=model_name,\n",
    "            version=version,\n",
    "            stage=ModelStage.DEVELOPMENT,\n",
    "            created_at=datetime.now(),\n",
    "            author=author,\n",
    "            description=description,\n",
    "            hyperparameters=hyperparameters,\n",
    "            training_data_version=f\"v_{data_hash}\",\n",
    "            training_data_hash=data_hash,\n",
    "            metrics=metrics,\n",
    "            tags=tags or []\n",
    "        )\n",
    "        \n",
    "        # Save model artifacts\n",
    "        model_dir = self.storage_path / model_name / version\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Serialize model\n",
    "        model_path = model_dir / \"model.pkl\"\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        model_version.model_artifact_path = str(model_path)\n",
    "        \n",
    "        # Serialize preprocessor if provided\n",
    "        if preprocessor:\n",
    "            preprocessor_path = model_dir / \"preprocessor.pkl\"\n",
    "            with open(preprocessor_path, 'wb') as f:\n",
    "                pickle.dump(preprocessor, f)\n",
    "            model_version.preprocessor_path = str(preprocessor_path)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = model_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(model_version.to_dict(), f, indent=2)\n",
    "        \n",
    "        # Store in registry\n",
    "        if model_name not in self.models:\n",
    "            self.models[model_name] = {}\n",
    "        self.models[model_name][version] = model_version\n",
    "        \n",
    "        # Log\n",
    "        self.registry_log.append({\n",
    "            'action': 'register',\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… Registered {model_name} v{version}\")\n",
    "        return model_version\n",
    "    \n",
    "    def transition_stage(self, model_name: str, version: str, \n",
    "                         new_stage: ModelStage) -> bool:\n",
    "        \"\"\"Transition model to a new lifecycle stage\"\"\"\n",
    "        if model_name not in self.models or version not in self.models[model_name]:\n",
    "            print(f\"âŒ Model {model_name} v{version} not found\")\n",
    "            return False\n",
    "        \n",
    "        model_version = self.models[model_name][version]\n",
    "        old_stage = model_version.stage\n",
    "        \n",
    "        # If promoting to production, archive current production model\n",
    "        if new_stage == ModelStage.PRODUCTION:\n",
    "            if model_name in self.production_models:\n",
    "                old_prod_version = self.production_models[model_name]\n",
    "                if old_prod_version != version:\n",
    "                    self.models[model_name][old_prod_version].stage = ModelStage.ARCHIVED\n",
    "                    print(f\"ðŸ“¦ Archived {model_name} v{old_prod_version}\")\n",
    "            \n",
    "            self.production_models[model_name] = version\n",
    "        \n",
    "        model_version.stage = new_stage\n",
    "        \n",
    "        # Log\n",
    "        self.registry_log.append({\n",
    "            'action': 'transition',\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'old_stage': old_stage.value,\n",
    "            'new_stage': new_stage.value,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… Transitioned {model_name} v{version}: {old_stage.value} â†’ {new_stage.value}\")\n",
    "        return True\n",
    "    \n",
    "    def get_model(self, model_name: str, version: Optional[str] = None,\n",
    "                  stage: Optional[ModelStage] = None) -> Tuple[Any, ModelVersion]:\n",
    "        \"\"\"Load a model from registry\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        # Determine version to load\n",
    "        if version:\n",
    "            target_version = version\n",
    "        elif stage == ModelStage.PRODUCTION and model_name in self.production_models:\n",
    "            target_version = self.production_models[model_name]\n",
    "        elif stage:\n",
    "            # Find latest version in requested stage\n",
    "            versions = [v for v, mv in self.models[model_name].items() if mv.stage == stage]\n",
    "            if not versions:\n",
    "                raise ValueError(f\"No model in stage {stage.value}\")\n",
    "            target_version = max(versions)\n",
    "        else:\n",
    "            # Get latest version\n",
    "            target_version = max(self.models[model_name].keys())\n",
    "        \n",
    "        model_version = self.models[model_name][target_version]\n",
    "        \n",
    "        # Load model artifact\n",
    "        with open(model_version.model_artifact_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        return model, model_version\n",
    "    \n",
    "    def list_models(self, model_name: Optional[str] = None,\n",
    "                    stage: Optional[ModelStage] = None) -> pd.DataFrame:\n",
    "        \"\"\"List registered models\"\"\"\n",
    "        records = []\n",
    "        \n",
    "        for name, versions in self.models.items():\n",
    "            if model_name and name != model_name:\n",
    "                continue\n",
    "            \n",
    "            for version, mv in versions.items():\n",
    "                if stage and mv.stage != stage:\n",
    "                    continue\n",
    "                \n",
    "                records.append({\n",
    "                    'model_name': name,\n",
    "                    'version': version,\n",
    "                    'stage': mv.stage.value,\n",
    "                    'created_at': mv.created_at,\n",
    "                    'author': mv.author,\n",
    "                    **mv.metrics\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def compare_versions(self, model_name: str, \n",
    "                         version_a: str, version_b: str) -> pd.DataFrame:\n",
    "        \"\"\"Compare two model versions\"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        mv_a = self.models[model_name].get(version_a)\n",
    "        mv_b = self.models[model_name].get(version_b)\n",
    "        \n",
    "        if not mv_a or not mv_b:\n",
    "            raise ValueError(\"One or both versions not found\")\n",
    "        \n",
    "        comparison = []\n",
    "        \n",
    "        # Compare metrics\n",
    "        all_metrics = set(mv_a.metrics.keys()) | set(mv_b.metrics.keys())\n",
    "        for metric in all_metrics:\n",
    "            val_a = mv_a.metrics.get(metric, np.nan)\n",
    "            val_b = mv_b.metrics.get(metric, np.nan)\n",
    "            diff = val_b - val_a if not np.isnan(val_a) and not np.isnan(val_b) else np.nan\n",
    "            comparison.append({\n",
    "                'metric': metric,\n",
    "                f'v{version_a}': val_a,\n",
    "                f'v{version_b}': val_b,\n",
    "                'difference': diff,\n",
    "                'improvement': 'âœ…' if diff > 0 else ('âŒ' if diff < 0 else 'âž¡ï¸')\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison)\n",
    "    \n",
    "    def get_production_model(self, model_name: str) -> Tuple[Any, ModelVersion]:\n",
    "        \"\"\"Get the production model for a given name\"\"\"\n",
    "        return self.get_model(model_name, stage=ModelStage.PRODUCTION)\n",
    "\n",
    "print(\"Model Registry implementation complete!\")\n",
    "print(\"Features: Versioning, Stage Management, Artifact Storage, Comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6.3 Demo: Model Registry in Action\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize registry\n",
    "registry = ModelRegistry(storage_path=\"./demo_model_registry\")\n",
    "\n",
    "# Prepare training data\n",
    "X_demo = features_with_target[feature_cols].dropna()\n",
    "y_demo = features_with_target.loc[X_demo.index, 'target']\n",
    "X_train_demo, X_test_demo, y_train_demo, y_test_demo = train_test_split(\n",
    "    X_demo, y_demo, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train and register multiple model versions\n",
    "print(\"=\" * 60)\n",
    "print(\"REGISTERING MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Version 1: Simple Random Forest\n",
    "model_v1 = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "model_v1.fit(X_train_demo, y_train_demo)\n",
    "y_pred_v1 = model_v1.predict(X_test_demo)\n",
    "y_proba_v1 = model_v1.predict_proba(X_test_demo)[:, 1]\n",
    "\n",
    "metrics_v1 = {\n",
    "    'accuracy': accuracy_score(y_test_demo, y_pred_v1),\n",
    "    'precision': precision_score(y_test_demo, y_pred_v1, zero_division=0),\n",
    "    'recall': recall_score(y_test_demo, y_pred_v1, zero_division=0),\n",
    "    'auc': roc_auc_score(y_test_demo, y_proba_v1)\n",
    "}\n",
    "\n",
    "mv1 = registry.register_model(\n",
    "    model_name=\"TradingSignalModel\",\n",
    "    model=model_v1,\n",
    "    hyperparameters={'n_estimators': 50, 'max_depth': 3},\n",
    "    metrics=metrics_v1,\n",
    "    training_data=features_with_target,\n",
    "    author=\"alice\",\n",
    "    description=\"Initial Random Forest model with basic features\",\n",
    "    tags=['baseline', 'random_forest']\n",
    ")\n",
    "\n",
    "# Version 2: Improved Random Forest\n",
    "model_v2 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "model_v2.fit(X_train_demo, y_train_demo)\n",
    "y_pred_v2 = model_v2.predict(X_test_demo)\n",
    "y_proba_v2 = model_v2.predict_proba(X_test_demo)[:, 1]\n",
    "\n",
    "metrics_v2 = {\n",
    "    'accuracy': accuracy_score(y_test_demo, y_pred_v2),\n",
    "    'precision': precision_score(y_test_demo, y_pred_v2, zero_division=0),\n",
    "    'recall': recall_score(y_test_demo, y_pred_v2, zero_division=0),\n",
    "    'auc': roc_auc_score(y_test_demo, y_proba_v2)\n",
    "}\n",
    "\n",
    "mv2 = registry.register_model(\n",
    "    model_name=\"TradingSignalModel\",\n",
    "    model=model_v2,\n",
    "    hyperparameters={'n_estimators': 100, 'max_depth': 5},\n",
    "    metrics=metrics_v2,\n",
    "    training_data=features_with_target,\n",
    "    author=\"bob\",\n",
    "    description=\"Improved model with more trees and depth\",\n",
    "    tags=['improved', 'random_forest']\n",
    ")\n",
    "\n",
    "# Version 3: Gradient Boosting\n",
    "model_v3 = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "model_v3.fit(X_train_demo, y_train_demo)\n",
    "y_pred_v3 = model_v3.predict(X_test_demo)\n",
    "y_proba_v3 = model_v3.predict_proba(X_test_demo)[:, 1]\n",
    "\n",
    "metrics_v3 = {\n",
    "    'accuracy': accuracy_score(y_test_demo, y_pred_v3),\n",
    "    'precision': precision_score(y_test_demo, y_pred_v3, zero_division=0),\n",
    "    'recall': recall_score(y_test_demo, y_pred_v3, zero_division=0),\n",
    "    'auc': roc_auc_score(y_test_demo, y_proba_v3)\n",
    "}\n",
    "\n",
    "mv3 = registry.register_model(\n",
    "    model_name=\"TradingSignalModel\",\n",
    "    model=model_v3,\n",
    "    hyperparameters={'n_estimators': 100, 'max_depth': 4, 'algorithm': 'gradient_boosting'},\n",
    "    metrics=metrics_v3,\n",
    "    training_data=features_with_target,\n",
    "    author=\"charlie\",\n",
    "    description=\"Gradient Boosting model for comparison\",\n",
    "    tags=['experiment', 'gradient_boosting']\n",
    ")\n",
    "\n",
    "# List all registered models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REGISTERED MODELS\")\n",
    "print(\"=\" * 60)\n",
    "models_df = registry.list_models()\n",
    "print(models_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd745eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lifecycle management - promote models through stages\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL LIFECYCLE MANAGEMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Promote v1.0.2 to staging\n",
    "registry.transition_stage(\"TradingSignalModel\", \"1.0.2\", ModelStage.STAGING)\n",
    "\n",
    "# Promote v1.0.2 to production (best performance)\n",
    "registry.transition_stage(\"TradingSignalModel\", \"1.0.2\", ModelStage.PRODUCTION)\n",
    "\n",
    "# Compare model versions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERSION COMPARISON: v1.0.0 vs v1.0.2\")\n",
    "print(\"=\" * 60)\n",
    "comparison = registry.compare_versions(\"TradingSignalModel\", \"1.0.0\", \"1.0.2\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Load production model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING PRODUCTION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "prod_model, prod_version = registry.get_production_model(\"TradingSignalModel\")\n",
    "print(f\"Loaded: {prod_version.model_name} v{prod_version.version}\")\n",
    "print(f\"Stage: {prod_version.stage.value}\")\n",
    "print(f\"Created: {prod_version.created_at}\")\n",
    "print(f\"Metrics: {prod_version.metrics}\")\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Metrics by version\n",
    "ax1 = axes[0]\n",
    "versions = ['1.0.0', '1.0.1', '1.0.2']\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'auc']\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, v in enumerate(versions):\n",
    "    mv = registry.models[\"TradingSignalModel\"][v]\n",
    "    values = [mv.metrics.get(m, 0) for m in metrics_names]\n",
    "    ax1.bar(x + i*width, values, width, label=f'v{v}', alpha=0.8)\n",
    "\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Model Metrics by Version')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels(metrics_names)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Model lifecycle\n",
    "ax2 = axes[1]\n",
    "stages = ['development', 'staging', 'production', 'archived']\n",
    "stage_colors = {'development': 'blue', 'staging': 'orange', 'production': 'green', 'archived': 'gray'}\n",
    "\n",
    "for i, v in enumerate(versions):\n",
    "    mv = registry.models[\"TradingSignalModel\"][v]\n",
    "    stage = mv.stage.value\n",
    "    color = stage_colors[stage]\n",
    "    ax2.barh(v, 1, color=color, alpha=0.7, label=stage if stage not in [bar.get_label() for bar in ax2.patches] else '')\n",
    "    ax2.text(0.5, i, f\"{stage.upper()}\", ha='center', va='center', fontweight='bold', color='white')\n",
    "\n",
    "ax2.set_xlabel('Lifecycle Position')\n",
    "ax2.set_title('Model Lifecycle Stages')\n",
    "ax2.set_xlim(0, 1)\n",
    "handles = [plt.Rectangle((0,0),1,1, color=c, alpha=0.7) for c in stage_colors.values()]\n",
    "ax2.legend(handles, stage_colors.keys(), loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show registry log\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REGISTRY ACTIVITY LOG\")\n",
    "print(\"=\" * 60)\n",
    "for entry in registry.registry_log[-5:]:\n",
    "    print(f\"  [{entry['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}] \"\n",
    "          f\"{entry['action'].upper()}: {entry['model_name']} \"\n",
    "          f\"v{entry.get('version', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5362b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### Production ML Systems for Trading\n",
    "\n",
    "We've covered the essential components of production ML systems:\n",
    "\n",
    "### 1. Pipeline Architecture\n",
    "- **Data Ingestion**: Validate and transform raw market data\n",
    "- **Feature Engineering**: Compute reproducible features\n",
    "- **Model Training**: Automated training with validation\n",
    "- **Serving**: Batch, real-time, and streaming inference\n",
    "\n",
    "### 2. Model Serving Patterns\n",
    "| Pattern | Use Case | Latency |\n",
    "|---------|----------|---------|\n",
    "| Batch | End-of-day signals | Minutes |\n",
    "| Real-time | Live trading | Milliseconds |\n",
    "| Streaming | Event-driven | Sub-second |\n",
    "\n",
    "### 3. Feature Store\n",
    "- Centralized feature management\n",
    "- Point-in-time correctness prevents look-ahead bias\n",
    "- Online/offline serving for different use cases\n",
    "\n",
    "### 4. Drift Detection\n",
    "- **PSI**: Population Stability Index for distribution shifts\n",
    "- **KS Test**: Statistical comparison of distributions\n",
    "- Monitor both data drift and model performance\n",
    "\n",
    "### 5. A/B Testing\n",
    "- Compare model variants with statistical rigor\n",
    "- Use trading-specific metrics (Sharpe, win rate)\n",
    "- Shadow deployment before full rollout\n",
    "\n",
    "### 6. Model Registry\n",
    "- Version control for models\n",
    "- Lifecycle management (dev â†’ staging â†’ production)\n",
    "- Audit trail for compliance\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always version everything**: Data, features, models, configs\n",
    "2. **Monitor continuously**: Set up alerts for drift and performance\n",
    "3. **Test before deploying**: A/B test with statistical significance\n",
    "4. **Document thoroughly**: Maintain audit trails\n",
    "5. **Plan for failure**: Implement rollback mechanisms\n",
    "6. **Automate**: CI/CD pipelines for reproducibility\n",
    "\n",
    "### Next Steps\n",
    "- Study `Trading_Strategy.ipynb` for practical implementation\n",
    "- Explore MLflow for production model tracking\n",
    "- Learn about Kubernetes for scalable deployments\n",
    "- Practice with real market data and backtesting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
