{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27262b8f",
   "metadata": {},
   "source": [
    "# Day 02: Model Monitoring & Drift Detection\n",
    "\n",
    "## Week 23: Production ML for Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand why model monitoring is critical for trading systems\n",
    "- Learn different types of drift: Data Drift, Concept Drift, Prediction Drift\n",
    "- Implement statistical tests for drift detection (KS test, PSI, Chi-squared)\n",
    "- Build automated alerting systems for production models\n",
    "- Apply monitoring techniques to financial time series\n",
    "\n",
    "### Why Model Monitoring Matters in Finance\n",
    "In production trading systems, models can degrade silently due to:\n",
    "- **Market regime changes** (bull ‚Üí bear, low ‚Üí high volatility)\n",
    "- **Feature distribution shifts** (data drift)\n",
    "- **Changing relationships** between features and returns (concept drift)\n",
    "- **Data quality issues** (missing data, API failures)\n",
    "\n",
    "A model that worked well in backtesting can become unprofitable without proper monitoring!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f543ad",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6575839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, chi2_contingency, entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285a14a5",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Training and Production Data\n",
    "\n",
    "We'll simulate a trading signal classification problem where:\n",
    "- **Training Data**: Collected during a \"normal\" market regime\n",
    "- **Production Data**: Collected after deployment, with intentional drift\n",
    "\n",
    "### Types of Drift We'll Simulate:\n",
    "1. **Data Drift**: Feature distributions shift (e.g., volatility regime change)\n",
    "2. **Concept Drift**: The relationship between features and target changes\n",
    "3. **Gradual Drift**: Slow changes over time\n",
    "4. **Sudden Drift**: Abrupt market regime change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55208cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_financial_features(n_samples: int, \n",
    "                                   regime: str = 'normal',\n",
    "                                   drift_factor: float = 0.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic financial features for a trading signal classifier.\n",
    "    \n",
    "    Features:\n",
    "    - momentum: Price momentum indicator\n",
    "    - volatility: Rolling volatility\n",
    "    - volume_ratio: Volume relative to average\n",
    "    - rsi: Relative Strength Index (normalized)\n",
    "    - spread: Bid-ask spread\n",
    "    - market_cap_log: Log market cap category\n",
    "    \"\"\"\n",
    "    \n",
    "    if regime == 'normal':\n",
    "        # Normal market conditions (training data)\n",
    "        momentum = np.random.normal(0.0, 1.0, n_samples)\n",
    "        volatility = np.abs(np.random.normal(0.15, 0.05, n_samples))\n",
    "        volume_ratio = np.random.lognormal(0, 0.3, n_samples)\n",
    "        rsi = np.random.beta(5, 5, n_samples)  # Centered around 0.5\n",
    "        spread = np.abs(np.random.normal(0.001, 0.0003, n_samples))\n",
    "        \n",
    "    elif regime == 'high_volatility':\n",
    "        # High volatility regime (data drift)\n",
    "        momentum = np.random.normal(0.0, 1.5, n_samples)  # Higher variance\n",
    "        volatility = np.abs(np.random.normal(0.25, 0.10, n_samples))  # Shifted mean\n",
    "        volume_ratio = np.random.lognormal(0.3, 0.5, n_samples)  # Higher volume\n",
    "        rsi = np.random.beta(3, 7, n_samples)  # Skewed towards oversold\n",
    "        spread = np.abs(np.random.normal(0.002, 0.0008, n_samples))  # Wider spreads\n",
    "        \n",
    "    elif regime == 'trending':\n",
    "        # Strong trending market\n",
    "        momentum = np.random.normal(0.5 + drift_factor, 0.8, n_samples)  # Positive bias\n",
    "        volatility = np.abs(np.random.normal(0.12, 0.03, n_samples))  # Lower vol\n",
    "        volume_ratio = np.random.lognormal(0.2, 0.25, n_samples)\n",
    "        rsi = np.random.beta(7, 3, n_samples)  # Skewed towards overbought\n",
    "        spread = np.abs(np.random.normal(0.0008, 0.0002, n_samples))\n",
    "        \n",
    "    else:  # gradual drift\n",
    "        # Gradual shift in distributions\n",
    "        momentum = np.random.normal(drift_factor * 0.3, 1.0 + drift_factor * 0.2, n_samples)\n",
    "        volatility = np.abs(np.random.normal(0.15 + drift_factor * 0.05, 0.05, n_samples))\n",
    "        volume_ratio = np.random.lognormal(drift_factor * 0.1, 0.3, n_samples)\n",
    "        rsi = np.random.beta(5 - drift_factor, 5 + drift_factor, n_samples)\n",
    "        spread = np.abs(np.random.normal(0.001 + drift_factor * 0.0005, 0.0003, n_samples))\n",
    "    \n",
    "    # Market cap category (categorical feature)\n",
    "    market_cap_log = np.random.choice([1, 2, 3, 4, 5], n_samples, \n",
    "                                       p=[0.1, 0.2, 0.3, 0.25, 0.15])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'momentum': momentum,\n",
    "        'volatility': volatility,\n",
    "        'volume_ratio': volume_ratio,\n",
    "        'rsi': rsi,\n",
    "        'spread': spread,\n",
    "        'market_cap': market_cap_log\n",
    "    })\n",
    "\n",
    "\n",
    "def generate_labels(features: pd.DataFrame, \n",
    "                    regime: str = 'normal',\n",
    "                    noise_level: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate binary labels (1 = profitable trade signal, 0 = not profitable).\n",
    "    \n",
    "    The relationship between features and labels changes based on regime\n",
    "    to simulate concept drift.\n",
    "    \"\"\"\n",
    "    n_samples = len(features)\n",
    "    \n",
    "    if regime == 'normal':\n",
    "        # Normal relationship: momentum and low volatility predict positive returns\n",
    "        logits = (\n",
    "            0.5 * features['momentum'] +\n",
    "            -2.0 * features['volatility'] +\n",
    "            0.3 * features['volume_ratio'] +\n",
    "            1.0 * (features['rsi'] - 0.5) +\n",
    "            -100 * features['spread']\n",
    "        )\n",
    "    elif regime == 'concept_drift':\n",
    "        # CONCEPT DRIFT: Relationship changes!\n",
    "        # Now high volatility and momentum reversal predict returns\n",
    "        logits = (\n",
    "            -0.3 * features['momentum'] +  # Momentum reversal now works\n",
    "            1.5 * features['volatility'] +  # High vol is now good\n",
    "            -0.2 * features['volume_ratio'] +\n",
    "            -0.5 * (features['rsi'] - 0.5) +  # RSI relationship inverted\n",
    "            50 * features['spread']\n",
    "        )\n",
    "    else:\n",
    "        # Same as normal\n",
    "        logits = (\n",
    "            0.5 * features['momentum'] +\n",
    "            -2.0 * features['volatility'] +\n",
    "            0.3 * features['volume_ratio'] +\n",
    "            1.0 * (features['rsi'] - 0.5) +\n",
    "            -100 * features['spread']\n",
    "        )\n",
    "    \n",
    "    # Convert to probabilities and add noise\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    probs = np.clip(probs + np.random.normal(0, noise_level, n_samples), 0.01, 0.99)\n",
    "    \n",
    "    # Generate binary labels\n",
    "    labels = (np.random.random(n_samples) < probs).astype(int)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "print(\"‚úÖ Data generation functions created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7da848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Training Data (Normal Market Regime)\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_train = 5000\n",
    "n_production = 1000\n",
    "\n",
    "# Training data - normal market conditions\n",
    "X_train = generate_financial_features(n_train, regime='normal')\n",
    "y_train = generate_labels(X_train, regime='normal')\n",
    "\n",
    "print(f\"\\nüìä Training Data (Normal Regime):\")\n",
    "print(f\"   Samples: {len(X_train)}\")\n",
    "print(f\"   Class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"   Class 1 ratio: {y_train.mean():.2%}\")\n",
    "\n",
    "# Production Data - Different Scenarios\n",
    "# Scenario 1: No Drift (baseline)\n",
    "X_prod_no_drift = generate_financial_features(n_production, regime='normal')\n",
    "y_prod_no_drift = generate_labels(X_prod_no_drift, regime='normal')\n",
    "\n",
    "# Scenario 2: Data Drift (high volatility regime)\n",
    "X_prod_data_drift = generate_financial_features(n_production, regime='high_volatility')\n",
    "y_prod_data_drift = generate_labels(X_prod_data_drift, regime='normal')  # Same relationship\n",
    "\n",
    "# Scenario 3: Concept Drift (relationship changes)\n",
    "X_prod_concept_drift = generate_financial_features(n_production, regime='normal')\n",
    "y_prod_concept_drift = generate_labels(X_prod_concept_drift, regime='concept_drift')\n",
    "\n",
    "# Scenario 4: Both Data and Concept Drift\n",
    "X_prod_both_drift = generate_financial_features(n_production, regime='high_volatility')\n",
    "y_prod_both_drift = generate_labels(X_prod_both_drift, regime='concept_drift')\n",
    "\n",
    "print(f\"\\nüìä Production Datasets Generated:\")\n",
    "print(f\"   1. No Drift: {len(X_prod_no_drift)} samples\")\n",
    "print(f\"   2. Data Drift (High Vol): {len(X_prod_data_drift)} samples\")\n",
    "print(f\"   3. Concept Drift: {len(X_prod_concept_drift)} samples\")\n",
    "print(f\"   4. Both Drifts: {len(X_prod_both_drift)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e53daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions: Training vs Production (Data Drift)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "features = ['momentum', 'volatility', 'volume_ratio', 'rsi', 'spread', 'market_cap']\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Training distribution\n",
    "    ax.hist(X_train[feature], bins=50, alpha=0.5, label='Training', density=True, color='blue')\n",
    "    # Production with data drift\n",
    "    ax.hist(X_prod_data_drift[feature], bins=50, alpha=0.5, label='Production (Data Drift)', \n",
    "            density=True, color='red')\n",
    "    \n",
    "    ax.set_title(f'{feature.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.suptitle('Feature Distributions: Training vs Production (Data Drift Scenario)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Notice how the distributions shifted in the Data Drift scenario:\")\n",
    "print(\"   - Volatility: Mean shifted from ~0.15 to ~0.25\")\n",
    "print(\"   - Spread: Wider spreads in production\")\n",
    "print(\"   - RSI: Skewed towards oversold conditions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c877a",
   "metadata": {},
   "source": [
    "## 3. Train a Baseline Model\n",
    "\n",
    "We'll train a Random Forest classifier to predict profitable trading signals. This model will serve as our baseline for monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae586402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train_split)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = model.predict(X_val_scaled)\n",
    "y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODEL PERFORMANCE (Validation Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy:  {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc_score(y_val, y_val_proba):.4f}\")\n",
    "\n",
    "# Store baseline metrics for monitoring\n",
    "baseline_metrics = {\n",
    "    'accuracy': accuracy_score(y_val, y_val_pred),\n",
    "    'precision': precision_score(y_val, y_val_pred),\n",
    "    'recall': recall_score(y_val, y_val_pred),\n",
    "    'f1': f1_score(y_val, y_val_pred),\n",
    "    'roc_auc': roc_auc_score(y_val, y_val_proba)\n",
    "}\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Feature Importance:\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71a062",
   "metadata": {},
   "source": [
    "## 4. Implement Data Drift Detection\n",
    "\n",
    "Data drift (also called covariate shift) occurs when the distribution of input features changes between training and production.\n",
    "\n",
    "### Statistical Tests for Drift Detection:\n",
    "1. **Kolmogorov-Smirnov (KS) Test**: Non-parametric test comparing two distributions\n",
    "2. **Population Stability Index (PSI)**: Common in finance/credit risk\n",
    "3. **Jensen-Shannon Divergence**: Symmetric measure of distribution similarity\n",
    "4. **Chi-Square Test**: For categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f3d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDriftDetector:\n",
    "    \"\"\"\n",
    "    Comprehensive data drift detection for production ML systems.\n",
    "    \n",
    "    Implements multiple statistical tests to detect distribution shifts\n",
    "    in input features between reference (training) and production data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data: pd.DataFrame, \n",
    "                 categorical_features: List[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize with reference (training) data.\n",
    "        \n",
    "        Args:\n",
    "            reference_data: Training/reference DataFrame\n",
    "            categorical_features: List of categorical column names\n",
    "        \"\"\"\n",
    "        self.reference_data = reference_data\n",
    "        self.categorical_features = categorical_features or []\n",
    "        self.numerical_features = [col for col in reference_data.columns \n",
    "                                   if col not in self.categorical_features]\n",
    "        \n",
    "        # Pre-compute reference statistics\n",
    "        self._compute_reference_stats()\n",
    "    \n",
    "    def _compute_reference_stats(self):\n",
    "        \"\"\"Pre-compute reference distribution statistics for efficiency.\"\"\"\n",
    "        self.reference_stats = {}\n",
    "        \n",
    "        for feature in self.numerical_features:\n",
    "            data = self.reference_data[feature].dropna()\n",
    "            self.reference_stats[feature] = {\n",
    "                'mean': data.mean(),\n",
    "                'std': data.std(),\n",
    "                'min': data.min(),\n",
    "                'max': data.max(),\n",
    "                'percentiles': np.percentile(data, [10, 25, 50, 75, 90])\n",
    "            }\n",
    "        \n",
    "        for feature in self.categorical_features:\n",
    "            self.reference_stats[feature] = {\n",
    "                'value_counts': self.reference_data[feature].value_counts(normalize=True)\n",
    "            }\n",
    "    \n",
    "    def ks_test(self, production_data: pd.DataFrame, \n",
    "                alpha: float = 0.05) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Kolmogorov-Smirnov test for numerical features.\n",
    "        \n",
    "        The KS test compares the empirical CDFs of two samples.\n",
    "        - H0: Both samples come from the same distribution\n",
    "        - If p-value < alpha, reject H0 (drift detected)\n",
    "        \n",
    "        Args:\n",
    "            production_data: Production DataFrame\n",
    "            alpha: Significance level\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with test results per feature\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for feature in self.numerical_features:\n",
    "            ref_data = self.reference_data[feature].dropna()\n",
    "            prod_data = production_data[feature].dropna()\n",
    "            \n",
    "            statistic, p_value = ks_2samp(ref_data, prod_data)\n",
    "            \n",
    "            results[feature] = {\n",
    "                'test': 'Kolmogorov-Smirnov',\n",
    "                'statistic': statistic,\n",
    "                'p_value': p_value,\n",
    "                'drift_detected': p_value < alpha,\n",
    "                'severity': self._classify_drift_severity(statistic, 'ks')\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_psi(self, production_data: pd.DataFrame, \n",
    "                      n_bins: int = 10) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Population Stability Index (PSI) for numerical features.\n",
    "        \n",
    "        PSI = Œ£ (Actual% - Expected%) √ó ln(Actual% / Expected%)\n",
    "        \n",
    "        Interpretation:\n",
    "        - PSI < 0.1: No significant change\n",
    "        - 0.1 ‚â§ PSI < 0.25: Moderate change, investigation needed\n",
    "        - PSI ‚â• 0.25: Significant change, action required\n",
    "        \n",
    "        Args:\n",
    "            production_data: Production DataFrame\n",
    "            n_bins: Number of bins for discretization\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with PSI values per feature\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for feature in self.numerical_features:\n",
    "            ref_data = self.reference_data[feature].dropna()\n",
    "            prod_data = production_data[feature].dropna()\n",
    "            \n",
    "            # Create bins based on reference data\n",
    "            bins = np.percentile(ref_data, np.linspace(0, 100, n_bins + 1))\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            \n",
    "            # Calculate proportions in each bin\n",
    "            ref_counts, _ = np.histogram(ref_data, bins=bins)\n",
    "            prod_counts, _ = np.histogram(prod_data, bins=bins)\n",
    "            \n",
    "            # Convert to proportions (add small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-10\n",
    "            ref_props = (ref_counts + epsilon) / (ref_counts.sum() + n_bins * epsilon)\n",
    "            prod_props = (prod_counts + epsilon) / (prod_counts.sum() + n_bins * epsilon)\n",
    "            \n",
    "            # Calculate PSI\n",
    "            psi = np.sum((prod_props - ref_props) * np.log(prod_props / ref_props))\n",
    "            \n",
    "            results[feature] = {\n",
    "                'test': 'Population Stability Index',\n",
    "                'psi': psi,\n",
    "                'drift_detected': psi >= 0.1,\n",
    "                'severity': self._classify_drift_severity(psi, 'psi'),\n",
    "                'interpretation': self._interpret_psi(psi)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def jensen_shannon_divergence(self, production_data: pd.DataFrame,\n",
    "                                   n_bins: int = 50) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Jensen-Shannon Divergence for numerical features.\n",
    "        \n",
    "        JSD is a symmetric, bounded (0 to 1) measure of similarity\n",
    "        between two probability distributions.\n",
    "        \n",
    "        Args:\n",
    "            production_data: Production DataFrame\n",
    "            n_bins: Number of bins for discretization\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with JSD values per feature\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for feature in self.numerical_features:\n",
    "            ref_data = self.reference_data[feature].dropna()\n",
    "            prod_data = production_data[feature].dropna()\n",
    "            \n",
    "            # Create common bin edges\n",
    "            all_data = np.concatenate([ref_data, prod_data])\n",
    "            bins = np.linspace(all_data.min(), all_data.max(), n_bins + 1)\n",
    "            \n",
    "            # Calculate histograms\n",
    "            ref_hist, _ = np.histogram(ref_data, bins=bins, density=True)\n",
    "            prod_hist, _ = np.histogram(prod_data, bins=bins, density=True)\n",
    "            \n",
    "            # Normalize to probability distributions\n",
    "            ref_hist = ref_hist / (ref_hist.sum() + 1e-10)\n",
    "            prod_hist = prod_hist / (prod_hist.sum() + 1e-10)\n",
    "            \n",
    "            # Calculate Jensen-Shannon divergence\n",
    "            jsd = jensenshannon(ref_hist, prod_hist) ** 2  # Square to get divergence\n",
    "            \n",
    "            results[feature] = {\n",
    "                'test': 'Jensen-Shannon Divergence',\n",
    "                'jsd': jsd,\n",
    "                'drift_detected': jsd > 0.1,\n",
    "                'severity': self._classify_drift_severity(jsd, 'jsd')\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def chi_square_test(self, production_data: pd.DataFrame,\n",
    "                        alpha: float = 0.05) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Chi-Square test for categorical features.\n",
    "        \n",
    "        Tests if the distribution of categories has changed significantly.\n",
    "        \n",
    "        Args:\n",
    "            production_data: Production DataFrame\n",
    "            alpha: Significance level\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with test results per categorical feature\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for feature in self.categorical_features:\n",
    "            # Get category counts\n",
    "            ref_counts = self.reference_data[feature].value_counts()\n",
    "            prod_counts = production_data[feature].value_counts()\n",
    "            \n",
    "            # Align categories\n",
    "            all_categories = set(ref_counts.index) | set(prod_counts.index)\n",
    "            ref_counts = ref_counts.reindex(all_categories, fill_value=0)\n",
    "            prod_counts = prod_counts.reindex(all_categories, fill_value=0)\n",
    "            \n",
    "            # Create contingency table\n",
    "            contingency = np.array([ref_counts.values, prod_counts.values])\n",
    "            \n",
    "            # Chi-square test\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency)\n",
    "            \n",
    "            results[feature] = {\n",
    "                'test': 'Chi-Square',\n",
    "                'statistic': chi2,\n",
    "                'p_value': p_value,\n",
    "                'degrees_of_freedom': dof,\n",
    "                'drift_detected': p_value < alpha\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _classify_drift_severity(self, value: float, test_type: str) -> str:\n",
    "        \"\"\"Classify drift severity based on test type and value.\"\"\"\n",
    "        if test_type == 'ks':\n",
    "            if value < 0.1:\n",
    "                return 'none'\n",
    "            elif value < 0.2:\n",
    "                return 'low'\n",
    "            elif value < 0.3:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'high'\n",
    "        elif test_type == 'psi':\n",
    "            if value < 0.1:\n",
    "                return 'none'\n",
    "            elif value < 0.25:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'high'\n",
    "        elif test_type == 'jsd':\n",
    "            if value < 0.05:\n",
    "                return 'none'\n",
    "            elif value < 0.1:\n",
    "                return 'low'\n",
    "            elif value < 0.2:\n",
    "                return 'medium'\n",
    "            else:\n",
    "                return 'high'\n",
    "        return 'unknown'\n",
    "    \n",
    "    def _interpret_psi(self, psi: float) -> str:\n",
    "        \"\"\"Provide human-readable interpretation of PSI value.\"\"\"\n",
    "        if psi < 0.1:\n",
    "            return \"No significant change - model is stable\"\n",
    "        elif psi < 0.25:\n",
    "            return \"Moderate change - investigate and monitor closely\"\n",
    "        else:\n",
    "            return \"Significant change - action required (retrain/review)\"\n",
    "    \n",
    "    def comprehensive_drift_report(self, production_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive drift report using all tests.\n",
    "        \n",
    "        Args:\n",
    "            production_data: Production DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with drift analysis for all features\n",
    "        \"\"\"\n",
    "        ks_results = self.ks_test(production_data)\n",
    "        psi_results = self.calculate_psi(production_data)\n",
    "        jsd_results = self.jensen_shannon_divergence(production_data)\n",
    "        \n",
    "        report_data = []\n",
    "        \n",
    "        for feature in self.numerical_features:\n",
    "            report_data.append({\n",
    "                'feature': feature,\n",
    "                'ks_statistic': ks_results[feature]['statistic'],\n",
    "                'ks_p_value': ks_results[feature]['p_value'],\n",
    "                'ks_drift': ks_results[feature]['drift_detected'],\n",
    "                'psi': psi_results[feature]['psi'],\n",
    "                'psi_drift': psi_results[feature]['drift_detected'],\n",
    "                'jsd': jsd_results[feature]['jsd'],\n",
    "                'jsd_drift': jsd_results[feature]['drift_detected'],\n",
    "                'overall_drift': (ks_results[feature]['drift_detected'] or \n",
    "                                  psi_results[feature]['drift_detected'])\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(report_data)\n",
    "\n",
    "print(\"‚úÖ DataDriftDetector class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drift detector with training data\n",
    "drift_detector = DataDriftDetector(\n",
    "    reference_data=X_train,\n",
    "    categorical_features=['market_cap']\n",
    ")\n",
    "\n",
    "# Test drift detection on different scenarios\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Scenario 1: No Drift\n",
    "print(\"\\nüìä SCENARIO 1: No Drift (Production = Normal Regime)\")\n",
    "print(\"-\" * 50)\n",
    "report_no_drift = drift_detector.comprehensive_drift_report(X_prod_no_drift)\n",
    "print(report_no_drift.to_string(index=False))\n",
    "n_drift_features = report_no_drift['overall_drift'].sum()\n",
    "print(f\"\\n‚úÖ Features with drift detected: {n_drift_features}/{len(report_no_drift)}\")\n",
    "\n",
    "# Scenario 2: Data Drift\n",
    "print(\"\\n\\nüìä SCENARIO 2: Data Drift (High Volatility Regime)\")\n",
    "print(\"-\" * 50)\n",
    "report_data_drift = drift_detector.comprehensive_drift_report(X_prod_data_drift)\n",
    "print(report_data_drift.to_string(index=False))\n",
    "n_drift_features = report_data_drift['overall_drift'].sum()\n",
    "print(f\"\\n‚ö†Ô∏è Features with drift detected: {n_drift_features}/{len(report_data_drift)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PSI values across scenarios\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PSI Comparison\n",
    "scenarios = ['No Drift', 'Data Drift']\n",
    "colors = ['green', 'red']\n",
    "\n",
    "for idx, (scenario, report, color) in enumerate([\n",
    "    ('No Drift', report_no_drift, 'green'),\n",
    "    ('Data Drift', report_data_drift, 'red')\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    features = report['feature']\n",
    "    psi_values = report['psi']\n",
    "    \n",
    "    bars = ax.barh(features, psi_values, color=color, alpha=0.7)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    ax.axvline(x=0.1, color='orange', linestyle='--', linewidth=2, label='Moderate (0.1)')\n",
    "    ax.axvline(x=0.25, color='red', linestyle='--', linewidth=2, label='High (0.25)')\n",
    "    \n",
    "    ax.set_xlabel('PSI Value')\n",
    "    ax.set_title(f'{scenario}: Population Stability Index', fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_xlim(0, max(0.5, psi_values.max() * 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà PSI Interpretation:\")\n",
    "print(\"   - PSI < 0.10: No significant change (Green zone)\")\n",
    "print(\"   - 0.10 ‚â§ PSI < 0.25: Moderate change, monitor closely (Orange zone)\")\n",
    "print(\"   - PSI ‚â• 0.25: Significant change, action required (Red zone)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7351cc",
   "metadata": {},
   "source": [
    "## 5. Implement Prediction Drift Detection\n",
    "\n",
    "Prediction drift monitors changes in the model's output distribution over time, regardless of whether input features have shifted. This can indicate:\n",
    "- Model degradation\n",
    "- Changes in the target population\n",
    "- Upstream data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51df646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDriftDetector:\n",
    "    \"\"\"\n",
    "    Monitor drift in model predictions over time.\n",
    "    \n",
    "    Tracks:\n",
    "    - Prediction probability distribution\n",
    "    - Class distribution (for classifiers)\n",
    "    - Prediction confidence levels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_predictions: np.ndarray,\n",
    "                 reference_probabilities: np.ndarray = None):\n",
    "        \"\"\"\n",
    "        Initialize with reference predictions from validation set.\n",
    "        \n",
    "        Args:\n",
    "            reference_predictions: Class predictions from validation\n",
    "            reference_probabilities: Probability predictions (if available)\n",
    "        \"\"\"\n",
    "        self.reference_predictions = reference_predictions\n",
    "        self.reference_probabilities = reference_probabilities\n",
    "        \n",
    "        # Store reference statistics\n",
    "        self.ref_class_distribution = np.bincount(reference_predictions) / len(reference_predictions)\n",
    "        \n",
    "        if reference_probabilities is not None:\n",
    "            self.ref_prob_mean = np.mean(reference_probabilities)\n",
    "            self.ref_prob_std = np.std(reference_probabilities)\n",
    "    \n",
    "    def detect_prediction_drift(self, production_predictions: np.ndarray,\n",
    "                                 production_probabilities: np.ndarray = None,\n",
    "                                 alpha: float = 0.05) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect drift in predictions using statistical tests.\n",
    "        \n",
    "        Args:\n",
    "            production_predictions: New predictions to compare\n",
    "            production_probabilities: New probability predictions\n",
    "            alpha: Significance level\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with drift detection results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # 1. Chi-Square test for class distribution\n",
    "        ref_counts = np.bincount(self.reference_predictions, \n",
    "                                  minlength=max(self.reference_predictions.max(),\n",
    "                                               production_predictions.max()) + 1)\n",
    "        prod_counts = np.bincount(production_predictions,\n",
    "                                   minlength=max(self.reference_predictions.max(),\n",
    "                                                production_predictions.max()) + 1)\n",
    "        \n",
    "        # Normalize to same scale\n",
    "        ref_counts_scaled = ref_counts * len(production_predictions) / len(self.reference_predictions)\n",
    "        \n",
    "        # Chi-square test\n",
    "        chi2_stat = np.sum((prod_counts - ref_counts_scaled) ** 2 / (ref_counts_scaled + 1e-10))\n",
    "        p_value = 1 - stats.chi2.cdf(chi2_stat, df=len(ref_counts) - 1)\n",
    "        \n",
    "        results['class_distribution'] = {\n",
    "            'chi2_statistic': chi2_stat,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < alpha,\n",
    "            'reference_distribution': self.ref_class_distribution,\n",
    "            'production_distribution': prod_counts / prod_counts.sum()\n",
    "        }\n",
    "        \n",
    "        # 2. KS test for probability distribution (if available)\n",
    "        if production_probabilities is not None and self.reference_probabilities is not None:\n",
    "            ks_stat, ks_p_value = ks_2samp(self.reference_probabilities, \n",
    "                                           production_probabilities)\n",
    "            \n",
    "            results['probability_distribution'] = {\n",
    "                'ks_statistic': ks_stat,\n",
    "                'p_value': ks_p_value,\n",
    "                'drift_detected': ks_p_value < alpha,\n",
    "                'reference_mean': self.ref_prob_mean,\n",
    "                'production_mean': np.mean(production_probabilities),\n",
    "                'mean_shift': np.mean(production_probabilities) - self.ref_prob_mean\n",
    "            }\n",
    "        \n",
    "        # 3. Confidence drift (are predictions becoming less confident?)\n",
    "        if production_probabilities is not None and self.reference_probabilities is not None:\n",
    "            # Confidence = distance from 0.5 decision boundary\n",
    "            ref_confidence = np.abs(self.reference_probabilities - 0.5)\n",
    "            prod_confidence = np.abs(production_probabilities - 0.5)\n",
    "            \n",
    "            conf_stat, conf_p_value = ks_2samp(ref_confidence, prod_confidence)\n",
    "            \n",
    "            results['confidence'] = {\n",
    "                'ks_statistic': conf_stat,\n",
    "                'p_value': conf_p_value,\n",
    "                'drift_detected': conf_p_value < alpha,\n",
    "                'reference_mean_confidence': np.mean(ref_confidence),\n",
    "                'production_mean_confidence': np.mean(prod_confidence)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_psi_predictions(self, production_probabilities: np.ndarray,\n",
    "                                   n_bins: int = 10) -> float:\n",
    "        \"\"\"Calculate PSI for probability predictions.\"\"\"\n",
    "        if self.reference_probabilities is None:\n",
    "            return None\n",
    "        \n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        \n",
    "        ref_counts, _ = np.histogram(self.reference_probabilities, bins=bins)\n",
    "        prod_counts, _ = np.histogram(production_probabilities, bins=bins)\n",
    "        \n",
    "        epsilon = 1e-10\n",
    "        ref_props = (ref_counts + epsilon) / (ref_counts.sum() + n_bins * epsilon)\n",
    "        prod_props = (prod_counts + epsilon) / (prod_counts.sum() + n_bins * epsilon)\n",
    "        \n",
    "        psi = np.sum((prod_props - ref_props) * np.log(prod_props / ref_props))\n",
    "        \n",
    "        return psi\n",
    "\n",
    "print(\"‚úÖ PredictionDriftDetector class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all scenarios\n",
    "def get_predictions(X, model, scaler):\n",
    "    \"\"\"Scale data and get model predictions.\"\"\"\n",
    "    X_scaled = scaler.transform(X)\n",
    "    predictions = model.predict(X_scaled)\n",
    "    probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "    return predictions, probabilities\n",
    "\n",
    "# Reference predictions (validation set)\n",
    "y_val_pred, y_val_proba = get_predictions(X_val, model, scaler)\n",
    "\n",
    "# Production predictions\n",
    "pred_no_drift, proba_no_drift = get_predictions(X_prod_no_drift, model, scaler)\n",
    "pred_data_drift, proba_data_drift = get_predictions(X_prod_data_drift, model, scaler)\n",
    "pred_concept_drift, proba_concept_drift = get_predictions(X_prod_concept_drift, model, scaler)\n",
    "pred_both_drift, proba_both_drift = get_predictions(X_prod_both_drift, model, scaler)\n",
    "\n",
    "# Initialize prediction drift detector\n",
    "pred_drift_detector = PredictionDriftDetector(\n",
    "    reference_predictions=y_val_pred,\n",
    "    reference_probabilities=y_val_proba\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION DRIFT DETECTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scenarios = [\n",
    "    ('No Drift', pred_no_drift, proba_no_drift),\n",
    "    ('Data Drift', pred_data_drift, proba_data_drift),\n",
    "    ('Concept Drift', pred_concept_drift, proba_concept_drift),\n",
    "    ('Both Drifts', pred_both_drift, proba_both_drift)\n",
    "]\n",
    "\n",
    "for name, preds, probas in scenarios:\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    results = pred_drift_detector.detect_prediction_drift(preds, probas)\n",
    "    \n",
    "    # Class distribution\n",
    "    class_drift = results['class_distribution']\n",
    "    print(f\"   Class Distribution Drift: {'‚ö†Ô∏è YES' if class_drift['drift_detected'] else '‚úÖ NO'}\")\n",
    "    print(f\"   - Chi¬≤ p-value: {class_drift['p_value']:.4f}\")\n",
    "    \n",
    "    # Probability distribution\n",
    "    prob_drift = results['probability_distribution']\n",
    "    print(f\"   Probability Distribution Drift: {'‚ö†Ô∏è YES' if prob_drift['drift_detected'] else '‚úÖ NO'}\")\n",
    "    print(f\"   - KS p-value: {prob_drift['p_value']:.4f}\")\n",
    "    print(f\"   - Mean shift: {prob_drift['mean_shift']:+.4f}\")\n",
    "    \n",
    "    # PSI\n",
    "    psi = pred_drift_detector.calculate_psi_predictions(probas)\n",
    "    print(f\"   Prediction PSI: {psi:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315a53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction probability distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "scenarios_data = [\n",
    "    ('No Drift', proba_no_drift, 'green'),\n",
    "    ('Data Drift', proba_data_drift, 'orange'),\n",
    "    ('Concept Drift', proba_concept_drift, 'red'),\n",
    "    ('Both Drifts', proba_both_drift, 'darkred')\n",
    "]\n",
    "\n",
    "for idx, (name, probas, color) in enumerate(scenarios_data):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Reference distribution\n",
    "    ax.hist(y_val_proba, bins=30, alpha=0.5, label='Reference (Validation)', \n",
    "            density=True, color='blue')\n",
    "    # Production distribution\n",
    "    ax.hist(probas, bins=30, alpha=0.5, label=f'Production ({name})',\n",
    "            density=True, color=color)\n",
    "    \n",
    "    ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{name}: Prediction Distribution', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "plt.suptitle('Prediction Drift: Reference vs Production Distributions', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae8333",
   "metadata": {},
   "source": [
    "## 6. Implement Concept Drift Detection\n",
    "\n",
    "Concept drift occurs when the relationship between features and the target variable changes. This is the most insidious type of drift because:\n",
    "- Input features may look normal (no data drift)\n",
    "- Predictions may look normal (no prediction drift)\n",
    "- But the model is making wrong predictions!\n",
    "\n",
    "### Detection Methods:\n",
    "1. **Performance Monitoring**: Track accuracy, precision, recall over time windows\n",
    "2. **ADWIN (Adaptive Windowing)**: Detects distribution changes in a stream\n",
    "3. **Page-Hinkley Test**: Sequential change detection\n",
    "4. **Error Rate Monitoring**: Track prediction errors over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7337835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDriftDetector:\n",
    "    \"\"\"\n",
    "    Detect concept drift using performance monitoring and statistical tests.\n",
    "    \n",
    "    Concept drift = the relationship P(Y|X) changes over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_metrics: Dict[str, float],\n",
    "                 window_size: int = 100,\n",
    "                 significance_threshold: float = 0.05):\n",
    "        \"\"\"\n",
    "        Initialize with baseline performance metrics.\n",
    "        \n",
    "        Args:\n",
    "            baseline_metrics: Dictionary of baseline metrics (accuracy, f1, etc.)\n",
    "            window_size: Size of rolling window for monitoring\n",
    "            significance_threshold: Threshold for statistical significance\n",
    "        \"\"\"\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.window_size = window_size\n",
    "        self.significance_threshold = significance_threshold\n",
    "        \n",
    "        # Store historical metrics\n",
    "        self.metric_history = {metric: [] for metric in baseline_metrics.keys()}\n",
    "        self.alert_history = []\n",
    "    \n",
    "    def update(self, y_true: np.ndarray, y_pred: np.ndarray, \n",
    "               y_proba: np.ndarray = None, timestamp: datetime = None):\n",
    "        \"\"\"\n",
    "        Update with new batch of predictions and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            y_proba: Predicted probabilities (optional)\n",
    "            timestamp: Timestamp for this batch\n",
    "        \"\"\"\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        # Compute metrics\n",
    "        current_metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                current_metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "            except:\n",
    "                current_metrics['roc_auc'] = 0.5\n",
    "        \n",
    "        # Store metrics\n",
    "        for metric, value in current_metrics.items():\n",
    "            if metric in self.metric_history:\n",
    "                self.metric_history[metric].append({\n",
    "                    'timestamp': timestamp,\n",
    "                    'value': value\n",
    "                })\n",
    "        \n",
    "        return current_metrics\n",
    "    \n",
    "    def detect_drift(self, current_metrics: Dict[str, float],\n",
    "                     method: str = 'threshold') -> Dict:\n",
    "        \"\"\"\n",
    "        Detect concept drift based on performance degradation.\n",
    "        \n",
    "        Args:\n",
    "            current_metrics: Current batch metrics\n",
    "            method: Detection method ('threshold', 'statistical', 'adwin')\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with drift detection results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for metric, current_value in current_metrics.items():\n",
    "            if metric not in self.baseline_metrics:\n",
    "                continue\n",
    "            \n",
    "            baseline_value = self.baseline_metrics[metric]\n",
    "            \n",
    "            if method == 'threshold':\n",
    "                # Simple threshold-based detection\n",
    "                # Alert if metric drops more than 10% from baseline\n",
    "                relative_drop = (baseline_value - current_value) / (baseline_value + 1e-10)\n",
    "                drift_detected = relative_drop > 0.10\n",
    "                \n",
    "                results[metric] = {\n",
    "                    'baseline': baseline_value,\n",
    "                    'current': current_value,\n",
    "                    'relative_drop': relative_drop,\n",
    "                    'drift_detected': drift_detected,\n",
    "                    'severity': self._classify_severity(relative_drop)\n",
    "                }\n",
    "            \n",
    "            elif method == 'statistical':\n",
    "                # Use historical data for statistical test\n",
    "                if len(self.metric_history.get(metric, [])) >= self.window_size:\n",
    "                    recent_values = [h['value'] for h in self.metric_history[metric][-self.window_size:]]\n",
    "                    \n",
    "                    # One-sample t-test against baseline\n",
    "                    t_stat, p_value = stats.ttest_1samp(recent_values, baseline_value)\n",
    "                    \n",
    "                    # Check if significantly lower (one-tailed)\n",
    "                    drift_detected = (p_value / 2 < self.significance_threshold and \n",
    "                                     np.mean(recent_values) < baseline_value)\n",
    "                    \n",
    "                    results[metric] = {\n",
    "                        'baseline': baseline_value,\n",
    "                        'current_mean': np.mean(recent_values),\n",
    "                        't_statistic': t_stat,\n",
    "                        'p_value': p_value,\n",
    "                        'drift_detected': drift_detected\n",
    "                    }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def page_hinkley_test(self, values: List[float], \n",
    "                          delta: float = 0.005,\n",
    "                          threshold: float = 50) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Page-Hinkley test for change detection in a sequence.\n",
    "        \n",
    "        Detects a change in the mean of a Gaussian signal.\n",
    "        \n",
    "        Args:\n",
    "            values: Sequence of values to test\n",
    "            delta: Minimum amplitude of change to detect\n",
    "            threshold: Decision threshold\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (change_detected, change_point_index)\n",
    "        \"\"\"\n",
    "        n = len(values)\n",
    "        if n < 10:\n",
    "            return False, -1\n",
    "        \n",
    "        mean = np.mean(values)\n",
    "        \n",
    "        # Cumulative sum statistics\n",
    "        m_t = 0\n",
    "        M_t = 0\n",
    "        \n",
    "        for t in range(n):\n",
    "            m_t += values[t] - mean - delta\n",
    "            M_t = max(M_t, m_t)\n",
    "            \n",
    "            ph_t = M_t - m_t\n",
    "            \n",
    "            if ph_t > threshold:\n",
    "                return True, t\n",
    "        \n",
    "        return False, -1\n",
    "    \n",
    "    def _classify_severity(self, relative_drop: float) -> str:\n",
    "        \"\"\"Classify drift severity based on relative performance drop.\"\"\"\n",
    "        if relative_drop < 0.05:\n",
    "            return 'none'\n",
    "        elif relative_drop < 0.10:\n",
    "            return 'low'\n",
    "        elif relative_drop < 0.20:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    def get_performance_trend(self, metric: str = 'accuracy') -> pd.DataFrame:\n",
    "        \"\"\"Get historical trend for a specific metric.\"\"\"\n",
    "        if metric not in self.metric_history:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame(self.metric_history[metric])\n",
    "\n",
    "print(\"‚úÖ ConceptDriftDetector class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55713fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance across all scenarios\n",
    "print(\"=\" * 70)\n",
    "print(\"CONCEPT DRIFT DETECTION: PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize concept drift detector\n",
    "concept_detector = ConceptDriftDetector(baseline_metrics)\n",
    "\n",
    "scenarios_full = [\n",
    "    ('No Drift', X_prod_no_drift, y_prod_no_drift),\n",
    "    ('Data Drift Only', X_prod_data_drift, y_prod_data_drift),\n",
    "    ('Concept Drift Only', X_prod_concept_drift, y_prod_concept_drift),\n",
    "    ('Both Drifts', X_prod_both_drift, y_prod_both_drift)\n",
    "]\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for name, X_prod, y_true in scenarios_full:\n",
    "    # Get predictions\n",
    "    X_scaled = scaler.transform(X_prod)\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    y_proba = model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        'scenario': name,\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.5\n",
    "    }\n",
    "    \n",
    "    performance_results.append(metrics)\n",
    "    \n",
    "    # Detect drift\n",
    "    current_metrics = {k: v for k, v in metrics.items() if k != 'scenario'}\n",
    "    drift_results = concept_detector.detect_drift(current_metrics)\n",
    "    \n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    for metric, result in drift_results.items():\n",
    "        status = 'üî¥' if result['drift_detected'] else 'üü¢'\n",
    "        drop = result['relative_drop']\n",
    "        print(f\"   {metric:12}: {result['current']:.4f} (baseline: {result['baseline']:.4f}, \"\n",
    "              f\"drop: {drop:+.1%}) {status}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(\"\\n\\nüìà Performance Summary Table:\")\n",
    "print(perf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d273e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming data with gradual concept drift\n",
    "print(\"=\" * 70)\n",
    "print(\"SIMULATING STREAMING DATA WITH GRADUAL DRIFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate time series of batches with gradual drift\n",
    "n_batches = 30\n",
    "batch_size = 200\n",
    "drift_start_batch = 15  # Drift starts at batch 15\n",
    "\n",
    "streaming_metrics = []\n",
    "\n",
    "for batch_idx in range(n_batches):\n",
    "    # Calculate drift intensity (0 before drift_start, increases after)\n",
    "    if batch_idx < drift_start_batch:\n",
    "        drift_factor = 0.0\n",
    "        regime = 'normal'\n",
    "        label_regime = 'normal'\n",
    "    else:\n",
    "        # Gradual increase in drift\n",
    "        drift_factor = (batch_idx - drift_start_batch) / (n_batches - drift_start_batch)\n",
    "        regime = 'gradual'\n",
    "        label_regime = 'concept_drift' if drift_factor > 0.3 else 'normal'\n",
    "    \n",
    "    # Generate batch data\n",
    "    X_batch = generate_financial_features(batch_size, regime=regime, drift_factor=drift_factor)\n",
    "    y_batch = generate_labels(X_batch, regime=label_regime, noise_level=0.1 + drift_factor * 0.1)\n",
    "    \n",
    "    # Get predictions\n",
    "    X_batch_scaled = scaler.transform(X_batch)\n",
    "    y_pred_batch = model.predict(X_batch_scaled)\n",
    "    y_proba_batch = model.predict_proba(X_batch_scaled)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    batch_metrics = {\n",
    "        'batch': batch_idx,\n",
    "        'drift_factor': drift_factor,\n",
    "        'accuracy': accuracy_score(y_batch, y_pred_batch),\n",
    "        'f1': f1_score(y_batch, y_pred_batch, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_batch, y_proba_batch) if len(np.unique(y_batch)) > 1 else 0.5\n",
    "    }\n",
    "    streaming_metrics.append(batch_metrics)\n",
    "\n",
    "streaming_df = pd.DataFrame(streaming_metrics)\n",
    "\n",
    "# Visualize performance over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Accuracy over time\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(streaming_df['batch'], streaming_df['accuracy'], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "ax1.axhline(y=baseline_metrics['accuracy'], color='green', linestyle='--', label='Baseline')\n",
    "ax1.axvline(x=drift_start_batch, color='red', linestyle='--', alpha=0.7, label='Drift Start')\n",
    "ax1.fill_between(streaming_df['batch'], \n",
    "                  baseline_metrics['accuracy'] * 0.9, baseline_metrics['accuracy'] * 1.1,\n",
    "                  alpha=0.2, color='green', label='¬±10% Threshold')\n",
    "ax1.set_xlabel('Batch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy Over Time', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score over time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(streaming_df['batch'], streaming_df['f1'], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "ax2.axhline(y=baseline_metrics['f1'], color='green', linestyle='--', label='Baseline')\n",
    "ax2.axvline(x=drift_start_batch, color='red', linestyle='--', alpha=0.7, label='Drift Start')\n",
    "ax2.set_xlabel('Batch')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('F1 Score Over Time', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-AUC over time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(streaming_df['batch'], streaming_df['roc_auc'], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "ax3.axhline(y=baseline_metrics['roc_auc'], color='green', linestyle='--', label='Baseline')\n",
    "ax3.axvline(x=drift_start_batch, color='red', linestyle='--', alpha=0.7, label='Drift Start')\n",
    "ax3.set_xlabel('Batch')\n",
    "ax3.set_ylabel('ROC-AUC')\n",
    "ax3.set_title('ROC-AUC Over Time', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Drift factor vs Performance\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(streaming_df['drift_factor'], streaming_df['accuracy'], c=streaming_df['batch'], \n",
    "            cmap='viridis', s=100, alpha=0.7)\n",
    "ax4.set_xlabel('Drift Factor')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title('Accuracy vs Drift Intensity', fontweight='bold')\n",
    "cbar = plt.colorbar(ax4.collections[0], ax=ax4)\n",
    "cbar.set_label('Batch Number')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Performance Degradation with Gradual Concept Drift', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìâ Key Observations:\")\n",
    "print(f\"   - Baseline Accuracy: {baseline_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - Final Accuracy: {streaming_df['accuracy'].iloc[-1]:.4f}\")\n",
    "print(f\"   - Performance Drop: {baseline_metrics['accuracy'] - streaming_df['accuracy'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0fc2e",
   "metadata": {},
   "source": [
    "## 7. Build Performance Monitoring Dashboard\n",
    "\n",
    "A comprehensive monitoring dashboard tracks multiple metrics and visualizes them in real-time. Here we create a reusable dashboard class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitoringDashboard:\n",
    "    \"\"\"\n",
    "    Comprehensive monitoring dashboard for production ML models.\n",
    "    \n",
    "    Tracks and visualizes:\n",
    "    - Model performance metrics over time\n",
    "    - Data drift indicators\n",
    "    - Prediction drift\n",
    "    - Feature distributions\n",
    "    - Alerts and anomalies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler, baseline_metrics: Dict,\n",
    "                 reference_data: pd.DataFrame,\n",
    "                 feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize monitoring dashboard.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            scaler: Feature scaler\n",
    "            baseline_metrics: Baseline performance metrics\n",
    "            reference_data: Reference (training) data\n",
    "            feature_names: List of feature names\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.reference_data = reference_data\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Initialize detectors\n",
    "        self.data_drift_detector = DataDriftDetector(reference_data)\n",
    "        \n",
    "        # Storage for historical data\n",
    "        self.history = {\n",
    "            'timestamps': [],\n",
    "            'metrics': [],\n",
    "            'data_drift': [],\n",
    "            'prediction_drift': [],\n",
    "            'alerts': []\n",
    "        }\n",
    "    \n",
    "    def log_batch(self, X: pd.DataFrame, y_true: np.ndarray,\n",
    "                  timestamp: datetime = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Log a new batch of predictions and compute all monitoring metrics.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature DataFrame\n",
    "            y_true: True labels\n",
    "            timestamp: Batch timestamp\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all monitoring results\n",
    "        \"\"\"\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        # Get predictions\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        y_pred = self.model.predict(X_scaled)\n",
    "        y_proba = self.model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        # 1. Compute performance metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.5\n",
    "        }\n",
    "        \n",
    "        # 2. Data drift detection\n",
    "        data_drift = self.data_drift_detector.comprehensive_drift_report(X)\n",
    "        n_drifted_features = data_drift['overall_drift'].sum()\n",
    "        \n",
    "        # 3. Prediction drift (compare to first batch or reference)\n",
    "        pred_drift_psi = self._calculate_prediction_psi(y_proba)\n",
    "        \n",
    "        # 4. Generate alerts\n",
    "        alerts = self._check_alerts(metrics, n_drifted_features, pred_drift_psi)\n",
    "        \n",
    "        # Store in history\n",
    "        self.history['timestamps'].append(timestamp)\n",
    "        self.history['metrics'].append(metrics)\n",
    "        self.history['data_drift'].append({\n",
    "            'n_drifted': n_drifted_features,\n",
    "            'total_features': len(self.feature_names)\n",
    "        })\n",
    "        self.history['prediction_drift'].append({'psi': pred_drift_psi})\n",
    "        self.history['alerts'].append(alerts)\n",
    "        \n",
    "        return {\n",
    "            'timestamp': timestamp,\n",
    "            'metrics': metrics,\n",
    "            'data_drift': {'n_drifted': n_drifted_features},\n",
    "            'prediction_drift_psi': pred_drift_psi,\n",
    "            'alerts': alerts\n",
    "        }\n",
    "    \n",
    "    def _calculate_prediction_psi(self, y_proba: np.ndarray, n_bins: int = 10) -> float:\n",
    "        \"\"\"Calculate PSI for prediction probabilities.\"\"\"\n",
    "        if len(self.history['timestamps']) == 0:\n",
    "            self._reference_proba = y_proba\n",
    "            return 0.0\n",
    "        \n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        \n",
    "        ref_counts, _ = np.histogram(self._reference_proba, bins=bins)\n",
    "        prod_counts, _ = np.histogram(y_proba, bins=bins)\n",
    "        \n",
    "        epsilon = 1e-10\n",
    "        ref_props = (ref_counts + epsilon) / (ref_counts.sum() + n_bins * epsilon)\n",
    "        prod_props = (prod_counts + epsilon) / (prod_counts.sum() + n_bins * epsilon)\n",
    "        \n",
    "        psi = np.sum((prod_props - ref_props) * np.log(prod_props / ref_props))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def _check_alerts(self, metrics: Dict, n_drifted: int, \n",
    "                      pred_psi: float) -> List[Dict]:\n",
    "        \"\"\"Check for alert conditions.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Performance degradation alerts\n",
    "        for metric, value in metrics.items():\n",
    "            if metric in self.baseline_metrics:\n",
    "                baseline = self.baseline_metrics[metric]\n",
    "                relative_drop = (baseline - value) / (baseline + 1e-10)\n",
    "                \n",
    "                if relative_drop > 0.20:\n",
    "                    alerts.append({\n",
    "                        'type': 'CRITICAL',\n",
    "                        'metric': metric,\n",
    "                        'message': f'{metric} dropped {relative_drop:.1%} from baseline'\n",
    "                    })\n",
    "                elif relative_drop > 0.10:\n",
    "                    alerts.append({\n",
    "                        'type': 'WARNING',\n",
    "                        'metric': metric,\n",
    "                        'message': f'{metric} dropped {relative_drop:.1%} from baseline'\n",
    "                    })\n",
    "        \n",
    "        # Data drift alert\n",
    "        if n_drifted >= len(self.feature_names) // 2:\n",
    "            alerts.append({\n",
    "                'type': 'WARNING',\n",
    "                'metric': 'data_drift',\n",
    "                'message': f'{n_drifted}/{len(self.feature_names)} features show drift'\n",
    "            })\n",
    "        \n",
    "        # Prediction drift alert\n",
    "        if pred_psi > 0.25:\n",
    "            alerts.append({\n",
    "                'type': 'CRITICAL',\n",
    "                'metric': 'prediction_drift',\n",
    "                'message': f'Prediction PSI = {pred_psi:.3f} (>0.25)'\n",
    "            })\n",
    "        elif pred_psi > 0.10:\n",
    "            alerts.append({\n",
    "                'type': 'WARNING',\n",
    "                'metric': 'prediction_drift',\n",
    "                'message': f'Prediction PSI = {pred_psi:.3f} (>0.10)'\n",
    "            })\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def plot_dashboard(self, figsize: Tuple[int, int] = (16, 12)):\n",
    "        \"\"\"Generate comprehensive monitoring dashboard visualization.\"\"\"\n",
    "        if len(self.history['timestamps']) < 2:\n",
    "            print(\"Not enough data to plot. Log at least 2 batches.\")\n",
    "            return\n",
    "        \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Create grid layout\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Prepare data\n",
    "        timestamps = range(len(self.history['timestamps']))\n",
    "        metrics_df = pd.DataFrame(self.history['metrics'])\n",
    "        \n",
    "        # 1. Accuracy over time (top left)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        ax1.plot(timestamps, metrics_df['accuracy'], 'b-', linewidth=2, marker='o', markersize=4)\n",
    "        ax1.axhline(y=self.baseline_metrics['accuracy'], color='green', linestyle='--', \n",
    "                    label='Baseline', linewidth=2)\n",
    "        ax1.fill_between(timestamps, \n",
    "                         self.baseline_metrics['accuracy'] * 0.9,\n",
    "                         self.baseline_metrics['accuracy'] * 1.1,\n",
    "                         alpha=0.2, color='green')\n",
    "        ax1.set_xlabel('Batch')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title('Accuracy', fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. F1 Score over time (top middle)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        ax2.plot(timestamps, metrics_df['f1'], 'purple', linewidth=2, marker='o', markersize=4)\n",
    "        ax2.axhline(y=self.baseline_metrics['f1'], color='green', linestyle='--', linewidth=2)\n",
    "        ax2.set_xlabel('Batch')\n",
    "        ax2.set_ylabel('F1 Score')\n",
    "        ax2.set_title('F1 Score', fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. ROC-AUC over time (top right)\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        ax3.plot(timestamps, metrics_df['roc_auc'], 'orange', linewidth=2, marker='o', markersize=4)\n",
    "        ax3.axhline(y=self.baseline_metrics['roc_auc'], color='green', linestyle='--', linewidth=2)\n",
    "        ax3.set_xlabel('Batch')\n",
    "        ax3.set_ylabel('ROC-AUC')\n",
    "        ax3.set_title('ROC-AUC', fontweight='bold')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Data drift - number of drifted features (middle left)\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        n_drifted = [d['n_drifted'] for d in self.history['data_drift']]\n",
    "        colors = ['green' if n < 2 else 'orange' if n < 4 else 'red' for n in n_drifted]\n",
    "        ax4.bar(timestamps, n_drifted, color=colors, alpha=0.7)\n",
    "        ax4.axhline(y=len(self.feature_names) // 2, color='red', linestyle='--', \n",
    "                    label='Alert Threshold')\n",
    "        ax4.set_xlabel('Batch')\n",
    "        ax4.set_ylabel('# Drifted Features')\n",
    "        ax4.set_title('Data Drift (# Features)', fontweight='bold')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Prediction PSI (middle center)\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        pred_psi = [d['psi'] for d in self.history['prediction_drift']]\n",
    "        ax5.plot(timestamps, pred_psi, 'red', linewidth=2, marker='s', markersize=4)\n",
    "        ax5.axhline(y=0.10, color='orange', linestyle='--', label='Moderate (0.10)')\n",
    "        ax5.axhline(y=0.25, color='red', linestyle='--', label='High (0.25)')\n",
    "        ax5.set_xlabel('Batch')\n",
    "        ax5.set_ylabel('PSI')\n",
    "        ax5.set_title('Prediction Drift (PSI)', fontweight='bold')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Precision & Recall (middle right)\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        ax6.plot(timestamps, metrics_df['precision'], 'b-', linewidth=2, marker='o', \n",
    "                 markersize=4, label='Precision')\n",
    "        ax6.plot(timestamps, metrics_df['recall'], 'r-', linewidth=2, marker='s', \n",
    "                 markersize=4, label='Recall')\n",
    "        ax6.set_xlabel('Batch')\n",
    "        ax6.set_ylabel('Score')\n",
    "        ax6.set_title('Precision & Recall', fontweight='bold')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. Alert summary (bottom, spans full width)\n",
    "        ax7 = fig.add_subplot(gs[2, :])\n",
    "        \n",
    "        # Count alerts by type\n",
    "        alert_counts = {'WARNING': [], 'CRITICAL': []}\n",
    "        for batch_alerts in self.history['alerts']:\n",
    "            warning_count = sum(1 for a in batch_alerts if a['type'] == 'WARNING')\n",
    "            critical_count = sum(1 for a in batch_alerts if a['type'] == 'CRITICAL')\n",
    "            alert_counts['WARNING'].append(warning_count)\n",
    "            alert_counts['CRITICAL'].append(critical_count)\n",
    "        \n",
    "        width = 0.35\n",
    "        x = np.array(timestamps)\n",
    "        ax7.bar(x - width/2, alert_counts['WARNING'], width, label='Warning', color='orange', alpha=0.7)\n",
    "        ax7.bar(x + width/2, alert_counts['CRITICAL'], width, label='Critical', color='red', alpha=0.7)\n",
    "        ax7.set_xlabel('Batch')\n",
    "        ax7.set_ylabel('# Alerts')\n",
    "        ax7.set_title('Alerts Over Time', fontweight='bold')\n",
    "        ax7.legend()\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Model Monitoring Dashboard', fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.show()\n",
    "    \n",
    "    def get_summary_report(self) -> str:\n",
    "        \"\"\"Generate a text summary report.\"\"\"\n",
    "        if len(self.history['timestamps']) == 0:\n",
    "            return \"No data logged yet.\"\n",
    "        \n",
    "        latest = self.history['metrics'][-1]\n",
    "        n_batches = len(self.history['timestamps'])\n",
    "        total_alerts = sum(len(a) for a in self.history['alerts'])\n",
    "        critical_alerts = sum(sum(1 for alert in a if alert['type'] == 'CRITICAL') \n",
    "                             for a in self.history['alerts'])\n",
    "        \n",
    "        report = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    MODEL MONITORING SUMMARY                       ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  Batches Processed: {n_batches:>5}                                        ‚ïë\n",
    "‚ïë  Total Alerts: {total_alerts:>5} (Critical: {critical_alerts})                           ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  LATEST METRICS:                                                  ‚ïë\n",
    "‚ïë    Accuracy:  {latest['accuracy']:.4f} (baseline: {self.baseline_metrics['accuracy']:.4f})              ‚ïë\n",
    "‚ïë    F1 Score:  {latest['f1']:.4f} (baseline: {self.baseline_metrics['f1']:.4f})              ‚ïë\n",
    "‚ïë    ROC-AUC:   {latest['roc_auc']:.4f} (baseline: {self.baseline_metrics['roc_auc']:.4f})              ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  DATA DRIFT: {self.history['data_drift'][-1]['n_drifted']}/{self.history['data_drift'][-1]['total_features']} features drifted                           ‚ïë\n",
    "‚ïë  PREDICTION PSI: {self.history['prediction_drift'][-1]['psi']:.4f}                                     ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "        return report\n",
    "\n",
    "print(\"‚úÖ ModelMonitoringDashboard class created!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
