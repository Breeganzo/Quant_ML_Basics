{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7afbe5fe",
   "metadata": {},
   "source": [
    "# Day 06: Model Deployment for Trading Systems\n",
    "\n",
    "## Production ML Week - Deploying Quantitative Models\n",
    "\n",
    "This notebook covers the critical aspects of deploying machine learning models for trading systems in production environments.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. **Model Serialization** - Save and load models using joblib, pickle, and ONNX\n",
    "2. **REST API Development** - Build Flask/FastAPI services for model inference\n",
    "3. **Real-time Pipelines** - Create low-latency prediction pipelines\n",
    "4. **Batch Processing** - Implement efficient batch prediction systems\n",
    "5. **Monitoring & Logging** - Track model performance and detect drift\n",
    "6. **Model Versioning** - Use MLflow for experiment tracking and model registry\n",
    "7. **Containerization** - Package models with Docker for consistent deployment\n",
    "8. **Deployment Strategies** - Implement blue-green and canary deployments\n",
    "\n",
    "### Production Considerations for Trading:\n",
    "- **Latency Requirements**: Sub-millisecond for HFT, seconds for swing trading\n",
    "- **Reliability**: 99.99% uptime requirements\n",
    "- **Scalability**: Handle market volatility spikes\n",
    "- **Compliance**: Audit trails and model explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce395254",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570672e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìÖ Date: 2026-01-24 02:15:14\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# API & Web Frameworks (conceptual - typically run separately)\n",
    "# from flask import Flask, request, jsonify\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# import uvicorn\n",
    "\n",
    "# Monitoring & Logging\n",
    "import logging\n",
    "from collections import deque\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('TradingModelDeployment')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d48e2",
   "metadata": {},
   "source": [
    "## 2. Create Sample Trading Model for Deployment\n",
    "\n",
    "First, let's create and train a sample trading model that we'll use throughout this deployment tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edaa792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Generated 10000 samples with 16 features\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>returns</th>\n",
       "      <th>volatility_20</th>\n",
       "      <th>momentum_10</th>\n",
       "      <th>momentum_20</th>\n",
       "      <th>rsi</th>\n",
       "      <th>volume</th>\n",
       "      <th>spread</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>volume_ma</th>\n",
       "      <th>volume_ratio</th>\n",
       "      <th>price_ma_20</th>\n",
       "      <th>price_ma_50</th>\n",
       "      <th>ma_crossover</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.998379</td>\n",
       "      <td>0.009934</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.970519</td>\n",
       "      <td>387510.029736</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.026734e+06</td>\n",
       "      <td>0.377420</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.719476</td>\n",
       "      <td>-0.002765</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.170626</td>\n",
       "      <td>108720.815639</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1.026734e+06</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102.032660</td>\n",
       "      <td>0.012954</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.864540</td>\n",
       "      <td>74516.181324</td>\n",
       "      <td>0.003510</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.026734e+06</td>\n",
       "      <td>0.072576</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105.188455</td>\n",
       "      <td>0.030461</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.865661</td>\n",
       "      <td>99020.630710</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1.026734e+06</td>\n",
       "      <td>0.096442</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104.697002</td>\n",
       "      <td>-0.004683</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.596106</td>\n",
       "      <td>874355.823733</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1.026734e+06</td>\n",
       "      <td>0.851590</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>100.998379</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        price   returns  volatility_20  momentum_10  momentum_20        rsi  \\\n",
       "0  100.998379  0.009934           0.01          0.0          0.0  52.970519   \n",
       "1  100.719476 -0.002765           0.01          0.0          0.0  49.170626   \n",
       "2  102.032660  0.012954           0.01          0.0          0.0  53.864540   \n",
       "3  105.188455  0.030461           0.01          0.0          0.0  58.865661   \n",
       "4  104.697002 -0.004683           0.01          0.0          0.0  48.596106   \n",
       "\n",
       "          volume    spread  hour  day_of_week     volume_ma  volume_ratio  \\\n",
       "0  387510.029736  0.000188    16            1  1.026734e+06      0.377420   \n",
       "1  108720.815639  0.000426    15            4  1.026734e+06      0.105890   \n",
       "2   74516.181324  0.003510    12            1  1.026734e+06      0.072576   \n",
       "3   99020.630710  0.001518    16            3  1.026734e+06      0.096442   \n",
       "4  874355.823733  0.000332    16            3  1.026734e+06      0.851590   \n",
       "\n",
       "   price_ma_20  price_ma_50  ma_crossover  target  \n",
       "0   100.998379   100.998379             0       0  \n",
       "1   100.998379   100.998379             0       1  \n",
       "2   100.998379   100.998379             0       1  \n",
       "3   100.998379   100.998379             0       0  \n",
       "4   100.998379   100.998379             0       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_trading_features(n_samples: int = 10000) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic trading data with features.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Price data simulation\n",
    "    returns = np.random.randn(n_samples) * 0.02\n",
    "    price = 100 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Technical indicators as features\n",
    "    df = pd.DataFrame({\n",
    "        'price': price,\n",
    "        'returns': returns,\n",
    "        'volatility_20': pd.Series(returns).rolling(20).std().fillna(0.01),\n",
    "        'momentum_10': pd.Series(price).pct_change(10).fillna(0),\n",
    "        'momentum_20': pd.Series(price).pct_change(20).fillna(0),\n",
    "        'rsi': 50 + 30 * np.tanh(returns * 10),  # Simplified RSI\n",
    "        'volume': np.random.exponential(1000000, n_samples),\n",
    "        'spread': np.random.exponential(0.001, n_samples),\n",
    "        'hour': np.random.randint(9, 17, n_samples),\n",
    "        'day_of_week': np.random.randint(0, 5, n_samples),\n",
    "    })\n",
    "    \n",
    "    # Add more complex features\n",
    "    df['volume_ma'] = df['volume'].rolling(20).mean().fillna(df['volume'].mean())\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_ma']\n",
    "    df['price_ma_20'] = df['price'].rolling(20).mean().fillna(df['price'].iloc[0])\n",
    "    df['price_ma_50'] = df['price'].rolling(50).mean().fillna(df['price'].iloc[0])\n",
    "    df['ma_crossover'] = (df['price_ma_20'] > df['price_ma_50']).astype(int)\n",
    "    \n",
    "    # Target: 1 if next return > 0.1%, else 0\n",
    "    future_returns = df['returns'].shift(-1).fillna(0)\n",
    "    df['target'] = (future_returns > 0.001).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate training data\n",
    "trading_data = generate_trading_features(10000)\n",
    "print(f\"üìä Generated {len(trading_data)} samples with {len(trading_data.columns)} features\")\n",
    "trading_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b04022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model Training Complete\n",
      "   Train Accuracy: 0.7742\n",
      "   Test Accuracy:  0.5040\n",
      "   Precision:      0.4680\n",
      "   Recall:         0.2437\n",
      "   F1 Score:       0.3205\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "FEATURE_COLUMNS = [\n",
    "    'returns', 'volatility_20', 'momentum_10', 'momentum_20', 'rsi',\n",
    "    'volume_ratio', 'ma_crossover', 'spread', 'hour', 'day_of_week'\n",
    "]\n",
    "\n",
    "# Prepare features and target\n",
    "X = trading_data[FEATURE_COLUMNS].values\n",
    "y = trading_data['target'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train a pipeline with preprocessing\n",
    "model_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = model_pipeline.score(X_train, y_train)\n",
    "test_acc = model_pipeline.score(X_test, y_test)\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"‚úÖ Model Training Complete\")\n",
    "print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"   Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"   Precision:      {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   Recall:         {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"   F1 Score:       {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7ab0a",
   "metadata": {},
   "source": [
    "## 3. Model Serialization Functions\n",
    "\n",
    "Model serialization is critical for deployment. We'll implement robust serialization with metadata tracking, versioning, and integrity verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39927beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:17,014 - TradingModelDeployment - INFO - ModelSerializer initialized with directory: deployed_models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ModelSerializer initialized\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelMetadata:\n",
    "    \"\"\"Metadata container for deployed models.\"\"\"\n",
    "    model_name: str\n",
    "    version: str\n",
    "    created_at: str\n",
    "    model_type: str\n",
    "    feature_names: List[str]\n",
    "    target_name: str\n",
    "    training_metrics: Dict[str, float]\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    checksum: str = \"\"\n",
    "    description: str = \"\"\n",
    "    author: str = \"\"\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'ModelMetadata':\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "class ModelSerializer:\n",
    "    \"\"\"\n",
    "    Production-grade model serialization with metadata and versioning.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple serialization formats (joblib, pickle)\n",
    "    - Checksum verification for integrity\n",
    "    - Metadata tracking\n",
    "    - Version management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir: str = \"./models\"):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"ModelSerializer initialized with directory: {self.model_dir}\")\n",
    "    \n",
    "    def _compute_checksum(self, filepath: Path) -> str:\n",
    "        \"\"\"Compute SHA256 checksum for model file.\"\"\"\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    \n",
    "    def save_model(\n",
    "        self,\n",
    "        model: Any,\n",
    "        model_name: str,\n",
    "        version: str,\n",
    "        feature_names: List[str],\n",
    "        training_metrics: Dict[str, float],\n",
    "        hyperparameters: Dict[str, Any],\n",
    "        target_name: str = \"target\",\n",
    "        format: str = \"joblib\",\n",
    "        description: str = \"\",\n",
    "        author: str = \"\"\n",
    "    ) -> Tuple[Path, ModelMetadata]:\n",
    "        \"\"\"\n",
    "        Save model with metadata and versioning.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : trained model object\n",
    "        model_name : name identifier for the model\n",
    "        version : semantic version string (e.g., \"1.0.0\")\n",
    "        feature_names : list of feature column names\n",
    "        training_metrics : dict of training metrics\n",
    "        hyperparameters : dict of model hyperparameters\n",
    "        format : \"joblib\" or \"pickle\"\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple of (model_path, metadata)\n",
    "        \"\"\"\n",
    "        # Create version directory\n",
    "        version_dir = self.model_dir / model_name / version\n",
    "        version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define file paths\n",
    "        model_filename = f\"{model_name}_{version}.{'joblib' if format == 'joblib' else 'pkl'}\"\n",
    "        model_path = version_dir / model_filename\n",
    "        metadata_path = version_dir / \"metadata.json\"\n",
    "        \n",
    "        # Save model\n",
    "        if format == \"joblib\":\n",
    "            joblib.dump(model, model_path)\n",
    "        else:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "        \n",
    "        # Compute checksum\n",
    "        checksum = self._compute_checksum(model_path)\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = ModelMetadata(\n",
    "            model_name=model_name,\n",
    "            version=version,\n",
    "            created_at=datetime.now().isoformat(),\n",
    "            model_type=type(model).__name__,\n",
    "            feature_names=feature_names,\n",
    "            target_name=target_name,\n",
    "            training_metrics=training_metrics,\n",
    "            hyperparameters=hyperparameters,\n",
    "            checksum=checksum,\n",
    "            description=description,\n",
    "            author=author\n",
    "        )\n",
    "        \n",
    "        # Save metadata\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata.to_dict(), f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Model saved: {model_path}\")\n",
    "        logger.info(f\"Checksum: {checksum[:16]}...\")\n",
    "        \n",
    "        return model_path, metadata\n",
    "    \n",
    "    def load_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        version: str = \"latest\",\n",
    "        verify_checksum: bool = True\n",
    "    ) -> Tuple[Any, ModelMetadata]:\n",
    "        \"\"\"\n",
    "        Load model with integrity verification.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_name : name identifier for the model\n",
    "        version : specific version or \"latest\"\n",
    "        verify_checksum : whether to verify file integrity\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Tuple of (model, metadata)\n",
    "        \"\"\"\n",
    "        model_base_dir = self.model_dir / model_name\n",
    "        \n",
    "        if not model_base_dir.exists():\n",
    "            raise FileNotFoundError(f\"Model '{model_name}' not found\")\n",
    "        \n",
    "        # Get version directory\n",
    "        if version == \"latest\":\n",
    "            versions = sorted([d.name for d in model_base_dir.iterdir() if d.is_dir()])\n",
    "            if not versions:\n",
    "                raise FileNotFoundError(f\"No versions found for model '{model_name}'\")\n",
    "            version = versions[-1]\n",
    "            logger.info(f\"Loading latest version: {version}\")\n",
    "        \n",
    "        version_dir = model_base_dir / version\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = version_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = ModelMetadata.from_dict(json.load(f))\n",
    "        \n",
    "        # Find model file\n",
    "        model_files = list(version_dir.glob(f\"{model_name}_{version}.*\"))\n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(f\"Model file not found in {version_dir}\")\n",
    "        \n",
    "        model_path = model_files[0]\n",
    "        \n",
    "        # Verify checksum\n",
    "        if verify_checksum:\n",
    "            computed_checksum = self._compute_checksum(model_path)\n",
    "            if computed_checksum != metadata.checksum:\n",
    "                raise ValueError(\"Model file checksum mismatch! File may be corrupted.\")\n",
    "            logger.info(\"‚úÖ Checksum verified\")\n",
    "        \n",
    "        # Load model\n",
    "        if model_path.suffix == '.joblib':\n",
    "            model = joblib.load(model_path)\n",
    "        else:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Model loaded: {model_name} v{version}\")\n",
    "        return model, metadata\n",
    "    \n",
    "    def list_versions(self, model_name: str) -> List[Dict]:\n",
    "        \"\"\"List all versions of a model with their metadata.\"\"\"\n",
    "        model_base_dir = self.model_dir / model_name\n",
    "        \n",
    "        if not model_base_dir.exists():\n",
    "            return []\n",
    "        \n",
    "        versions = []\n",
    "        for version_dir in sorted(model_base_dir.iterdir()):\n",
    "            if version_dir.is_dir():\n",
    "                metadata_path = version_dir / \"metadata.json\"\n",
    "                if metadata_path.exists():\n",
    "                    with open(metadata_path, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    versions.append({\n",
    "                        'version': version_dir.name,\n",
    "                        'created_at': metadata.get('created_at'),\n",
    "                        'metrics': metadata.get('training_metrics')\n",
    "                    })\n",
    "        \n",
    "        return versions\n",
    "\n",
    "# Initialize serializer\n",
    "serializer = ModelSerializer(model_dir=\"./deployed_models\")\n",
    "print(\"‚úÖ ModelSerializer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b710c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:17,162 - TradingModelDeployment - INFO - Model saved: deployed_models/trading_signal_classifier/1.0.0/trading_signal_classifier_1.0.0.joblib\n",
      "2026-01-24 02:15:17,162 - TradingModelDeployment - INFO - Checksum: e34d8bc09532bf5d...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Model saved to: deployed_models/trading_signal_classifier/1.0.0/trading_signal_classifier_1.0.0.joblib\n",
      "üìã Metadata:\n",
      "{\n",
      "  \"model_name\": \"trading_signal_classifier\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"created_at\": \"2026-01-24T02:15:17.161911\",\n",
      "  \"model_type\": \"Pipeline\",\n",
      "  \"feature_names\": [\n",
      "    \"returns\",\n",
      "    \"volatility_20\",\n",
      "    \"momentum_10\",\n",
      "    \"momentum_20\",\n",
      "    \"rsi\",\n",
      "    \"volume_ratio\",\n",
      "    \"ma_crossover\",\n",
      "    \"spread\",\n",
      "    \"hour\",\n",
      "    \"day_of_week\"\n",
      "  ],\n",
      "  \"target_name\": \"target\",\n",
      "  \"training_metrics\": {\n",
      "    \"accuracy\": 0.504,\n",
      "    \"precision\": 0.468,\n",
      "    \"recall\": 0.24375,\n",
      "    \"f1_score\": 0.32054794520547947\n",
      "  },\n",
      "  \"hyperparameters\": {\n",
      "    \"n_estimators\": 100,\n",
      "    \"max_depth\": 10,\n",
      "    \"min_samples_split\": 20\n",
      "  },\n",
      "  \"checksum\": \"e34d8bc09532bf5dbe3fa75328a58028a2666acfb3c7d766dd6421fcbe349a4e\",\n",
      "  \"description\": \"Random Forest classifier for trading signal prediction\",\n",
      "  \"author\": \"Quant Team\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save our trained model\n",
    "training_metrics = {\n",
    "    'accuracy': float(test_acc),\n",
    "    'precision': float(precision_score(y_test, y_pred)),\n",
    "    'recall': float(recall_score(y_test, y_pred)),\n",
    "    'f1_score': float(f1_score(y_test, y_pred))\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 20\n",
    "}\n",
    "\n",
    "model_path, metadata = serializer.save_model(\n",
    "    model=model_pipeline,\n",
    "    model_name=\"trading_signal_classifier\",\n",
    "    version=\"1.0.0\",\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    training_metrics=training_metrics,\n",
    "    hyperparameters=hyperparameters,\n",
    "    description=\"Random Forest classifier for trading signal prediction\",\n",
    "    author=\"Quant Team\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìÅ Model saved to: {model_path}\")\n",
    "print(f\"üìã Metadata:\")\n",
    "print(json.dumps(metadata.to_dict(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82d5a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:17,299 - TradingModelDeployment - INFO - ‚úÖ Checksum verified\n",
      "2026-01-24 02:15:17,310 - TradingModelDeployment - INFO - Model loaded: trading_signal_classifier v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model loaded and verified successfully\n",
      "   Test predictions: [0 0 0 0 0]\n",
      "   Model type: Pipeline\n",
      "   Features: ['returns', 'volatility_20', 'momentum_10', 'momentum_20', 'rsi', 'volume_ratio', 'ma_crossover', 'spread', 'hour', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "# Load and verify the model\n",
    "loaded_model, loaded_metadata = serializer.load_model(\n",
    "    model_name=\"trading_signal_classifier\",\n",
    "    version=\"1.0.0\",\n",
    "    verify_checksum=True\n",
    ")\n",
    "\n",
    "# Test that loaded model works correctly\n",
    "test_predictions = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\n‚úÖ Model loaded and verified successfully\")\n",
    "print(f\"   Test predictions: {test_predictions}\")\n",
    "print(f\"   Model type: {loaded_metadata.model_type}\")\n",
    "print(f\"   Features: {loaded_metadata.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e8821",
   "metadata": {},
   "source": [
    "## 4. Build a Prediction API with Flask\n",
    "\n",
    "This section demonstrates how to create a production-ready REST API for serving trading model predictions. The API includes health checks, input validation, and proper error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97ffafda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:17,377 - TradingModelDeployment - INFO - API initialized with model: trading_signal_classifier v1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trading Model API initialized\n"
     ]
    }
   ],
   "source": [
    "class TradingModelAPI:\n",
    "    \"\"\"\n",
    "    Production-ready API wrapper for trading model predictions.\n",
    "    \n",
    "    This class simulates a Flask/FastAPI service structure.\n",
    "    In production, this would be split into separate files.\n",
    "    \n",
    "    Endpoints:\n",
    "    - /health: Health check\n",
    "    - /predict: Single prediction\n",
    "    - /predict_batch: Batch predictions\n",
    "    - /model_info: Model metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        metadata: ModelMetadata,\n",
    "        max_batch_size: int = 1000\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.metadata = metadata\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.request_count = 0\n",
    "        self.total_latency = 0.0\n",
    "        self.startup_time = datetime.now()\n",
    "        \n",
    "        logger.info(f\"API initialized with model: {metadata.model_name} v{metadata.version}\")\n",
    "    \n",
    "    def health_check(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Health check endpoint.\n",
    "        Returns service status and basic metrics.\n",
    "        \"\"\"\n",
    "        uptime = (datetime.now() - self.startup_time).total_seconds()\n",
    "        avg_latency = self.total_latency / max(self.request_count, 1) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"model_name\": self.metadata.model_name,\n",
    "            \"model_version\": self.metadata.version,\n",
    "            \"uptime_seconds\": round(uptime, 2),\n",
    "            \"requests_served\": self.request_count,\n",
    "            \"avg_latency_ms\": round(avg_latency, 2),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def model_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Model information endpoint.\n",
    "        Returns model metadata and feature requirements.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"model_name\": self.metadata.model_name,\n",
    "            \"version\": self.metadata.version,\n",
    "            \"model_type\": self.metadata.model_type,\n",
    "            \"created_at\": self.metadata.created_at,\n",
    "            \"feature_names\": self.metadata.feature_names,\n",
    "            \"required_features\": len(self.metadata.feature_names),\n",
    "            \"training_metrics\": self.metadata.training_metrics,\n",
    "            \"description\": self.metadata.description\n",
    "        }\n",
    "    \n",
    "    def validate_input(self, features: Dict) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate input features against model requirements.\"\"\"\n",
    "        missing_features = []\n",
    "        for feature_name in self.metadata.feature_names:\n",
    "            if feature_name not in features:\n",
    "                missing_features.append(feature_name)\n",
    "        \n",
    "        if missing_features:\n",
    "            return False, f\"Missing features: {missing_features}\"\n",
    "        \n",
    "        # Check for valid numeric values\n",
    "        for feature_name, value in features.items():\n",
    "            if feature_name in self.metadata.feature_names:\n",
    "                if not isinstance(value, (int, float)):\n",
    "                    return False, f\"Feature '{feature_name}' must be numeric, got {type(value)}\"\n",
    "                if np.isnan(value) or np.isinf(value):\n",
    "                    return False, f\"Feature '{feature_name}' contains invalid value: {value}\"\n",
    "        \n",
    "        return True, \"Valid\"\n",
    "    \n",
    "    def predict(self, features: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Single prediction endpoint.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : dict with feature_name: value pairs\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict with prediction, probability, and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Validate input\n",
    "        is_valid, message = self.validate_input(features)\n",
    "        if not is_valid:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": message,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Prepare features array in correct order\n",
    "            feature_array = np.array([\n",
    "                [features[name] for name in self.metadata.feature_names]\n",
    "            ])\n",
    "            \n",
    "            # Get prediction and probability\n",
    "            prediction = self.model.predict(feature_array)[0]\n",
    "            \n",
    "            # Get probability if available\n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(feature_array)[0]\n",
    "                confidence = float(max(probabilities))\n",
    "            else:\n",
    "                probabilities = None\n",
    "                confidence = None\n",
    "            \n",
    "            # Calculate latency\n",
    "            latency = time.perf_counter() - start_time\n",
    "            self.request_count += 1\n",
    "            self.total_latency += latency\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"prediction\": int(prediction),\n",
    "                \"signal\": \"BUY\" if prediction == 1 else \"HOLD\",\n",
    "                \"confidence\": confidence,\n",
    "                \"probabilities\": {\n",
    "                    \"class_0\": float(probabilities[0]) if probabilities is not None else None,\n",
    "                    \"class_1\": float(probabilities[1]) if probabilities is not None else None\n",
    "                },\n",
    "                \"latency_ms\": round(latency * 1000, 3),\n",
    "                \"model_version\": self.metadata.version,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction error: {str(e)}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def predict_batch(self, features_list: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Batch prediction endpoint.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features_list : list of feature dicts\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict with list of predictions and batch metadata\n",
    "        \"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Check batch size\n",
    "        if len(features_list) > self.max_batch_size:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Batch size {len(features_list)} exceeds maximum {self.max_batch_size}\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        if len(features_list) == 0:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"Empty batch received\",\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Validate all inputs\n",
    "            for i, features in enumerate(features_list):\n",
    "                is_valid, message = self.validate_input(features)\n",
    "                if not is_valid:\n",
    "                    return {\n",
    "                        \"status\": \"error\",\n",
    "                        \"error\": f\"Invalid input at index {i}: {message}\",\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "            \n",
    "            # Prepare batch array\n",
    "            feature_array = np.array([\n",
    "                [features[name] for name in self.metadata.feature_names]\n",
    "                for features in features_list\n",
    "            ])\n",
    "            \n",
    "            # Batch prediction\n",
    "            predictions = self.model.predict(feature_array)\n",
    "            \n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(feature_array)\n",
    "            else:\n",
    "                probabilities = None\n",
    "            \n",
    "            # Format results\n",
    "            results = []\n",
    "            for i, pred in enumerate(predictions):\n",
    "                result = {\n",
    "                    \"index\": i,\n",
    "                    \"prediction\": int(pred),\n",
    "                    \"signal\": \"BUY\" if pred == 1 else \"HOLD\"\n",
    "                }\n",
    "                if probabilities is not None:\n",
    "                    result[\"confidence\"] = float(max(probabilities[i]))\n",
    "                results.append(result)\n",
    "            \n",
    "            latency = time.perf_counter() - start_time\n",
    "            self.request_count += len(features_list)\n",
    "            self.total_latency += latency\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"batch_size\": len(features_list),\n",
    "                \"predictions\": results,\n",
    "                \"total_latency_ms\": round(latency * 1000, 3),\n",
    "                \"avg_latency_per_sample_ms\": round(latency * 1000 / len(features_list), 3),\n",
    "                \"model_version\": self.metadata.version,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize API with our model\n",
    "api = TradingModelAPI(\n",
    "    model=loaded_model,\n",
    "    metadata=loaded_metadata\n",
    ")\n",
    "print(\"‚úÖ Trading Model API initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ea22ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üè• HEALTH CHECK\n",
      "============================================================\n",
      "{\n",
      "  \"status\": \"healthy\",\n",
      "  \"model_name\": \"trading_signal_classifier\",\n",
      "  \"model_version\": \"1.0.0\",\n",
      "  \"uptime_seconds\": 0.1,\n",
      "  \"requests_served\": 0,\n",
      "  \"avg_latency_ms\": 0.0,\n",
      "  \"timestamp\": \"2026-01-24T02:15:17.482575\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "üìã MODEL INFO\n",
      "============================================================\n",
      "{\n",
      "  \"model_name\": \"trading_signal_classifier\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"model_type\": \"Pipeline\",\n",
      "  \"created_at\": \"2026-01-24T02:15:17.161911\",\n",
      "  \"feature_names\": [\n",
      "    \"returns\",\n",
      "    \"volatility_20\",\n",
      "    \"momentum_10\",\n",
      "    \"momentum_20\",\n",
      "    \"rsi\",\n",
      "    \"volume_ratio\",\n",
      "    \"ma_crossover\",\n",
      "    \"spread\",\n",
      "    \"hour\",\n",
      "    \"day_of_week\"\n",
      "  ],\n",
      "  \"required_features\": 10,\n",
      "  \"training_metrics\": {\n",
      "    \"accuracy\": 0.504,\n",
      "    \"precision\": 0.468,\n",
      "    \"recall\": 0.24375,\n",
      "    \"f1_score\": 0.32054794520547947\n",
      "  },\n",
      "  \"description\": \"Random Forest classifier for trading signal prediction\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test the API endpoints\n",
    "\n",
    "# 1. Health Check\n",
    "print(\"=\" * 60)\n",
    "print(\"üè• HEALTH CHECK\")\n",
    "print(\"=\" * 60)\n",
    "health = api.health_check()\n",
    "print(json.dumps(health, indent=2))\n",
    "\n",
    "# 2. Model Info\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã MODEL INFO\")\n",
    "print(\"=\" * 60)\n",
    "info = api.model_info()\n",
    "print(json.dumps(info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d565c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ SINGLE PREDICTION\n",
      "============================================================\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"prediction\": 0,\n",
      "  \"signal\": \"HOLD\",\n",
      "  \"confidence\": 0.5709211661330337,\n",
      "  \"probabilities\": {\n",
      "    \"class_0\": 0.5709211661330337,\n",
      "    \"class_1\": 0.4290788338669664\n",
      "  },\n",
      "  \"latency_ms\": 29.442,\n",
      "  \"model_version\": \"1.0.0\",\n",
      "  \"timestamp\": \"2026-01-24T02:15:17.619410\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "üì¶ BATCH PREDICTION\n",
      "============================================================\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"batch_size\": 3,\n",
      "  \"predictions\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"prediction\": 0,\n",
      "      \"signal\": \"HOLD\",\n",
      "      \"confidence\": 0.5709211661330337\n",
      "    },\n",
      "    {\n",
      "      \"index\": 1,\n",
      "      \"prediction\": 0,\n",
      "      \"signal\": \"HOLD\",\n",
      "      \"confidence\": 0.5780127103813741\n",
      "    },\n",
      "    {\n",
      "      \"index\": 2,\n",
      "      \"prediction\": 0,\n",
      "      \"signal\": \"HOLD\",\n",
      "      \"confidence\": 0.507961695494945\n",
      "    }\n",
      "  ],\n",
      "  \"total_latency_ms\": 27.566,\n",
      "  \"avg_latency_per_sample_ms\": 9.189,\n",
      "  \"model_version\": \"1.0.0\",\n",
      "  \"timestamp\": \"2026-01-24T02:15:17.647279\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 3. Single Prediction\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ SINGLE PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_features = {\n",
    "    'returns': 0.005,\n",
    "    'volatility_20': 0.02,\n",
    "    'momentum_10': 0.03,\n",
    "    'momentum_20': 0.05,\n",
    "    'rsi': 65.0,\n",
    "    'volume_ratio': 1.2,\n",
    "    'ma_crossover': 1,\n",
    "    'spread': 0.0005,\n",
    "    'hour': 10,\n",
    "    'day_of_week': 2\n",
    "}\n",
    "\n",
    "prediction = api.predict(sample_features)\n",
    "print(json.dumps(prediction, indent=2))\n",
    "\n",
    "# 4. Batch Prediction\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üì¶ BATCH PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_features = [\n",
    "    {'returns': 0.005, 'volatility_20': 0.02, 'momentum_10': 0.03, 'momentum_20': 0.05,\n",
    "     'rsi': 65.0, 'volume_ratio': 1.2, 'ma_crossover': 1, 'spread': 0.0005, 'hour': 10, 'day_of_week': 2},\n",
    "    {'returns': -0.01, 'volatility_20': 0.03, 'momentum_10': -0.02, 'momentum_20': -0.03,\n",
    "     'rsi': 35.0, 'volume_ratio': 0.8, 'ma_crossover': 0, 'spread': 0.001, 'hour': 14, 'day_of_week': 4},\n",
    "    {'returns': 0.002, 'volatility_20': 0.015, 'momentum_10': 0.01, 'momentum_20': 0.02,\n",
    "     'rsi': 55.0, 'volume_ratio': 1.0, 'ma_crossover': 1, 'spread': 0.0003, 'hour': 11, 'day_of_week': 1},\n",
    "]\n",
    "\n",
    "batch_result = api.predict_batch(batch_features)\n",
    "print(json.dumps(batch_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87906ee",
   "metadata": {},
   "source": [
    "### Flask API Template\n",
    "\n",
    "Here's a complete Flask application template for production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18e965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Flask API Template\n",
      "============================================================\n",
      "\n",
      "\"\"\"\n",
      "Trading Model API - Flask Application\n",
      "\n",
      "Usage:\n",
      "    pip install flask gunicorn\n",
      "    gunicorn -w 4 -b 0.0.0.0:5000 app:app\n",
      "\"\"\"\n",
      "\n",
      "from flask import Flask, request, jsonify\n",
      "import joblib\n",
      "import json\n",
      "import numpy as np\n",
      "import time\n",
      "from datetime import datetime\n",
      "import logging\n",
      "\n",
      "# Configure logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# Load model and metadata at startup\n",
      "MODEL = None\n",
      "METADATA = None\n",
      "REQUEST_COUNT = 0\n",
      "STARTUP_TIME = datetime.now()\n",
      "\n",
      "def load_model():\n",
      "    \"\"\"Load model and metadata on startup.\"\"\"\n",
      "    global MODEL, METADATA\n",
      "    MODEL = joblib.load('./deployed_models/trading_signal_classifier/1.0.0/trading_signal_classifier_1.0.0.joblib')\n",
      "    with open('./deployed_models/trading_signal_classifier/1.0.0/metadata.json', 'r') as f:\n",
      "        METADATA = json.load(f)\n",
      "    logger.info(f\"Model loaded: {METADATA['model_name']} v{METADATA['version']}\")\n",
      "\n",
      "@app.route('/health', methods=['GET'])\n",
      "def health():\n",
      "    \"\"\"Health check endpoint.\"\"\"\n",
      "    uptime = (datetime.now() - STARTUP_TIME).total_seconds()\n",
      "    return jsonify({\n",
      "        'status': 'healthy',\n",
      "        'model_name': METADATA['model_name'],\n",
      "        'model_version': METADATA['version'],\n",
      "        'uptime_seconds': round(uptime, 2),\n",
      "        'requests_served': REQUEST_COUNT,\n",
      "        'timestamp': datetime.now().isoformat()\n",
      "    })\n",
      "\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "    \"\"\"Single prediction endpoint.\"\"\"\n",
      "    global REQUEST_COUNT\n",
      "    start_time = time.perf_counter()\n",
      "\n",
      "    try:\n",
      "        features = request.json\n",
      "\n",
      "        # Validate features\n",
      "        feature_names = METADATA['feature_names']\n",
      "        for name in feature_names:\n",
      "            if name not in features:\n",
      "                return jsonify({'status': 'error', 'error': f'Missing feature: {name}'}), 400\n",
      "\n",
      "        # Prepare feature array\n",
      "        feature_array = np.array([[features[name] for name in feature_names]])\n",
      "\n",
      "        # Predict\n",
      "        prediction = MODEL.predict(feature_array)[0]\n",
      "        probabilities = MODEL.predict_proba(feature_array)[0]\n",
      "\n",
      "        REQUEST_COUNT += 1\n",
      "        latency = time.perf_counter() - start_time\n",
      "\n",
      "        return jsonify({\n",
      "            'status': 'success',\n",
      "            'prediction': int(prediction),\n",
      "            'signal': 'BUY' if prediction == 1 else 'HOLD',\n",
      "            'confidence': float(max(probabilities)),\n",
      "            'latency_ms': round(latency * 1000, 3),\n",
      "            'timestamp': datetime.now().isoformat()\n",
      "        })\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Prediction error: {str(e)}\")\n",
      "        return jsonify({'status': 'error', 'error': str(e)}), 500\n",
      "\n",
      "@app.route('/predict_batch', methods=['POST'])\n",
      "def predict_batch():\n",
      "    \"\"\"Batch prediction endpoint.\"\"\"\n",
      "    global REQUEST_COUNT\n",
      "    start_time = time.perf_counter()\n",
      "\n",
      "    try:\n",
      "        features_list = request.json\n",
      "\n",
      "        if len(features_list) > 1000:\n",
      "            return jsonify({'status': 'error', 'error': 'Batch size exceeds limit'}), 400\n",
      "\n",
      "        feature_names = METADATA['feature_names']\n",
      "        feature_array = np.array([\n",
      "            [f[name] for name in feature_names] for f in features_list\n",
      "        ])\n",
      "\n",
      "        predictions = MODEL.predict(feature_array)\n",
      "        probabilities = MODEL.predict_proba(feature_array)\n",
      "\n",
      "        results = [\n",
      "            {'index': i, 'prediction': int(p), 'signal': 'BUY' if p == 1 else 'HOLD',\n",
      "             'confidence': float(max(proba))}\n",
      "            for i, (p, proba) in enumerate(zip(predictions, probabilities))\n",
      "        ]\n",
      "\n",
      "        REQUEST_COUNT += len(features_list)\n",
      "        latency = time.perf_counter() - start_time\n",
      "\n",
      "        return jsonify({\n",
      "            'status': 'success',\n",
      "            'batch_size': len(features_list),\n",
      "            'predictions': results,\n",
      "            'total_latency_ms': round(latency * 1000, 3),\n",
      "            'timestamp': datetime.now().isoformat()\n",
      "        })\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Batch prediction error: {str(e)}\")\n",
      "        return jsonify({'status': 'error', 'error': str(e)}), 500\n",
      "\n",
      "@app.route('/model_info', methods=['GET'])\n",
      "def model_info():\n",
      "    \"\"\"Model information endpoint.\"\"\"\n",
      "    return jsonify({\n",
      "        'model_name': METADATA['model_name'],\n",
      "        'version': METADATA['version'],\n",
      "        'model_type': METADATA['model_type'],\n",
      "        'feature_names': METADATA['feature_names'],\n",
      "        'training_metrics': METADATA['training_metrics']\n",
      "    })\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    load_model()\n",
      "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flask API Template (save as app.py for production deployment)\n",
    "\n",
    "flask_app_template = '''\n",
    "\"\"\"\n",
    "Trading Model API - Flask Application\n",
    "\n",
    "Usage:\n",
    "    pip install flask gunicorn\n",
    "    gunicorn -w 4 -b 0.0.0.0:5000 app:app\n",
    "\"\"\"\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model and metadata at startup\n",
    "MODEL = None\n",
    "METADATA = None\n",
    "REQUEST_COUNT = 0\n",
    "STARTUP_TIME = datetime.now()\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model and metadata on startup.\"\"\"\n",
    "    global MODEL, METADATA\n",
    "    MODEL = joblib.load('./deployed_models/trading_signal_classifier/1.0.0/trading_signal_classifier_1.0.0.joblib')\n",
    "    with open('./deployed_models/trading_signal_classifier/1.0.0/metadata.json', 'r') as f:\n",
    "        METADATA = json.load(f)\n",
    "    logger.info(f\"Model loaded: {METADATA['model_name']} v{METADATA['version']}\")\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    uptime = (datetime.now() - STARTUP_TIME).total_seconds()\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model_name': METADATA['model_name'],\n",
    "        'model_version': METADATA['version'],\n",
    "        'uptime_seconds': round(uptime, 2),\n",
    "        'requests_served': REQUEST_COUNT,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    })\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Single prediction endpoint.\"\"\"\n",
    "    global REQUEST_COUNT\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    try:\n",
    "        features = request.json\n",
    "        \n",
    "        # Validate features\n",
    "        feature_names = METADATA['feature_names']\n",
    "        for name in feature_names:\n",
    "            if name not in features:\n",
    "                return jsonify({'status': 'error', 'error': f'Missing feature: {name}'}), 400\n",
    "        \n",
    "        # Prepare feature array\n",
    "        feature_array = np.array([[features[name] for name in feature_names]])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = MODEL.predict(feature_array)[0]\n",
    "        probabilities = MODEL.predict_proba(feature_array)[0]\n",
    "        \n",
    "        REQUEST_COUNT += 1\n",
    "        latency = time.perf_counter() - start_time\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'prediction': int(prediction),\n",
    "            'signal': 'BUY' if prediction == 1 else 'HOLD',\n",
    "            'confidence': float(max(probabilities)),\n",
    "            'latency_ms': round(latency * 1000, 3),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        return jsonify({'status': 'error', 'error': str(e)}), 500\n",
    "\n",
    "@app.route('/predict_batch', methods=['POST'])\n",
    "def predict_batch():\n",
    "    \"\"\"Batch prediction endpoint.\"\"\"\n",
    "    global REQUEST_COUNT\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    try:\n",
    "        features_list = request.json\n",
    "        \n",
    "        if len(features_list) > 1000:\n",
    "            return jsonify({'status': 'error', 'error': 'Batch size exceeds limit'}), 400\n",
    "        \n",
    "        feature_names = METADATA['feature_names']\n",
    "        feature_array = np.array([\n",
    "            [f[name] for name in feature_names] for f in features_list\n",
    "        ])\n",
    "        \n",
    "        predictions = MODEL.predict(feature_array)\n",
    "        probabilities = MODEL.predict_proba(feature_array)\n",
    "        \n",
    "        results = [\n",
    "            {'index': i, 'prediction': int(p), 'signal': 'BUY' if p == 1 else 'HOLD',\n",
    "             'confidence': float(max(proba))}\n",
    "            for i, (p, proba) in enumerate(zip(predictions, probabilities))\n",
    "        ]\n",
    "        \n",
    "        REQUEST_COUNT += len(features_list)\n",
    "        latency = time.perf_counter() - start_time\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'batch_size': len(features_list),\n",
    "            'predictions': results,\n",
    "            'total_latency_ms': round(latency * 1000, 3),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Batch prediction error: {str(e)}\")\n",
    "        return jsonify({'status': 'error', 'error': str(e)}), 500\n",
    "\n",
    "@app.route('/model_info', methods=['GET'])\n",
    "def model_info():\n",
    "    \"\"\"Model information endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        'model_name': METADATA['model_name'],\n",
    "        'version': METADATA['version'],\n",
    "        'model_type': METADATA['model_type'],\n",
    "        'feature_names': METADATA['feature_names'],\n",
    "        'training_metrics': METADATA['training_metrics']\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_model()\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "'''\n",
    "\n",
    "print(\"üìù Flask API Template\")\n",
    "print(\"=\" * 60)\n",
    "print(flask_app_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c156e254",
   "metadata": {},
   "source": [
    "## 5. Implement Real-time Data Pipeline\n",
    "\n",
    "Real-time prediction pipelines are critical for trading systems. This section implements a low-latency pipeline for continuous market data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df7d968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:17,822 - TradingModelDeployment - INFO - RealTimePipeline initialized with window_size=100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Real-time Pipeline initialized\n"
     ]
    }
   ],
   "source": [
    "class RealTimePredictionPipeline:\n",
    "    \"\"\"\n",
    "    Real-time prediction pipeline for trading models.\n",
    "    \n",
    "    Features:\n",
    "    - Sliding window feature computation\n",
    "    - Low-latency predictions\n",
    "    - Automatic feature engineering\n",
    "    - Signal generation with confidence thresholds\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        feature_names: List[str],\n",
    "        window_size: int = 50,\n",
    "        confidence_threshold: float = 0.6\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.window_size = window_size\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Data buffers using deque for efficient sliding window\n",
    "        self.price_buffer = deque(maxlen=window_size)\n",
    "        self.volume_buffer = deque(maxlen=window_size)\n",
    "        self.timestamp_buffer = deque(maxlen=window_size)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.prediction_latencies = deque(maxlen=1000)\n",
    "        self.signals_generated = 0\n",
    "        \n",
    "        logger.info(f\"RealTimePipeline initialized with window_size={window_size}\")\n",
    "    \n",
    "    def compute_features(self, current_data: Dict) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Compute features from current market data and historical buffer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        current_data : dict with 'price', 'volume', 'timestamp', 'spread'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict of computed features or None if insufficient data\n",
    "        \"\"\"\n",
    "        # Add to buffers\n",
    "        self.price_buffer.append(current_data['price'])\n",
    "        self.volume_buffer.append(current_data['volume'])\n",
    "        self.timestamp_buffer.append(current_data['timestamp'])\n",
    "        \n",
    "        # Need enough history for feature computation\n",
    "        if len(self.price_buffer) < 20:\n",
    "            return None\n",
    "        \n",
    "        prices = np.array(self.price_buffer)\n",
    "        volumes = np.array(self.volume_buffer)\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = np.diff(prices) / prices[:-1]\n",
    "        current_return = returns[-1] if len(returns) > 0 else 0\n",
    "        \n",
    "        # Features\n",
    "        features = {\n",
    "            'returns': current_return,\n",
    "            'volatility_20': np.std(returns[-20:]) if len(returns) >= 20 else np.std(returns),\n",
    "            'momentum_10': (prices[-1] / prices[-10] - 1) if len(prices) >= 10 else 0,\n",
    "            'momentum_20': (prices[-1] / prices[-20] - 1) if len(prices) >= 20 else 0,\n",
    "            'rsi': self._compute_rsi(returns[-14:]) if len(returns) >= 14 else 50,\n",
    "            'volume_ratio': volumes[-1] / np.mean(volumes[-20:]) if len(volumes) >= 20 else 1.0,\n",
    "            'ma_crossover': 1 if np.mean(prices[-20:]) > np.mean(prices[-50:]) else 0 if len(prices) >= 50 else 0,\n",
    "            'spread': current_data.get('spread', 0.0005),\n",
    "            'hour': current_data['timestamp'].hour,\n",
    "            'day_of_week': current_data['timestamp'].weekday()\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _compute_rsi(self, returns: np.ndarray, period: int = 14) -> float:\n",
    "        \"\"\"Compute Relative Strength Index.\"\"\"\n",
    "        gains = np.maximum(returns, 0)\n",
    "        losses = np.abs(np.minimum(returns, 0))\n",
    "        \n",
    "        avg_gain = np.mean(gains)\n",
    "        avg_loss = np.mean(losses)\n",
    "        \n",
    "        if avg_loss == 0:\n",
    "            return 100.0\n",
    "        \n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        return float(rsi)\n",
    "    \n",
    "    def predict(self, current_data: Dict) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Generate prediction from current market data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        current_data : dict with market data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dict with signal or None if insufficient data\n",
    "        \"\"\"\n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "        # Compute features\n",
    "        features = self.compute_features(current_data)\n",
    "        \n",
    "        if features is None:\n",
    "            return {\n",
    "                'status': 'warming_up',\n",
    "                'buffer_size': len(self.price_buffer),\n",
    "                'required': 20\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Prepare feature array\n",
    "            feature_array = np.array([\n",
    "                [features[name] for name in self.feature_names]\n",
    "            ])\n",
    "            \n",
    "            # Get prediction and probability\n",
    "            prediction = self.model.predict(feature_array)[0]\n",
    "            probabilities = self.model.predict_proba(feature_array)[0]\n",
    "            confidence = float(max(probabilities))\n",
    "            \n",
    "            # Generate signal based on confidence threshold\n",
    "            if confidence >= self.confidence_threshold:\n",
    "                signal = \"BUY\" if prediction == 1 else \"SELL\"\n",
    "                self.signals_generated += 1\n",
    "            else:\n",
    "                signal = \"HOLD\"\n",
    "            \n",
    "            latency = time.perf_counter() - start_time\n",
    "            self.prediction_latencies.append(latency)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'timestamp': current_data['timestamp'].isoformat(),\n",
    "                'price': current_data['price'],\n",
    "                'prediction': int(prediction),\n",
    "                'signal': signal,\n",
    "                'confidence': round(confidence, 4),\n",
    "                'features': {k: round(v, 6) if isinstance(v, float) else v \n",
    "                           for k, v in features.items()},\n",
    "                'latency_us': round(latency * 1_000_000, 2)  # microseconds\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction error: {str(e)}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get pipeline statistics.\"\"\"\n",
    "        latencies = list(self.prediction_latencies)\n",
    "        \n",
    "        if not latencies:\n",
    "            return {'status': 'no_predictions'}\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': len(latencies),\n",
    "            'signals_generated': self.signals_generated,\n",
    "            'avg_latency_us': round(np.mean(latencies) * 1_000_000, 2),\n",
    "            'p50_latency_us': round(np.percentile(latencies, 50) * 1_000_000, 2),\n",
    "            'p95_latency_us': round(np.percentile(latencies, 95) * 1_000_000, 2),\n",
    "            'p99_latency_us': round(np.percentile(latencies, 99) * 1_000_000, 2),\n",
    "            'buffer_size': len(self.price_buffer)\n",
    "        }\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = RealTimePredictionPipeline(\n",
    "    model=loaded_model,\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    window_size=100,\n",
    "    confidence_threshold=0.6\n",
    ")\n",
    "print(\"‚úÖ Real-time Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ce52265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì° REAL-TIME PIPELINE SIMULATION\n",
      "============================================================\n",
      "üö® Signal: SELL at 99.75 (conf: 62.35%) - Latency: 28306¬µs\n",
      "\n",
      "============================================================\n",
      "üìä PIPELINE STATISTICS\n",
      "============================================================\n",
      "{\n",
      "  \"total_predictions\": 81,\n",
      "  \"signals_generated\": 1,\n",
      "  \"avg_latency_us\": 28536.03,\n",
      "  \"p50_latency_us\": 27986.83,\n",
      "  \"p95_latency_us\": 39488.75,\n",
      "  \"p99_latency_us\": 43854.83,\n",
      "  \"buffer_size\": 100\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Simulate real-time data stream\n",
    "def simulate_market_tick() -> Dict:\n",
    "    \"\"\"Simulate a single market data tick.\"\"\"\n",
    "    base_price = 100 + np.random.randn() * 2\n",
    "    return {\n",
    "        'price': base_price + np.random.randn() * 0.5,\n",
    "        'volume': np.random.exponential(1000000),\n",
    "        'spread': np.random.exponential(0.0005),\n",
    "        'timestamp': datetime.now()\n",
    "    }\n",
    "\n",
    "# Run simulation\n",
    "print(\"=\" * 60)\n",
    "print(\"üì° REAL-TIME PIPELINE SIMULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    tick = simulate_market_tick()\n",
    "    result = pipeline.predict(tick)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print significant signals\n",
    "    if result.get('signal') in ['BUY', 'SELL']:\n",
    "        print(f\"üö® Signal: {result['signal']} at {result['price']:.2f} \"\n",
    "              f\"(conf: {result['confidence']:.2%}) - Latency: {result['latency_us']:.0f}¬µs\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä PIPELINE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "stats = pipeline.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e295d51",
   "metadata": {},
   "source": [
    "## 6. Create Batch Prediction System\n",
    "\n",
    "Batch prediction systems are used for end-of-day processing, backtesting, and bulk signal generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7622f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:20,561 - TradingModelDeployment - INFO - BatchPredictionSystem initialized with chunk_size=5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch Prediction System initialized\n"
     ]
    }
   ],
   "source": [
    "class BatchPredictionSystem:\n",
    "    \"\"\"\n",
    "    Efficient batch prediction system for trading models.\n",
    "    \n",
    "    Features:\n",
    "    - Chunked processing for memory efficiency\n",
    "    - Progress tracking\n",
    "    - Parallel processing support\n",
    "    - Result aggregation and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        feature_names: List[str],\n",
    "        chunk_size: int = 10000\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        logger.info(f\"BatchPredictionSystem initialized with chunk_size={chunk_size}\")\n",
    "    \n",
    "    def predict_dataframe(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        output_file: Optional[str] = None,\n",
    "        include_probabilities: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate predictions for entire DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame with feature columns\n",
    "        output_file : optional path to save results\n",
    "        include_probabilities : whether to include probability scores\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with predictions added\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        n_samples = len(df)\n",
    "        n_chunks = (n_samples + self.chunk_size - 1) // self.chunk_size\n",
    "        \n",
    "        logger.info(f\"Processing {n_samples} samples in {n_chunks} chunks\")\n",
    "        \n",
    "        # Validate features\n",
    "        missing_features = set(self.feature_names) - set(df.columns)\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features: {missing_features}\")\n",
    "        \n",
    "        # Initialize result arrays\n",
    "        all_predictions = []\n",
    "        all_probabilities_0 = []\n",
    "        all_probabilities_1 = []\n",
    "        \n",
    "        # Process in chunks\n",
    "        for i, chunk_start in enumerate(range(0, n_samples, self.chunk_size)):\n",
    "            chunk_end = min(chunk_start + self.chunk_size, n_samples)\n",
    "            chunk_df = df.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            # Extract features\n",
    "            X_chunk = chunk_df[self.feature_names].values\n",
    "            \n",
    "            # Predict\n",
    "            predictions = self.model.predict(X_chunk)\n",
    "            all_predictions.extend(predictions)\n",
    "            \n",
    "            if include_probabilities and hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(X_chunk)\n",
    "                all_probabilities_0.extend(probabilities[:, 0])\n",
    "                all_probabilities_1.extend(probabilities[:, 1])\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (i + 1) / n_chunks * 100\n",
    "            if (i + 1) % max(1, n_chunks // 10) == 0:\n",
    "                logger.info(f\"Progress: {progress:.1f}% ({chunk_end}/{n_samples})\")\n",
    "        \n",
    "        # Create result DataFrame\n",
    "        result_df = df.copy()\n",
    "        result_df['prediction'] = all_predictions\n",
    "        result_df['signal'] = result_df['prediction'].map({1: 'BUY', 0: 'HOLD'})\n",
    "        \n",
    "        if include_probabilities and all_probabilities_0:\n",
    "            result_df['prob_class_0'] = all_probabilities_0\n",
    "            result_df['prob_class_1'] = all_probabilities_1\n",
    "            result_df['confidence'] = result_df[['prob_class_0', 'prob_class_1']].max(axis=1)\n",
    "        \n",
    "        # Calculate processing time\n",
    "        elapsed = time.time() - start_time\n",
    "        throughput = n_samples / elapsed\n",
    "        \n",
    "        logger.info(f\"Batch prediction complete: {n_samples} samples in {elapsed:.2f}s \"\n",
    "                   f\"({throughput:.0f} samples/sec)\")\n",
    "        \n",
    "        # Save if output file specified\n",
    "        if output_file:\n",
    "            result_df.to_parquet(output_file, index=False)\n",
    "            logger.info(f\"Results saved to {output_file}\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def generate_signal_report(self, result_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Generate summary report of batch predictions.\"\"\"\n",
    "        total = len(result_df)\n",
    "        buy_signals = (result_df['signal'] == 'BUY').sum()\n",
    "        hold_signals = (result_df['signal'] == 'HOLD').sum()\n",
    "        \n",
    "        report = {\n",
    "            'total_samples': total,\n",
    "            'buy_signals': int(buy_signals),\n",
    "            'hold_signals': int(hold_signals),\n",
    "            'buy_percentage': round(buy_signals / total * 100, 2),\n",
    "            'prediction_distribution': result_df['prediction'].value_counts().to_dict()\n",
    "        }\n",
    "        \n",
    "        if 'confidence' in result_df.columns:\n",
    "            report['confidence_stats'] = {\n",
    "                'mean': round(result_df['confidence'].mean(), 4),\n",
    "                'std': round(result_df['confidence'].std(), 4),\n",
    "                'min': round(result_df['confidence'].min(), 4),\n",
    "                'max': round(result_df['confidence'].max(), 4),\n",
    "                'high_confidence_count': int((result_df['confidence'] >= 0.7).sum())\n",
    "            }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Initialize batch system\n",
    "batch_system = BatchPredictionSystem(\n",
    "    model=loaded_model,\n",
    "    feature_names=FEATURE_COLUMNS,\n",
    "    chunk_size=5000\n",
    ")\n",
    "print(\"‚úÖ Batch Prediction System initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc2fffde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-24 02:15:20,580 - TradingModelDeployment - INFO - Processing 2000 samples in 1 chunks\n",
      "2026-01-24 02:15:20,623 - TradingModelDeployment - INFO - Progress: 100.0% (2000/2000)\n",
      "2026-01-24 02:15:20,627 - TradingModelDeployment - INFO - Batch prediction complete: 2000 samples in 0.05s (43168 samples/sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üì¶ BATCH PREDICTION DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "üìä SIGNAL REPORT\n",
      "============================================================\n",
      "{\n",
      "  \"total_samples\": 2000,\n",
      "  \"buy_signals\": 500,\n",
      "  \"hold_signals\": 1500,\n",
      "  \"buy_percentage\": 25.0,\n",
      "  \"prediction_distribution\": {\n",
      "    \"0\": 1500,\n",
      "    \"1\": 500\n",
      "  },\n",
      "  \"confidence_stats\": {\n",
      "    \"mean\": 0.5317,\n",
      "    \"std\": 0.026,\n",
      "    \"min\": 0.5,\n",
      "    \"max\": 0.6923,\n",
      "    \"high_confidence_count\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "üìã Sample Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>returns</th>\n",
       "      <th>rsi</th>\n",
       "      <th>prediction</th>\n",
       "      <th>signal</th>\n",
       "      <th>confidence</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052758</td>\n",
       "      <td>64.505788</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.592586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.020741</td>\n",
       "      <td>43.865324</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.510596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.006986</td>\n",
       "      <td>47.907503</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.518321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000032</td>\n",
       "      <td>50.009714</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.505610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021487</td>\n",
       "      <td>56.348597</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.538585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.018518</td>\n",
       "      <td>44.507301</td>\n",
       "      <td>1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>0.512731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004712</td>\n",
       "      <td>51.412642</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.526660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002989</td>\n",
       "      <td>50.896470</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.525755</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.013664</td>\n",
       "      <td>54.073951</td>\n",
       "      <td>0</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>0.576430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001111</td>\n",
       "      <td>49.666728</td>\n",
       "      <td>1</td>\n",
       "      <td>BUY</td>\n",
       "      <td>0.506601</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    returns        rsi  prediction signal  confidence  actual\n",
       "0  0.052758  64.505788           0   HOLD    0.592586       1\n",
       "1 -0.020741  43.865324           0   HOLD    0.510596       1\n",
       "2 -0.006986  47.907503           0   HOLD    0.518321       0\n",
       "3  0.000032  50.009714           0   HOLD    0.505610       1\n",
       "4  0.021487  56.348597           0   HOLD    0.538585       1\n",
       "5 -0.018518  44.507301           1    BUY    0.512731       0\n",
       "6  0.004712  51.412642           0   HOLD    0.526660       1\n",
       "7  0.002989  50.896470           0   HOLD    0.525755       1\n",
       "8  0.013664  54.073951           0   HOLD    0.576430       1\n",
       "9 -0.001111  49.666728           1    BUY    0.506601       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch prediction on test data\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ BATCH PREDICTION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create test DataFrame\n",
    "test_df = pd.DataFrame(X_test, columns=FEATURE_COLUMNS)\n",
    "test_df['actual'] = y_test\n",
    "\n",
    "# Run batch prediction\n",
    "results_df = batch_system.predict_dataframe(test_df, include_probabilities=True)\n",
    "\n",
    "# Generate report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä SIGNAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "report = batch_system.generate_signal_report(results_df)\n",
    "print(json.dumps(report, indent=2))\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nüìã Sample Results:\")\n",
    "results_df[['returns', 'rsi', 'prediction', 'signal', 'confidence', 'actual']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa548faf",
   "metadata": {},
   "source": [
    "## 7. Build Model Monitoring and Logging\n",
    "\n",
    "Comprehensive monitoring is essential for detecting model drift, performance degradation, and system issues in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
