{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c6796f",
   "metadata": {},
   "source": [
    "# Day 01: MLOps Fundamentals for Trading Systems\n",
    "\n",
    "## Week 23 - Production ML\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand MLOps principles and their application in quantitative trading\n",
    "- Build automated ML pipelines for trading models\n",
    "- Implement model versioning, tracking, and registry with MLflow\n",
    "- Create feature stores for consistent feature engineering\n",
    "- Set up monitoring for data drift and model performance degradation\n",
    "- Design automated retraining triggers based on performance metrics\n",
    "\n",
    "**Key Concepts:**\n",
    "- ML Lifecycle Management\n",
    "- Experiment Tracking & Model Registry\n",
    "- Feature Store Architecture\n",
    "- Data & Model Monitoring\n",
    "- CI/CD for ML Systems\n",
    "- Automated Retraining Pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf3f161",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Essential libraries for MLOps workflows including experiment tracking, pipeline orchestration, and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1358ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# Serialization and utilities\n",
    "import joblib\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "\n",
    "# Statistical tests for drift detection\n",
    "from scipy import stats\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766eb35d",
   "metadata": {},
   "source": [
    "## 2. MLOps Architecture Overview\n",
    "\n",
    "**MLOps** bridges the gap between ML development and production deployment. In trading systems, this is critical because:\n",
    "\n",
    "1. **Model Staleness**: Markets evolve - models trained on historical data degrade\n",
    "2. **Data Quality**: Real-time data can have quality issues not seen in backtests\n",
    "3. **Regulatory Compliance**: Need audit trails for model decisions\n",
    "4. **Risk Management**: Poor model performance can lead to significant losses\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MLOps Architecture for Trading                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚  Data    â”‚â”€â”€â–¶â”‚ Feature  â”‚â”€â”€â–¶â”‚  Model   â”‚â”€â”€â–¶â”‚  Model   â”‚         â”‚\n",
    "â”‚  â”‚  Source  â”‚   â”‚  Store   â”‚   â”‚ Training â”‚   â”‚ Registry â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚       â”‚              â”‚              â”‚              â”‚                 â”‚\n",
    "â”‚       â–¼              â–¼              â–¼              â–¼                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚              Experiment Tracking (MLflow)             â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                              â”‚                                       â”‚\n",
    "â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚       â–¼                      â–¼                      â–¼              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
    "â”‚  â”‚  Model   â”‚          â”‚ Monitoringâ”‚          â”‚ Retrainingâ”‚         â”‚\n",
    "â”‚  â”‚ Serving  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  & Drift  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Trigger  â”‚         â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚       â”‚                      â–²                                       â”‚\n",
    "â”‚       â–¼                      â”‚                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚  â”‚ Trading  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Prediction â”‚                                â”‚\n",
    "â”‚  â”‚ Signals  â”‚          â”‚  Logs     â”‚                                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70326e59",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Trading Data\n",
    "\n",
    "Create realistic trading data for demonstrating MLOps concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_data(\n",
    "    n_samples: int = 5000,\n",
    "    start_date: str = '2020-01-01',\n",
    "    seed: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate synthetic trading data with technical features.\n",
    "    \n",
    "    Features include price-based features, volume, volatility,\n",
    "    and momentum indicators commonly used in trading models.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate date index\n",
    "    dates = pd.date_range(start=start_date, periods=n_samples, freq='H')\n",
    "    \n",
    "    # Base price simulation (geometric brownian motion)\n",
    "    returns = np.random.normal(0.0001, 0.02, n_samples)\n",
    "    price = 100 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    # Generate OHLCV data\n",
    "    high = price * (1 + np.abs(np.random.normal(0, 0.01, n_samples)))\n",
    "    low = price * (1 - np.abs(np.random.normal(0, 0.01, n_samples)))\n",
    "    open_price = price + np.random.normal(0, 0.5, n_samples)\n",
    "    volume = np.random.lognormal(15, 1, n_samples)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': dates,\n",
    "        'open': open_price,\n",
    "        'high': high,\n",
    "        'low': low,\n",
    "        'close': price,\n",
    "        'volume': volume\n",
    "    })\n",
    "    \n",
    "    # Technical indicators\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    \n",
    "    # Moving averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df[f'sma_{window}'] = df['close'].rolling(window).mean()\n",
    "        df[f'ema_{window}'] = df['close'].ewm(span=window).mean()\n",
    "    \n",
    "    # Volatility features\n",
    "    df['volatility_20'] = df['returns'].rolling(20).std()\n",
    "    df['volatility_50'] = df['returns'].rolling(50).std()\n",
    "    \n",
    "    # Momentum features\n",
    "    df['momentum_5'] = df['close'] / df['close'].shift(5) - 1\n",
    "    df['momentum_10'] = df['close'] / df['close'].shift(10) - 1\n",
    "    df['momentum_20'] = df['close'] / df['close'].shift(20) - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df['close'].ewm(span=12).mean()\n",
    "    exp2 = df['close'].ewm(span=26).mean()\n",
    "    df['macd'] = exp1 - exp2\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df['bb_middle'] = df['close'].rolling(20).mean()\n",
    "    df['bb_std'] = df['close'].rolling(20).std()\n",
    "    df['bb_upper'] = df['bb_middle'] + 2 * df['bb_std']\n",
    "    df['bb_lower'] = df['bb_middle'] - 2 * df['bb_std']\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # Volume features\n",
    "    df['volume_sma_20'] = df['volume'].rolling(20).mean()\n",
    "    df['volume_ratio'] = df['volume'] / df['volume_sma_20']\n",
    "    \n",
    "    # Target: Next period direction (1 = up, 0 = down)\n",
    "    df['target'] = (df['close'].shift(-1) > df['close']).astype(int)\n",
    "    \n",
    "    # Drop NaN values\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_trading_data(n_samples=5000)\n",
    "print(f\"âœ… Generated {len(df)} samples\")\n",
    "print(f\"ðŸ“Š Features: {df.shape[1]} columns\")\n",
    "print(f\"ðŸ“… Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "print(f\"\\nðŸŽ¯ Target distribution:\")\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4b166",
   "metadata": {},
   "source": [
    "## 4. Data Validation and Quality Checks\n",
    "\n",
    "Robust data validation is critical in production trading systems. Bad data can lead to incorrect signals and significant losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a data validation check.\"\"\"\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    message: str\n",
    "    severity: str = \"ERROR\"  # ERROR, WARNING, INFO\n",
    "    details: Dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive data validation for trading data.\n",
    "    \n",
    "    Validates:\n",
    "    - Schema (required columns, data types)\n",
    "    - Data quality (missing values, duplicates)\n",
    "    - Statistical properties (outliers, distribution)\n",
    "    - Temporal consistency (gaps, ordering)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        self.config = config or self._default_config()\n",
    "        self.validation_results: List[ValidationResult] = []\n",
    "    \n",
    "    def _default_config(self) -> Dict:\n",
    "        return {\n",
    "            'required_columns': ['timestamp', 'open', 'high', 'low', 'close', 'volume'],\n",
    "            'numeric_columns': ['open', 'high', 'low', 'close', 'volume'],\n",
    "            'max_missing_pct': 0.01,  # 1%\n",
    "            'max_outlier_zscore': 5.0,\n",
    "            'min_samples': 100,\n",
    "            'max_gap_hours': 24,\n",
    "        }\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> Tuple[bool, List[ValidationResult]]:\n",
    "        \"\"\"Run all validation checks.\"\"\"\n",
    "        self.validation_results = []\n",
    "        \n",
    "        # Schema validation\n",
    "        self._check_required_columns(df)\n",
    "        self._check_data_types(df)\n",
    "        \n",
    "        # Data quality\n",
    "        self._check_missing_values(df)\n",
    "        self._check_duplicates(df)\n",
    "        self._check_sample_size(df)\n",
    "        \n",
    "        # Statistical validation\n",
    "        self._check_price_consistency(df)\n",
    "        self._check_outliers(df)\n",
    "        self._check_value_ranges(df)\n",
    "        \n",
    "        # Temporal validation\n",
    "        self._check_timestamp_ordering(df)\n",
    "        self._check_data_gaps(df)\n",
    "        \n",
    "        # Calculate overall pass/fail\n",
    "        errors = [r for r in self.validation_results if not r.passed and r.severity == \"ERROR\"]\n",
    "        overall_passed = len(errors) == 0\n",
    "        \n",
    "        return overall_passed, self.validation_results\n",
    "    \n",
    "    def _check_required_columns(self, df: pd.DataFrame):\n",
    "        \"\"\"Check that all required columns are present.\"\"\"\n",
    "        missing = set(self.config['required_columns']) - set(df.columns)\n",
    "        passed = len(missing) == 0\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"required_columns\",\n",
    "            passed=passed,\n",
    "            message=f\"Missing columns: {missing}\" if not passed else \"All required columns present\",\n",
    "            severity=\"ERROR\",\n",
    "            details={'missing_columns': list(missing)}\n",
    "        ))\n",
    "    \n",
    "    def _check_data_types(self, df: pd.DataFrame):\n",
    "        \"\"\"Validate data types of numeric columns.\"\"\"\n",
    "        invalid_types = {}\n",
    "        for col in self.config['numeric_columns']:\n",
    "            if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):\n",
    "                invalid_types[col] = str(df[col].dtype)\n",
    "        \n",
    "        passed = len(invalid_types) == 0\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"data_types\",\n",
    "            passed=passed,\n",
    "            message=f\"Invalid types: {invalid_types}\" if not passed else \"All data types valid\",\n",
    "            severity=\"ERROR\",\n",
    "            details={'invalid_types': invalid_types}\n",
    "        ))\n",
    "    \n",
    "    def _check_missing_values(self, df: pd.DataFrame):\n",
    "        \"\"\"Check for excessive missing values.\"\"\"\n",
    "        missing_pct = df.isnull().sum() / len(df)\n",
    "        high_missing = missing_pct[missing_pct > self.config['max_missing_pct']]\n",
    "        \n",
    "        passed = len(high_missing) == 0\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"missing_values\",\n",
    "            passed=passed,\n",
    "            message=f\"Columns with >1% missing: {list(high_missing.index)}\" if not passed else \"Missing values within threshold\",\n",
    "            severity=\"WARNING\",\n",
    "            details={'missing_percentages': high_missing.to_dict()}\n",
    "        ))\n",
    "    \n",
    "    def _check_duplicates(self, df: pd.DataFrame):\n",
    "        \"\"\"Check for duplicate timestamps.\"\"\"\n",
    "        if 'timestamp' in df.columns:\n",
    "            n_duplicates = df['timestamp'].duplicated().sum()\n",
    "            passed = n_duplicates == 0\n",
    "            self.validation_results.append(ValidationResult(\n",
    "                check_name=\"duplicates\",\n",
    "                passed=passed,\n",
    "                message=f\"Found {n_duplicates} duplicate timestamps\" if not passed else \"No duplicates found\",\n",
    "                severity=\"WARNING\",\n",
    "                details={'n_duplicates': int(n_duplicates)}\n",
    "            ))\n",
    "    \n",
    "    def _check_sample_size(self, df: pd.DataFrame):\n",
    "        \"\"\"Ensure minimum sample size.\"\"\"\n",
    "        passed = len(df) >= self.config['min_samples']\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"sample_size\",\n",
    "            passed=passed,\n",
    "            message=f\"Only {len(df)} samples (min: {self.config['min_samples']})\" if not passed else f\"Sample size OK ({len(df)})\",\n",
    "            severity=\"ERROR\",\n",
    "            details={'n_samples': len(df)}\n",
    "        ))\n",
    "    \n",
    "    def _check_price_consistency(self, df: pd.DataFrame):\n",
    "        \"\"\"Check OHLC price consistency (high >= low, etc.).\"\"\"\n",
    "        if all(col in df.columns for col in ['open', 'high', 'low', 'close']):\n",
    "            violations = (\n",
    "                (df['high'] < df['low']) | \n",
    "                (df['high'] < df['open']) | \n",
    "                (df['high'] < df['close']) |\n",
    "                (df['low'] > df['open']) | \n",
    "                (df['low'] > df['close'])\n",
    "            ).sum()\n",
    "            \n",
    "            passed = violations == 0\n",
    "            self.validation_results.append(ValidationResult(\n",
    "                check_name=\"price_consistency\",\n",
    "                passed=passed,\n",
    "                message=f\"Found {violations} OHLC inconsistencies\" if not passed else \"OHLC data consistent\",\n",
    "                severity=\"ERROR\",\n",
    "                details={'n_violations': int(violations)}\n",
    "            ))\n",
    "    \n",
    "    def _check_outliers(self, df: pd.DataFrame):\n",
    "        \"\"\"Check for extreme outliers using z-score.\"\"\"\n",
    "        outlier_cols = {}\n",
    "        for col in self.config['numeric_columns']:\n",
    "            if col in df.columns:\n",
    "                z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "                n_outliers = (z_scores > self.config['max_outlier_zscore']).sum()\n",
    "                if n_outliers > 0:\n",
    "                    outlier_cols[col] = int(n_outliers)\n",
    "        \n",
    "        passed = len(outlier_cols) == 0\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"outliers\",\n",
    "            passed=passed,\n",
    "            message=f\"Outliers detected: {outlier_cols}\" if not passed else \"No extreme outliers\",\n",
    "            severity=\"WARNING\",\n",
    "            details={'outlier_counts': outlier_cols}\n",
    "        ))\n",
    "    \n",
    "    def _check_value_ranges(self, df: pd.DataFrame):\n",
    "        \"\"\"Check for non-positive prices or volumes.\"\"\"\n",
    "        issues = {}\n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            if col in df.columns:\n",
    "                n_negative = (df[col] <= 0).sum()\n",
    "                if n_negative > 0:\n",
    "                    issues[col] = int(n_negative)\n",
    "        \n",
    "        if 'volume' in df.columns:\n",
    "            n_negative = (df['volume'] < 0).sum()\n",
    "            if n_negative > 0:\n",
    "                issues['volume'] = int(n_negative)\n",
    "        \n",
    "        passed = len(issues) == 0\n",
    "        self.validation_results.append(ValidationResult(\n",
    "            check_name=\"value_ranges\",\n",
    "            passed=passed,\n",
    "            message=f\"Invalid values: {issues}\" if not passed else \"All values in valid range\",\n",
    "            severity=\"ERROR\",\n",
    "            details={'invalid_counts': issues}\n",
    "        ))\n",
    "    \n",
    "    def _check_timestamp_ordering(self, df: pd.DataFrame):\n",
    "        \"\"\"Check that timestamps are in order.\"\"\"\n",
    "        if 'timestamp' in df.columns:\n",
    "            is_sorted = df['timestamp'].is_monotonic_increasing\n",
    "            self.validation_results.append(ValidationResult(\n",
    "                check_name=\"timestamp_ordering\",\n",
    "                passed=is_sorted,\n",
    "                message=\"Timestamps not in chronological order\" if not is_sorted else \"Timestamps properly ordered\",\n",
    "                severity=\"ERROR\"\n",
    "            ))\n",
    "    \n",
    "    def _check_data_gaps(self, df: pd.DataFrame):\n",
    "        \"\"\"Check for large gaps in time series.\"\"\"\n",
    "        if 'timestamp' in df.columns:\n",
    "            gaps = df['timestamp'].diff()\n",
    "            max_gap_hours = gaps.max().total_seconds() / 3600 if len(gaps) > 0 else 0\n",
    "            \n",
    "            passed = max_gap_hours <= self.config['max_gap_hours']\n",
    "            self.validation_results.append(ValidationResult(\n",
    "                check_name=\"data_gaps\",\n",
    "                passed=passed,\n",
    "                message=f\"Max gap: {max_gap_hours:.1f} hours\" if not passed else f\"Data gaps within threshold ({max_gap_hours:.1f}h)\",\n",
    "                severity=\"WARNING\",\n",
    "                details={'max_gap_hours': max_gap_hours}\n",
    "            ))\n",
    "    \n",
    "    def summary(self) -> pd.DataFrame:\n",
    "        \"\"\"Get validation summary as DataFrame.\"\"\"\n",
    "        return pd.DataFrame([\n",
    "            {\n",
    "                'Check': r.check_name,\n",
    "                'Passed': 'âœ…' if r.passed else 'âŒ',\n",
    "                'Severity': r.severity,\n",
    "                'Message': r.message\n",
    "            }\n",
    "            for r in self.validation_results\n",
    "        ])\n",
    "\n",
    "\n",
    "# Run validation on our data\n",
    "validator = DataValidator()\n",
    "passed, results = validator.validate(df)\n",
    "\n",
    "print(f\"{'âœ… All checks passed!' if passed else 'âŒ Some checks failed!'}\\n\")\n",
    "validator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931d5de",
   "metadata": {},
   "source": [
    "## 5. Feature Store Implementation\n",
    "\n",
    "A **Feature Store** is a centralized repository for storing, managing, and serving ML features. Key benefits:\n",
    "- **Consistency**: Same features for training and inference\n",
    "- **Reusability**: Features can be shared across models\n",
    "- **Versioning**: Track feature changes over time\n",
    "- **Freshness**: Ensure features are up-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FeatureMetadata:\n",
    "    \"\"\"Metadata for a feature set.\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "    created_at: datetime\n",
    "    features: List[str]\n",
    "    description: str\n",
    "    data_hash: str\n",
    "    n_samples: int\n",
    "    statistics: Dict[str, Dict] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class FeatureStore:\n",
    "    \"\"\"\n",
    "    Simple feature store implementation for trading features.\n",
    "    \n",
    "    In production, you'd use tools like:\n",
    "    - Feast (open source)\n",
    "    - Tecton (managed)\n",
    "    - AWS Feature Store\n",
    "    - Databricks Feature Store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"./feature_store\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.metadata_file = self.storage_path / \"metadata.json\"\n",
    "        self.metadata: Dict[str, Dict] = self._load_metadata()\n",
    "    \n",
    "    def _load_metadata(self) -> Dict:\n",
    "        \"\"\"Load feature store metadata.\"\"\"\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        \"\"\"Persist metadata to disk.\"\"\"\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2, default=str)\n",
    "    \n",
    "    def _compute_hash(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Compute hash of DataFrame for versioning.\"\"\"\n",
    "        return hashlib.md5(\n",
    "            pd.util.hash_pandas_object(df).values.tobytes()\n",
    "        ).hexdigest()[:12]\n",
    "    \n",
    "    def _compute_statistics(self, df: pd.DataFrame, features: List[str]) -> Dict:\n",
    "        \"\"\"Compute feature statistics for monitoring.\"\"\"\n",
    "        stats = {}\n",
    "        for feature in features:\n",
    "            if feature in df.columns and pd.api.types.is_numeric_dtype(df[feature]):\n",
    "                stats[feature] = {\n",
    "                    'mean': float(df[feature].mean()),\n",
    "                    'std': float(df[feature].std()),\n",
    "                    'min': float(df[feature].min()),\n",
    "                    'max': float(df[feature].max()),\n",
    "                    'median': float(df[feature].median()),\n",
    "                    'q25': float(df[feature].quantile(0.25)),\n",
    "                    'q75': float(df[feature].quantile(0.75)),\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def register_feature_set(\n",
    "        self,\n",
    "        name: str,\n",
    "        df: pd.DataFrame,\n",
    "        features: List[str],\n",
    "        description: str = \"\",\n",
    "        version: Optional[str] = None\n",
    "    ) -> FeatureMetadata:\n",
    "        \"\"\"\n",
    "        Register a new feature set in the store.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the feature set\n",
    "            df: DataFrame containing features\n",
    "            features: List of feature column names\n",
    "            description: Description of the feature set\n",
    "            version: Optional version string (auto-generated if not provided)\n",
    "        \n",
    "        Returns:\n",
    "            FeatureMetadata object\n",
    "        \"\"\"\n",
    "        # Compute version from data hash if not provided\n",
    "        data_hash = self._compute_hash(df[features])\n",
    "        if version is None:\n",
    "            version = f\"v_{data_hash}\"\n",
    "        \n",
    "        # Compute statistics\n",
    "        statistics = self._compute_statistics(df, features)\n",
    "        \n",
    "        # Create metadata\n",
    "        metadata = FeatureMetadata(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            created_at=datetime.now(),\n",
    "            features=features,\n",
    "            description=description,\n",
    "            data_hash=data_hash,\n",
    "            n_samples=len(df),\n",
    "            statistics=statistics\n",
    "        )\n",
    "        \n",
    "        # Save feature data\n",
    "        feature_path = self.storage_path / name / version\n",
    "        feature_path.mkdir(parents=True, exist_ok=True)\n",
    "        df[features].to_parquet(feature_path / \"features.parquet\")\n",
    "        \n",
    "        # Update metadata\n",
    "        if name not in self.metadata:\n",
    "            self.metadata[name] = {'versions': {}}\n",
    "        \n",
    "        self.metadata[name]['versions'][version] = {\n",
    "            'created_at': metadata.created_at.isoformat(),\n",
    "            'features': features,\n",
    "            'description': description,\n",
    "            'data_hash': data_hash,\n",
    "            'n_samples': len(df),\n",
    "            'statistics': statistics\n",
    "        }\n",
    "        self.metadata[name]['latest'] = version\n",
    "        self._save_metadata()\n",
    "        \n",
    "        logger.info(f\"Registered feature set '{name}' version '{version}'\")\n",
    "        return metadata\n",
    "    \n",
    "    def get_feature_set(\n",
    "        self,\n",
    "        name: str,\n",
    "        version: Optional[str] = None\n",
    "    ) -> Tuple[pd.DataFrame, FeatureMetadata]:\n",
    "        \"\"\"\n",
    "        Retrieve a feature set from the store.\n",
    "        \n",
    "        Args:\n",
    "            name: Name of the feature set\n",
    "            version: Specific version (defaults to latest)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (DataFrame, FeatureMetadata)\n",
    "        \"\"\"\n",
    "        if name not in self.metadata:\n",
    "            raise ValueError(f\"Feature set '{name}' not found\")\n",
    "        \n",
    "        if version is None:\n",
    "            version = self.metadata[name]['latest']\n",
    "        \n",
    "        if version not in self.metadata[name]['versions']:\n",
    "            raise ValueError(f\"Version '{version}' not found for '{name}'\")\n",
    "        \n",
    "        # Load feature data\n",
    "        feature_path = self.storage_path / name / version / \"features.parquet\"\n",
    "        df = pd.read_parquet(feature_path)\n",
    "        \n",
    "        # Create metadata object\n",
    "        meta_dict = self.metadata[name]['versions'][version]\n",
    "        metadata = FeatureMetadata(\n",
    "            name=name,\n",
    "            version=version,\n",
    "            created_at=datetime.fromisoformat(meta_dict['created_at']),\n",
    "            features=meta_dict['features'],\n",
    "            description=meta_dict['description'],\n",
    "            data_hash=meta_dict['data_hash'],\n",
    "            n_samples=meta_dict['n_samples'],\n",
    "            statistics=meta_dict.get('statistics', {})\n",
    "        )\n",
    "        \n",
    "        return df, metadata\n",
    "    \n",
    "    def list_feature_sets(self) -> pd.DataFrame:\n",
    "        \"\"\"List all registered feature sets.\"\"\"\n",
    "        records = []\n",
    "        for name, info in self.metadata.items():\n",
    "            latest = info.get('latest', '')\n",
    "            if latest in info.get('versions', {}):\n",
    "                version_info = info['versions'][latest]\n",
    "                records.append({\n",
    "                    'Name': name,\n",
    "                    'Latest Version': latest,\n",
    "                    'Features': len(version_info['features']),\n",
    "                    'Samples': version_info['n_samples'],\n",
    "                    'Created': version_info['created_at'][:10]\n",
    "                })\n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def get_feature_statistics(self, name: str, version: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Get statistics for a feature set.\"\"\"\n",
    "        if name not in self.metadata:\n",
    "            raise ValueError(f\"Feature set '{name}' not found\")\n",
    "        \n",
    "        if version is None:\n",
    "            version = self.metadata[name]['latest']\n",
    "        \n",
    "        stats = self.metadata[name]['versions'][version].get('statistics', {})\n",
    "        return pd.DataFrame(stats).T\n",
    "\n",
    "\n",
    "# Initialize feature store\n",
    "feature_store = FeatureStore(\"./mlops_demo/feature_store\")\n",
    "\n",
    "# Define feature groups\n",
    "price_features = ['returns', 'log_returns', 'momentum_5', 'momentum_10', 'momentum_20']\n",
    "technical_features = ['rsi', 'macd', 'macd_signal', 'bb_position']\n",
    "volatility_features = ['volatility_20', 'volatility_50']\n",
    "volume_features = ['volume_ratio']\n",
    "\n",
    "all_features = price_features + technical_features + volatility_features + volume_features\n",
    "\n",
    "# Register feature set\n",
    "metadata = feature_store.register_feature_set(\n",
    "    name=\"trading_signals_v1\",\n",
    "    df=df,\n",
    "    features=all_features,\n",
    "    description=\"Core trading signal features including price momentum, technicals, and volatility\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Registered feature set: {metadata.name}\")\n",
    "print(f\"ðŸ“Œ Version: {metadata.version}\")\n",
    "print(f\"ðŸ“Š Features: {len(metadata.features)}\")\n",
    "print(f\"ðŸ“ˆ Samples: {metadata.n_samples}\")\n",
    "print(f\"\\nðŸ“‹ Feature Statistics:\")\n",
    "feature_store.get_feature_statistics(\"trading_signals_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33f4ac",
   "metadata": {},
   "source": [
    "## 6. Experiment Tracking & Model Registry\n",
    "\n",
    "Experiment tracking is essential for:\n",
    "- Reproducibility of results\n",
    "- Comparing model performance\n",
    "- Auditing model decisions (regulatory compliance)\n",
    "- Managing model lifecycle\n",
    "\n",
    "We'll build a lightweight tracker (in production, use **MLflow**, **Weights & Biases**, or **Neptune**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentRun:\n",
    "    \"\"\"Represents a single experiment run.\"\"\"\n",
    "    run_id: str\n",
    "    experiment_name: str\n",
    "    model_name: str\n",
    "    parameters: Dict[str, Any]\n",
    "    metrics: Dict[str, float]\n",
    "    artifacts: Dict[str, str]\n",
    "    tags: Dict[str, str]\n",
    "    start_time: datetime\n",
    "    end_time: Optional[datetime] = None\n",
    "    status: str = \"RUNNING\"\n",
    "\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"\n",
    "    Lightweight experiment tracker for ML experiments.\n",
    "    \n",
    "    Tracks:\n",
    "    - Model parameters\n",
    "    - Training metrics\n",
    "    - Model artifacts\n",
    "    - Metadata and tags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"./experiments\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.experiments_file = self.storage_path / \"experiments.json\"\n",
    "        self.experiments: Dict[str, List[Dict]] = self._load_experiments()\n",
    "        self.current_run: Optional[ExperimentRun] = None\n",
    "    \n",
    "    def _load_experiments(self) -> Dict:\n",
    "        \"\"\"Load experiments from disk.\"\"\"\n",
    "        if self.experiments_file.exists():\n",
    "            with open(self.experiments_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_experiments(self):\n",
    "        \"\"\"Persist experiments to disk.\"\"\"\n",
    "        with open(self.experiments_file, 'w') as f:\n",
    "            json.dump(self.experiments, f, indent=2, default=str)\n",
    "    \n",
    "    def start_run(\n",
    "        self,\n",
    "        experiment_name: str,\n",
    "        model_name: str,\n",
    "        parameters: Dict[str, Any],\n",
    "        tags: Optional[Dict[str, str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Start a new experiment run.\"\"\"\n",
    "        run_id = str(uuid.uuid4())[:8]\n",
    "        \n",
    "        self.current_run = ExperimentRun(\n",
    "            run_id=run_id,\n",
    "            experiment_name=experiment_name,\n",
    "            model_name=model_name,\n",
    "            parameters=parameters,\n",
    "            metrics={},\n",
    "            artifacts={},\n",
    "            tags=tags or {},\n",
    "            start_time=datetime.now()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Started run {run_id} for experiment '{experiment_name}'\")\n",
    "        return run_id\n",
    "    \n",
    "    def log_metric(self, name: str, value: float):\n",
    "        \"\"\"Log a metric for the current run.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise RuntimeError(\"No active run. Call start_run() first.\")\n",
    "        self.current_run.metrics[name] = value\n",
    "    \n",
    "    def log_metrics(self, metrics: Dict[str, float]):\n",
    "        \"\"\"Log multiple metrics.\"\"\"\n",
    "        for name, value in metrics.items():\n",
    "            self.log_metric(name, value)\n",
    "    \n",
    "    def log_artifact(self, name: str, artifact: Any, artifact_type: str = \"model\"):\n",
    "        \"\"\"Save an artifact (model, plot, data).\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise RuntimeError(\"No active run. Call start_run() first.\")\n",
    "        \n",
    "        # Create artifact directory\n",
    "        artifact_dir = self.storage_path / self.current_run.experiment_name / self.current_run.run_id\n",
    "        artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save artifact based on type\n",
    "        if artifact_type == \"model\":\n",
    "            artifact_path = artifact_dir / f\"{name}.joblib\"\n",
    "            joblib.dump(artifact, artifact_path)\n",
    "        elif artifact_type == \"dataframe\":\n",
    "            artifact_path = artifact_dir / f\"{name}.parquet\"\n",
    "            artifact.to_parquet(artifact_path)\n",
    "        elif artifact_type == \"figure\":\n",
    "            artifact_path = artifact_dir / f\"{name}.png\"\n",
    "            artifact.savefig(artifact_path, dpi=150, bbox_inches='tight')\n",
    "        else:\n",
    "            artifact_path = artifact_dir / f\"{name}.json\"\n",
    "            with open(artifact_path, 'w') as f:\n",
    "                json.dump(artifact, f, indent=2, default=str)\n",
    "        \n",
    "        self.current_run.artifacts[name] = str(artifact_path)\n",
    "    \n",
    "    def end_run(self, status: str = \"COMPLETED\"):\n",
    "        \"\"\"End the current run.\"\"\"\n",
    "        if self.current_run is None:\n",
    "            raise RuntimeError(\"No active run to end.\")\n",
    "        \n",
    "        self.current_run.end_time = datetime.now()\n",
    "        self.current_run.status = status\n",
    "        \n",
    "        # Save to experiments\n",
    "        exp_name = self.current_run.experiment_name\n",
    "        if exp_name not in self.experiments:\n",
    "            self.experiments[exp_name] = []\n",
    "        \n",
    "        self.experiments[exp_name].append({\n",
    "            'run_id': self.current_run.run_id,\n",
    "            'model_name': self.current_run.model_name,\n",
    "            'parameters': self.current_run.parameters,\n",
    "            'metrics': self.current_run.metrics,\n",
    "            'artifacts': self.current_run.artifacts,\n",
    "            'tags': self.current_run.tags,\n",
    "            'start_time': self.current_run.start_time.isoformat(),\n",
    "            'end_time': self.current_run.end_time.isoformat(),\n",
    "            'status': self.current_run.status,\n",
    "            'duration_seconds': (self.current_run.end_time - self.current_run.start_time).total_seconds()\n",
    "        })\n",
    "        \n",
    "        self._save_experiments()\n",
    "        logger.info(f\"Ended run {self.current_run.run_id} with status {status}\")\n",
    "        \n",
    "        run_id = self.current_run.run_id\n",
    "        self.current_run = None\n",
    "        return run_id\n",
    "    \n",
    "    def get_experiment_runs(self, experiment_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Get all runs for an experiment.\"\"\"\n",
    "        if experiment_name not in self.experiments:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        runs = self.experiments[experiment_name]\n",
    "        records = []\n",
    "        for run in runs:\n",
    "            record = {\n",
    "                'run_id': run['run_id'],\n",
    "                'model_name': run['model_name'],\n",
    "                'status': run['status'],\n",
    "                'duration_s': run.get('duration_seconds', 0),\n",
    "                **{f\"param_{k}\": v for k, v in run['parameters'].items()},\n",
    "                **{f\"metric_{k}\": v for k, v in run['metrics'].items()}\n",
    "            }\n",
    "            records.append(record)\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def get_best_run(self, experiment_name: str, metric: str, minimize: bool = False) -> Dict:\n",
    "        \"\"\"Get the best run based on a metric.\"\"\"\n",
    "        if experiment_name not in self.experiments:\n",
    "            raise ValueError(f\"Experiment '{experiment_name}' not found\")\n",
    "        \n",
    "        runs = self.experiments[experiment_name]\n",
    "        valid_runs = [r for r in runs if metric in r['metrics']]\n",
    "        \n",
    "        if not valid_runs:\n",
    "            raise ValueError(f\"No runs with metric '{metric}'\")\n",
    "        \n",
    "        key = lambda r: r['metrics'][metric]\n",
    "        best_run = min(valid_runs, key=key) if minimize else max(valid_runs, key=key)\n",
    "        return best_run\n",
    "    \n",
    "    def load_artifact(self, experiment_name: str, run_id: str, artifact_name: str):\n",
    "        \"\"\"Load an artifact from a run.\"\"\"\n",
    "        artifact_dir = self.storage_path / experiment_name / run_id\n",
    "        \n",
    "        # Try different extensions\n",
    "        for ext in ['.joblib', '.parquet', '.json', '.png']:\n",
    "            artifact_path = artifact_dir / f\"{artifact_name}{ext}\"\n",
    "            if artifact_path.exists():\n",
    "                if ext == '.joblib':\n",
    "                    return joblib.load(artifact_path)\n",
    "                elif ext == '.parquet':\n",
    "                    return pd.read_parquet(artifact_path)\n",
    "                elif ext == '.json':\n",
    "                    with open(artifact_path, 'r') as f:\n",
    "                        return json.load(f)\n",
    "        \n",
    "        raise FileNotFoundError(f\"Artifact '{artifact_name}' not found\")\n",
    "\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = ExperimentTracker(\"./mlops_demo/experiments\")\n",
    "print(\"âœ… Experiment tracker initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a7ef4",
   "metadata": {},
   "source": [
    "## 7. Model Training Pipeline\n",
    "\n",
    "Build an automated training pipeline that:\n",
    "1. Loads features from the feature store\n",
    "2. Trains multiple model variants\n",
    "3. Logs all parameters, metrics, and artifacts\n",
    "4. Tracks experiments for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9e4052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingModelPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end training pipeline for trading signal models.\n",
    "    \n",
    "    Integrates with feature store and experiment tracker.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_store: FeatureStore,\n",
    "        tracker: ExperimentTracker,\n",
    "        experiment_name: str = \"trading_signals\"\n",
    "    ):\n",
    "        self.feature_store = feature_store\n",
    "        self.tracker = tracker\n",
    "        self.experiment_name = experiment_name\n",
    "    \n",
    "    def prepare_data(\n",
    "        self,\n",
    "        feature_set_name: str,\n",
    "        target_column: str,\n",
    "        train_ratio: float = 0.7,\n",
    "        val_ratio: float = 0.15\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare train/val/test splits using time-based splitting.\n",
    "        \n",
    "        For time series, we always split chronologically to prevent look-ahead bias.\n",
    "        \"\"\"\n",
    "        # Get features from store\n",
    "        features_df, metadata = self.feature_store.get_feature_set(feature_set_name)\n",
    "        \n",
    "        # Get target from original data\n",
    "        # In production, target would also be in feature store\n",
    "        X = features_df.values\n",
    "        y = df['target'].iloc[:len(X)].values\n",
    "        \n",
    "        # Time-based split (no shuffling!)\n",
    "        n = len(X)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * (train_ratio + val_ratio))\n",
    "        \n",
    "        X_train, y_train = X[:train_end], y[:train_end]\n",
    "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "        X_test, y_test = X[val_end:], y[val_end:]\n",
    "        \n",
    "        logger.info(f\"Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def train_model(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        model,\n",
    "        X_train: np.ndarray,\n",
    "        X_val: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        y_test: np.ndarray,\n",
    "        parameters: Dict[str, Any],\n",
    "        tags: Optional[Dict[str, str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Train a model with full experiment tracking.\n",
    "        \n",
    "        Returns the run_id for the experiment.\n",
    "        \"\"\"\n",
    "        # Start experiment run\n",
    "        run_id = self.tracker.start_run(\n",
    "            experiment_name=self.experiment_name,\n",
    "            model_name=model_name,\n",
    "            parameters=parameters,\n",
    "            tags=tags or {}\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Create pipeline with scaling\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            \n",
    "            # Train\n",
    "            logger.info(f\"Training {model_name}...\")\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predictions\n",
    "            y_train_pred = pipeline.predict(X_train)\n",
    "            y_val_pred = pipeline.predict(X_val)\n",
    "            y_test_pred = pipeline.predict(X_test)\n",
    "            \n",
    "            # Probabilities for AUC\n",
    "            if hasattr(pipeline, 'predict_proba'):\n",
    "                y_train_proba = pipeline.predict_proba(X_train)[:, 1]\n",
    "                y_val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "                y_test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_train_proba = y_val_proba = y_test_proba = None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = {\n",
    "                'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "                'val_accuracy': accuracy_score(y_val, y_val_pred),\n",
    "                'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "                'train_precision': precision_score(y_train, y_train_pred, zero_division=0),\n",
    "                'val_precision': precision_score(y_val, y_val_pred, zero_division=0),\n",
    "                'test_precision': precision_score(y_test, y_test_pred, zero_division=0),\n",
    "                'train_recall': recall_score(y_train, y_train_pred, zero_division=0),\n",
    "                'val_recall': recall_score(y_val, y_val_pred, zero_division=0),\n",
    "                'test_recall': recall_score(y_test, y_test_pred, zero_division=0),\n",
    "                'train_f1': f1_score(y_train, y_train_pred, zero_division=0),\n",
    "                'val_f1': f1_score(y_val, y_val_pred, zero_division=0),\n",
    "                'test_f1': f1_score(y_test, y_test_pred, zero_division=0),\n",
    "            }\n",
    "            \n",
    "            # Add AUC if probabilities available\n",
    "            if y_train_proba is not None:\n",
    "                metrics['train_auc'] = roc_auc_score(y_train, y_train_proba)\n",
    "                metrics['val_auc'] = roc_auc_score(y_val, y_val_proba)\n",
    "                metrics['test_auc'] = roc_auc_score(y_test, y_test_proba)\n",
    "            \n",
    "            # Log metrics\n",
    "            self.tracker.log_metrics(metrics)\n",
    "            \n",
    "            # Log model artifact\n",
    "            self.tracker.log_artifact('model', pipeline, artifact_type='model')\n",
    "            \n",
    "            # Log confusion matrix plot\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            for ax, (name, y_true, y_pred) in zip(\n",
    "                axes, \n",
    "                [('Train', y_train, y_train_pred), \n",
    "                 ('Val', y_val, y_val_pred), \n",
    "                 ('Test', y_test, y_test_pred)]\n",
    "            ):\n",
    "                cm = confusion_matrix(y_true, y_pred)\n",
    "                ax.imshow(cm, cmap='Blues')\n",
    "                ax.set_title(f'{name} Confusion Matrix')\n",
    "                ax.set_xlabel('Predicted')\n",
    "                ax.set_ylabel('Actual')\n",
    "                for i in range(2):\n",
    "                    for j in range(2):\n",
    "                        ax.text(j, i, cm[i, j], ha='center', va='center')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            self.tracker.log_artifact('confusion_matrices', fig, artifact_type='figure')\n",
    "            plt.close()\n",
    "            \n",
    "            # End run successfully\n",
    "            self.tracker.end_run(status=\"COMPLETED\")\n",
    "            \n",
    "            logger.info(f\"âœ… Training completed - Run ID: {run_id}\")\n",
    "            logger.info(f\"   Val Accuracy: {metrics['val_accuracy']:.4f}\")\n",
    "            logger.info(f\"   Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "            \n",
    "            return run_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {e}\")\n",
    "            self.tracker.end_run(status=\"FAILED\")\n",
    "            raise\n",
    "    \n",
    "    def run_experiments(\n",
    "        self,\n",
    "        feature_set_name: str,\n",
    "        models: Dict[str, Tuple[Any, Dict[str, Any]]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run multiple model experiments.\n",
    "        \n",
    "        Args:\n",
    "            feature_set_name: Name of feature set in feature store\n",
    "            models: Dict of model_name -> (model_instance, parameters)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with all experiment results\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_data(feature_set_name, 'target')\n",
    "        \n",
    "        run_ids = []\n",
    "        for model_name, (model, params) in models.items():\n",
    "            run_id = self.train_model(\n",
    "                model_name=model_name,\n",
    "                model=model,\n",
    "                X_train=X_train, X_val=X_val, X_test=X_test,\n",
    "                y_train=y_train, y_val=y_val, y_test=y_test,\n",
    "                parameters=params,\n",
    "                tags={'feature_set': feature_set_name}\n",
    "            )\n",
    "            run_ids.append(run_id)\n",
    "        \n",
    "        return self.tracker.get_experiment_runs(self.experiment_name)\n",
    "\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TradingModelPipeline(feature_store, tracker, experiment_name=\"trading_signals_experiment\")\n",
    "\n",
    "# Define models to experiment with\n",
    "models = {\n",
    "    'logistic_regression': (\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        {'C': 1.0, 'penalty': 'l2', 'max_iter': 1000}\n",
    "    ),\n",
    "    'random_forest': (\n",
    "        RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "        {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    ),\n",
    "    'gradient_boosting': (\n",
    "        GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
    "    )\n",
    "}\n",
    "\n",
    "# Run experiments\n",
    "results = pipeline.run_experiments(\"trading_signals_v1\", models)\n",
    "print(\"\\nðŸ“Š Experiment Results:\")\n",
    "results[['run_id', 'model_name', 'metric_val_accuracy', 'metric_test_accuracy', 'metric_val_auc', 'metric_test_auc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8561f18",
   "metadata": {},
   "source": [
    "## 8. Model Registry & Deployment\n",
    "\n",
    "The **Model Registry** manages model versions and their deployment stages:\n",
    "- **Staging**: Models under evaluation\n",
    "- **Production**: Live models serving predictions\n",
    "- **Archived**: Retired models kept for audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Model registry for managing model versions and deployment stages.\n",
    "    \n",
    "    Stages:\n",
    "    - None: Just registered\n",
    "    - Staging: Under evaluation\n",
    "    - Production: Live model\n",
    "    - Archived: Retired\n",
    "    \"\"\"\n",
    "    \n",
    "    VALID_STAGES = ['staging', 'production', 'archived']\n",
    "    \n",
    "    def __init__(self, storage_path: str = \"./model_registry\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.registry_file = self.storage_path / \"registry.json\"\n",
    "        self.registry: Dict[str, Dict] = self._load_registry()\n",
    "    \n",
    "    def _load_registry(self) -> Dict:\n",
    "        \"\"\"Load registry from disk.\"\"\"\n",
    "        if self.registry_file.exists():\n",
    "            with open(self.registry_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_registry(self):\n",
    "        \"\"\"Persist registry to disk.\"\"\"\n",
    "        with open(self.registry_file, 'w') as f:\n",
    "            json.dump(self.registry, f, indent=2, default=str)\n",
    "    \n",
    "    def register_model(\n",
    "        self,\n",
    "        name: str,\n",
    "        model,\n",
    "        version: str,\n",
    "        metrics: Dict[str, float],\n",
    "        experiment_run_id: Optional[str] = None,\n",
    "        description: str = \"\"\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Register a new model version.\n",
    "        \n",
    "        Args:\n",
    "            name: Model name\n",
    "            model: Model object (sklearn pipeline, etc.)\n",
    "            version: Version string\n",
    "            metrics: Model metrics\n",
    "            experiment_run_id: Link to experiment run\n",
    "            description: Model description\n",
    "        \n",
    "        Returns:\n",
    "            Full model identifier (name/version)\n",
    "        \"\"\"\n",
    "        # Create model directory\n",
    "        model_dir = self.storage_path / name / version\n",
    "        model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = model_dir / \"model.joblib\"\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Create registry entry\n",
    "        if name not in self.registry:\n",
    "            self.registry[name] = {'versions': {}}\n",
    "        \n",
    "        self.registry[name]['versions'][version] = {\n",
    "            'registered_at': datetime.now().isoformat(),\n",
    "            'metrics': metrics,\n",
    "            'experiment_run_id': experiment_run_id,\n",
    "            'description': description,\n",
    "            'stage': None,\n",
    "            'model_path': str(model_path)\n",
    "        }\n",
    "        \n",
    "        self._save_registry()\n",
    "        logger.info(f\"Registered model '{name}' version '{version}'\")\n",
    "        \n",
    "        return f\"{name}/{version}\"\n",
    "    \n",
    "    def transition_stage(self, name: str, version: str, stage: str):\n",
    "        \"\"\"\n",
    "        Transition a model to a new stage.\n",
    "        \n",
    "        Only one model can be in 'production' stage per model name.\n",
    "        \"\"\"\n",
    "        if stage not in self.VALID_STAGES:\n",
    "            raise ValueError(f\"Invalid stage. Must be one of {self.VALID_STAGES}\")\n",
    "        \n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"Model '{name}' not found\")\n",
    "        \n",
    "        if version not in self.registry[name]['versions']:\n",
    "            raise ValueError(f\"Version '{version}' not found for model '{name}'\")\n",
    "        \n",
    "        # If transitioning to production, archive current production model\n",
    "        if stage == 'production':\n",
    "            for v, info in self.registry[name]['versions'].items():\n",
    "                if info.get('stage') == 'production':\n",
    "                    info['stage'] = 'archived'\n",
    "                    logger.info(f\"Archived previous production model '{name}/{v}'\")\n",
    "        \n",
    "        # Update stage\n",
    "        self.registry[name]['versions'][version]['stage'] = stage\n",
    "        self.registry[name]['versions'][version]['stage_updated_at'] = datetime.now().isoformat()\n",
    "        \n",
    "        self._save_registry()\n",
    "        logger.info(f\"Transitioned '{name}/{version}' to stage '{stage}'\")\n",
    "    \n",
    "    def get_production_model(self, name: str):\n",
    "        \"\"\"Get the production model for a given name.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"Model '{name}' not found\")\n",
    "        \n",
    "        for version, info in self.registry[name]['versions'].items():\n",
    "            if info.get('stage') == 'production':\n",
    "                model_path = info['model_path']\n",
    "                return joblib.load(model_path), version, info\n",
    "        \n",
    "        raise ValueError(f\"No production model found for '{name}'\")\n",
    "    \n",
    "    def get_model(self, name: str, version: str):\n",
    "        \"\"\"Get a specific model version.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"Model '{name}' not found\")\n",
    "        \n",
    "        if version not in self.registry[name]['versions']:\n",
    "            raise ValueError(f\"Version '{version}' not found\")\n",
    "        \n",
    "        info = self.registry[name]['versions'][version]\n",
    "        model = joblib.load(info['model_path'])\n",
    "        return model, info\n",
    "    \n",
    "    def list_models(self) -> pd.DataFrame:\n",
    "        \"\"\"List all registered models.\"\"\"\n",
    "        records = []\n",
    "        for name, model_info in self.registry.items():\n",
    "            for version, info in model_info['versions'].items():\n",
    "                records.append({\n",
    "                    'Name': name,\n",
    "                    'Version': version,\n",
    "                    'Stage': info.get('stage', 'None'),\n",
    "                    'Registered': info['registered_at'][:10],\n",
    "                    'Test Accuracy': info['metrics'].get('test_accuracy', 'N/A'),\n",
    "                    'Test AUC': info['metrics'].get('test_auc', 'N/A')\n",
    "                })\n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def compare_versions(self, name: str) -> pd.DataFrame:\n",
    "        \"\"\"Compare all versions of a model.\"\"\"\n",
    "        if name not in self.registry:\n",
    "            raise ValueError(f\"Model '{name}' not found\")\n",
    "        \n",
    "        records = []\n",
    "        for version, info in self.registry[name]['versions'].items():\n",
    "            record = {'Version': version, 'Stage': info.get('stage', 'None')}\n",
    "            record.update(info['metrics'])\n",
    "            records.append(record)\n",
    "        \n",
    "        return pd.DataFrame(records).sort_values('test_accuracy', ascending=False)\n",
    "\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry(\"./mlops_demo/model_registry\")\n",
    "\n",
    "# Get best model from experiments and register it\n",
    "best_run = tracker.get_best_run(\"trading_signals_experiment\", metric=\"val_accuracy\")\n",
    "best_model = tracker.load_artifact(\"trading_signals_experiment\", best_run['run_id'], \"model\")\n",
    "\n",
    "# Register the best model\n",
    "model_registry.register_model(\n",
    "    name=\"trading_signal_classifier\",\n",
    "    model=best_model,\n",
    "    version=\"1.0.0\",\n",
    "    metrics=best_run['metrics'],\n",
    "    experiment_run_id=best_run['run_id'],\n",
    "    description=f\"Best model from experiments - {best_run['model_name']}\"\n",
    ")\n",
    "\n",
    "# Promote to staging\n",
    "model_registry.transition_stage(\"trading_signal_classifier\", \"1.0.0\", \"staging\")\n",
    "\n",
    "# After validation, promote to production\n",
    "model_registry.transition_stage(\"trading_signal_classifier\", \"1.0.0\", \"production\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Model Registry:\")\n",
    "model_registry.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e542c1",
   "metadata": {},
   "source": [
    "## 9. Model Serving and Inference\n",
    "\n",
    "Production model serving patterns for trading systems:\n",
    "1. **Batch Inference**: Generate signals for all assets periodically\n",
    "2. **Real-time Inference**: On-demand predictions for trading decisions\n",
    "3. **Streaming Inference**: Continuous predictions on data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelServer:\n",
    "    \"\"\"\n",
    "    Model serving layer for trading signal predictions.\n",
    "    \n",
    "    Handles:\n",
    "    - Loading production models\n",
    "    - Feature preparation\n",
    "    - Prediction generation\n",
    "    - Prediction logging for monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_registry: ModelRegistry,\n",
    "        model_name: str,\n",
    "        log_predictions: bool = True\n",
    "    ):\n",
    "        self.model_registry = model_registry\n",
    "        self.model_name = model_name\n",
    "        self.log_predictions = log_predictions\n",
    "        self.prediction_log: List[Dict] = []\n",
    "        \n",
    "        # Load production model\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the current production model.\"\"\"\n",
    "        try:\n",
    "            self.model, self.model_version, self.model_info = \\\n",
    "                self.model_registry.get_production_model(self.model_name)\n",
    "            logger.info(f\"Loaded production model: {self.model_name} v{self.model_version}\")\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def reload_model(self):\n",
    "        \"\"\"Reload model (for hot-reloading on model updates).\"\"\"\n",
    "        self._load_model()\n",
    "    \n",
    "    def predict(self, features: np.ndarray, return_proba: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate predictions for given features.\n",
    "        \n",
    "        Args:\n",
    "            features: Feature array (n_samples, n_features)\n",
    "            return_proba: Whether to return probabilities\n",
    "        \n",
    "        Returns:\n",
    "            Dict with predictions and metadata\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Ensure 2D array\n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(1, -1)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = self.model.predict(features)\n",
    "        \n",
    "        result = {\n",
    "            'predictions': predictions.tolist(),\n",
    "            'model_version': self.model_version,\n",
    "            'timestamp': start_time.isoformat(),\n",
    "            'n_samples': len(features)\n",
    "        }\n",
    "        \n",
    "        if return_proba and hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(features)\n",
    "            result['probabilities'] = probabilities.tolist()\n",
    "            result['confidence'] = np.max(probabilities, axis=1).tolist()\n",
    "        \n",
    "        # Calculate latency\n",
    "        latency_ms = (datetime.now() - start_time).total_seconds() * 1000\n",
    "        result['latency_ms'] = latency_ms\n",
    "        \n",
    "        # Log prediction for monitoring\n",
    "        if self.log_predictions:\n",
    "            self._log_prediction(features, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _log_prediction(self, features: np.ndarray, result: Dict):\n",
    "        \"\"\"Log prediction for monitoring and drift detection.\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': result['timestamp'],\n",
    "            'model_version': result['model_version'],\n",
    "            'n_samples': result['n_samples'],\n",
    "            'predictions': result['predictions'],\n",
    "            'latency_ms': result['latency_ms'],\n",
    "            'feature_stats': {\n",
    "                'mean': float(np.mean(features)),\n",
    "                'std': float(np.std(features)),\n",
    "                'min': float(np.min(features)),\n",
    "                'max': float(np.max(features))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if 'confidence' in result:\n",
    "            log_entry['avg_confidence'] = float(np.mean(result['confidence']))\n",
    "        \n",
    "        self.prediction_log.append(log_entry)\n",
    "    \n",
    "    def batch_predict(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        feature_columns: List[str],\n",
    "        output_column: str = 'prediction'\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Batch prediction on a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            feature_columns: List of feature column names\n",
    "            output_column: Name for prediction column\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with predictions added\n",
    "        \"\"\"\n",
    "        features = df[feature_columns].values\n",
    "        result = self.predict(features, return_proba=True)\n",
    "        \n",
    "        df_out = df.copy()\n",
    "        df_out[output_column] = result['predictions']\n",
    "        \n",
    "        if 'probabilities' in result:\n",
    "            df_out[f'{output_column}_prob'] = [p[1] for p in result['probabilities']]\n",
    "            df_out[f'{output_column}_confidence'] = result['confidence']\n",
    "        \n",
    "        return df_out\n",
    "    \n",
    "    def get_prediction_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics from prediction logs.\"\"\"\n",
    "        if not self.prediction_log:\n",
    "            return {}\n",
    "        \n",
    "        total_predictions = sum(log['n_samples'] for log in self.prediction_log)\n",
    "        latencies = [log['latency_ms'] for log in self.prediction_log]\n",
    "        \n",
    "        return {\n",
    "            'total_requests': len(self.prediction_log),\n",
    "            'total_predictions': total_predictions,\n",
    "            'avg_latency_ms': np.mean(latencies),\n",
    "            'p50_latency_ms': np.percentile(latencies, 50),\n",
    "            'p99_latency_ms': np.percentile(latencies, 99),\n",
    "            'prediction_distribution': {\n",
    "                'up': sum(sum(1 for p in log['predictions'] if p == 1) for log in self.prediction_log),\n",
    "                'down': sum(sum(1 for p in log['predictions'] if p == 0) for log in self.prediction_log)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize model server\n",
    "server = ModelServer(model_registry, \"trading_signal_classifier\")\n",
    "\n",
    "# Demo: Single prediction\n",
    "sample_features = df[all_features].iloc[-10:].values\n",
    "result = server.predict(sample_features, return_proba=True)\n",
    "\n",
    "print(\"ðŸ”® Single Batch Prediction:\")\n",
    "print(f\"   Predictions: {result['predictions']}\")\n",
    "print(f\"   Confidence: {[f'{c:.2f}' for c in result['confidence']]}\")\n",
    "print(f\"   Latency: {result['latency_ms']:.2f}ms\")\n",
    "print(f\"   Model Version: {result['model_version']}\")\n",
    "\n",
    "# Demo: Batch prediction on DataFrame\n",
    "df_predictions = server.batch_predict(df.tail(100), all_features, 'signal')\n",
    "print(f\"\\nðŸ“Š Batch Prediction Results:\")\n",
    "print(df_predictions[['timestamp', 'close', 'signal', 'signal_prob', 'signal_confidence']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acdf1be",
   "metadata": {},
   "source": [
    "## 10. Monitoring and Drift Detection\n",
    "\n",
    "Model monitoring is critical in trading because:\n",
    "1. **Market Regime Changes**: Models trained in bull markets may fail in bear markets\n",
    "2. **Data Distribution Shifts**: Feature distributions change over time\n",
    "3. **Concept Drift**: The relationship between features and target changes\n",
    "\n",
    "Types of drift:\n",
    "- **Data Drift**: Input feature distributions change\n",
    "- **Concept Drift**: P(Y|X) changes - same features, different outcomes\n",
    "- **Prediction Drift**: Model output distribution changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DriftResult:\n",
    "    \"\"\"Result of a drift detection test.\"\"\"\n",
    "    feature: str\n",
    "    test_name: str\n",
    "    statistic: float\n",
    "    p_value: float\n",
    "    drift_detected: bool\n",
    "    threshold: float\n",
    "\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"\n",
    "    Drift detection for production ML models.\n",
    "    \n",
    "    Uses statistical tests to compare reference (training) and\n",
    "    production data distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        reference_data: pd.DataFrame,\n",
    "        features: List[str],\n",
    "        p_value_threshold: float = 0.05,\n",
    "        psi_threshold: float = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize drift detector with reference data.\n",
    "        \n",
    "        Args:\n",
    "            reference_data: Training/baseline data\n",
    "            features: Features to monitor\n",
    "            p_value_threshold: Threshold for statistical tests\n",
    "            psi_threshold: Population Stability Index threshold\n",
    "        \"\"\"\n",
    "        self.reference_data = reference_data[features].copy()\n",
    "        self.features = features\n",
    "        self.p_value_threshold = p_value_threshold\n",
    "        self.psi_threshold = psi_threshold\n",
    "        \n",
    "        # Compute reference statistics\n",
    "        self.reference_stats = self._compute_stats(self.reference_data)\n",
    "        \n",
    "        # Store historical drift results\n",
    "        self.drift_history: List[Dict] = []\n",
    "    \n",
    "    def _compute_stats(self, df: pd.DataFrame) -> Dict[str, Dict]:\n",
    "        \"\"\"Compute statistics for each feature.\"\"\"\n",
    "        stats = {}\n",
    "        for feature in self.features:\n",
    "            if feature in df.columns:\n",
    "                data = df[feature].dropna()\n",
    "                stats[feature] = {\n",
    "                    'mean': float(data.mean()),\n",
    "                    'std': float(data.std()),\n",
    "                    'min': float(data.min()),\n",
    "                    'max': float(data.max()),\n",
    "                    'median': float(data.median()),\n",
    "                    'skew': float(data.skew()),\n",
    "                    'kurtosis': float(data.kurtosis()),\n",
    "                    'histogram': np.histogram(data, bins=10)\n",
    "                }\n",
    "        return stats\n",
    "    \n",
    "    def _ks_test(self, feature: str, production_data: pd.Series) -> DriftResult:\n",
    "        \"\"\"Kolmogorov-Smirnov test for distribution comparison.\"\"\"\n",
    "        reference = self.reference_data[feature].dropna()\n",
    "        production = production_data.dropna()\n",
    "        \n",
    "        statistic, p_value = stats.ks_2samp(reference, production)\n",
    "        \n",
    "        return DriftResult(\n",
    "            feature=feature,\n",
    "            test_name=\"Kolmogorov-Smirnov\",\n",
    "            statistic=statistic,\n",
    "            p_value=p_value,\n",
    "            drift_detected=p_value < self.p_value_threshold,\n",
    "            threshold=self.p_value_threshold\n",
    "        )\n",
    "    \n",
    "    def _psi(self, feature: str, production_data: pd.Series, n_bins: int = 10) -> DriftResult:\n",
    "        \"\"\"\n",
    "        Population Stability Index (PSI).\n",
    "        \n",
    "        PSI < 0.1: No significant change\n",
    "        0.1 <= PSI < 0.2: Moderate change\n",
    "        PSI >= 0.2: Significant change\n",
    "        \"\"\"\n",
    "        reference = self.reference_data[feature].dropna()\n",
    "        production = production_data.dropna()\n",
    "        \n",
    "        # Create bins from reference data\n",
    "        _, bin_edges = np.histogram(reference, bins=n_bins)\n",
    "        \n",
    "        # Get percentages in each bin\n",
    "        ref_counts, _ = np.histogram(reference, bins=bin_edges)\n",
    "        prod_counts, _ = np.histogram(production, bins=bin_edges)\n",
    "        \n",
    "        # Convert to percentages (add small epsilon to avoid division by zero)\n",
    "        epsilon = 1e-10\n",
    "        ref_pct = ref_counts / len(reference) + epsilon\n",
    "        prod_pct = prod_counts / len(production) + epsilon\n",
    "        \n",
    "        # Calculate PSI\n",
    "        psi_value = np.sum((prod_pct - ref_pct) * np.log(prod_pct / ref_pct))\n",
    "        \n",
    "        return DriftResult(\n",
    "            feature=feature,\n",
    "            test_name=\"PSI\",\n",
    "            statistic=psi_value,\n",
    "            p_value=psi_value,  # PSI doesn't have p-value, using value itself\n",
    "            drift_detected=psi_value >= self.psi_threshold,\n",
    "            threshold=self.psi_threshold\n",
    "        )\n",
    "    \n",
    "    def _chi_squared_test(self, feature: str, production_data: pd.Series, n_bins: int = 10) -> DriftResult:\n",
    "        \"\"\"Chi-squared test for categorical/binned data.\"\"\"\n",
    "        reference = self.reference_data[feature].dropna()\n",
    "        production = production_data.dropna()\n",
    "        \n",
    "        # Create bins from reference data\n",
    "        _, bin_edges = np.histogram(reference, bins=n_bins)\n",
    "        \n",
    "        # Get counts in each bin\n",
    "        ref_counts, _ = np.histogram(reference, bins=bin_edges)\n",
    "        prod_counts, _ = np.histogram(production, bins=bin_edges)\n",
    "        \n",
    "        # Normalize to same sample size\n",
    "        expected = ref_counts * (len(production) / len(reference))\n",
    "        \n",
    "        # Filter out zeros\n",
    "        mask = (expected > 0) & (prod_counts > 0)\n",
    "        if mask.sum() < 2:\n",
    "            return DriftResult(\n",
    "                feature=feature,\n",
    "                test_name=\"Chi-Squared\",\n",
    "                statistic=0,\n",
    "                p_value=1.0,\n",
    "                drift_detected=False,\n",
    "                threshold=self.p_value_threshold\n",
    "            )\n",
    "        \n",
    "        statistic, p_value = stats.chisquare(prod_counts[mask], expected[mask])\n",
    "        \n",
    "        return DriftResult(\n",
    "            feature=feature,\n",
    "            test_name=\"Chi-Squared\",\n",
    "            statistic=statistic,\n",
    "            p_value=p_value,\n",
    "            drift_detected=p_value < self.p_value_threshold,\n",
    "            threshold=self.p_value_threshold\n",
    "        )\n",
    "    \n",
    "    def detect_drift(\n",
    "        self,\n",
    "        production_data: pd.DataFrame,\n",
    "        tests: List[str] = ['ks', 'psi']\n",
    "    ) -> Tuple[bool, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Detect drift in production data.\n",
    "        \n",
    "        Args:\n",
    "            production_data: Recent production data\n",
    "            tests: List of tests to run ('ks', 'psi', 'chi2')\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (any_drift_detected, results_dataframe)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for feature in self.features:\n",
    "            if feature not in production_data.columns:\n",
    "                continue\n",
    "            \n",
    "            prod_series = production_data[feature]\n",
    "            \n",
    "            if 'ks' in tests:\n",
    "                results.append(self._ks_test(feature, prod_series))\n",
    "            if 'psi' in tests:\n",
    "                results.append(self._psi(feature, prod_series))\n",
    "            if 'chi2' in tests:\n",
    "                results.append(self._chi_squared_test(feature, prod_series))\n",
    "        \n",
    "        # Create results dataframe\n",
    "        results_df = pd.DataFrame([\n",
    "            {\n",
    "                'Feature': r.feature,\n",
    "                'Test': r.test_name,\n",
    "                'Statistic': r.statistic,\n",
    "                'P-Value/PSI': r.p_value,\n",
    "                'Threshold': r.threshold,\n",
    "                'Drift': 'âš ï¸ YES' if r.drift_detected else 'âœ… NO'\n",
    "            }\n",
    "            for r in results\n",
    "        ])\n",
    "        \n",
    "        # Log to history\n",
    "        timestamp = datetime.now()\n",
    "        for r in results:\n",
    "            self.drift_history.append({\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'feature': r.feature,\n",
    "                'test': r.test_name,\n",
    "                'statistic': r.statistic,\n",
    "                'p_value': r.p_value,\n",
    "                'drift_detected': r.drift_detected\n",
    "            })\n",
    "        \n",
    "        any_drift = any(r.drift_detected for r in results)\n",
    "        \n",
    "        return any_drift, results_df\n",
    "    \n",
    "    def get_drift_summary(self, production_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Get summary statistics comparing reference and production data.\"\"\"\n",
    "        prod_stats = self._compute_stats(production_data)\n",
    "        \n",
    "        records = []\n",
    "        for feature in self.features:\n",
    "            if feature in prod_stats:\n",
    "                ref = self.reference_stats[feature]\n",
    "                prod = prod_stats[feature]\n",
    "                \n",
    "                records.append({\n",
    "                    'Feature': feature,\n",
    "                    'Ref Mean': f\"{ref['mean']:.4f}\",\n",
    "                    'Prod Mean': f\"{prod['mean']:.4f}\",\n",
    "                    'Mean Î”%': f\"{((prod['mean'] - ref['mean']) / (abs(ref['mean']) + 1e-10) * 100):.1f}%\",\n",
    "                    'Ref Std': f\"{ref['std']:.4f}\",\n",
    "                    'Prod Std': f\"{prod['std']:.4f}\",\n",
    "                    'Std Î”%': f\"{((prod['std'] - ref['std']) / (abs(ref['std']) + 1e-10) * 100):.1f}%\"\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Split data into \"training\" (reference) and \"production\" (simulating drift)\n",
    "n_reference = int(len(df) * 0.7)\n",
    "reference_data = df.iloc[:n_reference]\n",
    "production_data = df.iloc[n_reference:]\n",
    "\n",
    "# Initialize drift detector\n",
    "drift_detector = DriftDetector(\n",
    "    reference_data=reference_data,\n",
    "    features=all_features,\n",
    "    p_value_threshold=0.05,\n",
    "    psi_threshold=0.2\n",
    ")\n",
    "\n",
    "# Check for drift\n",
    "drift_detected, drift_results = drift_detector.detect_drift(production_data)\n",
    "\n",
    "print(f\"{'âš ï¸ DRIFT DETECTED!' if drift_detected else 'âœ… No significant drift detected'}\\n\")\n",
    "print(\"ðŸ“Š Drift Detection Results:\")\n",
    "drift_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(all_features[:12]):\n",
    "    ax = axes[idx]\n",
    "    ax.hist(reference_data[feature].dropna(), bins=30, alpha=0.5, label='Reference', density=True)\n",
    "    ax.hist(production_data[feature].dropna(), bins=30, alpha=0.5, label='Production', density=True)\n",
    "    ax.set_title(feature)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Feature Distribution Comparison: Reference vs Production\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Statistics summary\n",
    "print(\"\\nðŸ“ˆ Distribution Statistics Comparison:\")\n",
    "drift_detector.get_drift_summary(production_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495eefe",
   "metadata": {},
   "source": [
    "## 11. Performance Monitoring\n",
    "\n",
    "Track model performance metrics over time to detect degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd16491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"\n",
    "    Monitor model performance over time.\n",
    "    \n",
    "    Tracks:\n",
    "    - Prediction accuracy\n",
    "    - Precision/Recall\n",
    "    - Trading-specific metrics (Sharpe, returns)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        baseline_metrics: Dict[str, float],\n",
    "        alert_thresholds: Optional[Dict[str, float]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize performance monitor.\n",
    "        \n",
    "        Args:\n",
    "            baseline_metrics: Baseline metrics from training\n",
    "            alert_thresholds: Thresholds for alerts (relative degradation)\n",
    "        \"\"\"\n",
    "        self.baseline_metrics = baseline_metrics\n",
    "        self.alert_thresholds = alert_thresholds or {\n",
    "            'accuracy': 0.10,   # Alert if accuracy drops by 10%\n",
    "            'precision': 0.15,  # Alert if precision drops by 15%\n",
    "            'recall': 0.15,     # Alert if recall drops by 15%\n",
    "            'auc': 0.10,        # Alert if AUC drops by 10%\n",
    "        }\n",
    "        \n",
    "        self.performance_history: List[Dict] = []\n",
    "        self.alerts: List[Dict] = []\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        y_true: np.ndarray,\n",
    "        y_pred: np.ndarray,\n",
    "        y_proba: Optional[np.ndarray] = None,\n",
    "        timestamp: Optional[datetime] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance and check for degradation.\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels\n",
    "            y_pred: Predicted labels\n",
    "            y_proba: Prediction probabilities (optional)\n",
    "            timestamp: Timestamp for this evaluation\n",
    "        \n",
    "        Returns:\n",
    "            Dict with metrics and alerts\n",
    "        \"\"\"\n",
    "        timestamp = timestamp or datetime.now()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        }\n",
    "        \n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                metrics['auc'] = roc_auc_score(y_true, y_proba)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check for degradation\n",
    "        current_alerts = []\n",
    "        for metric_name, threshold in self.alert_thresholds.items():\n",
    "            if metric_name in metrics and metric_name in self.baseline_metrics:\n",
    "                baseline = self.baseline_metrics[metric_name]\n",
    "                current = metrics[metric_name]\n",
    "                degradation = (baseline - current) / baseline if baseline > 0 else 0\n",
    "                \n",
    "                if degradation > threshold:\n",
    "                    alert = {\n",
    "                        'timestamp': timestamp.isoformat(),\n",
    "                        'metric': metric_name,\n",
    "                        'baseline': baseline,\n",
    "                        'current': current,\n",
    "                        'degradation_pct': degradation * 100,\n",
    "                        'threshold_pct': threshold * 100,\n",
    "                        'severity': 'HIGH' if degradation > threshold * 2 else 'MEDIUM'\n",
    "                    }\n",
    "                    current_alerts.append(alert)\n",
    "                    self.alerts.append(alert)\n",
    "        \n",
    "        # Store in history\n",
    "        record = {\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'n_samples': len(y_true),\n",
    "            'metrics': metrics,\n",
    "            'alerts': len(current_alerts)\n",
    "        }\n",
    "        self.performance_history.append(record)\n",
    "        \n",
    "        return {\n",
    "            'metrics': metrics,\n",
    "            'alerts': current_alerts,\n",
    "            'degradation_detected': len(current_alerts) > 0\n",
    "        }\n",
    "    \n",
    "    def get_performance_trend(self) -> pd.DataFrame:\n",
    "        \"\"\"Get performance metrics over time.\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        records = []\n",
    "        for entry in self.performance_history:\n",
    "            record = {'timestamp': entry['timestamp']}\n",
    "            record.update(entry['metrics'])\n",
    "            records.append(record)\n",
    "        \n",
    "        return pd.DataFrame(records)\n",
    "    \n",
    "    def get_alerts(self) -> pd.DataFrame:\n",
    "        \"\"\"Get all performance alerts.\"\"\"\n",
    "        if not self.alerts:\n",
    "            return pd.DataFrame()\n",
    "        return pd.DataFrame(self.alerts)\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        \"\"\"Visualize performance trends.\"\"\"\n",
    "        df = self.get_performance_trend()\n",
    "        if df.empty:\n",
    "            print(\"No performance data to plot\")\n",
    "            return\n",
    "        \n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        available_metrics = [m for m in metrics_to_plot if m in df.columns]\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, metric in enumerate(available_metrics):\n",
    "            ax = axes[idx]\n",
    "            values = df[metric].values\n",
    "            ax.plot(values, marker='o', label=metric.capitalize())\n",
    "            ax.axhline(y=self.baseline_metrics.get(metric, 0), \n",
    "                      color='r', linestyle='--', label='Baseline')\n",
    "            \n",
    "            # Threshold line\n",
    "            baseline = self.baseline_metrics.get(metric, 0)\n",
    "            threshold = self.alert_thresholds.get(metric, 0)\n",
    "            ax.axhline(y=baseline * (1 - threshold), \n",
    "                      color='orange', linestyle=':', label='Alert Threshold')\n",
    "            \n",
    "            ax.set_title(f'{metric.capitalize()} Over Time')\n",
    "            ax.set_xlabel('Evaluation Period')\n",
    "            ax.set_ylabel(metric.capitalize())\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Initialize performance monitor with baseline from training\n",
    "baseline_metrics = {\n",
    "    'accuracy': best_run['metrics']['test_accuracy'],\n",
    "    'precision': best_run['metrics']['test_precision'],\n",
    "    'recall': best_run['metrics']['test_recall'],\n",
    "    'auc': best_run['metrics'].get('test_auc', 0.5)\n",
    "}\n",
    "\n",
    "performance_monitor = PerformanceMonitor(\n",
    "    baseline_metrics=baseline_metrics,\n",
    "    alert_thresholds={\n",
    "        'accuracy': 0.05,   # 5% drop triggers alert\n",
    "        'precision': 0.10,  \n",
    "        'recall': 0.10,\n",
    "        'auc': 0.05\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate periodic performance evaluation\n",
    "print(\"ðŸ“Š Simulating Performance Monitoring...\\n\")\n",
    "\n",
    "# Split production data into chunks to simulate time periods\n",
    "chunk_size = len(production_data) // 5\n",
    "production_model, _, _ = model_registry.get_production_model(\"trading_signal_classifier\")\n",
    "\n",
    "for i in range(5):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size\n",
    "    \n",
    "    chunk = production_data.iloc[start_idx:end_idx]\n",
    "    X_chunk = chunk[all_features].values\n",
    "    y_chunk = chunk['target'].values\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = production_model.predict(X_chunk)\n",
    "    y_proba = production_model.predict_proba(X_chunk)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    result = performance_monitor.evaluate(\n",
    "        y_true=y_chunk,\n",
    "        y_pred=y_pred,\n",
    "        y_proba=y_proba,\n",
    "        timestamp=datetime.now() + timedelta(days=i*7)\n",
    "    )\n",
    "    \n",
    "    print(f\"Period {i+1}: Accuracy={result['metrics']['accuracy']:.3f}, \"\n",
    "          f\"AUC={result['metrics'].get('auc', 'N/A'):.3f}, \"\n",
    "          f\"Alerts={len(result['alerts'])}\")\n",
    "\n",
    "# Plot performance trends\n",
    "performance_monitor.plot_performance()\n",
    "\n",
    "# Show any alerts\n",
    "alerts_df = performance_monitor.get_alerts()\n",
    "if not alerts_df.empty:\n",
    "    print(\"\\nâš ï¸ Performance Alerts:\")\n",
    "    print(alerts_df)\n",
    "else:\n",
    "    print(\"\\nâœ… No performance alerts generated\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
