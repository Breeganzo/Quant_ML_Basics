{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f720f98",
   "metadata": {},
   "source": [
    "# Week 11: Feature Engineering & Model Explainability\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What You'll Learn This Week\n",
    "\n",
    "Raw data is rarely predictive. Feature engineering transforms data into meaningful signals that models can learn from. Explainability helps us understand why models make predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Creating financial features (technical indicators, momentum, volatility)\n",
    "- Feature selection methods\n",
    "- SHAP for model interpretation\n",
    "- Preventing data leakage\n",
    "\n",
    "**Why This Matters:**\n",
    "In quantitative finance, feature engineering is where alpha is created. Understanding why models work is crucial for trust, debugging, and regulatory compliance.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Financial Feature Engineering\n",
    "2. Technical Indicators\n",
    "3. Feature Selection\n",
    "4. Model Explainability (SHAP)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31323d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard 5 equities for analysis\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'SPY']\n",
    "\n",
    "# Fetch 5 years of data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(\"üì• Downloading market data...\")\n",
    "data = yf.download(TICKERS, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "prices = data['Close'].dropna()\n",
    "volumes = data['Volume'].dropna()\n",
    "highs = data['High'].dropna()\n",
    "lows = data['Low'].dropna()\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(prices)} days of data for {len(TICKERS)} tickers\")\n",
    "print(f\"üìÖ Date range: {prices.index[0].strftime('%Y-%m-%d')} to {prices.index[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cf166",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Financial Feature Engineering\n",
    "\n",
    "### ü§î What Makes a Good Feature?\n",
    "\n",
    "A good trading feature should have:\n",
    "1. **Predictive power** (correlation with future returns)\n",
    "2. **Economic intuition** (makes sense why it should work)\n",
    "3. **Low correlation with existing features** (adds new information)\n",
    "4. **Stability** (works across different time periods)\n",
    "\n",
    "### Common Feature Categories:\n",
    "\n",
    "| Category | Examples |\n",
    "|----------|----------|\n",
    "| Price-Based | Returns, momentum, moving averages |\n",
    "| Volume | Volume ratio, OBV, VWAP |\n",
    "| Volatility | Rolling std, ATR, Bollinger position |\n",
    "| Technical | RSI, MACD, Stochastic |\n",
    "| Cross-Sectional | Sector rank, relative strength |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(prices, volumes, highs, lows, ticker):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for a single ticker.\n",
    "    All features use ONLY past data (no look-ahead bias).\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(index=prices.index)\n",
    "    close = prices[ticker]\n",
    "    volume = volumes[ticker]\n",
    "    high = highs[ticker]\n",
    "    low = lows[ticker]\n",
    "    \n",
    "    # ============ RETURNS ============\n",
    "    df['return_1d'] = close.pct_change(1)\n",
    "    df['return_5d'] = close.pct_change(5)\n",
    "    df['return_10d'] = close.pct_change(10)\n",
    "    df['return_20d'] = close.pct_change(20)\n",
    "    \n",
    "    # Log returns\n",
    "    df['log_return_1d'] = np.log(close / close.shift(1))\n",
    "    \n",
    "    # ============ MOMENTUM ============\n",
    "    df['momentum_5d'] = close / close.shift(5) - 1\n",
    "    df['momentum_20d'] = close / close.shift(20) - 1\n",
    "    \n",
    "    # Rate of change\n",
    "    df['roc_10d'] = (close - close.shift(10)) / close.shift(10)\n",
    "    \n",
    "    # ============ MOVING AVERAGES ============\n",
    "    df['sma_5'] = close.rolling(5).mean()\n",
    "    df['sma_20'] = close.rolling(20).mean()\n",
    "    df['sma_50'] = close.rolling(50).mean()\n",
    "    \n",
    "    # MA ratios (trend indicators)\n",
    "    df['ma_ratio_5_20'] = df['sma_5'] / df['sma_20']\n",
    "    df['ma_ratio_20_50'] = df['sma_20'] / df['sma_50']\n",
    "    \n",
    "    # Price relative to MAs\n",
    "    df['price_to_sma20'] = close / df['sma_20']\n",
    "    \n",
    "    # ============ VOLATILITY ============\n",
    "    df['volatility_5d'] = close.pct_change().rolling(5).std()\n",
    "    df['volatility_20d'] = close.pct_change().rolling(20).std()\n",
    "    \n",
    "    # Volatility ratio\n",
    "    df['vol_ratio'] = df['volatility_5d'] / df['volatility_20d']\n",
    "    \n",
    "    # ATR (Average True Range)\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift(1))\n",
    "    tr3 = abs(low - close.shift(1))\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    df['atr_14'] = tr.rolling(14).mean()\n",
    "    df['atr_ratio'] = df['atr_14'] / close\n",
    "    \n",
    "    # ============ BOLLINGER BANDS ============\n",
    "    bb_middle = close.rolling(20).mean()\n",
    "    bb_std = close.rolling(20).std()\n",
    "    df['bb_upper'] = bb_middle + 2 * bb_std\n",
    "    df['bb_lower'] = bb_middle - 2 * bb_std\n",
    "    df['bb_position'] = (close - bb_middle) / (2 * bb_std)\n",
    "    df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / bb_middle\n",
    "    \n",
    "    # ============ RSI ============\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # ============ MACD ============\n",
    "    ema_12 = close.ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = close.ewm(span=26, adjust=False).mean()\n",
    "    df['macd'] = ema_12 - ema_26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df['macd_hist'] = df['macd'] - df['macd_signal']\n",
    "    \n",
    "    # ============ VOLUME ============\n",
    "    df['volume_sma_20'] = volume.rolling(20).mean()\n",
    "    df['volume_ratio'] = volume / df['volume_sma_20']\n",
    "    \n",
    "    # On-Balance Volume trend\n",
    "    obv = (np.sign(close.diff()) * volume).cumsum()\n",
    "    df['obv_slope'] = obv.diff(5) / obv.shift(5)\n",
    "    \n",
    "    # ============ PRICE PATTERNS ============\n",
    "    # Higher highs, lower lows\n",
    "    df['higher_high'] = (high > high.shift(1)).astype(int)\n",
    "    df['lower_low'] = (low < low.shift(1)).astype(int)\n",
    "    \n",
    "    # Gap up/down\n",
    "    df['gap'] = (close.shift(1) - close.shift(1)) / close.shift(1)\n",
    "    \n",
    "    # Drop intermediate calculation columns\n",
    "    df = df.drop(['sma_5', 'sma_20', 'sma_50', 'bb_upper', 'bb_lower', \n",
    "                  'volume_sma_20', 'macd_signal'], axis=1, errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features for SPY\n",
    "features = create_features(prices, volumes, highs, lows, 'SPY')\n",
    "print(f\"Created {len(features.columns)} features\")\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, col in enumerate(features.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature statistics\n",
    "features_clean = features.dropna()\n",
    "print(f\"\\nFeature Statistics (after dropping NaN):\")\n",
    "print(f\"Samples: {len(features_clean)}\")\n",
    "print(\"=\"*70)\n",
    "print(features_clean.describe().T[['mean', 'std', 'min', 'max']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774cefe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Technical Indicators Deep Dive\n",
    "\n",
    "### RSI (Relative Strength Index)\n",
    "$$RSI = 100 - \\frac{100}{1 + RS}$$\n",
    "where $RS = \\frac{\\text{Avg Gain}}{\\text{Avg Loss}}$\n",
    "\n",
    "- **RSI > 70**: Overbought\n",
    "- **RSI < 30**: Oversold\n",
    "\n",
    "### Bollinger Band Position\n",
    "$$BB_{position} = \\frac{P_t - SMA_{20}}{2 \\times \\sigma_{20}}$$\n",
    "\n",
    "- **Position > 1**: Above upper band (overbought)\n",
    "- **Position < -1**: Below lower band (oversold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89837477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key technical indicators\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Price\n",
    "spy_prices = prices['SPY'].loc[features_clean.index]\n",
    "axes[0].plot(spy_prices, 'b-', alpha=0.8)\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].set_title('SPY Price')\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(features_clean['rsi_14'], 'purple', alpha=0.8)\n",
    "axes[1].axhline(y=70, color='r', linestyle='--', alpha=0.5, label='Overbought')\n",
    "axes[1].axhline(y=30, color='g', linestyle='--', alpha=0.5, label='Oversold')\n",
    "axes[1].set_ylabel('RSI')\n",
    "axes[1].set_title('RSI(14)')\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "# Bollinger Band Position\n",
    "axes[2].plot(features_clean['bb_position'], 'orange', alpha=0.8)\n",
    "axes[2].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Upper Band')\n",
    "axes[2].axhline(y=-1, color='g', linestyle='--', alpha=0.5, label='Lower Band')\n",
    "axes[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[2].set_ylabel('BB Position')\n",
    "axes[2].set_title('Bollinger Band Position')\n",
    "axes[2].legend()\n",
    "\n",
    "# MACD\n",
    "axes[3].plot(features_clean['macd'], 'blue', alpha=0.8, label='MACD')\n",
    "axes[3].bar(features_clean.index, features_clean['macd_hist'], alpha=0.5, color='gray', label='Histogram')\n",
    "axes[3].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[3].set_ylabel('MACD')\n",
    "axes[3].set_title('MACD')\n",
    "axes[3].legend()\n",
    "axes[3].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d11ea6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Selection\n",
    "\n",
    "### ü§î Why Select Features?\n",
    "\n",
    "Too many features can lead to:\n",
    "1. **Overfitting**: Model memorizes noise\n",
    "2. **Multicollinearity**: Correlated features confuse the model\n",
    "3. **Computational cost**: Slower training and prediction\n",
    "\n",
    "### Methods:\n",
    "\n",
    "| Method | Approach |\n",
    "|--------|----------|\n",
    "| Correlation Filter | Remove highly correlated features |\n",
    "| Variance Threshold | Remove low-variance features |\n",
    "| Importance-Based | Use model feature importance |\n",
    "| Recursive Elimination | Iteratively remove weakest |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feature selection\n",
    "# Target: 5-day forward return\n",
    "target = returns['SPY'].shift(-5)\n",
    "\n",
    "# Align data\n",
    "common_idx = features_clean.index.intersection(target.dropna().index)\n",
    "X = features_clean.loc[common_idx]\n",
    "y = target.loc[common_idx]\n",
    "\n",
    "print(f\"Samples for modeling: {len(X)}\")\n",
    "\n",
    "# Time-series split (no shuffling!)\n",
    "split_idx = int(len(X) * 0.7)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if corr_matrix.iloc[i, j] > 0.8:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature 1': corr_matrix.columns[i],\n",
    "                'Feature 2': corr_matrix.columns[j],\n",
    "                'Correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(\"Highly Correlated Feature Pairs (|r| > 0.8):\")\n",
    "print(\"=\"*60)\n",
    "for pair in sorted(high_corr_pairs, key=lambda x: -x['Correlation'])[:10]:\n",
    "    print(f\"  {pair['Feature 1']} <-> {pair['Feature 2']}: {pair['Correlation']:.3f}\")\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(corr_matrix.columns)))\n",
    "ax.set_yticks(range(len(corr_matrix.columns)))\n",
    "ax.set_xticklabels(corr_matrix.columns, rotation=90, fontsize=8)\n",
    "ax.set_yticklabels(corr_matrix.columns, fontsize=8)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(\"=\"*50)\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# Model performance\n",
    "train_r2 = rf.score(X_train_scaled, y_train)\n",
    "test_r2 = rf.score(X_test_scaled, y_test)\n",
    "print(f\"\\nüìä Train R¬≤: {train_r2:.4f}\")\n",
    "print(f\"üìä Test R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Importance bar chart\n",
    "importance.plot.barh(x='feature', y='importance', ax=axes[0], legend=False, color='steelblue')\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Top 10 features only for clarity\n",
    "top_10 = importance.head(10)\n",
    "axes[1].barh(top_10['feature'], top_10['importance'], color='coral')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top 10 Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbc0af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Explainability (SHAP)\n",
    "\n",
    "### ü§î What is SHAP?\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) comes from game theory. It answers:\n",
    "> \"How much did each feature contribute to this specific prediction?\"\n",
    "\n",
    "### Key Properties:\n",
    "1. **Local accuracy**: SHAP values sum to prediction - baseline\n",
    "2. **Consistency**: More important ‚Üí higher SHAP value\n",
    "3. **Directional**: Tells you if feature pushes prediction up or down\n",
    "\n",
    "### Why It Matters in Finance:\n",
    "- Understand what drives model decisions\n",
    "- Debug unexpected predictions\n",
    "- Regulatory compliance (explainable AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    print(\"SHAP Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Use a subset for speed\n",
    "    X_explain = X_test.iloc[:200]\n",
    "    X_explain_scaled = scaler.transform(X_explain)\n",
    "    \n",
    "    # TreeExplainer for Random Forest\n",
    "    explainer = shap.TreeExplainer(rf)\n",
    "    shap_values = explainer.shap_values(X_explain_scaled)\n",
    "    \n",
    "    # Global SHAP importance\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'shap_importance': shap_importance\n",
    "    }).sort_values('shap_importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nGlobal SHAP Importance:\")\n",
    "    print(shap_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualize SHAP\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Summary plot (bar)\n",
    "    shap_df_top = shap_df.head(15)\n",
    "    axes[0].barh(shap_df_top['feature'], shap_df_top['shap_importance'], color='steelblue')\n",
    "    axes[0].set_xlabel('Mean |SHAP Value|')\n",
    "    axes[0].set_title('SHAP Feature Importance (Top 15)')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Compare RF importance vs SHAP\n",
    "    merged = importance.merge(shap_df, on='feature')\n",
    "    axes[1].scatter(merged['importance'], merged['shap_importance'], alpha=0.7)\n",
    "    axes[1].set_xlabel('Random Forest Importance')\n",
    "    axes[1].set_ylabel('SHAP Importance')\n",
    "    axes[1].set_title('RF Importance vs SHAP Importance')\n",
    "    \n",
    "    # Add labels for top features\n",
    "    for idx, row in merged.head(5).iterrows():\n",
    "        axes[1].annotate(row['feature'], (row['importance'], row['shap_importance']), fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP summary plot (beeswarm)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_explain, feature_names=X.columns.tolist(), show=False)\n",
    "    plt.title('SHAP Summary Plot')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not installed. Install with: pip install shap\")\n",
    "    print(\"Skipping SHAP analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89632e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection based on importance\n",
    "# Select top N features\n",
    "N_FEATURES = 10\n",
    "top_features = importance.head(N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"Selected Top {N_FEATURES} Features:\")\n",
    "print(\"=\"*40)\n",
    "for i, f in enumerate(top_features, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "# Train model with selected features only\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "scaler_selected = StandardScaler()\n",
    "X_train_sel_scaled = scaler_selected.fit_transform(X_train_selected)\n",
    "X_test_sel_scaled = scaler_selected.transform(X_test_selected)\n",
    "\n",
    "rf_selected = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_selected.fit(X_train_sel_scaled, y_train)\n",
    "\n",
    "train_r2_sel = rf_selected.score(X_train_sel_scaled, y_train)\n",
    "test_r2_sel = rf_selected.score(X_test_sel_scaled, y_test)\n",
    "\n",
    "print(f\"\\nüìä Model Comparison:\")\n",
    "print(f\"All features ({len(X.columns)}): Train R¬≤ = {train_r2:.4f}, Test R¬≤ = {test_r2:.4f}\")\n",
    "print(f\"Top {N_FEATURES} features: Train R¬≤ = {train_r2_sel:.4f}, Test R¬≤ = {test_r2_sel:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_all = rf.predict(X_test_scaled)\n",
    "y_pred_sel = rf_selected.predict(X_test_sel_scaled)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].scatter(y_test, y_pred_all, alpha=0.5, label='All Features')\n",
    "axes[0].scatter(y_test, y_pred_sel, alpha=0.5, label=f'Top {N_FEATURES}')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[0].set_xlabel('Actual Return')\n",
    "axes[0].set_ylabel('Predicted Return')\n",
    "axes[0].set_title('Predictions: All vs Selected Features')\n",
    "axes[0].legend()\n",
    "\n",
    "# Prediction time series\n",
    "axes[1].plot(y_test.index[-50:], y_test.values[-50:], 'k-', label='Actual', alpha=0.7)\n",
    "axes[1].plot(y_test.index[-50:], y_pred_sel[-50:], 'b--', label='Predicted', alpha=0.7)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('5-Day Return')\n",
    "axes[1].set_title('Prediction Time Series (Last 50 Days)')\n",
    "axes[1].legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5246a1b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Common Pitfalls in Feature Engineering\n",
    "\n",
    "### 1. Data Leakage\n",
    "Using future information in features - **deadly for backtests!**\n",
    "\n",
    "```\n",
    "‚ùå WRONG: Using today's close to predict today's return\n",
    "‚ùå WRONG: Normalizing with full dataset statistics\n",
    "‚ùå WRONG: Using future earnings announcements\n",
    "\n",
    "‚úÖ RIGHT: Use only past data at each point\n",
    "‚úÖ RIGHT: Rolling window calculations\n",
    "‚úÖ RIGHT: Point-in-time data\n",
    "```\n",
    "\n",
    "### 2. Look-Ahead Bias\n",
    "```\n",
    "‚ùå WRONG: features = data.pct_change()  # Uses t+1 for return at t\n",
    "‚úÖ RIGHT: features = data.pct_change().shift(1)  # Past returns only\n",
    "```\n",
    "\n",
    "### 3. Survivorship Bias\n",
    "Only using stocks that exist today ‚Üí Overestimates performance\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Feature Engineering**: Create meaningful signals from raw data\n",
    "2. **Technical Indicators**: RSI, MACD, Bollinger Bands, etc.\n",
    "3. **Feature Selection**: Remove redundant/noisy features\n",
    "4. **SHAP**: Understand what drives model predictions\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "| Practice | Why |\n",
    "|----------|-----|\n",
    "| Use only past data | Prevent leakage |\n",
    "| Normalize features | Fair comparison across features |\n",
    "| Check correlations | Remove redundancy |\n",
    "| Understand features | Economic intuition matters |\n",
    "\n",
    "---\n",
    "\n",
    "## üîë Interview Questions\n",
    "\n",
    "1. **How do you prevent data leakage?**\n",
    "2. **What makes a good trading feature?**\n",
    "3. **How do you interpret SHAP values?**\n",
    "4. **Why is feature selection important?**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
