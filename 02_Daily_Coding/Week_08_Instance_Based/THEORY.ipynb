{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9755c297",
   "metadata": {},
   "source": [
    "# Week 8: Instance-Based Learning - KNN & SVM Regression\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What You'll Learn This Week\n",
    "\n",
    "Instance-based methods make predictions by comparing new data points to similar historical examples. They're like asking: \"What happened the last few times the market looked like this?\"\n",
    "\n",
    "**Key Concepts:**\n",
    "- K-Nearest Neighbors (KNN) for regression and classification\n",
    "- Distance metrics (Euclidean, Manhattan, Mahalanobis)\n",
    "- Support Vector Regression (SVR)\n",
    "- Finance applications: regime matching, pattern recognition\n",
    "\n",
    "**Why This Matters:**\n",
    "In trading, history often rhymes. Instance-based methods find similar historical periods and use those outcomes to inform predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. K-Nearest Neighbors (KNN)\n",
    "2. Distance Metrics\n",
    "3. Support Vector Regression (SVR)\n",
    "4. Financial Applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614168c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard 5 equities for analysis\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'SPY']\n",
    "\n",
    "# Fetch 5 years of data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(\"ðŸ“¥ Downloading market data...\")\n",
    "data = yf.download(TICKERS, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "prices = data['Close'].dropna()\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "print(f\"âœ… Loaded {len(prices)} days of data for {len(TICKERS)} tickers\")\n",
    "print(f\"ðŸ“… Date range: {prices.index[0].strftime('%Y-%m-%d')} to {prices.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(prices.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b634d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. K-Nearest Neighbors (KNN)\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "KNN is like asking your neighbors for advice. When you want to predict something:\n",
    "1. Find the k most similar historical examples\n",
    "2. Average their outcomes (regression) or vote (classification)\n",
    "\n",
    "**The Algorithm:**\n",
    "$$\\hat{y} = \\frac{1}{k}\\sum_{i \\in N_k(x)} y_i$$\n",
    "\n",
    "Where $N_k(x)$ is the set of k nearest neighbors.\n",
    "\n",
    "### Key Trade-off: Choosing k\n",
    "- **Small k (e.g., 1-3)**: Very responsive, but sensitive to noise\n",
    "- **Large k (e.g., 20-50)**: Smoother predictions, but may miss local patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ed38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for KNN\n",
    "# We'll predict next-day returns using recent market indicators\n",
    "\n",
    "def create_knn_features(prices_df, returns_df, lookback=20):\n",
    "    \"\"\"Create features for KNN-based prediction.\"\"\"\n",
    "    features = pd.DataFrame(index=returns_df.index)\n",
    "    \n",
    "    # For each ticker, create features based on market-wide indicators\n",
    "    spy_returns = returns_df['SPY']\n",
    "    \n",
    "    # Recent return patterns\n",
    "    features['ret_1d'] = spy_returns\n",
    "    features['ret_5d'] = spy_returns.rolling(5).sum()\n",
    "    features['ret_20d'] = spy_returns.rolling(20).sum()\n",
    "    \n",
    "    # Volatility\n",
    "    features['vol_5d'] = spy_returns.rolling(5).std()\n",
    "    features['vol_20d'] = spy_returns.rolling(20).std()\n",
    "    \n",
    "    # Volume proxy (using price range)\n",
    "    spy_prices = prices_df['SPY']\n",
    "    features['range'] = (spy_prices.rolling(5).max() - spy_prices.rolling(5).min()) / spy_prices\n",
    "    \n",
    "    # Moving average ratio\n",
    "    features['ma_ratio'] = spy_prices.rolling(5).mean() / spy_prices.rolling(20).mean()\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "# Create features\n",
    "features = create_knn_features(prices, returns)\n",
    "target = returns['SPY'].shift(-1)  # Next-day return\n",
    "\n",
    "# Align data\n",
    "common_idx = features.index.intersection(target.dropna().index)\n",
    "X = features.loc[common_idx]\n",
    "y = target.loc[common_idx]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Feature names: {list(X.columns)}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (CRITICAL for KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data (time-series aware - no shuffle!)\n",
    "split_idx = int(len(X) * 0.7)\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(\"KNN: Effect of k (number of neighbors)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different k values\n",
    "k_values = [1, 3, 5, 10, 20, 50]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    \n",
    "    results.append({'k': k, 'train_r2': train_score, 'test_r2': test_score})\n",
    "    print(f\"k={k:2d}: Train RÂ² = {train_score:.4f}, Test RÂ² = {test_score:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['k'], results_df['train_r2'], 'b-o', label='Train RÂ²')\n",
    "plt.plot(results_df['k'], results_df['test_r2'], 'r-o', label='Test RÂ²')\n",
    "plt.xlabel('k (Number of Neighbors)')\n",
    "plt.ylabel('RÂ² Score')\n",
    "plt.title('KNN: Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Best k model predictions\n",
    "best_k = results_df.loc[results_df['test_r2'].idxmax(), 'k']\n",
    "knn_best = KNeighborsRegressor(n_neighbors=int(best_k))\n",
    "knn_best.fit(X_train, y_train)\n",
    "y_pred = knn_best.predict(X_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Actual Return')\n",
    "plt.ylabel('Predicted Return')\n",
    "plt.title(f'KNN Predictions (k={int(best_k)})')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Best k = {int(best_k)} with Test RÂ² = {results_df['test_r2'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e2d13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Distance Metrics\n",
    "\n",
    "### ðŸ¤” How Do We Measure \"Similar\"?\n",
    "\n",
    "Different distance metrics capture different notions of similarity:\n",
    "\n",
    "| Metric | Formula | Best For |\n",
    "|--------|---------|----------|\n",
    "| Euclidean (L2) | $\\sqrt{\\sum_i (x_i - y_i)^2}$ | Standard, continuous |\n",
    "| Manhattan (L1) | $\\sum_i \\|x_i - y_i\\|$ | Robust to outliers |\n",
    "| Chebyshev | $\\max_i \\|x_i - y_i\\|$ | Max deviation matters |\n",
    "\n",
    "**Key Insight:** The \"best\" metric depends on your data and what \"similar\" means in your context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10374df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different distance metrics\n",
    "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
    "\n",
    "print(\"Distance Metric Comparison (k=10)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metric_results = []\n",
    "for metric in metrics:\n",
    "    knn = KNeighborsRegressor(n_neighbors=10, metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = knn.score(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    \n",
    "    metric_results.append({\n",
    "        'metric': metric,\n",
    "        'train_r2': train_score,\n",
    "        'test_r2': test_score\n",
    "    })\n",
    "    print(f\"{metric:12}: Train RÂ² = {train_score:.4f}, Test RÂ² = {test_score:.4f}\")\n",
    "\n",
    "# Visualize distance metric comparison\n",
    "metric_df = pd.DataFrame(metric_results)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, metric_df['train_r2'], width, label='Train RÂ²', alpha=0.8)\n",
    "ax.bar(x + width/2, metric_df['test_r2'], width, label='Test RÂ²', alpha=0.8)\n",
    "ax.set_xlabel('Distance Metric')\n",
    "ax.set_ylabel('RÂ² Score')\n",
    "ax.set_title('KNN Performance by Distance Metric')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a98da9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Support Vector Regression (SVR)\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "SVR creates a \"tube\" around predictions. Points inside the tube have zero error. Only points outside contribute to the loss.\n",
    "\n",
    "**Îµ-Insensitive Loss:**\n",
    "$$L_\\epsilon(y, f(x)) = \\max(0, |y - f(x)| - \\epsilon)$$\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- **C**: Penalty for points outside tube (higher = stricter)\n",
    "- **epsilon (Îµ)**: Width of the tube\n",
    "- **kernel**: RBF (non-linear) vs Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71349dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR Comparison\n",
    "print(\"Support Vector Regression (SVR)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svr_results = []\n",
    "\n",
    "for kernel in kernels:\n",
    "    svr = SVR(kernel=kernel, C=1.0, epsilon=0.001)\n",
    "    svr.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = svr.score(X_train, y_train)\n",
    "    test_score = svr.score(X_test, y_test)\n",
    "    \n",
    "    svr_results.append({\n",
    "        'kernel': kernel,\n",
    "        'train_r2': train_score,\n",
    "        'test_r2': test_score\n",
    "    })\n",
    "    print(f\"{kernel:8} kernel: Train RÂ² = {train_score:.4f}, Test RÂ² = {test_score:.4f}\")\n",
    "\n",
    "# Best SVR model\n",
    "best_svr = SVR(kernel='rbf', C=1.0, epsilon=0.001)\n",
    "best_svr.fit(X_train, y_train)\n",
    "y_pred_svr = best_svr.predict(X_test)\n",
    "\n",
    "# Compare KNN vs SVR\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].scatter(y_test, y_pred, alpha=0.5, label='KNN')\n",
    "axes[0].scatter(y_test, y_pred_svr, alpha=0.5, label='SVR')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "axes[0].set_xlabel('Actual Return')\n",
    "axes[0].set_ylabel('Predicted Return')\n",
    "axes[0].set_title('KNN vs SVR Predictions')\n",
    "axes[0].legend()\n",
    "\n",
    "# Prediction comparison over time\n",
    "test_dates = y.iloc[split_idx:].index\n",
    "axes[1].plot(test_dates[-50:], y_test.values[-50:], 'k-', label='Actual', alpha=0.7)\n",
    "axes[1].plot(test_dates[-50:], y_pred[-50:], 'b--', label='KNN', alpha=0.7)\n",
    "axes[1].plot(test_dates[-50:], y_pred_svr[-50:], 'g--', label='SVR', alpha=0.7)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Return')\n",
    "axes[1].set_title('Prediction Time Series (Last 50 Days)')\n",
    "axes[1].legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c65cd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Financial Application: Market Regime Matching\n",
    "\n",
    "### ðŸ¤” The Idea\n",
    "\n",
    "Use KNN to find similar historical market conditions and see what happened next.\n",
    "\n",
    "**Features:**\n",
    "- Volatility level\n",
    "- Recent momentum\n",
    "- MA ratio (trend indicator)\n",
    "\n",
    "This is like a quantitative way of saying: \"What happened the last 10 times VIX was this high and momentum was this low?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e245c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market Regime Matching for all tickers\n",
    "print(\"Market Regime Matching - Multi-Asset Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create regime features using market-wide data\n",
    "regime_features = pd.DataFrame(index=returns.index)\n",
    "regime_features['vol_20d'] = returns['SPY'].rolling(20).std() * np.sqrt(252)  # Annualized vol\n",
    "regime_features['momentum_20d'] = prices['SPY'].pct_change(20)\n",
    "regime_features['ma_ratio'] = prices['SPY'].rolling(5).mean() / prices['SPY'].rolling(20).mean()\n",
    "regime_features = regime_features.dropna()\n",
    "\n",
    "# Scale features\n",
    "scaler_regime = StandardScaler()\n",
    "X_regime = scaler_regime.fit_transform(regime_features)\n",
    "\n",
    "# For each ticker, use KNN to predict returns based on regime\n",
    "regime_results = {}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    ticker_returns = returns[ticker].shift(-5)  # 5-day forward return\n",
    "    common_idx = regime_features.index.intersection(ticker_returns.dropna().index)\n",
    "    \n",
    "    y_regime = ticker_returns.loc[common_idx]\n",
    "    X_regime_aligned = scaler_regime.transform(regime_features.loc[common_idx])\n",
    "    \n",
    "    # Time-series split\n",
    "    split = int(len(X_regime_aligned) * 0.7)\n",
    "    X_tr, X_te = X_regime_aligned[:split], X_regime_aligned[split:]\n",
    "    y_tr, y_te = y_regime.iloc[:split], y_regime.iloc[split:]\n",
    "    \n",
    "    # KNN model\n",
    "    knn_regime = KNeighborsRegressor(n_neighbors=20)\n",
    "    knn_regime.fit(X_tr, y_tr)\n",
    "    \n",
    "    train_r2 = knn_regime.score(X_tr, y_tr)\n",
    "    test_r2 = knn_regime.score(X_te, y_te)\n",
    "    \n",
    "    regime_results[ticker] = {\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'predictions': knn_regime.predict(X_te),\n",
    "        'actual': y_te.values\n",
    "    }\n",
    "    \n",
    "    print(f\"{ticker}: Train RÂ² = {train_r2:.4f}, Test RÂ² = {test_r2:.4f}\")\n",
    "\n",
    "# Visualize regime-based predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ticker in enumerate(TICKERS):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(regime_results[ticker]['actual'], regime_results[ticker]['predictions'], alpha=0.5)\n",
    "    ax.plot([regime_results[ticker]['actual'].min(), regime_results[ticker]['actual'].max()],\n",
    "            [regime_results[ticker]['actual'].min(), regime_results[ticker]['actual'].max()], 'r--')\n",
    "    ax.set_xlabel('Actual 5-Day Return')\n",
    "    ax.set_ylabel('Predicted 5-Day Return')\n",
    "    ax.set_title(f'{ticker} (RÂ² = {regime_results[ticker][\"test_r2\"]:.4f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].axis('off')  # Hide extra subplot\n",
    "plt.suptitle('Regime-Based KNN Predictions (5-Day Forward Returns)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af94d0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **KNN Basics**: Find k similar historical examples, average their outcomes\n",
    "2. **k Selection**: Small k = high variance, large k = high bias\n",
    "3. **Distance Metrics**: Euclidean is default, but others may work better\n",
    "4. **SVR**: Creates Îµ-tube, more robust to outliers\n",
    "5. **Scaling is CRITICAL**: Always standardize features!\n",
    "\n",
    "### Trading Applications:\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| KNN Regression | Predict returns from similar regimes |\n",
    "| KNN Classification | Classify market state (bull/bear) |\n",
    "| SVR | Robust prediction with outlier handling |\n",
    "\n",
    "### Key Formula Summary:\n",
    "\n",
    "$$\\text{KNN Prediction: } \\hat{y} = \\frac{1}{k}\\sum_{i \\in N_k(x)} y_i$$\n",
    "\n",
    "$$\\text{SVR Loss: } L_\\epsilon = \\max(0, |y - f(x)| - \\epsilon)$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Interview Questions\n",
    "\n",
    "1. **When would you use KNN over a parametric model?**\n",
    "2. **How does k affect bias-variance tradeoff?**\n",
    "3. **What preprocessing is essential for KNN?**\n",
    "4. **Compare KNN vs SVR for financial prediction.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
