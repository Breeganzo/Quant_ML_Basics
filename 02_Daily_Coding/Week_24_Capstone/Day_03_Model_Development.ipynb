{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5825f586",
   "metadata": {},
   "source": [
    "# Day 03: Model Development for Capstone\n",
    "\n",
    "## Week 24 - Capstone Project\n",
    "\n",
    "### Learning Objectives\n",
    "- Build a complete ML model pipeline for trading signal generation\n",
    "- Implement multiple model architectures (Linear, Tree, Neural Network)\n",
    "- Create ensemble methods for robust predictions\n",
    "- Apply proper cross-validation for time series\n",
    "- Track experiments and model versioning\n",
    "\n",
    "### Why Model Development Matters\n",
    "In production quant systems, model development is **iterative and disciplined**:\n",
    "- Start simple (baseline models)\n",
    "- Add complexity only when justified\n",
    "- Validate rigorously with time-series aware methods\n",
    "- Document everything for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data\n",
    "import yfinance as yf\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Typing\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fa944",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'GS']\n",
    "START_DATE = '2019-01-01'\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"ðŸ“¥ Downloading data for {len(TICKERS)} tickers...\")\n",
    "data = yf.download(TICKERS, start=START_DATE, end=END_DATE, progress=False, auto_adjust=True)\n",
    "prices = data['Close'].dropna()\n",
    "\n",
    "print(f\"âœ… Data loaded: {prices.shape[0]} days, {prices.shape[1]} tickers\")\n",
    "print(f\"ðŸ“… Date range: {prices.index[0].strftime('%Y-%m-%d')} to {prices.index[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(prices: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"Create features for ML model.\"\"\"\n",
    "    df = pd.DataFrame(index=prices.index)\n",
    "    price = prices[ticker]\n",
    "    \n",
    "    # Returns\n",
    "    df['returns'] = price.pct_change()\n",
    "    df['log_returns'] = np.log(price / price.shift(1))\n",
    "    \n",
    "    # Moving Averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df[f'sma_{window}'] = price.rolling(window).mean()\n",
    "        df[f'returns_sma_{window}'] = df['returns'].rolling(window).mean()\n",
    "    \n",
    "    # Volatility\n",
    "    for window in [10, 20, 60]:\n",
    "        df[f'volatility_{window}'] = df['returns'].rolling(window).std()\n",
    "    \n",
    "    # Momentum\n",
    "    for window in [5, 10, 20]:\n",
    "        df[f'momentum_{window}'] = price / price.shift(window) - 1\n",
    "    \n",
    "    # RSI\n",
    "    delta = price.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi_14'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = price.ewm(span=12).mean()\n",
    "    ema26 = price.ewm(span=26).mean()\n",
    "    df['macd'] = ema12 - ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    \n",
    "    # Target: Next day return direction\n",
    "    df['target'] = (df['returns'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    # Price relative to moving averages\n",
    "    df['price_sma_20_ratio'] = price / df['sma_20']\n",
    "    df['price_sma_50_ratio'] = price / df['sma_50']\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "# Create features for AAPL as example\n",
    "features_df = create_features(prices, 'AAPL')\n",
    "print(f\"\\nðŸ“Š Features created: {features_df.shape[1]} columns, {features_df.shape[0]} rows\")\n",
    "print(f\"\\nFeature columns: {list(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29944e",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65670713",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Container for model evaluation metrics.\"\"\"\n",
    "    accuracy: float\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    auc_roc: float\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'accuracy': self.accuracy,\n",
    "            'precision': self.precision,\n",
    "            'recall': self.recall,\n",
    "            'f1': self.f1,\n",
    "            'auc_roc': self.auc_roc\n",
    "        }\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"Abstract base class for all models.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> ModelMetrics:\n",
    "        \"\"\"Evaluate model performance.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        y_proba = self.predict_proba(X)[:, 1]\n",
    "        \n",
    "        return ModelMetrics(\n",
    "            accuracy=accuracy_score(y, y_pred),\n",
    "            precision=precision_score(y, y_pred, zero_division=0),\n",
    "            recall=recall_score(y, y_pred, zero_division=0),\n",
    "            f1=f1_score(y, y_pred, zero_division=0),\n",
    "            auc_roc=roc_auc_score(y, y_proba)\n",
    "        )\n",
    "\n",
    "\n",
    "class LogisticModel(BaseModel):\n",
    "    \"\"\"Logistic Regression baseline model.\"\"\"\n",
    "    \n",
    "    def __init__(self, C: float = 1.0):\n",
    "        self.model = LogisticRegression(C=C, max_iter=1000, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.name = \"Logistic Regression\"\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "\n",
    "class RandomForestModel(BaseModel):\n",
    "    \"\"\"Random Forest model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 100, max_depth: int = 10):\n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.name = \"Random Forest\"\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "\n",
    "class GradientBoostModel(BaseModel):\n",
    "    \"\"\"Gradient Boosting model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators: int = 100, learning_rate: float = 0.1):\n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.name = \"Gradient Boosting\"\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict_proba(X_scaled)\n",
    "\n",
    "print(\"âœ… Model classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66050ad4",
   "metadata": {},
   "source": [
    "## 3. Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingMLP(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron for trading signals.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32], dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 2))  # Binary classification\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class NeuralNetModel(BaseModel):\n",
    "    \"\"\"Neural Network wrapper for trading.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32], \n",
    "                 epochs: int = 100, lr: float = 0.001, batch_size: int = 32):\n",
    "        self.model = TradingMLP(input_dim, hidden_dims)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.name = \"Neural Network\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.LongTensor(y).to(self.device)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for X_batch, y_batch in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        return predicted.cpu().numpy()\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "print(\"âœ… Neural Network model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78290178",
   "metadata": {},
   "source": [
    "## 4. Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model: BaseModel, X: np.ndarray, y: np.ndarray, \n",
    "                   n_splits: int = 5) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Perform time series cross-validation.\n",
    "    \n",
    "    Uses expanding window to ensure no future data leakage.\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc_roc': []\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Create fresh model instance\n",
    "        if isinstance(model, LogisticModel):\n",
    "            fold_model = LogisticModel()\n",
    "        elif isinstance(model, RandomForestModel):\n",
    "            fold_model = RandomForestModel()\n",
    "        elif isinstance(model, GradientBoostModel):\n",
    "            fold_model = GradientBoostModel()\n",
    "        else:\n",
    "            fold_model = model\n",
    "        \n",
    "        fold_model.fit(X_train, y_train)\n",
    "        metrics = fold_model.evaluate(X_test, y_test)\n",
    "        \n",
    "        for key, value in metrics.to_dict().items():\n",
    "            results[key].append(value)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare data for cross-validation\n",
    "feature_cols = [c for c in features_df.columns if c not in ['target', 'returns', 'log_returns']]\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['target'].values\n",
    "\n",
    "print(f\"ðŸ“Š Feature matrix shape: {X.shape}\")\n",
    "print(f\"ðŸ“Š Target distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63f300",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = [\n",
    "    LogisticModel(),\n",
    "    RandomForestModel(),\n",
    "    GradientBoostModel()\n",
    "]\n",
    "\n",
    "# Run cross-validation for each model\n",
    "results_summary = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON - Time Series Cross-Validation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model in models:\n",
    "    print(f\"\\nâ³ Evaluating {model.name}...\")\n",
    "    cv_results = time_series_cv(model, X, y, n_splits=5)\n",
    "    \n",
    "    results_summary[model.name] = {\n",
    "        metric: f\"{np.mean(values):.4f} Â± {np.std(values):.4f}\"\n",
    "        for metric, values in cv_results.items()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Accuracy: {np.mean(cv_results['accuracy']):.4f} Â± {np.std(cv_results['accuracy']):.4f}\")\n",
    "    print(f\"   AUC-ROC:  {np.mean(cv_results['auc_roc']):.4f} Â± {np.std(cv_results['auc_roc']):.4f}\")\n",
    "    print(f\"   F1 Score: {np.mean(cv_results['f1']):.4f} Â± {np.std(cv_results['f1']):.4f}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "summary_df = pd.DataFrame(results_summary).T\n",
    "print(summary_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0dec1f",
   "metadata": {},
   "source": [
    "## 6. Train Final Model & Generate Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19892b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all data\n",
    "final_model = GradientBoostModel(n_estimators=100, learning_rate=0.1)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Generate predictions for all tickers\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ¯ TRADING SIGNALS FOR ALL TICKERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "\n",
    "all_signals = {}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    ticker_features = create_features(prices, ticker)\n",
    "    feature_cols = [c for c in ticker_features.columns if c not in ['target', 'returns', 'log_returns']]\n",
    "    \n",
    "    # Get last row for prediction\n",
    "    X_last = ticker_features[feature_cols].iloc[-1:].values\n",
    "    \n",
    "    # Predict probability\n",
    "    prob = final_model.predict_proba(X_last)[0, 1]\n",
    "    \n",
    "    # Determine signal\n",
    "    if prob > 0.60:\n",
    "        signal = \"ðŸŸ¢ STRONG BUY\"\n",
    "        action = \"Consider buying shares or CALL options\"\n",
    "    elif prob > 0.55:\n",
    "        signal = \"ðŸŸ¡ WEAK BUY\"\n",
    "        action = \"Consider small position\"\n",
    "    elif prob < 0.40:\n",
    "        signal = \"ðŸ”´ STRONG SELL\"\n",
    "        action = \"Consider selling or PUT options\"\n",
    "    elif prob < 0.45:\n",
    "        signal = \"ðŸŸ  WEAK SELL\"\n",
    "        action = \"Consider reducing position\"\n",
    "    else:\n",
    "        signal = \"âšª HOLD\"\n",
    "        action = \"No clear edge - maintain position\"\n",
    "    \n",
    "    all_signals[ticker] = {\n",
    "        'probability': prob,\n",
    "        'signal': signal,\n",
    "        'action': action,\n",
    "        'price': prices[ticker].iloc[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"{'='*30} {ticker} {'='*30}\")\n",
    "    print(f\"   Current Price: ${prices[ticker].iloc[-1]:.2f}\")\n",
    "    print(f\"   Up Probability: {prob:.2%}\")\n",
    "    print(f\"   Signal: {signal}\")\n",
    "    print(f\"   Action: {action}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd526d6",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### What We Built Today:\n",
    "1. **Feature Engineering Pipeline**: Technical indicators, momentum, volatility\n",
    "2. **Multiple Models**: Logistic Regression, Random Forest, Gradient Boosting\n",
    "3. **Time Series CV**: Proper validation without data leakage\n",
    "4. **Signal Generation**: Probability-based trading signals for all tickers\n",
    "\n",
    "### Tomorrow's Preview: Backtesting Framework\n",
    "- Full backtesting with transaction costs\n",
    "- Walk-forward optimization\n",
    "- Risk metrics (Sharpe, Sortino, Max Drawdown)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
