{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1286d936",
   "metadata": {},
   "source": [
    "# Day 02: Data Pipeline for Capstone Project\n",
    "\n",
    "## Week 24 - Capstone Project\n",
    "\n",
    "**Objective**: Build a robust, production-ready data pipeline for our ML trading system capstone project.\n",
    "\n",
    "### Topics Covered:\n",
    "1. Data Pipeline Architecture Overview\n",
    "2. Data Sources Configuration\n",
    "3. Data Extraction Layer (APIs, market data)\n",
    "4. Data Transformation Pipeline\n",
    "5. Data Validation and Quality Checks\n",
    "6. Feature Engineering Pipeline\n",
    "7. Data Storage and Caching\n",
    "8. Pipeline Orchestration\n",
    "9. End-to-End Pipeline Testing\n",
    "\n",
    "---\n",
    "\n",
    "### Why Data Pipelines Matter in Quant Finance\n",
    "\n",
    "A well-designed data pipeline is the foundation of any successful ML trading system:\n",
    "\n",
    "- **Reliability**: Consistent, clean data is critical for model performance\n",
    "- **Reproducibility**: Pipelines ensure experiments can be replicated\n",
    "- **Scalability**: Handle growing data volumes efficiently\n",
    "- **Automation**: Reduce manual intervention and human error\n",
    "- **Monitoring**: Track data quality and pipeline health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26014173",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Fetching\n",
    "import yfinance as yf\n",
    "\n",
    "# Data Validation\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# File I/O and Caching\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('DataPipeline')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c13296",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline Architecture\n",
    "\n",
    "### Pipeline Design Pattern\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                      DATA PIPELINE ARCHITECTURE                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ   ‚îÇ  DATA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  DATA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  DATA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ FEATURE  ‚îÇ     ‚îÇ\n",
    "‚îÇ   ‚îÇ SOURCES  ‚îÇ    ‚îÇEXTRACTION‚îÇ    ‚îÇTRANSFORM ‚îÇ    ‚îÇENGINEERING‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ        ‚îÇ               ‚îÇ               ‚îÇ               ‚îÇ            ‚îÇ\n",
    "‚îÇ        ‚ñº               ‚ñº               ‚ñº               ‚ñº            ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n",
    "‚îÇ   ‚îÇ  CONFIG  ‚îÇ    ‚îÇVALIDATION‚îÇ    ‚îÇ QUALITY  ‚îÇ    ‚îÇ  FEATURE ‚îÇ     ‚îÇ\n",
    "‚îÇ   ‚îÇ  LAYER   ‚îÇ    ‚îÇ  LAYER   ‚îÇ    ‚îÇ  CHECKS  ‚îÇ    ‚îÇ   STORE  ‚îÇ     ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n",
    "‚îÇ                                                                      ‚îÇ\n",
    "‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ\n",
    "‚îÇ                    ‚îÇ   ORCHESTRATION    ‚îÇ                           ‚îÇ\n",
    "‚îÇ                    ‚îÇ   & MONITORING     ‚îÇ                           ‚îÇ\n",
    "‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f9849",
   "metadata": {},
   "source": [
    "## 3. Define Data Sources and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d917563",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for the data pipeline.\"\"\"\n",
    "    \n",
    "    # Ticker Configuration\n",
    "    tickers: List[str] = field(default_factory=lambda: [\n",
    "        'SPY', 'QQQ', 'IWM',  # Major ETFs\n",
    "        'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META',  # Tech Giants\n",
    "        'JPM', 'GS', 'BAC',  # Financials\n",
    "        'XLE', 'GLD', 'TLT'  # Sectors/Commodities/Bonds\n",
    "    ])\n",
    "    \n",
    "    # Date Range\n",
    "    start_date: str = '2020-01-01'\n",
    "    end_date: str = '2025-12-31'\n",
    "    \n",
    "    # Data Frequency\n",
    "    frequency: str = '1d'  # '1d', '1h', '5m', etc.\n",
    "    \n",
    "    # Storage Configuration\n",
    "    data_dir: str = './pipeline_data'\n",
    "    cache_dir: str = './pipeline_cache'\n",
    "    \n",
    "    # Validation Thresholds\n",
    "    max_missing_pct: float = 0.05  # Max 5% missing data\n",
    "    min_trading_days: int = 252  # At least 1 year of data\n",
    "    max_price_change_pct: float = 0.50  # Flag >50% daily moves\n",
    "    \n",
    "    # Feature Engineering\n",
    "    lookback_windows: List[int] = field(default_factory=lambda: [5, 10, 20, 60, 120, 252])\n",
    "    \n",
    "    # Pipeline Settings\n",
    "    enable_caching: bool = True\n",
    "    parallel_downloads: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        Path(self.data_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "config = PipelineConfig()\n",
    "print(f\"üìã Pipeline Configuration:\")\n",
    "print(f\"   Tickers: {len(config.tickers)} symbols\")\n",
    "print(f\"   Date Range: {config.start_date} to {config.end_date}\")\n",
    "print(f\"   Data Directory: {config.data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aeca33",
   "metadata": {},
   "source": [
    "## 4. Create Data Extraction Functions\n",
    "\n",
    "### 4.1 Base Data Extractor Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079423a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor(ABC):\n",
    "    \"\"\"Abstract base class for data extractors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, symbols: List[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data for given symbols and date range.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_source_name(self) -> str:\n",
    "        \"\"\"Return the name of the data source.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class YFinanceExtractor(DataExtractor):\n",
    "    \"\"\"Extract market data using yfinance API.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('YFinanceExtractor')\n",
    "    \n",
    "    def get_source_name(self) -> str:\n",
    "        return \"Yahoo Finance\"\n",
    "    \n",
    "    def extract(self, symbols: List[str], start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract OHLCV data for multiple symbols.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with MultiIndex columns (ticker, field)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Extracting data for {len(symbols)} symbols...\")\n",
    "        \n",
    "        all_data = {}\n",
    "        failed_symbols = []\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                ticker = yf.Ticker(symbol)\n",
    "                df = ticker.history(start=start_date, end=end_date, interval=self.config.frequency)\n",
    "                \n",
    "                if df.empty:\n",
    "                    self.logger.warning(f\"No data returned for {symbol}\")\n",
    "                    failed_symbols.append(symbol)\n",
    "                    continue\n",
    "                \n",
    "                # Standardize column names\n",
    "                df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "                \n",
    "                # Keep essential columns\n",
    "                essential_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "                available_cols = [col for col in essential_cols if col in df.columns]\n",
    "                df = df[available_cols]\n",
    "                \n",
    "                all_data[symbol] = df\n",
    "                self.logger.info(f\"‚úì {symbol}: {len(df)} records\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error extracting {symbol}: {str(e)}\")\n",
    "                failed_symbols.append(symbol)\n",
    "        \n",
    "        if not all_data:\n",
    "            raise ValueError(\"No data extracted for any symbol\")\n",
    "        \n",
    "        # Combine into single DataFrame with MultiIndex columns\n",
    "        combined_df = pd.concat(all_data, axis=1)\n",
    "        combined_df.index = pd.to_datetime(combined_df.index).tz_localize(None)\n",
    "        \n",
    "        self.logger.info(f\"‚úÖ Extraction complete: {len(all_data)} symbols, {len(combined_df)} records\")\n",
    "        if failed_symbols:\n",
    "            self.logger.warning(f\"Failed symbols: {failed_symbols}\")\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def extract_single(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract data for a single symbol with additional metadata.\"\"\"\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        \n",
    "        # Get price data\n",
    "        price_data = ticker.history(start=start_date, end=end_date, interval=self.config.frequency)\n",
    "        price_data.columns = price_data.columns.str.lower().str.replace(' ', '_')\n",
    "        \n",
    "        # Get fundamental info (for enrichment)\n",
    "        try:\n",
    "            info = ticker.info\n",
    "            sector = info.get('sector', 'Unknown')\n",
    "            industry = info.get('industry', 'Unknown')\n",
    "            market_cap = info.get('marketCap', np.nan)\n",
    "        except:\n",
    "            sector, industry, market_cap = 'Unknown', 'Unknown', np.nan\n",
    "        \n",
    "        # Add metadata columns\n",
    "        price_data['symbol'] = symbol\n",
    "        price_data['sector'] = sector\n",
    "        price_data['industry'] = industry\n",
    "        price_data['market_cap'] = market_cap\n",
    "        \n",
    "        return price_data\n",
    "\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = YFinanceExtractor(config)\n",
    "print(f\"‚úÖ Data Extractor initialized: {extractor.get_source_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c229cb1",
   "metadata": {},
   "source": [
    "## 5. Implement Data Transformation Pipeline\n",
    "\n",
    "### 5.1 Data Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"Transform and clean raw market data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('DataTransformer')\n",
    "    \n",
    "    def handle_missing_values(self, df: pd.DataFrame, method: str = 'ffill') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle missing values in the dataset.\n",
    "        \n",
    "        Methods:\n",
    "        - 'ffill': Forward fill (appropriate for prices)\n",
    "        - 'interpolate': Linear interpolation\n",
    "        - 'drop': Drop rows with missing values\n",
    "        \"\"\"\n",
    "        missing_before = df.isna().sum().sum()\n",
    "        \n",
    "        if method == 'ffill':\n",
    "            df = df.ffill().bfill()  # Forward fill, then backfill for start\n",
    "        elif method == 'interpolate':\n",
    "            df = df.interpolate(method='time')\n",
    "        elif method == 'drop':\n",
    "            df = df.dropna()\n",
    "        \n",
    "        missing_after = df.isna().sum().sum()\n",
    "        self.logger.info(f\"Missing values: {missing_before} ‚Üí {missing_after}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_outliers(self, series: pd.Series, method: str = 'zscore', \n",
    "                        threshold: float = 3.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect outliers in a series.\n",
    "        \n",
    "        Methods:\n",
    "        - 'zscore': Z-score method\n",
    "        - 'iqr': Interquartile range method\n",
    "        - 'mad': Median absolute deviation\n",
    "        \"\"\"\n",
    "        if method == 'zscore':\n",
    "            z_scores = np.abs((series - series.mean()) / series.std())\n",
    "            return z_scores > threshold\n",
    "        \n",
    "        elif method == 'iqr':\n",
    "            Q1, Q3 = series.quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            return (series < lower_bound) | (series > upper_bound)\n",
    "        \n",
    "        elif method == 'mad':\n",
    "            median = series.median()\n",
    "            mad = np.median(np.abs(series - median))\n",
    "            modified_z = 0.6745 * (series - median) / mad\n",
    "            return np.abs(modified_z) > threshold\n",
    "        \n",
    "        return pd.Series(False, index=series.index)\n",
    "    \n",
    "    def handle_outliers(self, df: pd.DataFrame, columns: List[str] = None,\n",
    "                        method: str = 'clip') -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Handle outliers in specified columns.\n",
    "        \n",
    "        Methods:\n",
    "        - 'clip': Clip to percentile bounds (winsorize)\n",
    "        - 'remove': Remove outlier rows\n",
    "        - 'replace': Replace with median\n",
    "        \"\"\"\n",
    "        if columns is None:\n",
    "            # Only apply to return columns\n",
    "            columns = [col for col in df.columns if 'return' in str(col).lower()]\n",
    "        \n",
    "        outlier_report = {}\n",
    "        \n",
    "        for col in columns:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            outliers = self.detect_outliers(df[col])\n",
    "            outlier_count = outliers.sum()\n",
    "            outlier_report[col] = outlier_count\n",
    "            \n",
    "            if outlier_count > 0:\n",
    "                if method == 'clip':\n",
    "                    lower = df[col].quantile(0.01)\n",
    "                    upper = df[col].quantile(0.99)\n",
    "                    df[col] = df[col].clip(lower, upper)\n",
    "                elif method == 'replace':\n",
    "                    median = df[col].median()\n",
    "                    df.loc[outliers, col] = median\n",
    "                elif method == 'remove':\n",
    "                    df = df[~outliers]\n",
    "        \n",
    "        self.logger.info(f\"Outliers handled: {sum(outlier_report.values())} total\")\n",
    "        return df, outlier_report\n",
    "    \n",
    "    def normalize_data(self, df: pd.DataFrame, columns: List[str],\n",
    "                       method: str = 'zscore') -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Normalize specified columns.\n",
    "        \n",
    "        Methods:\n",
    "        - 'zscore': Standardization (mean=0, std=1)\n",
    "        - 'minmax': Min-Max scaling [0, 1]\n",
    "        - 'robust': Robust scaling using median/IQR\n",
    "        \"\"\"\n",
    "        scalers = {}\n",
    "        df_normalized = df.copy()\n",
    "        \n",
    "        for col in columns:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            if method == 'zscore':\n",
    "                scaler = StandardScaler()\n",
    "            elif method == 'minmax':\n",
    "                scaler = MinMaxScaler()\n",
    "            elif method == 'robust':\n",
    "                scaler = RobustScaler()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown normalization method: {method}\")\n",
    "            \n",
    "            values = df[col].values.reshape(-1, 1)\n",
    "            df_normalized[col] = scaler.fit_transform(values).flatten()\n",
    "            scalers[col] = scaler\n",
    "        \n",
    "        return df_normalized, scalers\n",
    "    \n",
    "    def resample_data(self, df: pd.DataFrame, frequency: str = 'W') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Resample time series data to different frequency.\n",
    "        \n",
    "        Frequencies:\n",
    "        - 'D': Daily\n",
    "        - 'W': Weekly\n",
    "        - 'M': Monthly\n",
    "        - 'Q': Quarterly\n",
    "        \"\"\"\n",
    "        # OHLCV aggregation rules\n",
    "        agg_rules = {\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }\n",
    "        \n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            # Handle MultiIndex columns\n",
    "            resampled = {}\n",
    "            for symbol in df.columns.get_level_values(0).unique():\n",
    "                symbol_df = df[symbol]\n",
    "                symbol_agg = {col: agg_rules.get(col, 'last') \n",
    "                            for col in symbol_df.columns if col in agg_rules}\n",
    "                resampled[symbol] = symbol_df.resample(frequency).agg(symbol_agg)\n",
    "            return pd.concat(resampled, axis=1)\n",
    "        else:\n",
    "            available_rules = {col: agg_rules.get(col, 'last') \n",
    "                            for col in df.columns if col in agg_rules}\n",
    "            return df.resample(frequency).agg(available_rules)\n",
    "\n",
    "\n",
    "# Initialize transformer\n",
    "transformer = DataTransformer(config)\n",
    "print(\"‚úÖ Data Transformer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36074745",
   "metadata": {},
   "source": [
    "## 6. Build Data Validation Layer\n",
    "\n",
    "### 6.1 Validation Rules and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e6793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a validation check.\"\"\"\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    message: str\n",
    "    details: Dict = field(default_factory=dict)\n",
    "    severity: str = 'error'  # 'error', 'warning', 'info'\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validate data quality and integrity.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger('DataValidator')\n",
    "        self.validation_results: List[ValidationResult] = []\n",
    "    \n",
    "    def check_missing_values(self, df: pd.DataFrame, threshold: float = None) -> ValidationResult:\n",
    "        \"\"\"Check for missing values exceeding threshold.\"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = self.config.max_missing_pct\n",
    "        \n",
    "        missing_pct = df.isna().sum().sum() / df.size\n",
    "        passed = missing_pct <= threshold\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name=\"missing_values\",\n",
    "            passed=passed,\n",
    "            message=f\"Missing values: {missing_pct:.2%} (threshold: {threshold:.2%})\",\n",
    "            details={'missing_pct': missing_pct, 'threshold': threshold},\n",
    "            severity='error' if not passed else 'info'\n",
    "        )\n",
    "        self.validation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_date_continuity(self, df: pd.DataFrame, max_gap_days: int = 5) -> ValidationResult:\n",
    "        \"\"\"Check for gaps in date series.\"\"\"\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            return ValidationResult(\n",
    "                check_name=\"date_continuity\",\n",
    "                passed=False,\n",
    "                message=\"Index is not DatetimeIndex\",\n",
    "                severity='error'\n",
    "            )\n",
    "        \n",
    "        date_diffs = df.index.to_series().diff().dt.days\n",
    "        max_gap = date_diffs.max()\n",
    "        gap_dates = df.index[date_diffs > max_gap_days].tolist()\n",
    "        \n",
    "        passed = len(gap_dates) == 0\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name=\"date_continuity\",\n",
    "            passed=passed,\n",
    "            message=f\"Max date gap: {max_gap} days, Gaps > {max_gap_days} days: {len(gap_dates)}\",\n",
    "            details={'max_gap': max_gap, 'gap_dates': gap_dates[:5]},  # First 5 gaps\n",
    "            severity='warning' if not passed else 'info'\n",
    "        )\n",
    "        self.validation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_price_sanity(self, df: pd.DataFrame, price_col: str = 'close') -> ValidationResult:\n",
    "        \"\"\"Check for unrealistic price movements.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check for negative prices\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            for symbol in df.columns.get_level_values(0).unique():\n",
    "                if (symbol, price_col) in df.columns:\n",
    "                    prices = df[(symbol, price_col)]\n",
    "                    if (prices <= 0).any():\n",
    "                        issues.append(f\"{symbol}: negative/zero prices\")\n",
    "                    \n",
    "                    returns = prices.pct_change()\n",
    "                    extreme_moves = (returns.abs() > self.config.max_price_change_pct).sum()\n",
    "                    if extreme_moves > 0:\n",
    "                        issues.append(f\"{symbol}: {extreme_moves} moves > {self.config.max_price_change_pct:.0%}\")\n",
    "        else:\n",
    "            if price_col in df.columns:\n",
    "                prices = df[price_col]\n",
    "                if (prices <= 0).any():\n",
    "                    issues.append(\"Negative/zero prices found\")\n",
    "                \n",
    "                returns = prices.pct_change()\n",
    "                extreme_moves = (returns.abs() > self.config.max_price_change_pct).sum()\n",
    "                if extreme_moves > 0:\n",
    "                    issues.append(f\"{extreme_moves} moves > {self.config.max_price_change_pct:.0%}\")\n",
    "        \n",
    "        passed = len(issues) == 0\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name=\"price_sanity\",\n",
    "            passed=passed,\n",
    "            message=f\"Price sanity check: {len(issues)} issues found\",\n",
    "            details={'issues': issues},\n",
    "            severity='warning' if not passed else 'info'\n",
    "        )\n",
    "        self.validation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_minimum_history(self, df: pd.DataFrame, min_days: int = None) -> ValidationResult:\n",
    "        \"\"\"Check for minimum historical data.\"\"\"\n",
    "        if min_days is None:\n",
    "            min_days = self.config.min_trading_days\n",
    "        \n",
    "        actual_days = len(df)\n",
    "        passed = actual_days >= min_days\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name=\"minimum_history\",\n",
    "            passed=passed,\n",
    "            message=f\"Trading days: {actual_days} (minimum: {min_days})\",\n",
    "            details={'actual_days': actual_days, 'min_days': min_days},\n",
    "            severity='error' if not passed else 'info'\n",
    "        )\n",
    "        self.validation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def check_schema(self, df: pd.DataFrame, required_columns: List[str]) -> ValidationResult:\n",
    "        \"\"\"Check if required columns exist.\"\"\"\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            available_cols = df.columns.get_level_values(1).unique().tolist()\n",
    "        else:\n",
    "            available_cols = df.columns.tolist()\n",
    "        \n",
    "        missing_cols = [col for col in required_columns if col not in available_cols]\n",
    "        passed = len(missing_cols) == 0\n",
    "        \n",
    "        result = ValidationResult(\n",
    "            check_name=\"schema_validation\",\n",
    "            passed=passed,\n",
    "            message=f\"Schema check: {len(missing_cols)} missing columns\",\n",
    "            details={'missing_columns': missing_cols, 'available_columns': available_cols},\n",
    "            severity='error' if not passed else 'info'\n",
    "        )\n",
    "        self.validation_results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def validate_all(self, df: pd.DataFrame) -> Tuple[bool, List[ValidationResult]]:\n",
    "        \"\"\"Run all validation checks.\"\"\"\n",
    "        self.validation_results = []\n",
    "        \n",
    "        # Run all checks\n",
    "        self.check_missing_values(df)\n",
    "        self.check_date_continuity(df)\n",
    "        self.check_price_sanity(df)\n",
    "        self.check_minimum_history(df)\n",
    "        self.check_schema(df, ['open', 'high', 'low', 'close', 'volume'])\n",
    "        \n",
    "        # Determine overall pass/fail\n",
    "        errors = [r for r in self.validation_results if r.severity == 'error' and not r.passed]\n",
    "        all_passed = len(errors) == 0\n",
    "        \n",
    "        return all_passed, self.validation_results\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print validation report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä DATA VALIDATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for result in self.validation_results:\n",
    "            status = \"‚úÖ\" if result.passed else (\"‚ö†Ô∏è\" if result.severity == 'warning' else \"‚ùå\")\n",
    "            print(f\"{status} {result.check_name}: {result.message}\")\n",
    "        \n",
    "        errors = sum(1 for r in self.validation_results if not r.passed and r.severity == 'error')\n",
    "        warnings = sum(1 for r in self.validation_results if not r.passed and r.severity == 'warning')\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Summary: {errors} errors, {warnings} warnings\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# Initialize validator\n",
    "validator = DataValidator(config)\n",
    "print(\"‚úÖ Data Validator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c827f65",
   "metadata": {},
   "source": [
    "## 7. Create Feature Engineering Pipeline\n",
    "\n",
    "### 7.1 Technical Indicators and Features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
