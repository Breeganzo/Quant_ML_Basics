{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1043e107",
   "metadata": {},
   "source": [
    "# Day 01: Capstone Project Planning & Requirements\n",
    "\n",
    "## Week 24 - Bringing It All Together\n",
    "\n",
    "### Learning Objectives\n",
    "- Define project scope, objectives, and success criteria for a quant ML trading system\n",
    "- Create a structured project architecture with proper directory organization\n",
    "- Establish requirements, dependencies, and configuration frameworks\n",
    "- Design data pipelines and feature engineering specifications\n",
    "- Build project timelines with milestones and deliverables\n",
    "- Set up logging, testing, and documentation standards\n",
    "\n",
    "### Why Project Planning Matters in Quant Finance\n",
    "In production quantitative finance, **poor planning is the #1 cause of project failure**. Unlike academic projects, real trading systems must handle:\n",
    "- **Live market data** with strict latency requirements\n",
    "- **Regulatory compliance** and audit trails\n",
    "- **Risk management** integration\n",
    "- **24/7 reliability** for global markets\n",
    "- **Version control** for models and strategies\n",
    "\n",
    "*\"Weeks of coding can save you hours of planning.\"* - Anonymous Quant Developer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034f6ef",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Define Project Scope and Objectives\n",
    "\n",
    "### The SMART Framework for Quant Projects\n",
    "Every successful capstone project starts with clearly defined goals using the **SMART** framework:\n",
    "- **S**pecific: What exactly are you building?\n",
    "- **M**easurable: How will you quantify success?\n",
    "- **A**chievable: Is it realistic given your timeline and skills?\n",
    "- **R**elevant: Does it demonstrate marketable quant skills?\n",
    "- **T**ime-bound: What are your milestones and deadlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa34def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Definition Framework\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import json\n",
    "\n",
    "class ProjectType(Enum):\n",
    "    \"\"\"Available capstone project types\"\"\"\n",
    "    MULTI_ASSET_ML_TRADING = \"Multi-Asset ML Trading System\"\n",
    "    DEEP_HEDGING_OPTIONS = \"Deep Hedging & Options Market Making\"\n",
    "    NLP_ALTERNATIVE_DATA = \"Alternative Data NLP Alpha Research\"\n",
    "    HFT_MICROSTRUCTURE = \"HFT Microstructure & Execution Optimization\"\n",
    "\n",
    "class Priority(Enum):\n",
    "    CRITICAL = \"P0 - Critical\"\n",
    "    HIGH = \"P1 - High\"\n",
    "    MEDIUM = \"P2 - Medium\"\n",
    "    LOW = \"P3 - Low\"\n",
    "\n",
    "@dataclass\n",
    "class SuccessMetric:\n",
    "    \"\"\"Define measurable success criteria\"\"\"\n",
    "    name: str\n",
    "    target_value: float\n",
    "    unit: str\n",
    "    minimum_acceptable: float\n",
    "    measurement_method: str\n",
    "    \n",
    "    def evaluate(self, actual_value: float) -> dict:\n",
    "        \"\"\"Evaluate if metric meets success criteria\"\"\"\n",
    "        return {\n",
    "            \"metric\": self.name,\n",
    "            \"target\": f\"{self.target_value} {self.unit}\",\n",
    "            \"actual\": f\"{actual_value} {self.unit}\",\n",
    "            \"meets_minimum\": actual_value >= self.minimum_acceptable,\n",
    "            \"meets_target\": actual_value >= self.target_value,\n",
    "            \"achievement_rate\": min(actual_value / self.target_value * 100, 100) if self.target_value > 0 else 0\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class Deliverable:\n",
    "    \"\"\"Project deliverable with acceptance criteria\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    acceptance_criteria: List[str]\n",
    "    due_date: datetime\n",
    "    priority: Priority\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    completed: bool = False\n",
    "\n",
    "@dataclass\n",
    "class CapstoneProject:\n",
    "    \"\"\"Complete capstone project definition\"\"\"\n",
    "    # Basic Info\n",
    "    project_name: str\n",
    "    project_type: ProjectType\n",
    "    author: str\n",
    "    start_date: datetime\n",
    "    end_date: datetime\n",
    "    \n",
    "    # Scope Definition\n",
    "    problem_statement: str\n",
    "    objectives: List[str]\n",
    "    out_of_scope: List[str]\n",
    "    \n",
    "    # Success Criteria\n",
    "    success_metrics: List[SuccessMetric]\n",
    "    deliverables: List[Deliverable]\n",
    "    \n",
    "    # Technical Specs\n",
    "    target_markets: List[str]\n",
    "    data_sources: List[str]\n",
    "    ml_models: List[str]\n",
    "    \n",
    "    # Constraints\n",
    "    constraints: Dict[str, str] = field(default_factory=dict)\n",
    "    assumptions: List[str] = field(default_factory=list)\n",
    "    risks: List[Dict] = field(default_factory=list)\n",
    "    \n",
    "    def to_charter(self) -> str:\n",
    "        \"\"\"Generate project charter document\"\"\"\n",
    "        charter = f\"\"\"\n",
    "{'='*70}\n",
    "                    PROJECT CHARTER\n",
    "{'='*70}\n",
    "\n",
    "PROJECT NAME: {self.project_name}\n",
    "PROJECT TYPE: {self.project_type.value}\n",
    "AUTHOR: {self.author}\n",
    "DURATION: {self.start_date.strftime('%Y-%m-%d')} to {self.end_date.strftime('%Y-%m-%d')}\n",
    "         ({(self.end_date - self.start_date).days} days)\n",
    "\n",
    "{'='*70}\n",
    "                    PROBLEM STATEMENT\n",
    "{'='*70}\n",
    "{self.problem_statement}\n",
    "\n",
    "{'='*70}\n",
    "                    OBJECTIVES\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        for i, obj in enumerate(self.objectives, 1):\n",
    "            charter += f\"{i}. {obj}\\n\"\n",
    "        \n",
    "        charter += f\"\"\"\n",
    "{'='*70}\n",
    "                    OUT OF SCOPE\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        for item in self.out_of_scope:\n",
    "            charter += f\"â€¢ {item}\\n\"\n",
    "        \n",
    "        charter += f\"\"\"\n",
    "{'='*70}\n",
    "                    SUCCESS METRICS\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        for metric in self.success_metrics:\n",
    "            charter += f\"â€¢ {metric.name}: Target {metric.target_value} {metric.unit} (Min: {metric.minimum_acceptable} {metric.unit})\\n\"\n",
    "        \n",
    "        charter += f\"\"\"\n",
    "{'='*70}\n",
    "                    DELIVERABLES\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "        for d in self.deliverables:\n",
    "            charter += f\"\\n[{d.priority.value}] {d.name}\\n\"\n",
    "            charter += f\"    Due: {d.due_date.strftime('%Y-%m-%d')}\\n\"\n",
    "            charter += f\"    {d.description}\\n\"\n",
    "            charter += \"    Acceptance Criteria:\\n\"\n",
    "            for ac in d.acceptance_criteria:\n",
    "                charter += f\"      âœ“ {ac}\\n\"\n",
    "        \n",
    "        return charter\n",
    "\n",
    "print(\"âœ… Project Definition Framework loaded successfully!\")\n",
    "print(\"\\nAvailable Project Types:\")\n",
    "for pt in ProjectType:\n",
    "    print(f\"  â€¢ {pt.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae48ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Define a Multi-Asset ML Trading System Capstone\n",
    "\n",
    "# Define success metrics\n",
    "success_metrics = [\n",
    "    SuccessMetric(\n",
    "        name=\"Sharpe Ratio\",\n",
    "        target_value=1.5,\n",
    "        unit=\"\",\n",
    "        minimum_acceptable=1.0,\n",
    "        measurement_method=\"Annualized Sharpe on out-of-sample backtest (2023-2024)\"\n",
    "    ),\n",
    "    SuccessMetric(\n",
    "        name=\"Maximum Drawdown\",\n",
    "        target_value=15.0,\n",
    "        unit=\"%\",\n",
    "        minimum_acceptable=25.0,  # Lower is better, so min acceptable is higher\n",
    "        measurement_method=\"Peak-to-trough drawdown during backtest period\"\n",
    "    ),\n",
    "    SuccessMetric(\n",
    "        name=\"Win Rate\",\n",
    "        target_value=55.0,\n",
    "        unit=\"%\",\n",
    "        minimum_acceptable=50.0,\n",
    "        measurement_method=\"Percentage of profitable trades\"\n",
    "    ),\n",
    "    SuccessMetric(\n",
    "        name=\"Code Coverage\",\n",
    "        target_value=80.0,\n",
    "        unit=\"%\",\n",
    "        minimum_acceptable=70.0,\n",
    "        measurement_method=\"Unit test coverage via pytest-cov\"\n",
    "    ),\n",
    "    SuccessMetric(\n",
    "        name=\"Prediction Latency\",\n",
    "        target_value=100.0,\n",
    "        unit=\"ms\",\n",
    "        minimum_acceptable=500.0,\n",
    "        measurement_method=\"P95 inference time on CPU\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define deliverables with dates\n",
    "project_start = datetime(2026, 1, 24)\n",
    "deliverables = [\n",
    "    Deliverable(\n",
    "        name=\"Project Charter & Architecture\",\n",
    "        description=\"Complete project specification with system design diagrams\",\n",
    "        acceptance_criteria=[\n",
    "            \"SMART objectives defined\",\n",
    "            \"System architecture diagram created\",\n",
    "            \"Data flow documented\",\n",
    "            \"Risk assessment completed\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=1),\n",
    "        priority=Priority.CRITICAL\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"Data Pipeline\",\n",
    "        description=\"Automated data ingestion, cleaning, and feature engineering pipeline\",\n",
    "        acceptance_criteria=[\n",
    "            \"Fetches data from at least 2 sources\",\n",
    "            \"Handles missing data appropriately\",\n",
    "            \"Generates 20+ features\",\n",
    "            \"Includes data validation checks\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=2),\n",
    "        priority=Priority.CRITICAL,\n",
    "        dependencies=[\"Project Charter & Architecture\"]\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"ML Model Training Pipeline\",\n",
    "        description=\"Complete ML training workflow with hyperparameter tuning\",\n",
    "        acceptance_criteria=[\n",
    "            \"Implements at least 3 model types\",\n",
    "            \"Uses cross-validation\",\n",
    "            \"Includes hyperparameter optimization\",\n",
    "            \"Saves model artifacts with versioning\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=3),\n",
    "        priority=Priority.HIGH,\n",
    "        dependencies=[\"Data Pipeline\"]\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"Backtesting Framework\",\n",
    "        description=\"Event-driven backtesting engine with realistic execution simulation\",\n",
    "        acceptance_criteria=[\n",
    "            \"Accounts for transaction costs\",\n",
    "            \"Implements slippage modeling\",\n",
    "            \"Generates comprehensive performance metrics\",\n",
    "            \"Avoids lookahead bias\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=4),\n",
    "        priority=Priority.HIGH,\n",
    "        dependencies=[\"ML Model Training Pipeline\"]\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"Risk Management Module\",\n",
    "        description=\"Position sizing, portfolio constraints, and risk monitoring\",\n",
    "        acceptance_criteria=[\n",
    "            \"Implements position limits\",\n",
    "            \"VaR/CVaR calculations\",\n",
    "            \"Drawdown monitoring\",\n",
    "            \"Correlation-aware allocation\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=5),\n",
    "        priority=Priority.HIGH,\n",
    "        dependencies=[\"Backtesting Framework\"]\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"Production-Ready Package\",\n",
    "        description=\"Deployable system with documentation and tests\",\n",
    "        acceptance_criteria=[\n",
    "            \"70%+ test coverage\",\n",
    "            \"Docker containerization\",\n",
    "            \"API endpoints documented\",\n",
    "            \"README with setup instructions\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=6),\n",
    "        priority=Priority.MEDIUM,\n",
    "        dependencies=[\"Risk Management Module\"]\n",
    "    ),\n",
    "    Deliverable(\n",
    "        name=\"Portfolio Presentation\",\n",
    "        description=\"GitHub repository ready for job applications\",\n",
    "        acceptance_criteria=[\n",
    "            \"Clean commit history\",\n",
    "            \"Professional README with results\",\n",
    "            \"Interactive demo/notebook\",\n",
    "            \"Performance visualizations\"\n",
    "        ],\n",
    "        due_date=project_start + timedelta(days=7),\n",
    "        priority=Priority.MEDIUM,\n",
    "        dependencies=[\"Production-Ready Package\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the project definition\n",
    "ml_trading_project = CapstoneProject(\n",
    "    project_name=\"Alpha Signals: Multi-Asset ML Trading System\",\n",
    "    project_type=ProjectType.MULTI_ASSET_ML_TRADING,\n",
    "    author=\"Your Name\",\n",
    "    start_date=project_start,\n",
    "    end_date=project_start + timedelta(days=7),\n",
    "    \n",
    "    problem_statement=\"\"\"\n",
    "    Retail and institutional investors struggle to systematically identify \n",
    "    alpha-generating opportunities across multiple asset classes. This project \n",
    "    develops an ML-driven trading system that combines technical indicators, \n",
    "    fundamental factors, and cross-asset signals to generate risk-adjusted \n",
    "    returns superior to passive benchmarks.\n",
    "    \"\"\",\n",
    "    \n",
    "    objectives=[\n",
    "        \"Build a modular data pipeline supporting equities, ETFs, and crypto\",\n",
    "        \"Implement ensemble ML models (XGBoost, LSTM, Transformer) for return prediction\",\n",
    "        \"Create an event-driven backtesting engine with realistic execution modeling\",\n",
    "        \"Develop risk management module with dynamic position sizing\",\n",
    "        \"Achieve Sharpe ratio > 1.5 on out-of-sample data (2023-2024)\",\n",
    "        \"Package as production-ready Python module with Docker deployment\"\n",
    "    ],\n",
    "    \n",
    "    out_of_scope=[\n",
    "        \"Live trading integration (paper trading only)\",\n",
    "        \"High-frequency strategies (< 1 minute horizon)\",\n",
    "        \"Options and derivatives trading\",\n",
    "        \"Multi-account portfolio management\",\n",
    "        \"Regulatory compliance implementation\"\n",
    "    ],\n",
    "    \n",
    "    success_metrics=success_metrics,\n",
    "    deliverables=deliverables,\n",
    "    \n",
    "    target_markets=[\"US Equities (S&P 500)\", \"Sector ETFs\", \"Major Crypto (BTC, ETH)\"],\n",
    "    data_sources=[\"Yahoo Finance\", \"Alpha Vantage\", \"FRED Economic Data\"],\n",
    "    ml_models=[\"XGBoost\", \"LightGBM\", \"LSTM\", \"Transformer Encoder\"],\n",
    "    \n",
    "    constraints={\n",
    "        \"Timeline\": \"7 days for complete implementation\",\n",
    "        \"Compute\": \"Standard laptop/Colab (no GPU required for inference)\",\n",
    "        \"Data\": \"Free data sources only (no Bloomberg/Refinitiv)\",\n",
    "        \"Deployment\": \"Docker container, no cloud infrastructure required\"\n",
    "    },\n",
    "    \n",
    "    assumptions=[\n",
    "        \"Historical patterns have some predictive power for future returns\",\n",
    "        \"Transaction costs are approximately 10 bps for equities\",\n",
    "        \"Sufficient liquidity for position sizes up to $100K notional\",\n",
    "        \"No major market regime changes during backtest period\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the project charter\n",
    "print(ml_trading_project.to_charter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb85ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Set Up Project Directory Structure\n",
    "\n",
    "### Professional Project Organization\n",
    "A well-organized project structure is crucial for:\n",
    "- **Maintainability**: Easy to navigate and modify\n",
    "- **Collaboration**: Team members understand the layout instantly\n",
    "- **Production Readiness**: Follows Python packaging best practices\n",
    "- **Interview Showcase**: Demonstrates professional software engineering skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe53bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Structure Generator\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class ProjectStructureGenerator:\n",
    "    \"\"\"Generate professional project directory structure\"\"\"\n",
    "    \n",
    "    # Standard structure for ML trading projects\n",
    "    STANDARD_STRUCTURE = {\n",
    "        \"src\": {\n",
    "            \"__init__.py\": \"# Alpha Signals Trading System\",\n",
    "            \"data\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"fetchers.py\": \"# Data fetching modules\",\n",
    "                \"processors.py\": \"# Data cleaning and processing\",\n",
    "                \"features.py\": \"# Feature engineering\",\n",
    "                \"validators.py\": \"# Data validation\"\n",
    "            },\n",
    "            \"models\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"base.py\": \"# Base model interface\",\n",
    "                \"ensemble.py\": \"# Ensemble models (XGBoost, LightGBM)\",\n",
    "                \"deep_learning.py\": \"# Neural network models\",\n",
    "                \"factory.py\": \"# Model factory pattern\"\n",
    "            },\n",
    "            \"strategy\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"signals.py\": \"# Signal generation\",\n",
    "                \"portfolio.py\": \"# Portfolio construction\",\n",
    "                \"execution.py\": \"# Order execution logic\"\n",
    "            },\n",
    "            \"backtest\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"engine.py\": \"# Backtesting engine\",\n",
    "                \"metrics.py\": \"# Performance metrics\",\n",
    "                \"visualization.py\": \"# Results plotting\"\n",
    "            },\n",
    "            \"risk\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"position_sizing.py\": \"# Position sizing algorithms\",\n",
    "                \"var.py\": \"# VaR calculations\",\n",
    "                \"monitoring.py\": \"# Risk monitoring\"\n",
    "            },\n",
    "            \"utils\": {\n",
    "                \"__init__.py\": \"\",\n",
    "                \"logging.py\": \"# Logging utilities\",\n",
    "                \"config.py\": \"# Configuration management\",\n",
    "                \"database.py\": \"# Database connections\"\n",
    "            }\n",
    "        },\n",
    "        \"tests\": {\n",
    "            \"__init__.py\": \"\",\n",
    "            \"test_data.py\": \"# Data pipeline tests\",\n",
    "            \"test_models.py\": \"# Model tests\",\n",
    "            \"test_backtest.py\": \"# Backtesting tests\",\n",
    "            \"test_risk.py\": \"# Risk module tests\",\n",
    "            \"conftest.py\": \"# Pytest fixtures\"\n",
    "        },\n",
    "        \"notebooks\": {\n",
    "            \"01_data_exploration.ipynb\": None,\n",
    "            \"02_feature_engineering.ipynb\": None,\n",
    "            \"03_model_development.ipynb\": None,\n",
    "            \"04_backtesting_results.ipynb\": None,\n",
    "            \"05_final_presentation.ipynb\": None\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"raw\": {\".gitkeep\": \"\"},\n",
    "            \"processed\": {\".gitkeep\": \"\"},\n",
    "            \"features\": {\".gitkeep\": \"\"},\n",
    "            \"models\": {\".gitkeep\": \"\"}\n",
    "        },\n",
    "        \"configs\": {\n",
    "            \"config.yaml\": None,\n",
    "            \"logging.yaml\": None,\n",
    "            \"model_params.yaml\": None\n",
    "        },\n",
    "        \"docs\": {\n",
    "            \"api.md\": \"# API Documentation\",\n",
    "            \"architecture.md\": \"# System Architecture\",\n",
    "            \"setup.md\": \"# Setup Guide\"\n",
    "        },\n",
    "        \"scripts\": {\n",
    "            \"train.py\": \"# Training script\",\n",
    "            \"backtest.py\": \"# Backtesting script\",\n",
    "            \"deploy.py\": \"# Deployment script\"\n",
    "        },\n",
    "        # Root files\n",
    "        \"README.md\": None,\n",
    "        \"requirements.txt\": None,\n",
    "        \"setup.py\": None,\n",
    "        \"Dockerfile\": None,\n",
    "        \".gitignore\": None,\n",
    "        \"pyproject.toml\": None,\n",
    "        \"Makefile\": None\n",
    "    }\n",
    "    \n",
    "    def __init__(self, project_name: str, base_path: str = \".\"):\n",
    "        self.project_name = project_name\n",
    "        self.base_path = Path(base_path)\n",
    "        self.project_path = self.base_path / project_name\n",
    "    \n",
    "    def generate_structure(self, structure: dict = None, current_path: Path = None):\n",
    "        \"\"\"Recursively create directory structure\"\"\"\n",
    "        if structure is None:\n",
    "            structure = self.STANDARD_STRUCTURE\n",
    "        if current_path is None:\n",
    "            current_path = self.project_path\n",
    "        \n",
    "        created_items = []\n",
    "        \n",
    "        for name, content in structure.items():\n",
    "            item_path = current_path / name\n",
    "            \n",
    "            if isinstance(content, dict):\n",
    "                # It's a directory\n",
    "                item_path.mkdir(parents=True, exist_ok=True)\n",
    "                created_items.append(f\"ğŸ“ {item_path.relative_to(self.project_path)}/\")\n",
    "                # Recurse into subdirectory\n",
    "                created_items.extend(self.generate_structure(content, item_path))\n",
    "            else:\n",
    "                # It's a file\n",
    "                item_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if content is not None:\n",
    "                    item_path.write_text(content)\n",
    "                else:\n",
    "                    item_path.touch()\n",
    "                created_items.append(f\"ğŸ“„ {item_path.relative_to(self.project_path)}\")\n",
    "        \n",
    "        return created_items\n",
    "    \n",
    "    def display_tree(self, path: Path = None, prefix: str = \"\", is_last: bool = True):\n",
    "        \"\"\"Display directory tree\"\"\"\n",
    "        if path is None:\n",
    "            path = self.project_path\n",
    "            print(f\"\\nğŸ“¦ {self.project_name}/\")\n",
    "        \n",
    "        items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "        \n",
    "        for i, item in enumerate(items):\n",
    "            is_last_item = (i == len(items) - 1)\n",
    "            connector = \"â””â”€â”€ \" if is_last_item else \"â”œâ”€â”€ \"\n",
    "            \n",
    "            if item.is_dir():\n",
    "                print(f\"{prefix}{connector}ğŸ“ {item.name}/\")\n",
    "                extension = \"    \" if is_last_item else \"â”‚   \"\n",
    "                self.display_tree(item, prefix + extension, is_last_item)\n",
    "            else:\n",
    "                print(f\"{prefix}{connector}ğŸ“„ {item.name}\")\n",
    "    \n",
    "    def get_structure_summary(self) -> Dict:\n",
    "        \"\"\"Get summary of project structure\"\"\"\n",
    "        def count_items(path: Path):\n",
    "            dirs = files = 0\n",
    "            for item in path.rglob(\"*\"):\n",
    "                if item.is_dir():\n",
    "                    dirs += 1\n",
    "                else:\n",
    "                    files += 1\n",
    "            return dirs, files\n",
    "        \n",
    "        dirs, files = count_items(self.project_path)\n",
    "        return {\n",
    "            \"project_name\": self.project_name,\n",
    "            \"total_directories\": dirs,\n",
    "            \"total_files\": files,\n",
    "            \"path\": str(self.project_path.absolute())\n",
    "        }\n",
    "\n",
    "# Display the standard structure (without creating it)\n",
    "print(\"ğŸ“‹ Standard ML Trading Project Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def print_structure(structure, indent=0):\n",
    "    for name, content in structure.items():\n",
    "        if isinstance(content, dict):\n",
    "            print(\"  \" * indent + f\"ğŸ“ {name}/\")\n",
    "            print_structure(content, indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent + f\"ğŸ“„ {name}\")\n",
    "\n",
    "print_structure(ProjectStructureGenerator.STANDARD_STRUCTURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate project structure (commented out to avoid creating files)\n",
    "# Uncomment to actually create the structure\n",
    "\n",
    "\"\"\"\n",
    "# Create project structure\n",
    "generator = ProjectStructureGenerator(\n",
    "    project_name=\"alpha_signals\",\n",
    "    base_path=\"/path/to/your/projects\"\n",
    ")\n",
    "\n",
    "# Generate all directories and files\n",
    "created = generator.generate_structure()\n",
    "\n",
    "# Display the tree\n",
    "generator.display_tree()\n",
    "\n",
    "# Get summary\n",
    "summary = generator.get_structure_summary()\n",
    "print(f\"\\nâœ… Created {summary['total_directories']} directories and {summary['total_files']} files\")\n",
    "\"\"\"\n",
    "\n",
    "# Simulated output\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ¯ Project Structure Generator - Preview Mode\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "To create this structure, use:\n",
    "\n",
    "    generator = ProjectStructureGenerator(\n",
    "        project_name=\"alpha_signals\",\n",
    "        base_path=\"./projects\"  \n",
    "    )\n",
    "    generator.generate_structure()\n",
    "    \n",
    "This will create:\n",
    "    âœ… 15 directories\n",
    "    âœ… 35+ files with starter templates\n",
    "    âœ… Proper Python package structure\n",
    "    âœ… Testing framework setup\n",
    "    âœ… Documentation scaffolding\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917d7af",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Create Requirements Configuration\n",
    "\n",
    "### Dependency Management Best Practices\n",
    "Professional Python projects use multiple requirements files:\n",
    "- `requirements.txt`: Production dependencies\n",
    "- `requirements-dev.txt`: Development tools (testing, linting)\n",
    "- `requirements-docs.txt`: Documentation generation\n",
    "- `pyproject.toml`: Modern Python packaging standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements Generator for Quant ML Projects\n",
    "\n",
    "class RequirementsGenerator:\n",
    "    \"\"\"Generate comprehensive requirements files for trading ML projects\"\"\"\n",
    "    \n",
    "    # Core dependencies by category\n",
    "    DEPENDENCIES = {\n",
    "        \"data_processing\": {\n",
    "            \"pandas\": \">=2.0.0\",\n",
    "            \"numpy\": \">=1.24.0\",\n",
    "            \"polars\": \">=0.20.0\",  # Fast DataFrame alternative\n",
    "            \"pyarrow\": \">=14.0.0\",  # Parquet support\n",
    "        },\n",
    "        \"data_sources\": {\n",
    "            \"yfinance\": \">=0.2.30\",\n",
    "            \"alpha_vantage\": \">=2.3.1\",\n",
    "            \"pandas-datareader\": \">=0.10.0\",\n",
    "            \"fredapi\": \">=0.5.0\",\n",
    "            \"ccxt\": \">=4.0.0\",  # Crypto exchanges\n",
    "        },\n",
    "        \"ml_traditional\": {\n",
    "            \"scikit-learn\": \">=1.3.0\",\n",
    "            \"xgboost\": \">=2.0.0\",\n",
    "            \"lightgbm\": \">=4.0.0\",\n",
    "            \"catboost\": \">=1.2.0\",\n",
    "            \"optuna\": \">=3.4.0\",  # Hyperparameter optimization\n",
    "            \"shap\": \">=0.44.0\",  # Model interpretability\n",
    "        },\n",
    "        \"ml_deep_learning\": {\n",
    "            \"torch\": \">=2.1.0\",\n",
    "            \"pytorch-lightning\": \">=2.1.0\",\n",
    "            \"transformers\": \">=4.35.0\",\n",
    "        },\n",
    "        \"backtesting\": {\n",
    "            \"vectorbt\": \">=0.26.0\",\n",
    "            \"bt\": \">=0.2.10\",\n",
    "            \"empyrical\": \">=0.5.5\",\n",
    "            \"pyfolio-reloaded\": \">=0.9.5\",\n",
    "        },\n",
    "        \"risk_analytics\": {\n",
    "            \"scipy\": \">=1.11.0\",\n",
    "            \"statsmodels\": \">=0.14.0\",\n",
    "            \"arch\": \">=6.2.0\",  # GARCH models\n",
    "            \"cvxpy\": \">=1.4.0\",  # Portfolio optimization\n",
    "        },\n",
    "        \"visualization\": {\n",
    "            \"matplotlib\": \">=3.8.0\",\n",
    "            \"seaborn\": \">=0.13.0\",\n",
    "            \"plotly\": \">=5.18.0\",\n",
    "            \"mplfinance\": \">=0.12.10\",\n",
    "        },\n",
    "        \"utilities\": {\n",
    "            \"pyyaml\": \">=6.0.1\",\n",
    "            \"python-dotenv\": \">=1.0.0\",\n",
    "            \"tqdm\": \">=4.66.0\",\n",
    "            \"loguru\": \">=0.7.0\",\n",
    "            \"requests\": \">=2.31.0\",\n",
    "            \"aiohttp\": \">=3.9.0\",\n",
    "        },\n",
    "        \"database\": {\n",
    "            \"sqlalchemy\": \">=2.0.0\",\n",
    "            \"redis\": \">=5.0.0\",\n",
    "            \"pymongo\": \">=4.6.0\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    DEV_DEPENDENCIES = {\n",
    "        \"testing\": {\n",
    "            \"pytest\": \">=7.4.0\",\n",
    "            \"pytest-cov\": \">=4.1.0\",\n",
    "            \"pytest-asyncio\": \">=0.21.0\",\n",
    "            \"hypothesis\": \">=6.90.0\",  # Property-based testing\n",
    "        },\n",
    "        \"code_quality\": {\n",
    "            \"black\": \">=23.12.0\",\n",
    "            \"isort\": \">=5.13.0\",\n",
    "            \"flake8\": \">=6.1.0\",\n",
    "            \"mypy\": \">=1.7.0\",\n",
    "            \"pre-commit\": \">=3.6.0\",\n",
    "        },\n",
    "        \"documentation\": {\n",
    "            \"sphinx\": \">=7.2.0\",\n",
    "            \"mkdocs\": \">=1.5.0\",\n",
    "            \"mkdocs-material\": \">=9.5.0\",\n",
    "        },\n",
    "        \"development\": {\n",
    "            \"jupyter\": \">=1.0.0\",\n",
    "            \"ipykernel\": \">=6.27.0\",\n",
    "            \"ipywidgets\": \">=8.1.0\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_requirements_txt(cls, categories: List[str] = None, \n",
    "                                   include_dev: bool = False) -> str:\n",
    "        \"\"\"Generate requirements.txt content\"\"\"\n",
    "        lines = [\n",
    "            \"# ============================================\",\n",
    "            \"# Alpha Signals - ML Trading System\",\n",
    "            \"# Requirements File\",\n",
    "            \"# Generated by Project Planning Framework\",\n",
    "            \"# ============================================\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        if categories is None:\n",
    "            categories = list(cls.DEPENDENCIES.keys())\n",
    "        \n",
    "        for category in categories:\n",
    "            if category in cls.DEPENDENCIES:\n",
    "                lines.append(f\"# --- {category.replace('_', ' ').title()} ---\")\n",
    "                for package, version in cls.DEPENDENCIES[category].items():\n",
    "                    lines.append(f\"{package}{version}\")\n",
    "                lines.append(\"\")\n",
    "        \n",
    "        if include_dev:\n",
    "            lines.append(\"# ============================================\")\n",
    "            lines.append(\"# Development Dependencies\")\n",
    "            lines.append(\"# ============================================\")\n",
    "            lines.append(\"\")\n",
    "            for category, packages in cls.DEV_DEPENDENCIES.items():\n",
    "                lines.append(f\"# --- {category.replace('_', ' ').title()} ---\")\n",
    "                for package, version in packages.items():\n",
    "                    lines.append(f\"{package}{version}\")\n",
    "                lines.append(\"\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_pyproject_toml(cls, project_name: str, \n",
    "                                 author: str,\n",
    "                                 description: str) -> str:\n",
    "        \"\"\"Generate modern pyproject.toml\"\"\"\n",
    "        # Flatten dependencies\n",
    "        all_deps = []\n",
    "        for packages in cls.DEPENDENCIES.values():\n",
    "            for pkg, ver in packages.items():\n",
    "                all_deps.append(f'    \"{pkg}{ver}\",')\n",
    "        \n",
    "        dev_deps = []\n",
    "        for packages in cls.DEV_DEPENDENCIES.values():\n",
    "            for pkg, ver in packages.items():\n",
    "                dev_deps.append(f'    \"{pkg}{ver}\",')\n",
    "        \n",
    "        toml = f'''[build-system]\n",
    "requires = [\"setuptools>=68.0\", \"wheel\"]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "[project]\n",
    "name = \"{project_name}\"\n",
    "version = \"0.1.0\"\n",
    "description = \"{description}\"\n",
    "authors = [\n",
    "    {{name = \"{author}\", email = \"{author.lower().replace(\" \", \".\")}@example.com\"}}\n",
    "]\n",
    "readme = \"README.md\"\n",
    "requires-python = \">=3.10\"\n",
    "classifiers = [\n",
    "    \"Development Status :: 3 - Alpha\",\n",
    "    \"Intended Audience :: Financial and Insurance Industry\",\n",
    "    \"License :: OSI Approved :: MIT License\",\n",
    "    \"Programming Language :: Python :: 3.10\",\n",
    "    \"Programming Language :: Python :: 3.11\",\n",
    "    \"Topic :: Office/Business :: Financial :: Investment\",\n",
    "]\n",
    "dependencies = [\n",
    "{chr(10).join(all_deps)}\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "{chr(10).join(dev_deps)}\n",
    "]\n",
    "\n",
    "[project.scripts]\n",
    "alpha-train = \"{project_name}.scripts.train:main\"\n",
    "alpha-backtest = \"{project_name}.scripts.backtest:main\"\n",
    "\n",
    "[tool.pytest.ini_options]\n",
    "testpaths = [\"tests\"]\n",
    "python_files = [\"test_*.py\"]\n",
    "addopts = \"-v --cov=src --cov-report=term-missing\"\n",
    "\n",
    "[tool.black]\n",
    "line-length = 100\n",
    "target-version = [\"py310\", \"py311\"]\n",
    "include = \"\\\\\\\\.pyi?$\"\n",
    "\n",
    "[tool.isort]\n",
    "profile = \"black\"\n",
    "line_length = 100\n",
    "\n",
    "[tool.mypy]\n",
    "python_version = \"3.10\"\n",
    "warn_return_any = true\n",
    "warn_unused_configs = true\n",
    "ignore_missing_imports = true\n",
    "'''\n",
    "        return toml\n",
    "\n",
    "# Generate and display requirements\n",
    "print(\"ğŸ“¦ Production Requirements (requirements.txt)\")\n",
    "print(\"=\" * 60)\n",
    "requirements_content = RequirementsGenerator.generate_requirements_txt(\n",
    "    categories=[\"data_processing\", \"data_sources\", \"ml_traditional\", \n",
    "                \"backtesting\", \"risk_analytics\", \"visualization\", \"utilities\"]\n",
    ")\n",
    "print(requirements_content[:2000] + \"\\n... [truncated for display]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca32faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pyproject.toml preview\n",
    "print(\"\\nğŸ“¦ Modern Python Packaging (pyproject.toml)\")\n",
    "print(\"=\" * 60)\n",
    "pyproject_content = RequirementsGenerator.generate_pyproject_toml(\n",
    "    project_name=\"alpha_signals\",\n",
    "    author=\"Your Name\",\n",
    "    description=\"Multi-Asset ML Trading System for Alpha Generation\"\n",
    ")\n",
    "print(pyproject_content[:2500] + \"\\n... [truncated for display]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dd73f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Define Data Pipeline Requirements\n",
    "\n",
    "### Data Pipeline Architecture\n",
    "A robust data pipeline is the foundation of any ML trading system. It must handle:\n",
    "- **Multiple data sources** with different APIs and formats\n",
    "- **Historical and real-time data** with appropriate update frequencies\n",
    "- **Data validation** to catch anomalies before they corrupt models\n",
    "- **Feature engineering** with proper train/test separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pipeline Configuration Framework\n",
    "import yaml\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Optional, Union\n",
    "from enum import Enum\n",
    "\n",
    "class DataFrequency(Enum):\n",
    "    TICK = \"tick\"\n",
    "    SECOND = \"1s\"\n",
    "    MINUTE = \"1m\"\n",
    "    FIVE_MINUTE = \"5m\"\n",
    "    FIFTEEN_MINUTE = \"15m\"\n",
    "    HOURLY = \"1h\"\n",
    "    DAILY = \"1d\"\n",
    "    WEEKLY = \"1w\"\n",
    "    MONTHLY = \"1M\"\n",
    "\n",
    "class AssetClass(Enum):\n",
    "    EQUITY = \"equity\"\n",
    "    ETF = \"etf\"\n",
    "    CRYPTO = \"crypto\"\n",
    "    FOREX = \"forex\"\n",
    "    FUTURES = \"futures\"\n",
    "    OPTIONS = \"options\"\n",
    "\n",
    "@dataclass\n",
    "class DataSourceConfig:\n",
    "    \"\"\"Configuration for a single data source\"\"\"\n",
    "    name: str\n",
    "    source_type: str  # \"api\", \"file\", \"database\"\n",
    "    provider: str  # \"yfinance\", \"alpha_vantage\", \"fred\", etc.\n",
    "    api_key_env: Optional[str] = None\n",
    "    rate_limit_per_minute: int = 5\n",
    "    retry_attempts: int = 3\n",
    "    cache_enabled: bool = True\n",
    "    cache_ttl_hours: int = 24\n",
    "\n",
    "@dataclass\n",
    "class UniverseConfig:\n",
    "    \"\"\"Configuration for trading universe\"\"\"\n",
    "    name: str\n",
    "    asset_class: AssetClass\n",
    "    symbols: List[str]\n",
    "    benchmark: str\n",
    "    frequency: DataFrequency\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    \n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    \"\"\"Configuration for feature engineering\"\"\"\n",
    "    name: str\n",
    "    category: str  # \"technical\", \"fundamental\", \"alternative\", \"cross_asset\"\n",
    "    lookback_periods: List[int]\n",
    "    dependencies: List[str] = field(default_factory=list)\n",
    "    description: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class TargetConfig:\n",
    "    \"\"\"Configuration for target variable\"\"\"\n",
    "    name: str\n",
    "    type: str  # \"regression\", \"classification\", \"multi_class\"\n",
    "    horizon: int  # prediction horizon in bars\n",
    "    threshold: Optional[float] = None  # for classification\n",
    "    \n",
    "@dataclass\n",
    "class DataPipelineConfig:\n",
    "    \"\"\"Complete data pipeline configuration\"\"\"\n",
    "    project_name: str\n",
    "    version: str\n",
    "    \n",
    "    # Data sources\n",
    "    data_sources: List[DataSourceConfig]\n",
    "    \n",
    "    # Universe\n",
    "    universes: List[UniverseConfig]\n",
    "    \n",
    "    # Features\n",
    "    features: List[FeatureConfig]\n",
    "    \n",
    "    # Targets\n",
    "    targets: List[TargetConfig]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    train_start: str\n",
    "    train_end: str\n",
    "    test_start: str\n",
    "    test_end: str\n",
    "    validation_method: str = \"time_series_cv\"  # \"time_series_cv\", \"walk_forward\", \"holdout\"\n",
    "    n_folds: int = 5\n",
    "    \n",
    "    # Data quality\n",
    "    max_missing_pct: float = 0.05\n",
    "    outlier_method: str = \"iqr\"  # \"iqr\", \"zscore\", \"mad\"\n",
    "    outlier_threshold: float = 3.0\n",
    "    \n",
    "    def to_yaml(self) -> str:\n",
    "        \"\"\"Export configuration to YAML format\"\"\"\n",
    "        config_dict = {\n",
    "            \"project\": {\n",
    "                \"name\": self.project_name,\n",
    "                \"version\": self.version\n",
    "            },\n",
    "            \"data_sources\": [\n",
    "                {\n",
    "                    \"name\": ds.name,\n",
    "                    \"type\": ds.source_type,\n",
    "                    \"provider\": ds.provider,\n",
    "                    \"api_key_env\": ds.api_key_env,\n",
    "                    \"rate_limit\": ds.rate_limit_per_minute,\n",
    "                    \"cache\": {\"enabled\": ds.cache_enabled, \"ttl_hours\": ds.cache_ttl_hours}\n",
    "                }\n",
    "                for ds in self.data_sources\n",
    "            ],\n",
    "            \"universes\": [\n",
    "                {\n",
    "                    \"name\": u.name,\n",
    "                    \"asset_class\": u.asset_class.value,\n",
    "                    \"symbols\": u.symbols,\n",
    "                    \"benchmark\": u.benchmark,\n",
    "                    \"frequency\": u.frequency.value,\n",
    "                    \"date_range\": {\"start\": u.start_date, \"end\": u.end_date}\n",
    "                }\n",
    "                for u in self.universes\n",
    "            ],\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"name\": f.name,\n",
    "                    \"category\": f.category,\n",
    "                    \"lookback_periods\": f.lookback_periods,\n",
    "                    \"dependencies\": f.dependencies,\n",
    "                    \"description\": f.description\n",
    "                }\n",
    "                for f in self.features\n",
    "            ],\n",
    "            \"targets\": [\n",
    "                {\n",
    "                    \"name\": t.name,\n",
    "                    \"type\": t.type,\n",
    "                    \"horizon\": t.horizon,\n",
    "                    \"threshold\": t.threshold\n",
    "                }\n",
    "                for t in self.targets\n",
    "            ],\n",
    "            \"train_test_split\": {\n",
    "                \"train\": {\"start\": self.train_start, \"end\": self.train_end},\n",
    "                \"test\": {\"start\": self.test_start, \"end\": self.test_end},\n",
    "                \"validation\": {\n",
    "                    \"method\": self.validation_method,\n",
    "                    \"n_folds\": self.n_folds\n",
    "                }\n",
    "            },\n",
    "            \"data_quality\": {\n",
    "                \"max_missing_pct\": self.max_missing_pct,\n",
    "                \"outlier_detection\": {\n",
    "                    \"method\": self.outlier_method,\n",
    "                    \"threshold\": self.outlier_threshold\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return yaml.dump(config_dict, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Get pipeline configuration summary\"\"\"\n",
    "        total_symbols = sum(len(u.symbols) for u in self.universes)\n",
    "        total_features = sum(len(f.lookback_periods) for f in self.features)\n",
    "        \n",
    "        return f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    DATA PIPELINE CONFIGURATION                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ Project: {self.project_name:<54} â•‘\n",
    "â•‘ Version: {self.version:<54} â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ DATA SOURCES: {len(self.data_sources):<50} â•‘\n",
    "â•‘ UNIVERSES: {len(self.universes)} ({total_symbols} total symbols){' ':<35} â•‘\n",
    "â•‘ FEATURES: {len(self.features)} categories ({total_features} total){' ':<33} â•‘\n",
    "â•‘ TARGETS: {len(self.targets):<55} â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘ TRAIN PERIOD: {self.train_start} to {self.train_end:<27} â•‘\n",
    "â•‘ TEST PERIOD:  {self.test_start} to {self.test_end:<27} â•‘\n",
    "â•‘ VALIDATION:   {self.validation_method} ({self.n_folds} folds){' ':<27} â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Data Pipeline Configuration Framework loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a comprehensive data pipeline configuration\n",
    "\n",
    "# Define data sources\n",
    "data_sources = [\n",
    "    DataSourceConfig(\n",
    "        name=\"Yahoo Finance\",\n",
    "        source_type=\"api\",\n",
    "        provider=\"yfinance\",\n",
    "        rate_limit_per_minute=2000,\n",
    "        cache_enabled=True,\n",
    "        cache_ttl_hours=24\n",
    "    ),\n",
    "    DataSourceConfig(\n",
    "        name=\"FRED\",\n",
    "        source_type=\"api\", \n",
    "        provider=\"fredapi\",\n",
    "        api_key_env=\"FRED_API_KEY\",\n",
    "        rate_limit_per_minute=120,\n",
    "        cache_enabled=True,\n",
    "        cache_ttl_hours=168  # Weekly data, cache for a week\n",
    "    ),\n",
    "    DataSourceConfig(\n",
    "        name=\"Alpha Vantage\",\n",
    "        source_type=\"api\",\n",
    "        provider=\"alpha_vantage\",\n",
    "        api_key_env=\"ALPHA_VANTAGE_KEY\",\n",
    "        rate_limit_per_minute=5,\n",
    "        cache_enabled=True,\n",
    "        cache_ttl_hours=24\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define universes\n",
    "universes = [\n",
    "    UniverseConfig(\n",
    "        name=\"SP500_Large_Cap\",\n",
    "        asset_class=AssetClass.EQUITY,\n",
    "        symbols=[\n",
    "            \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"TSLA\", \"BRK-B\",\n",
    "            \"UNH\", \"JNJ\", \"JPM\", \"V\", \"PG\", \"XOM\", \"HD\", \"CVX\", \"MA\", \"ABBV\",\n",
    "            \"MRK\", \"PEP\", \"KO\", \"COST\", \"AVGO\", \"WMT\", \"LLY\"\n",
    "        ],\n",
    "        benchmark=\"SPY\",\n",
    "        frequency=DataFrequency.DAILY,\n",
    "        start_date=\"2018-01-01\",\n",
    "        end_date=\"2025-12-31\"\n",
    "    ),\n",
    "    UniverseConfig(\n",
    "        name=\"Sector_ETFs\",\n",
    "        asset_class=AssetClass.ETF,\n",
    "        symbols=[\n",
    "            \"XLK\", \"XLF\", \"XLE\", \"XLV\", \"XLI\", \"XLY\", \"XLP\", \"XLU\", \"XLB\", \"XLRE\",\n",
    "            \"QQQ\", \"IWM\", \"DIA\", \"VTI\"\n",
    "        ],\n",
    "        benchmark=\"SPY\",\n",
    "        frequency=DataFrequency.DAILY,\n",
    "        start_date=\"2018-01-01\",\n",
    "        end_date=\"2025-12-31\"\n",
    "    ),\n",
    "    UniverseConfig(\n",
    "        name=\"Crypto_Major\",\n",
    "        asset_class=AssetClass.CRYPTO,\n",
    "        symbols=[\"BTC-USD\", \"ETH-USD\", \"SOL-USD\", \"BNB-USD\"],\n",
    "        benchmark=\"BTC-USD\",\n",
    "        frequency=DataFrequency.DAILY,\n",
    "        start_date=\"2020-01-01\",\n",
    "        end_date=\"2025-12-31\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define features\n",
    "features = [\n",
    "    FeatureConfig(\n",
    "        name=\"returns\",\n",
    "        category=\"price\",\n",
    "        lookback_periods=[1, 5, 10, 21, 63, 126, 252],\n",
    "        description=\"Log returns over various horizons\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"volatility\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[10, 21, 63],\n",
    "        description=\"Rolling realized volatility (std of returns)\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"momentum\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[5, 10, 21, 63, 126],\n",
    "        dependencies=[\"returns\"],\n",
    "        description=\"Price momentum indicators\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"rsi\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[7, 14, 21],\n",
    "        dependencies=[\"returns\"],\n",
    "        description=\"Relative Strength Index\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"macd\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[12, 26],\n",
    "        description=\"MACD indicator components\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"bollinger\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[20],\n",
    "        dependencies=[\"volatility\"],\n",
    "        description=\"Bollinger Band signals\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"volume_indicators\",\n",
    "        category=\"technical\",\n",
    "        lookback_periods=[10, 21],\n",
    "        description=\"Volume-based indicators (OBV, VWAP ratio)\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"cross_sectional_rank\",\n",
    "        category=\"cross_asset\",\n",
    "        lookback_periods=[1, 5, 21],\n",
    "        dependencies=[\"returns\", \"momentum\"],\n",
    "        description=\"Cross-sectional ranking within universe\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"sector_momentum\",\n",
    "        category=\"cross_asset\",\n",
    "        lookback_periods=[5, 21],\n",
    "        description=\"Sector-level momentum relative to market\"\n",
    "    ),\n",
    "    FeatureConfig(\n",
    "        name=\"macro_indicators\",\n",
    "        category=\"fundamental\",\n",
    "        lookback_periods=[1],\n",
    "        description=\"Economic indicators (VIX, rates, spreads)\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define targets\n",
    "targets = [\n",
    "    TargetConfig(\n",
    "        name=\"forward_return_5d\",\n",
    "        type=\"regression\",\n",
    "        horizon=5,\n",
    "        threshold=None\n",
    "    ),\n",
    "    TargetConfig(\n",
    "        name=\"direction_5d\",\n",
    "        type=\"classification\",\n",
    "        horizon=5,\n",
    "        threshold=0.0  # Binary: up or down\n",
    "    ),\n",
    "    TargetConfig(\n",
    "        name=\"quintile_5d\",\n",
    "        type=\"multi_class\",\n",
    "        horizon=5,\n",
    "        threshold=None  # 5-class based on return quintiles\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create complete pipeline configuration\n",
    "pipeline_config = DataPipelineConfig(\n",
    "    project_name=\"alpha_signals\",\n",
    "    version=\"1.0.0\",\n",
    "    data_sources=data_sources,\n",
    "    universes=universes,\n",
    "    features=features,\n",
    "    targets=targets,\n",
    "    train_start=\"2018-01-01\",\n",
    "    train_end=\"2022-12-31\",\n",
    "    test_start=\"2023-01-01\",\n",
    "    test_end=\"2024-12-31\",\n",
    "    validation_method=\"time_series_cv\",\n",
    "    n_folds=5,\n",
    "    max_missing_pct=0.05,\n",
    "    outlier_method=\"iqr\",\n",
    "    outlier_threshold=3.0\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(pipeline_config.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1792b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration to YAML\n",
    "yaml_config = pipeline_config.to_yaml()\n",
    "print(\"ğŸ“„ Data Pipeline Configuration (config.yaml)\")\n",
    "print(\"=\" * 60)\n",
    "print(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae51625",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Create Project Timeline with Milestones\n",
    "\n",
    "### Gantt Chart Project Planning\n",
    "A visual project timeline helps:\n",
    "- Track progress against deadlines\n",
    "- Identify dependencies and critical path\n",
    "- Communicate status to stakeholders\n",
    "- Adjust plans when issues arise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67601430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Timeline and Gantt Chart Generator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "class ProjectTimeline:\n",
    "    \"\"\"Create and visualize project timelines\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str, start_date: datetime):\n",
    "        self.project_name = project_name\n",
    "        self.start_date = start_date\n",
    "        self.tasks = []\n",
    "        \n",
    "    def add_task(self, name: str, duration_days: float, \n",
    "                 dependencies: List[str] = None,\n",
    "                 category: str = \"Development\",\n",
    "                 description: str = \"\",\n",
    "                 deliverables: List[str] = None):\n",
    "        \"\"\"Add a task to the timeline\"\"\"\n",
    "        self.tasks.append({\n",
    "            \"name\": name,\n",
    "            \"duration_days\": duration_days,\n",
    "            \"dependencies\": dependencies or [],\n",
    "            \"category\": category,\n",
    "            \"description\": description,\n",
    "            \"deliverables\": deliverables or []\n",
    "        })\n",
    "    \n",
    "    def calculate_schedule(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate start and end dates based on dependencies\"\"\"\n",
    "        task_dict = {t[\"name\"]: t for t in self.tasks}\n",
    "        scheduled = {}\n",
    "        \n",
    "        def get_task_end(task_name):\n",
    "            if task_name in scheduled:\n",
    "                return scheduled[task_name][\"end\"]\n",
    "            \n",
    "            task = task_dict[task_name]\n",
    "            \n",
    "            # Calculate start based on dependencies\n",
    "            if task[\"dependencies\"]:\n",
    "                start = max(get_task_end(dep) for dep in task[\"dependencies\"])\n",
    "            else:\n",
    "                start = self.start_date\n",
    "            \n",
    "            end = start + timedelta(days=task[\"duration_days\"])\n",
    "            \n",
    "            scheduled[task_name] = {\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"duration\": task[\"duration_days\"],\n",
    "                \"category\": task[\"category\"],\n",
    "                \"description\": task[\"description\"],\n",
    "                \"deliverables\": task[\"deliverables\"],\n",
    "                \"dependencies\": task[\"dependencies\"]\n",
    "            }\n",
    "            return end\n",
    "        \n",
    "        # Process all tasks\n",
    "        for task in self.tasks:\n",
    "            get_task_end(task[\"name\"])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"Task\": name,\n",
    "                \"Start\": info[\"start\"],\n",
    "                \"End\": info[\"end\"],\n",
    "                \"Duration (days)\": info[\"duration\"],\n",
    "                \"Category\": info[\"category\"],\n",
    "                \"Dependencies\": \", \".join(info[\"dependencies\"]) if info[\"dependencies\"] else \"None\",\n",
    "                \"Deliverables\": \"; \".join(info[\"deliverables\"]) if info[\"deliverables\"] else \"\"\n",
    "            }\n",
    "            for name, info in scheduled.items()\n",
    "        ])\n",
    "        \n",
    "        return df.sort_values(\"Start\").reset_index(drop=True)\n",
    "    \n",
    "    def create_gantt_chart(self, figsize=(14, 8)):\n",
    "        \"\"\"Create a Gantt chart visualization\"\"\"\n",
    "        schedule = self.calculate_schedule()\n",
    "        \n",
    "        # Define colors for categories\n",
    "        category_colors = {\n",
    "            \"Planning\": \"#3498db\",       # Blue\n",
    "            \"Data\": \"#2ecc71\",           # Green\n",
    "            \"Modeling\": \"#e74c3c\",       # Red\n",
    "            \"Backtesting\": \"#9b59b6\",    # Purple\n",
    "            \"Risk\": \"#f39c12\",           # Orange\n",
    "            \"Production\": \"#1abc9c\",     # Teal\n",
    "            \"Documentation\": \"#95a5a6\"   # Gray\n",
    "        }\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot bars\n",
    "        y_positions = range(len(schedule))\n",
    "        \n",
    "        for i, (idx, row) in enumerate(schedule.iterrows()):\n",
    "            start_num = (row[\"Start\"] - self.start_date).days\n",
    "            duration = row[\"Duration (days)\"]\n",
    "            color = category_colors.get(row[\"Category\"], \"#7f8c8d\")\n",
    "            \n",
    "            # Main bar\n",
    "            ax.barh(i, duration, left=start_num, height=0.6, \n",
    "                   color=color, alpha=0.8, edgecolor=\"black\", linewidth=0.5)\n",
    "            \n",
    "            # Task label\n",
    "            ax.text(start_num + duration/2, i, row[\"Task\"], \n",
    "                   ha='center', va='center', fontsize=8, fontweight='bold',\n",
    "                   color='white' if duration > 0.3 else 'black')\n",
    "        \n",
    "        # Customize appearance\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(schedule[\"Task\"])\n",
    "        ax.set_xlabel(\"Days from Project Start\", fontsize=12)\n",
    "        ax.set_title(f\"ğŸ“Š {self.project_name} - Project Timeline\", fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Add grid\n",
    "        ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "        ax.set_axisbelow(True)\n",
    "        \n",
    "        # Create legend\n",
    "        legend_handles = [mpatches.Patch(color=color, label=cat) \n",
    "                         for cat, color in category_colors.items() \n",
    "                         if cat in schedule[\"Category\"].values]\n",
    "        ax.legend(handles=legend_handles, loc='upper right', fontsize=9)\n",
    "        \n",
    "        # Add milestone markers\n",
    "        project_end = (schedule[\"End\"].max() - self.start_date).days\n",
    "        ax.axvline(x=project_end, color='red', linestyle='--', linewidth=2, label='Project End')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n",
    "    \n",
    "    def get_critical_path(self) -> List[str]:\n",
    "        \"\"\"Identify the critical path (longest dependency chain)\"\"\"\n",
    "        schedule = self.calculate_schedule()\n",
    "        project_end = schedule[\"End\"].max()\n",
    "        \n",
    "        # Find tasks that end at project end\n",
    "        critical_tasks = schedule[schedule[\"End\"] == project_end][\"Task\"].tolist()\n",
    "        \n",
    "        return critical_tasks\n",
    "\n",
    "# Create comprehensive 7-day capstone timeline\n",
    "timeline = ProjectTimeline(\n",
    "    project_name=\"Alpha Signals - ML Trading System\",\n",
    "    start_date=datetime(2026, 1, 24)\n",
    ")\n",
    "\n",
    "# Day 1: Project Setup & Planning\n",
    "timeline.add_task(\"Project Charter & Setup\", 0.5, \n",
    "                  category=\"Planning\",\n",
    "                  description=\"Define scope, objectives, and success metrics\",\n",
    "                  deliverables=[\"Project charter document\", \"Directory structure\", \"Git repository\"])\n",
    "\n",
    "timeline.add_task(\"Architecture Design\", 0.5, \n",
    "                  dependencies=[\"Project Charter & Setup\"],\n",
    "                  category=\"Planning\",\n",
    "                  description=\"Design system architecture and data flow\",\n",
    "                  deliverables=[\"Architecture diagram\", \"Component specifications\"])\n",
    "\n",
    "# Day 2: Data Pipeline\n",
    "timeline.add_task(\"Data Fetching Module\", 0.5,\n",
    "                  dependencies=[\"Architecture Design\"],\n",
    "                  category=\"Data\",\n",
    "                  description=\"Implement data fetchers for all sources\",\n",
    "                  deliverables=[\"YFinance fetcher\", \"FRED fetcher\", \"Data caching\"])\n",
    "\n",
    "timeline.add_task(\"Data Processing & Cleaning\", 0.5,\n",
    "                  dependencies=[\"Data Fetching Module\"],\n",
    "                  category=\"Data\",\n",
    "                  description=\"Data validation and cleaning pipeline\",\n",
    "                  deliverables=[\"Missing data handler\", \"Outlier detection\", \"Data validators\"])\n",
    "\n",
    "# Day 3: Feature Engineering\n",
    "timeline.add_task(\"Feature Engineering\", 1.0,\n",
    "                  dependencies=[\"Data Processing & Cleaning\"],\n",
    "                  category=\"Data\",\n",
    "                  description=\"Compute all technical and cross-asset features\",\n",
    "                  deliverables=[\"Technical indicators\", \"Cross-sectional features\", \"Feature store\"])\n",
    "\n",
    "# Day 4: Model Development\n",
    "timeline.add_task(\"Model Training Pipeline\", 1.0,\n",
    "                  dependencies=[\"Feature Engineering\"],\n",
    "                  category=\"Modeling\",\n",
    "                  description=\"Train and tune ML models\",\n",
    "                  deliverables=[\"XGBoost model\", \"LightGBM model\", \"LSTM model\", \"Ensemble\"])\n",
    "\n",
    "# Day 5: Backtesting\n",
    "timeline.add_task(\"Backtesting Engine\", 0.75,\n",
    "                  dependencies=[\"Model Training Pipeline\"],\n",
    "                  category=\"Backtesting\",\n",
    "                  description=\"Build event-driven backtesting framework\",\n",
    "                  deliverables=[\"Backtest engine\", \"Transaction cost model\", \"Performance metrics\"])\n",
    "\n",
    "timeline.add_task(\"Strategy Optimization\", 0.25,\n",
    "                  dependencies=[\"Backtesting Engine\"],\n",
    "                  category=\"Backtesting\",\n",
    "                  description=\"Optimize strategy parameters\",\n",
    "                  deliverables=[\"Optimal parameters\", \"Walk-forward results\"])\n",
    "\n",
    "# Day 6: Risk Management & Production\n",
    "timeline.add_task(\"Risk Management Module\", 0.5,\n",
    "                  dependencies=[\"Strategy Optimization\"],\n",
    "                  category=\"Risk\",\n",
    "                  description=\"Implement risk controls and position sizing\",\n",
    "                  deliverables=[\"Position sizer\", \"VaR calculator\", \"Drawdown monitor\"])\n",
    "\n",
    "timeline.add_task(\"Production Packaging\", 0.5,\n",
    "                  dependencies=[\"Risk Management Module\"],\n",
    "                  category=\"Production\",\n",
    "                  description=\"Package for deployment\",\n",
    "                  deliverables=[\"Docker container\", \"API endpoints\", \"Configuration management\"])\n",
    "\n",
    "# Day 7: Testing & Documentation\n",
    "timeline.add_task(\"Testing Suite\", 0.5,\n",
    "                  dependencies=[\"Production Packaging\"],\n",
    "                  category=\"Production\",\n",
    "                  description=\"Write comprehensive tests\",\n",
    "                  deliverables=[\"Unit tests\", \"Integration tests\", \">70% coverage\"])\n",
    "\n",
    "timeline.add_task(\"Documentation & Portfolio\", 0.5,\n",
    "                  dependencies=[\"Testing Suite\"],\n",
    "                  category=\"Documentation\",\n",
    "                  description=\"Final documentation and GitHub presentation\",\n",
    "                  deliverables=[\"README.md\", \"API docs\", \"Results notebook\", \"Demo video\"])\n",
    "\n",
    "# Display schedule\n",
    "print(\"ğŸ“… Project Schedule\")\n",
    "print(\"=\" * 80)\n",
    "schedule_df = timeline.calculate_schedule()\n",
    "print(schedule_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gantt chart visualization\n",
    "fig, ax = timeline.create_gantt_chart(figsize=(14, 8))\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('MVP Complete', xy=(5, 6), xytext=(5.5, 8),\n",
    "            arrowprops=dict(arrowstyle='->', color='green'),\n",
    "            fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "plt.savefig('project_gantt_chart.png', dpi=150, bbox_inches='tight', \n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Gantt chart saved to 'project_gantt_chart.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea92ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Set Up Logging and Configuration Framework\n",
    "\n",
    "### Professional Logging Standards\n",
    "Production trading systems require comprehensive logging for:\n",
    "- **Debugging**: Trace execution flow and errors\n",
    "- **Auditing**: Record all trading decisions for compliance\n",
    "- **Monitoring**: Track system health and performance\n",
    "- **Analysis**: Post-trade analysis and model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bde58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Logging Configuration\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "class LoggingConfig:\n",
    "    \"\"\"Professional logging configuration for trading systems\"\"\"\n",
    "    \n",
    "    # Logging YAML template\n",
    "    LOGGING_YAML_TEMPLATE = '''\n",
    "# Logging Configuration for Alpha Signals Trading System\n",
    "version: 1\n",
    "disable_existing_loggers: false\n",
    "\n",
    "formatters:\n",
    "  standard:\n",
    "    format: \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n",
    "    datefmt: \"%Y-%m-%d %H:%M:%S\"\n",
    "  \n",
    "  detailed:\n",
    "    format: \"%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d | %(message)s\"\n",
    "    datefmt: \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "  \n",
    "  json:\n",
    "    format: '{\"timestamp\": \"%(asctime)s\", \"level\": \"%(levelname)s\", \"logger\": \"%(name)s\", \"message\": \"%(message)s\"}'\n",
    "    datefmt: \"%Y-%m-%dT%H:%M:%S\"\n",
    "  \n",
    "  trade:\n",
    "    format: \"%(asctime)s | TRADE | %(message)s\"\n",
    "    datefmt: \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "handlers:\n",
    "  console:\n",
    "    class: logging.StreamHandler\n",
    "    level: INFO\n",
    "    formatter: standard\n",
    "    stream: ext://sys.stdout\n",
    "  \n",
    "  file_debug:\n",
    "    class: logging.handlers.RotatingFileHandler\n",
    "    level: DEBUG\n",
    "    formatter: detailed\n",
    "    filename: logs/debug.log\n",
    "    maxBytes: 10485760  # 10MB\n",
    "    backupCount: 5\n",
    "  \n",
    "  file_error:\n",
    "    class: logging.handlers.RotatingFileHandler\n",
    "    level: ERROR\n",
    "    formatter: detailed\n",
    "    filename: logs/error.log\n",
    "    maxBytes: 10485760\n",
    "    backupCount: 10\n",
    "  \n",
    "  file_trade:\n",
    "    class: logging.handlers.RotatingFileHandler\n",
    "    level: INFO\n",
    "    formatter: trade\n",
    "    filename: logs/trades.log\n",
    "    maxBytes: 52428800  # 50MB\n",
    "    backupCount: 30\n",
    "  \n",
    "  file_performance:\n",
    "    class: logging.handlers.TimedRotatingFileHandler\n",
    "    level: INFO\n",
    "    formatter: json\n",
    "    filename: logs/performance.log\n",
    "    when: midnight\n",
    "    backupCount: 90\n",
    "\n",
    "loggers:\n",
    "  alpha_signals:\n",
    "    level: DEBUG\n",
    "    handlers: [console, file_debug]\n",
    "    propagate: false\n",
    "  \n",
    "  alpha_signals.data:\n",
    "    level: INFO\n",
    "    handlers: [console, file_debug]\n",
    "    propagate: false\n",
    "  \n",
    "  alpha_signals.models:\n",
    "    level: INFO\n",
    "    handlers: [console, file_debug]\n",
    "    propagate: false\n",
    "  \n",
    "  alpha_signals.strategy:\n",
    "    level: DEBUG\n",
    "    handlers: [console, file_debug, file_trade]\n",
    "    propagate: false\n",
    "  \n",
    "  alpha_signals.backtest:\n",
    "    level: INFO\n",
    "    handlers: [console, file_performance]\n",
    "    propagate: false\n",
    "  \n",
    "  alpha_signals.risk:\n",
    "    level: WARNING\n",
    "    handlers: [console, file_error]\n",
    "    propagate: false\n",
    "\n",
    "root:\n",
    "  level: WARNING\n",
    "  handlers: [console]\n",
    "'''\n",
    "\n",
    "    @classmethod\n",
    "    def setup_logging(cls, \n",
    "                      log_dir: str = \"logs\",\n",
    "                      level: str = \"INFO\",\n",
    "                      log_to_file: bool = True):\n",
    "        \"\"\"Set up basic logging configuration\"\"\"\n",
    "        \n",
    "        # Create log directory\n",
    "        Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Configure root logger\n",
    "        log_level = getattr(logging, level.upper())\n",
    "        \n",
    "        # Create formatters\n",
    "        console_formatter = logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "        file_formatter = logging.Formatter(\n",
    "            \"%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s:%(lineno)d | %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "        )\n",
    "        \n",
    "        # Console handler\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setLevel(log_level)\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        \n",
    "        handlers = [console_handler]\n",
    "        \n",
    "        # File handler\n",
    "        if log_to_file:\n",
    "            file_handler = logging.FileHandler(f\"{log_dir}/alpha_signals.log\")\n",
    "            file_handler.setLevel(logging.DEBUG)\n",
    "            file_handler.setFormatter(file_formatter)\n",
    "            handlers.append(file_handler)\n",
    "        \n",
    "        # Configure root logger\n",
    "        logging.basicConfig(\n",
    "            level=log_level,\n",
    "            handlers=handlers\n",
    "        )\n",
    "        \n",
    "        return logging.getLogger(\"alpha_signals\")\n",
    "    \n",
    "    @classmethod\n",
    "    def get_logger(cls, name: str) -> logging.Logger:\n",
    "        \"\"\"Get a named logger\"\"\"\n",
    "        return logging.getLogger(f\"alpha_signals.{name}\")\n",
    "\n",
    "# Display logging configuration template\n",
    "print(\"ğŸ“ Logging Configuration Template (logging.yaml)\")\n",
    "print(\"=\" * 60)\n",
    "print(LoggingConfig.LOGGING_YAML_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized Configuration Management\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database connection configuration\"\"\"\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 5432\n",
    "    database: str = \"alpha_signals\"\n",
    "    username: str = \"postgres\"\n",
    "    password: str = field(default_factory=lambda: os.getenv(\"DB_PASSWORD\", \"\"))\n",
    "    \n",
    "    @property\n",
    "    def connection_string(self) -> str:\n",
    "        return f\"postgresql://{self.username}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "\n",
    "@dataclass\n",
    "class APIConfig:\n",
    "    \"\"\"External API configuration\"\"\"\n",
    "    alpha_vantage_key: str = field(default_factory=lambda: os.getenv(\"ALPHA_VANTAGE_KEY\", \"\"))\n",
    "    fred_api_key: str = field(default_factory=lambda: os.getenv(\"FRED_API_KEY\", \"\"))\n",
    "    quandl_api_key: str = field(default_factory=lambda: os.getenv(\"QUANDL_API_KEY\", \"\"))\n",
    "    rate_limit_per_minute: int = 5\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"ML model configuration\"\"\"\n",
    "    model_dir: str = \"data/models\"\n",
    "    default_model: str = \"xgboost_v1\"\n",
    "    ensemble_weights: Dict[str, float] = field(default_factory=lambda: {\n",
    "        \"xgboost\": 0.4,\n",
    "        \"lightgbm\": 0.3,\n",
    "        \"lstm\": 0.3\n",
    "    })\n",
    "    prediction_threshold: float = 0.5\n",
    "    retrain_frequency_days: int = 30\n",
    "\n",
    "@dataclass \n",
    "class BacktestConfig:\n",
    "    \"\"\"Backtesting configuration\"\"\"\n",
    "    initial_capital: float = 100000.0\n",
    "    transaction_cost_bps: float = 10.0\n",
    "    slippage_bps: float = 5.0\n",
    "    max_position_pct: float = 0.1\n",
    "    enable_shorting: bool = True\n",
    "    margin_requirement: float = 0.5\n",
    "\n",
    "@dataclass\n",
    "class RiskConfig:\n",
    "    \"\"\"Risk management configuration\"\"\"\n",
    "    max_drawdown_pct: float = 0.20\n",
    "    max_position_size: float = 0.10\n",
    "    max_sector_exposure: float = 0.30\n",
    "    var_confidence: float = 0.95\n",
    "    var_horizon_days: int = 1\n",
    "    stop_loss_pct: float = 0.05\n",
    "    take_profit_pct: float = 0.15\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Master configuration class\"\"\"\n",
    "    # Environment\n",
    "    environment: str = \"development\"  # development, staging, production\n",
    "    debug: bool = True\n",
    "    \n",
    "    # Paths\n",
    "    data_dir: str = \"data\"\n",
    "    log_dir: str = \"logs\"\n",
    "    output_dir: str = \"output\"\n",
    "    \n",
    "    # Sub-configurations\n",
    "    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n",
    "    api: APIConfig = field(default_factory=APIConfig)\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    backtest: BacktestConfig = field(default_factory=BacktestConfig)\n",
    "    risk: RiskConfig = field(default_factory=RiskConfig)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_env(cls, env: str = \"development\") -> \"Config\":\n",
    "        \"\"\"Load configuration based on environment\"\"\"\n",
    "        config = cls(environment=env)\n",
    "        \n",
    "        if env == \"production\":\n",
    "            config.debug = False\n",
    "            config.backtest.transaction_cost_bps = 15.0  # Conservative in production\n",
    "            config.risk.max_drawdown_pct = 0.15  # Tighter risk controls\n",
    "            \n",
    "        return config\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Export configuration as dictionary\"\"\"\n",
    "        return {\n",
    "            \"environment\": self.environment,\n",
    "            \"debug\": self.debug,\n",
    "            \"paths\": {\n",
    "                \"data\": self.data_dir,\n",
    "                \"logs\": self.log_dir,\n",
    "                \"output\": self.output_dir\n",
    "            },\n",
    "            \"database\": {\n",
    "                \"host\": self.database.host,\n",
    "                \"port\": self.database.port,\n",
    "                \"database\": self.database.database\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"model_dir\": self.model.model_dir,\n",
    "                \"default_model\": self.model.default_model,\n",
    "                \"ensemble_weights\": self.model.ensemble_weights\n",
    "            },\n",
    "            \"backtest\": {\n",
    "                \"initial_capital\": self.backtest.initial_capital,\n",
    "                \"transaction_cost_bps\": self.backtest.transaction_cost_bps,\n",
    "                \"slippage_bps\": self.backtest.slippage_bps\n",
    "            },\n",
    "            \"risk\": {\n",
    "                \"max_drawdown_pct\": self.risk.max_drawdown_pct,\n",
    "                \"max_position_size\": self.risk.max_position_size,\n",
    "                \"var_confidence\": self.risk.var_confidence\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create and display configuration\n",
    "config = Config.from_env(\"development\")\n",
    "\n",
    "print(\"âš™ï¸ Configuration Management System\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nğŸ“Œ Environment: {config.environment}\")\n",
    "print(f\"ğŸ”§ Debug Mode: {config.debug}\")\n",
    "print(f\"\\nğŸ“ Paths:\")\n",
    "print(f\"   Data:   {config.data_dir}\")\n",
    "print(f\"   Logs:   {config.log_dir}\")\n",
    "print(f\"   Output: {config.output_dir}\")\n",
    "print(f\"\\nğŸ¤– Model Configuration:\")\n",
    "print(f\"   Model Dir: {config.model.model_dir}\")\n",
    "print(f\"   Default:   {config.model.default_model}\")\n",
    "print(f\"   Ensemble:  {config.model.ensemble_weights}\")\n",
    "print(f\"\\nğŸ“Š Backtest Configuration:\")\n",
    "print(f\"   Capital:      ${config.backtest.initial_capital:,.0f}\")\n",
    "print(f\"   Trans Costs:  {config.backtest.transaction_cost_bps} bps\")\n",
    "print(f\"   Slippage:     {config.backtest.slippage_bps} bps\")\n",
    "print(f\"\\nğŸ›¡ï¸ Risk Configuration:\")\n",
    "print(f\"   Max Drawdown: {config.risk.max_drawdown_pct:.0%}\")\n",
    "print(f\"   Max Position: {config.risk.max_position_size:.0%}\")\n",
    "print(f\"   VaR Level:    {config.risk.var_confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d953fb5",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Risk Assessment and Mitigation\n",
    "\n",
    "### Project Risk Management\n",
    "Identifying and mitigating risks early is crucial for project success. Common risks in quant ML projects include:\n",
    "- **Technical risks**: Model failures, data quality issues, infrastructure problems\n",
    "- **Schedule risks**: Scope creep, underestimated complexity, dependencies\n",
    "- **Resource risks**: Data access, compute limitations, skill gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afef884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Risk Assessment Framework\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class ProjectRisk:\n",
    "    \"\"\"Define a project risk with assessment\"\"\"\n",
    "    id: str\n",
    "    category: str\n",
    "    description: str\n",
    "    probability: float  # 0-1\n",
    "    impact: float  # 0-1\n",
    "    mitigation: str\n",
    "    contingency: str\n",
    "    owner: str\n",
    "    \n",
    "    @property\n",
    "    def risk_score(self) -> float:\n",
    "        \"\"\"Calculate risk score (probability Ã— impact)\"\"\"\n",
    "        return self.probability * self.impact\n",
    "    \n",
    "    @property\n",
    "    def risk_level(self) -> str:\n",
    "        \"\"\"Categorize risk level\"\"\"\n",
    "        score = self.risk_score\n",
    "        if score >= 0.5:\n",
    "            return \"ğŸ”´ Critical\"\n",
    "        elif score >= 0.25:\n",
    "            return \"ğŸŸ  High\"\n",
    "        elif score >= 0.1:\n",
    "            return \"ğŸŸ¡ Medium\"\n",
    "        else:\n",
    "            return \"ğŸŸ¢ Low\"\n",
    "\n",
    "class RiskRegister:\n",
    "    \"\"\"Project risk register for tracking and visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name: str):\n",
    "        self.project_name = project_name\n",
    "        self.risks: List[ProjectRisk] = []\n",
    "    \n",
    "    def add_risk(self, risk: ProjectRisk):\n",
    "        self.risks.append(risk)\n",
    "    \n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get risk summary statistics\"\"\"\n",
    "        if not self.risks:\n",
    "            return {}\n",
    "        \n",
    "        scores = [r.risk_score for r in self.risks]\n",
    "        return {\n",
    "            \"total_risks\": len(self.risks),\n",
    "            \"avg_risk_score\": np.mean(scores),\n",
    "            \"max_risk_score\": max(scores),\n",
    "            \"critical_count\": sum(1 for r in self.risks if r.risk_score >= 0.5),\n",
    "            \"high_count\": sum(1 for r in self.risks if 0.25 <= r.risk_score < 0.5),\n",
    "            \"medium_count\": sum(1 for r in self.risks if 0.1 <= r.risk_score < 0.25),\n",
    "            \"low_count\": sum(1 for r in self.risks if r.risk_score < 0.1)\n",
    "        }\n",
    "    \n",
    "    def plot_risk_matrix(self, figsize=(10, 8)):\n",
    "        \"\"\"Create a risk matrix visualization\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Create heatmap background\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        y = np.linspace(0, 1, 100)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = X * Y  # Risk score\n",
    "        \n",
    "        # Custom colormap (green -> yellow -> red)\n",
    "        from matplotlib.colors import LinearSegmentedColormap\n",
    "        colors = ['#27ae60', '#f1c40f', '#e74c3c']\n",
    "        cmap = LinearSegmentedColormap.from_list('risk', colors)\n",
    "        \n",
    "        im = ax.contourf(X, Y, Z, levels=20, cmap=cmap, alpha=0.3)\n",
    "        \n",
    "        # Plot risk points\n",
    "        categories = list(set(r.category for r in self.risks))\n",
    "        markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p']\n",
    "        colors_cat = plt.cm.tab10(np.linspace(0, 1, len(categories)))\n",
    "        \n",
    "        for i, cat in enumerate(categories):\n",
    "            cat_risks = [r for r in self.risks if r.category == cat]\n",
    "            probs = [r.probability for r in cat_risks]\n",
    "            impacts = [r.impact for r in cat_risks]\n",
    "            \n",
    "            ax.scatter(probs, impacts, \n",
    "                      s=200, marker=markers[i % len(markers)],\n",
    "                      c=[colors_cat[i]], label=cat, \n",
    "                      edgecolors='black', linewidth=2, alpha=0.8)\n",
    "            \n",
    "            # Add risk IDs as labels\n",
    "            for r in cat_risks:\n",
    "                ax.annotate(r.id, (r.probability, r.impact),\n",
    "                           xytext=(5, 5), textcoords='offset points',\n",
    "                           fontsize=9, fontweight='bold')\n",
    "        \n",
    "        # Add quadrant labels\n",
    "        ax.text(0.25, 0.75, 'Low Prob\\nHigh Impact', ha='center', va='center', fontsize=10, alpha=0.5)\n",
    "        ax.text(0.75, 0.75, 'High Prob\\nHigh Impact', ha='center', va='center', fontsize=10, \n",
    "                color='red', fontweight='bold')\n",
    "        ax.text(0.25, 0.25, 'Low Prob\\nLow Impact', ha='center', va='center', fontsize=10, alpha=0.5)\n",
    "        ax.text(0.75, 0.25, 'High Prob\\nLow Impact', ha='center', va='center', fontsize=10, alpha=0.5)\n",
    "        \n",
    "        # Add threshold lines\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Probability', fontsize=12)\n",
    "        ax.set_ylabel('Impact', fontsize=12)\n",
    "        ax.set_title(f'ğŸ¯ Risk Matrix - {self.project_name}', fontsize=14, fontweight='bold')\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, label='Risk Score')\n",
    "        plt.tight_layout()\n",
    "        return fig, ax\n",
    "    \n",
    "    def print_register(self):\n",
    "        \"\"\"Print formatted risk register\"\"\"\n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(f\"                    RISK REGISTER - {self.project_name}\")\n",
    "        print(f\"{'='*90}\")\n",
    "        \n",
    "        sorted_risks = sorted(self.risks, key=lambda r: r.risk_score, reverse=True)\n",
    "        \n",
    "        for r in sorted_risks:\n",
    "            print(f\"\\n{r.risk_level} [{r.id}] {r.description}\")\n",
    "            print(f\"   Category:    {r.category}\")\n",
    "            print(f\"   Probability: {r.probability:.0%} | Impact: {r.impact:.0%} | Score: {r.risk_score:.2f}\")\n",
    "            print(f\"   Mitigation:  {r.mitigation}\")\n",
    "            print(f\"   Contingency: {r.contingency}\")\n",
    "            print(f\"   Owner:       {r.owner}\")\n",
    "\n",
    "# Create risk register for ML trading project\n",
    "risk_register = RiskRegister(\"Alpha Signals - ML Trading System\")\n",
    "\n",
    "# Technical Risks\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"T1\",\n",
    "    category=\"Technical\",\n",
    "    description=\"Model overfitting leads to poor out-of-sample performance\",\n",
    "    probability=0.6,\n",
    "    impact=0.8,\n",
    "    mitigation=\"Use proper cross-validation, regularization, and holdout test sets\",\n",
    "    contingency=\"Fall back to simpler baseline models with proven generalization\",\n",
    "    owner=\"ML Engineer\"\n",
    "))\n",
    "\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"T2\",\n",
    "    category=\"Technical\",\n",
    "    description=\"Data quality issues corrupt training datasets\",\n",
    "    probability=0.5,\n",
    "    impact=0.7,\n",
    "    mitigation=\"Implement comprehensive data validation and anomaly detection\",\n",
    "    contingency=\"Maintain clean backup data; manual inspection protocols\",\n",
    "    owner=\"Data Engineer\"\n",
    "))\n",
    "\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"T3\",\n",
    "    category=\"Technical\",\n",
    "    description=\"Backtesting framework has lookahead bias\",\n",
    "    probability=0.4,\n",
    "    impact=0.9,\n",
    "    mitigation=\"Code review focused on time leakage; use walk-forward validation\",\n",
    "    contingency=\"Paper trading validation before any live deployment\",\n",
    "    owner=\"Quant Developer\"\n",
    "))\n",
    "\n",
    "# Schedule Risks\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"S1\",\n",
    "    category=\"Schedule\",\n",
    "    description=\"Feature engineering takes longer than estimated\",\n",
    "    probability=0.7,\n",
    "    impact=0.5,\n",
    "    mitigation=\"Prioritize most impactful features; use established libraries\",\n",
    "    contingency=\"Reduce feature set to core technical indicators only\",\n",
    "    owner=\"Project Lead\"\n",
    "))\n",
    "\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"S2\",\n",
    "    category=\"Schedule\",\n",
    "    description=\"Integration testing reveals major issues late in project\",\n",
    "    probability=0.4,\n",
    "    impact=0.6,\n",
    "    mitigation=\"Continuous integration from day 1; daily integration builds\",\n",
    "    contingency=\"Descope to MVP with core functionality only\",\n",
    "    owner=\"Project Lead\"\n",
    "))\n",
    "\n",
    "# Resource Risks\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"R1\",\n",
    "    category=\"Resource\",\n",
    "    description=\"Free data APIs rate limits slow development\",\n",
    "    probability=0.8,\n",
    "    impact=0.3,\n",
    "    mitigation=\"Implement aggressive caching; use local data stores\",\n",
    "    contingency=\"Use pre-downloaded sample datasets for development\",\n",
    "    owner=\"Data Engineer\"\n",
    "))\n",
    "\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"R2\",\n",
    "    category=\"Resource\",\n",
    "    description=\"Training deep learning models exceeds compute budget\",\n",
    "    probability=0.5,\n",
    "    impact=0.4,\n",
    "    mitigation=\"Start with smaller models; use Google Colab for GPU access\",\n",
    "    contingency=\"Focus on gradient boosting models; skip deep learning\",\n",
    "    owner=\"ML Engineer\"\n",
    "))\n",
    "\n",
    "# Market/Domain Risks\n",
    "risk_register.add_risk(ProjectRisk(\n",
    "    id=\"M1\",\n",
    "    category=\"Market\",\n",
    "    description=\"Strategy doesn't generate meaningful alpha\",\n",
    "    probability=0.5,\n",
    "    impact=0.7,\n",
    "    mitigation=\"Research-first approach; validate signals before full implementation\",\n",
    "    contingency=\"Document learnings; pivot to different strategy type\",\n",
    "    owner=\"Quant Researcher\"\n",
    "))\n",
    "\n",
    "# Print the risk register\n",
    "risk_register.print_register()\n",
    "\n",
    "# Summary statistics\n",
    "summary = risk_register.get_summary()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ğŸ“Š RISK SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total Risks: {summary['total_risks']}\")\n",
    "print(f\"Average Score: {summary['avg_risk_score']:.2f}\")\n",
    "print(f\"ğŸ”´ Critical: {summary['critical_count']}\")\n",
    "print(f\"ğŸŸ  High: {summary['high_count']}\")\n",
    "print(f\"ğŸŸ¡ Medium: {summary['medium_count']}\")\n",
    "print(f\"ğŸŸ¢ Low: {summary['low_count']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
