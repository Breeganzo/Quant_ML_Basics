{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d367cd57",
   "metadata": {},
   "source": [
    "# ðŸ“š Week 6.1: Classification Methods for Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "This week covers **classification methods** essential for trading signal generation:\n",
    "\n",
    "| Day | Topic | Key Concepts |\n",
    "|-----|-------|--------------|\n",
    "| 1 | Logistic Regression | Sigmoid function, log-odds, MLE |\n",
    "| 2 | Support Vector Machines | Margin maximization, kernels |\n",
    "| 3 | Triple Barrier Method | Event-based labeling |\n",
    "| 4 | Meta-Labeling | Bet sizing, secondary model |\n",
    "| 5 | Multiclass Classification | One-vs-rest, softmax |\n",
    "| 6 | Class Imbalance | SMOTE, class weights |\n",
    "| 7 | Interview Review | Practice questions |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS & DATA\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Sample data\n",
    "ticker = 'SPY'\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=756)\n",
    "\n",
    "print(\"ðŸ“¥ Downloading data...\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "returns = data['Close'].pct_change().dropna()\n",
    "print(f\"âœ… Loaded {len(returns)} trading days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a2b45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 1: Logistic Regression\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Logistic Regression** models the probability of a binary outcome using the **sigmoid function**:\n",
    "\n",
    "$$P(Y=1|X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where $z = \\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n$\n",
    "\n",
    "### Log-Odds (Logit)\n",
    "\n",
    "$$\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n$$\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "The likelihood function:\n",
    "$$L(\\beta) = \\prod_{i=1}^{n} p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "\n",
    "Log-likelihood:\n",
    "$$\\ell(\\beta) = \\sum_{i=1}^{n} [y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]$$\n",
    "\n",
    "### Financial Application\n",
    "- Predict UP (1) vs DOWN (0) market moves\n",
    "- Probability output useful for position sizing\n",
    "- Coefficients show feature importance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 1: LOGISTIC REGRESSION\n",
    "# ============================================================\n",
    "\n",
    "# Create features and labels\n",
    "df = pd.DataFrame(index=returns.index)\n",
    "df['return'] = returns\n",
    "df['lag_1'] = returns.shift(1)\n",
    "df['lag_2'] = returns.shift(2)\n",
    "df['lag_5'] = returns.shift(5)\n",
    "df['vol_5'] = returns.rolling(5).std()\n",
    "df['vol_20'] = returns.rolling(20).std()\n",
    "df['momentum_10'] = data['Close'].pct_change(10)\n",
    "df['label'] = (returns.shift(-1) > 0).astype(int)  # Next day UP/DOWN\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare data\n",
    "features = ['lag_1', 'lag_2', 'lag_5', 'vol_5', 'vol_20', 'momentum_10']\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"ðŸ“Š LOGISTIC REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {(y_pred == y_test).mean():.2%}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feat, coef in zip(features, log_reg.coef_[0]):\n",
    "    print(f\"   {feat}: {coef:+.4f}\")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sigmoid function\n",
    "ax1 = axes[0]\n",
    "z = np.linspace(-6, 6, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "ax1.plot(z, sigmoid, 'b-', linewidth=2)\n",
    "ax1.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('z')\n",
    "ax1.set_ylabel('Ïƒ(z)')\n",
    "ax1.set_title('Sigmoid Function', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ROC Curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f56cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 2: Support Vector Machines (SVM)\n",
    "\n",
    "## Theory\n",
    "\n",
    "**SVM** finds the hyperplane that maximizes the **margin** between classes.\n",
    "\n",
    "### Linear SVM Optimization\n",
    "\n",
    "$$\\min_{w,b} \\frac{1}{2}||w||^2$$\n",
    "\n",
    "Subject to: $y_i(w \\cdot x_i + b) \\geq 1$ for all $i$\n",
    "\n",
    "### Soft Margin (C parameter)\n",
    "\n",
    "$$\\min_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum_{i=1}^{n}\\xi_i$$\n",
    "\n",
    "- **C large:** Smaller margin, fewer misclassifications\n",
    "- **C small:** Larger margin, more tolerant of errors\n",
    "\n",
    "### Kernel Trick\n",
    "\n",
    "Transform data to higher dimensions:\n",
    "- **Linear:** $K(x_i, x_j) = x_i \\cdot x_j$\n",
    "- **RBF:** $K(x_i, x_j) = \\exp(-\\gamma||x_i - x_j||^2)$\n",
    "- **Polynomial:** $K(x_i, x_j) = (x_i \\cdot x_j + r)^d$\n",
    "\n",
    "### Financial Application\n",
    "- Non-linear decision boundaries for complex patterns\n",
    "- Works well with high-dimensional feature spaces\n",
    "- Robust to outliers with soft margin\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37180bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 2: SUPPORT VECTOR MACHINES\n",
    "# ============================================================\n",
    "\n",
    "# Compare different kernels\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "svm_results = {}\n",
    "\n",
    "print(\"ðŸ“Š SVM KERNEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for kernel in kernels:\n",
    "    svm = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_svm = svm.predict(X_test)\n",
    "    accuracy = (y_pred_svm == y_test).mean()\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(svm, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    svm_results[kernel] = {\n",
    "        'model': svm,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{kernel.upper()} Kernel:\")\n",
    "    print(f\"   Test Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   CV Accuracy: {cv_scores.mean():.2%} Â± {cv_scores.std():.2%}\")\n",
    "\n",
    "# Visualize decision boundaries (2D projection)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, kernel in zip(axes, kernels):\n",
    "    # Use first 2 features for visualization\n",
    "    X_2d = X_scaled[:, :2]\n",
    "    X_train_2d, X_test_2d, _, _ = train_test_split(X_2d, y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    svm_2d = SVC(kernel=kernel, random_state=42)\n",
    "    svm_2d.fit(X_train_2d, y_train)\n",
    "    \n",
    "    # Create meshgrid\n",
    "    xx, yy = np.meshgrid(np.linspace(X_2d[:, 0].min()-1, X_2d[:, 0].max()+1, 100),\n",
    "                         np.linspace(X_2d[:, 1].min()-1, X_2d[:, 1].max()+1, 100))\n",
    "    Z = svm_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test, cmap='coolwarm', edgecolors='k', s=30)\n",
    "    ax.set_title(f'{kernel.upper()} Kernel', fontweight='bold')\n",
    "    ax.set_xlabel('Feature 1 (scaled)')\n",
    "    ax.set_ylabel('Feature 2 (scaled)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324024a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 3: Triple Barrier Method\n",
    "\n",
    "## Theory\n",
    "\n",
    "The **Triple Barrier Method** (Marcos Lopez de Prado) creates labels based on which barrier is touched first:\n",
    "\n",
    "### Three Barriers:\n",
    "1. **Upper Barrier (Take Profit):** Price rises by `pt` (profit taking)\n",
    "2. **Lower Barrier (Stop Loss):** Price falls by `sl` (stop loss)\n",
    "3. **Vertical Barrier (Time):** Maximum holding period `t_max`\n",
    "\n",
    "### Labels:\n",
    "- **+1 (BUY):** Upper barrier touched first\n",
    "- **-1 (SELL):** Lower barrier touched first\n",
    "- **0 (NEUTRAL):** Vertical barrier touched first\n",
    "\n",
    "### Advantages:\n",
    "- Event-driven labeling (not fixed horizon)\n",
    "- Incorporates risk management (stop-loss)\n",
    "- More realistic trading scenario\n",
    "- Adjustable barriers based on volatility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 3: TRIPLE BARRIER METHOD\n",
    "# ============================================================\n",
    "\n",
    "def triple_barrier_label(prices, pt=0.02, sl=0.02, max_holding=10):\n",
    "    \"\"\"\n",
    "    Triple barrier labeling method.\n",
    "    \n",
    "    Args:\n",
    "        prices: Price series\n",
    "        pt: Profit taking threshold (e.g., 0.02 = 2%)\n",
    "        sl: Stop loss threshold\n",
    "        max_holding: Maximum holding period in days\n",
    "    \n",
    "    Returns:\n",
    "        Labels: +1 (up), -1 (down), 0 (neutral)\n",
    "    \"\"\"\n",
    "    labels = pd.Series(index=prices.index, dtype=float)\n",
    "    \n",
    "    for i in range(len(prices) - max_holding):\n",
    "        entry_price = prices.iloc[i]\n",
    "        upper = entry_price * (1 + pt)\n",
    "        lower = entry_price * (1 - sl)\n",
    "        \n",
    "        # Check future prices\n",
    "        future_prices = prices.iloc[i+1:i+max_holding+1]\n",
    "        \n",
    "        # Find first barrier touch\n",
    "        upper_touch = (future_prices >= upper).idxmax() if (future_prices >= upper).any() else None\n",
    "        lower_touch = (future_prices <= lower).idxmax() if (future_prices <= lower).any() else None\n",
    "        \n",
    "        if upper_touch is None and lower_touch is None:\n",
    "            labels.iloc[i] = 0  # Vertical barrier (time)\n",
    "        elif upper_touch is None:\n",
    "            labels.iloc[i] = -1  # Stop loss hit\n",
    "        elif lower_touch is None:\n",
    "            labels.iloc[i] = 1  # Take profit hit\n",
    "        elif upper_touch <= lower_touch:\n",
    "            labels.iloc[i] = 1  # Take profit first\n",
    "        else:\n",
    "            labels.iloc[i] = -1  # Stop loss first\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Apply triple barrier\n",
    "prices = data['Close']\n",
    "tb_labels = triple_barrier_label(prices, pt=0.02, sl=0.02, max_holding=10)\n",
    "tb_labels = tb_labels.dropna()\n",
    "\n",
    "print(\"ðŸ“Š TRIPLE BARRIER LABELING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(f\"   +1 (Buy):    {(tb_labels == 1).sum():>5} ({(tb_labels == 1).mean():.1%})\")\n",
    "print(f\"   -1 (Sell):   {(tb_labels == -1).sum():>5} ({(tb_labels == -1).mean():.1%})\")\n",
    "print(f\"    0 (Neutral):{(tb_labels == 0).sum():>5} ({(tb_labels == 0).mean():.1%})\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Label distribution\n",
    "ax1 = axes[0]\n",
    "label_counts = tb_labels.value_counts().sort_index()\n",
    "colors = ['red', 'gray', 'green']\n",
    "ax1.bar(['Sell (-1)', 'Neutral (0)', 'Buy (+1)'], label_counts.values, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Triple Barrier Label Distribution', fontweight='bold')\n",
    "\n",
    "# Example visualization\n",
    "ax2 = axes[1]\n",
    "sample_idx = 100\n",
    "sample_price = prices.iloc[sample_idx]\n",
    "future = prices.iloc[sample_idx:sample_idx+15]\n",
    "ax2.plot(range(len(future)), future, 'b-', linewidth=2, marker='o')\n",
    "ax2.axhline(y=sample_price * 1.02, color='green', linestyle='--', label='Upper (+2%)')\n",
    "ax2.axhline(y=sample_price * 0.98, color='red', linestyle='--', label='Lower (-2%)')\n",
    "ax2.axvline(x=10, color='gray', linestyle='--', label='Time Barrier')\n",
    "ax2.scatter([0], [sample_price], color='blue', s=100, zorder=5, label='Entry')\n",
    "ax2.set_xlabel('Days')\n",
    "ax2.set_ylabel('Price')\n",
    "ax2.set_title('Triple Barrier Example', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a706fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 4: Meta-Labeling\n",
    "\n",
    "## Theory\n",
    "\n",
    "**Meta-Labeling** is a two-stage approach:\n",
    "\n",
    "### Stage 1: Primary Model\n",
    "- Predicts direction (buy/sell signal)\n",
    "- Focus on **recall** (catch all opportunities)\n",
    "\n",
    "### Stage 2: Secondary Model (Meta-Labeler)\n",
    "- Predicts whether to **act** on primary signal\n",
    "- Focus on **precision** (filter false positives)\n",
    "- Output: Probability â†’ **bet sizing**\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "$$\\text{Final Signal} = \\text{Primary Signal} \\times P(\\text{Meta-Label} = 1)$$\n",
    "\n",
    "### Advantages:\n",
    "- Decouples direction prediction from bet sizing\n",
    "- Allows different features for each model\n",
    "- Improves risk-adjusted returns\n",
    "- Reduces false positive trades\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b50dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 4: META-LABELING\n",
    "# ============================================================\n",
    "\n",
    "# Primary Model: Simple momentum signal\n",
    "df['primary_signal'] = np.where(df['momentum_10'] > 0, 1, -1)\n",
    "\n",
    "# Meta-label: Did the primary signal make money?\n",
    "df['meta_label'] = ((df['primary_signal'] * df['return'].shift(-1)) > 0).astype(int)\n",
    "\n",
    "# Prepare meta-labeling data\n",
    "meta_features = ['lag_1', 'lag_2', 'vol_5', 'vol_20']\n",
    "X_meta = df[meta_features].dropna()\n",
    "y_meta = df['meta_label'].loc[X_meta.index]\n",
    "\n",
    "X_train_meta, X_test_meta, y_train_meta, y_test_meta = train_test_split(\n",
    "    X_meta, y_meta, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler_meta = StandardScaler()\n",
    "X_train_meta_scaled = scaler_meta.fit_transform(X_train_meta)\n",
    "X_test_meta_scaled = scaler_meta.transform(X_test_meta)\n",
    "\n",
    "# Train meta-labeler\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(X_train_meta_scaled, y_train_meta)\n",
    "\n",
    "# Predictions\n",
    "meta_prob = meta_model.predict_proba(X_test_meta_scaled)[:, 1]\n",
    "meta_pred = meta_model.predict(X_test_meta_scaled)\n",
    "\n",
    "print(\"ðŸ“Š META-LABELING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMeta-Model Accuracy: {(meta_pred == y_test_meta).mean():.2%}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"   - High probability â†’ Strong conviction, larger position\")\n",
    "print(f\"   - Low probability â†’ Weak conviction, smaller/no position\")\n",
    "\n",
    "# Position sizing based on meta-label\n",
    "print(f\"\\nPosition Sizing Example:\")\n",
    "print(f\"   Probability > 0.7: Full position (100%)\")\n",
    "print(f\"   Probability 0.5-0.7: Half position (50%)\")\n",
    "print(f\"   Probability < 0.5: No position (0%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(meta_prob[y_test_meta == 1], bins=20, alpha=0.7, label='Correct Primary Signals', color='green')\n",
    "ax.hist(meta_prob[y_test_meta == 0], bins=20, alpha=0.7, label='Incorrect Primary Signals', color='red')\n",
    "ax.axvline(x=0.5, color='k', linestyle='--', label='Threshold')\n",
    "ax.set_xlabel('Meta-Label Probability')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Meta-Label Probability Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82170221",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 5: Multiclass Classification\n",
    "\n",
    "## Theory\n",
    "\n",
    "### One-vs-Rest (OvR)\n",
    "\n",
    "Train K binary classifiers, one for each class:\n",
    "- Classifier k: Class k vs. all other classes\n",
    "- Predict: Class with highest confidence\n",
    "\n",
    "### One-vs-One (OvO)\n",
    "\n",
    "Train K(K-1)/2 binary classifiers:\n",
    "- One for each pair of classes\n",
    "- Predict: Class with most votes\n",
    "\n",
    "### Softmax (Multinomial Logistic)\n",
    "\n",
    "$$P(Y=k|X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "### Financial Application\n",
    "- 3 classes: Strong Buy, Hold, Strong Sell\n",
    "- 5 classes: Quintile predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 5: MULTICLASS CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "# Create 3-class labels\n",
    "df['multiclass'] = pd.cut(df['return'].shift(-1), \n",
    "                          bins=[-np.inf, -0.01, 0.01, np.inf],\n",
    "                          labels=[0, 1, 2])  # 0=Sell, 1=Hold, 2=Buy\n",
    "df_multi = df.dropna()\n",
    "\n",
    "X_multi = df_multi[features]\n",
    "y_multi = df_multi['multiclass'].astype(int)\n",
    "\n",
    "# Scale and split\n",
    "X_multi_scaled = scaler.fit_transform(X_multi)\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_multi_scaled, y_multi, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "# Multiclass Logistic Regression\n",
    "multi_model = LogisticRegression(multi_class='multinomial', random_state=42)\n",
    "multi_model.fit(X_train_m, y_train_m)\n",
    "\n",
    "y_pred_m = multi_model.predict(X_test_m)\n",
    "y_prob_m = multi_model.predict_proba(X_test_m)\n",
    "\n",
    "print(\"ðŸ“Š MULTICLASS CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy: {(y_pred_m == y_test_m).mean():.2%}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"   Sell (0):  {(y_test_m == 0).sum():>4} ({(y_test_m == 0).mean():.1%})\")\n",
    "print(f\"   Hold (1):  {(y_test_m == 1).sum():>4} ({(y_test_m == 1).mean():.1%})\")\n",
    "print(f\"   Buy (2):   {(y_test_m == 2).sum():>4} ({(y_test_m == 2).mean():.1%})\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_m, y_pred_m, target_names=['Sell', 'Hold', 'Buy']))\n",
    "\n",
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test_m, y_pred_m)\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_xticklabels(['Sell', 'Hold', 'Buy'])\n",
    "ax.set_yticklabels(['Sell', 'Hold', 'Buy'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix', fontweight='bold')\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', fontsize=14)\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c1bda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 6: Handling Class Imbalance\n",
    "\n",
    "## Theory\n",
    "\n",
    "### The Problem\n",
    "Financial data is often imbalanced:\n",
    "- Rare events (crashes, breakouts)\n",
    "- Extreme returns underrepresented\n",
    "\n",
    "### Solutions\n",
    "\n",
    "**1. Resampling:**\n",
    "- **Oversampling:** SMOTE (Synthetic Minority Over-sampling)\n",
    "- **Undersampling:** Random undersampling\n",
    "\n",
    "**2. Class Weights:**\n",
    "$$w_k = \\frac{n}{K \\cdot n_k}$$\n",
    "\n",
    "**3. Evaluation Metrics:**\n",
    "- **Precision:** $\\frac{TP}{TP + FP}$\n",
    "- **Recall:** $\\frac{TP}{TP + FN}$\n",
    "- **F1-Score:** $2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$\n",
    "- **AUC-ROC:** Area under ROC curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10452fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 6: CLASS IMBALANCE\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Create imbalanced scenario (extreme returns)\n",
    "df['extreme'] = (np.abs(df['return'].shift(-1)) > 0.02).astype(int)\n",
    "df_imb = df.dropna()\n",
    "\n",
    "X_imb = df_imb[features]\n",
    "y_imb = df_imb['extreme']\n",
    "\n",
    "print(\"ðŸ“Š CLASS IMBALANCE HANDLING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal Class Distribution:\")\n",
    "print(f\"   Normal (0): {(y_imb == 0).sum():>5} ({(y_imb == 0).mean():.1%})\")\n",
    "print(f\"   Extreme (1): {(y_imb == 1).sum():>5} ({(y_imb == 1).mean():.1%})\")\n",
    "\n",
    "# Scale and split\n",
    "X_imb_scaled = scaler.fit_transform(X_imb)\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
    "    X_imb_scaled, y_imb, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "# Compare: No weights vs. Class weights\n",
    "results_imb = {}\n",
    "\n",
    "# Without class weights\n",
    "model_no_weight = LogisticRegression(random_state=42)\n",
    "model_no_weight.fit(X_train_i, y_train_i)\n",
    "y_pred_nw = model_no_weight.predict(X_test_i)\n",
    "results_imb['No Weights'] = {\n",
    "    'recall_1': (y_pred_nw[y_test_i == 1] == 1).mean(),\n",
    "    'precision_1': (y_test_i[y_pred_nw == 1] == 1).mean() if (y_pred_nw == 1).sum() > 0 else 0\n",
    "}\n",
    "\n",
    "# With class weights\n",
    "model_weighted = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "model_weighted.fit(X_train_i, y_train_i)\n",
    "y_pred_w = model_weighted.predict(X_test_i)\n",
    "results_imb['Balanced Weights'] = {\n",
    "    'recall_1': (y_pred_w[y_test_i == 1] == 1).mean(),\n",
    "    'precision_1': (y_test_i[y_pred_w == 1] == 1).mean() if (y_pred_w == 1).sum() > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"{'Method':<20} {'Recall (Extreme)':>18} {'Precision (Extreme)':>20}\")\n",
    "print(\"-\" * 60)\n",
    "for method, metrics in results_imb.items():\n",
    "    print(f\"{method:<20} {metrics['recall_1']:>17.2%} {metrics['precision_1']:>19.2%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "cm_nw = confusion_matrix(y_test_i, y_pred_nw)\n",
    "im1 = ax1.imshow(cm_nw, cmap='Reds')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['Normal', 'Extreme'])\n",
    "ax1.set_yticklabels(['Normal', 'Extreme'])\n",
    "ax1.set_title('Without Class Weights', fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax1.text(j, i, cm_nw[i, j], ha='center', va='center', fontsize=14)\n",
    "\n",
    "ax2 = axes[1]\n",
    "cm_w = confusion_matrix(y_test_i, y_pred_w)\n",
    "im2 = ax2.imshow(cm_w, cmap='Greens')\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_yticks([0, 1])\n",
    "ax2.set_xticklabels(['Normal', 'Extreme'])\n",
    "ax2.set_yticklabels(['Normal', 'Extreme'])\n",
    "ax2.set_title('With Balanced Weights', fontweight='bold')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax2.text(j, i, cm_w[i, j], ha='center', va='center', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e8917",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 7: Interview Review\n",
    "\n",
    "## Common Interview Questions\n",
    "\n",
    "### Q1: When to use Logistic Regression vs SVM?\n",
    "**Answer:**\n",
    "- **Logistic Regression:** When you need probability outputs, interpretable coefficients, baseline model\n",
    "- **SVM:** When decision boundary is non-linear, high-dimensional data, robust to outliers\n",
    "\n",
    "### Q2: Explain the Triple Barrier Method\n",
    "**Answer:**\n",
    "- Event-based labeling with 3 barriers: profit taking, stop loss, time\n",
    "- Labels based on which barrier is touched first\n",
    "- More realistic than fixed-horizon returns\n",
    "\n",
    "### Q3: What is Meta-Labeling?\n",
    "**Answer:**\n",
    "- Two-stage approach: primary model (direction) + secondary model (bet sizing)\n",
    "- Allows different optimization objectives for each stage\n",
    "- Improves risk-adjusted returns\n",
    "\n",
    "### Q4: How do you handle class imbalance?\n",
    "**Answer:**\n",
    "- Class weights (inverse frequency)\n",
    "- SMOTE oversampling\n",
    "- Appropriate metrics (F1, AUC-ROC)\n",
    "- Cost-sensitive learning\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Method | Pros | Cons | Use Case |\n",
    "|--------|------|------|----------|\n",
    "| Logistic | Interpretable, probabilities | Linear only | Baseline, probabilities |\n",
    "| SVM | Non-linear, robust | No probabilities (default) | Complex boundaries |\n",
    "| Triple Barrier | Realistic labels | Parameter tuning | Signal generation |\n",
    "| Meta-Label | Bet sizing | Complexity | Position sizing |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
