{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc64ea68",
   "metadata": {},
   "source": [
    "# Week 16 - Day 6: PPO Trading\n",
    "\n",
    "## Proximal Policy Optimization for Algorithmic Trading\n",
    "\n",
    "**Date:** Day 6 of Week 16 - Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **PPO Algorithm Basics** - Understand the theory behind Proximal Policy Optimization\n",
    "2. **Clipped Objectives** - Learn how PPO stabilizes training with clipped surrogate objectives\n",
    "3. **Full Trading Environment** - Build a complete gym-style trading environment\n",
    "4. **Training and Evaluation** - Train PPO agent and evaluate trading performance\n",
    "\n",
    "---\n",
    "\n",
    "## Why PPO for Trading?\n",
    "\n",
    "PPO is one of the most successful deep reinforcement learning algorithms, offering:\n",
    "- **Stability**: Constrains policy updates to prevent catastrophic changes\n",
    "- **Sample Efficiency**: Better data utilization than vanilla policy gradient\n",
    "- **Simplicity**: Easier to implement and tune than TRPO\n",
    "- **Robustness**: Works well across diverse environments including finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385af4e",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2caa4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f122263",
   "metadata": {},
   "source": [
    "## Part 2: PPO Algorithm Basics\n",
    "\n",
    "### 2.1 Policy Gradient Fundamentals\n",
    "\n",
    "The standard policy gradient objective is:\n",
    "\n",
    "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\log \\pi_\\theta(a_t | s_t) \\hat{A}_t \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ is the policy network\n",
    "- $\\hat{A}_t$ is the advantage estimate at time $t$\n",
    "\n",
    "### 2.2 The Problem with Vanilla Policy Gradient\n",
    "\n",
    "- Large policy updates can destroy learned behavior\n",
    "- Training is unstable and sensitive to hyperparameters\n",
    "- No constraint on how much the policy can change\n",
    "\n",
    "### 2.3 PPO Solution: Clipped Surrogate Objective\n",
    "\n",
    "PPO introduces the probability ratio:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{old}}(a_t | s_t)}$$\n",
    "\n",
    "And the clipped objective:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "Where $\\epsilon$ (typically 0.1-0.3) limits how much the policy can change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57132a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PPO clipping mechanism\n",
    "def visualize_ppo_clipping(epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Visualize how PPO clips the objective function\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Ratio values\n",
    "    r = np.linspace(0, 2, 1000)\n",
    "    \n",
    "    # Case 1: Positive advantage (A > 0)\n",
    "    ax1 = axes[0]\n",
    "    A_pos = 1.0\n",
    "    \n",
    "    # Unclipped objective\n",
    "    L_unclipped_pos = r * A_pos\n",
    "    \n",
    "    # Clipped objective\n",
    "    r_clipped = np.clip(r, 1 - epsilon, 1 + epsilon)\n",
    "    L_clipped_pos = r_clipped * A_pos\n",
    "    \n",
    "    # PPO objective (min of both)\n",
    "    L_ppo_pos = np.minimum(L_unclipped_pos, L_clipped_pos)\n",
    "    \n",
    "    ax1.plot(r, L_unclipped_pos, 'b--', label='Unclipped', alpha=0.7, linewidth=2)\n",
    "    ax1.plot(r, L_clipped_pos, 'r--', label='Clipped', alpha=0.7, linewidth=2)\n",
    "    ax1.plot(r, L_ppo_pos, 'g-', label='PPO (min)', linewidth=3)\n",
    "    ax1.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax1.axvline(x=1-epsilon, color='orange', linestyle='--', alpha=0.5, label=f'Clip bounds')\n",
    "    ax1.axvline(x=1+epsilon, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax1.fill_between(r, L_ppo_pos, alpha=0.2, color='green')\n",
    "    ax1.set_xlabel('Probability Ratio r(Î¸)', fontsize=12)\n",
    "    ax1.set_ylabel('Objective L', fontsize=12)\n",
    "    ax1.set_title('Positive Advantage (A > 0)\\nIncentivizes action, but clips to prevent too much change', fontsize=11)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 2)\n",
    "    \n",
    "    # Case 2: Negative advantage (A < 0)\n",
    "    ax2 = axes[1]\n",
    "    A_neg = -1.0\n",
    "    \n",
    "    L_unclipped_neg = r * A_neg\n",
    "    L_clipped_neg = r_clipped * A_neg\n",
    "    L_ppo_neg = np.minimum(L_unclipped_neg, L_clipped_neg)\n",
    "    \n",
    "    ax2.plot(r, L_unclipped_neg, 'b--', label='Unclipped', alpha=0.7, linewidth=2)\n",
    "    ax2.plot(r, L_clipped_neg, 'r--', label='Clipped', alpha=0.7, linewidth=2)\n",
    "    ax2.plot(r, L_ppo_neg, 'g-', label='PPO (min)', linewidth=3)\n",
    "    ax2.axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax2.axvline(x=1-epsilon, color='orange', linestyle='--', alpha=0.5, label=f'Clip bounds')\n",
    "    ax2.axvline(x=1+epsilon, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax2.fill_between(r, L_ppo_neg, alpha=0.2, color='green')\n",
    "    ax2.set_xlabel('Probability Ratio r(Î¸)', fontsize=12)\n",
    "    ax2.set_ylabel('Objective L', fontsize=12)\n",
    "    ax2.set_title('Negative Advantage (A < 0)\\nDiscourages action, but clips to prevent too much change', fontsize=11)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, 2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'PPO Clipped Objective (Îµ = {epsilon})', fontsize=14, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Key Insights:\")\n",
    "    print(\"â€¢ When A > 0: PPO encourages increasing action probability, but caps the benefit\")\n",
    "    print(\"â€¢ When A < 0: PPO encourages decreasing action probability, but limits the penalty\")\n",
    "    print(f\"â€¢ The clipping range [{1-epsilon:.1f}, {1+epsilon:.1f}] creates a 'trust region'\")\n",
    "\n",
    "visualize_ppo_clipping(epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130d62f",
   "metadata": {},
   "source": [
    "## Part 3: Full Trading Environment\n",
    "\n",
    "### 3.1 Environment Design\n",
    "\n",
    "Our trading environment will have:\n",
    "- **State Space**: Price features, technical indicators, portfolio state\n",
    "- **Action Space**: Hold (0), Buy (1), Sell (2)\n",
    "- **Reward**: Portfolio return with risk adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download market data\n",
    "def download_data(ticker='SPY', start='2018-01-01', end='2024-01-01'):\n",
    "    \"\"\"\n",
    "    Download and prepare market data\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ“¥ Downloading {ticker} data...\")\n",
    "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
    "    \n",
    "    # Use only Close price as requested\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df = df['Close'].to_frame(name='Close')\n",
    "    else:\n",
    "        df = df[['Close']]\n",
    "    \n",
    "    df = df.dropna()\n",
    "    print(f\"âœ… Downloaded {len(df)} trading days\")\n",
    "    print(f\"   Date range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Download data\n",
    "df = download_data('SPY', start='2018-01-01', end='2024-01-01')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    \"\"\"\n",
    "    A full trading environment for PPO\n",
    "    \n",
    "    Features:\n",
    "    - Realistic transaction costs\n",
    "    - Position tracking\n",
    "    - Multiple technical indicators as state\n",
    "    - Risk-adjusted rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, initial_balance=100000, transaction_cost=0.001, \n",
    "                 window_size=20, max_position=1.0):\n",
    "        \"\"\"\n",
    "        Initialize trading environment\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with 'Close' prices\n",
    "            initial_balance: Starting capital\n",
    "            transaction_cost: Cost per trade (0.1%)\n",
    "            window_size: Lookback window for features\n",
    "            max_position: Maximum position size (1.0 = 100% of capital)\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.window_size = window_size\n",
    "        self.max_position = max_position\n",
    "        \n",
    "        # Prepare features\n",
    "        self._prepare_features()\n",
    "        \n",
    "        # Action space: 0=Hold, 1=Buy, 2=Sell\n",
    "        self.action_space = 3\n",
    "        self.state_dim = self._get_state_dim()\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _prepare_features(self):\n",
    "        \"\"\"\n",
    "        Calculate technical indicators from Close price\n",
    "        \"\"\"\n",
    "        close = self.df['Close']\n",
    "        \n",
    "        # Returns\n",
    "        self.df['Returns'] = close.pct_change()\n",
    "        \n",
    "        # Moving averages\n",
    "        self.df['SMA_10'] = close.rolling(10).mean()\n",
    "        self.df['SMA_20'] = close.rolling(20).mean()\n",
    "        self.df['SMA_50'] = close.rolling(50).mean()\n",
    "        \n",
    "        # Price relative to MAs\n",
    "        self.df['Price_SMA10_Ratio'] = close / self.df['SMA_10']\n",
    "        self.df['Price_SMA20_Ratio'] = close / self.df['SMA_20']\n",
    "        self.df['SMA10_SMA20_Ratio'] = self.df['SMA_10'] / self.df['SMA_20']\n",
    "        \n",
    "        # Volatility\n",
    "        self.df['Volatility_10'] = self.df['Returns'].rolling(10).std()\n",
    "        self.df['Volatility_20'] = self.df['Returns'].rolling(20).std()\n",
    "        \n",
    "        # Momentum\n",
    "        self.df['Momentum_5'] = close.pct_change(5)\n",
    "        self.df['Momentum_10'] = close.pct_change(10)\n",
    "        self.df['Momentum_20'] = close.pct_change(20)\n",
    "        \n",
    "        # RSI\n",
    "        delta = close.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        self.df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        self.df['RSI_Normalized'] = (self.df['RSI'] - 50) / 50  # Normalize to [-1, 1]\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        self.df['BB_Middle'] = close.rolling(20).mean()\n",
    "        bb_std = close.rolling(20).std()\n",
    "        self.df['BB_Upper'] = self.df['BB_Middle'] + 2 * bb_std\n",
    "        self.df['BB_Lower'] = self.df['BB_Middle'] - 2 * bb_std\n",
    "        self.df['BB_Position'] = (close - self.df['BB_Lower']) / (self.df['BB_Upper'] - self.df['BB_Lower'])\n",
    "        \n",
    "        # Drop NaN rows\n",
    "        self.df = self.df.dropna()\n",
    "        \n",
    "        # Feature columns for state\n",
    "        self.feature_columns = [\n",
    "            'Returns', 'Price_SMA10_Ratio', 'Price_SMA20_Ratio', 'SMA10_SMA20_Ratio',\n",
    "            'Volatility_10', 'Volatility_20', 'Momentum_5', 'Momentum_10', 'Momentum_20',\n",
    "            'RSI_Normalized', 'BB_Position'\n",
    "        ]\n",
    "    \n",
    "    def _get_state_dim(self):\n",
    "        \"\"\"\n",
    "        Get state dimension: features + portfolio state\n",
    "        \"\"\"\n",
    "        # Features + position + unrealized_pnl_pct + cash_ratio\n",
    "        return len(self.feature_columns) + 3\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment to initial state\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.cost_basis = 0\n",
    "        self.total_trades = 0\n",
    "        self.total_profit = 0\n",
    "        \n",
    "        # Track history for analysis\n",
    "        self.history = {\n",
    "            'portfolio_value': [self.initial_balance],\n",
    "            'actions': [],\n",
    "            'positions': [0],\n",
    "            'returns': []\n",
    "        }\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Get current state observation\n",
    "        \"\"\"\n",
    "        # Market features\n",
    "        features = self.df[self.feature_columns].iloc[self.current_step].values\n",
    "        \n",
    "        # Portfolio state\n",
    "        current_price = self.df['Close'].iloc[self.current_step]\n",
    "        portfolio_value = self.balance + self.shares_held * current_price\n",
    "        \n",
    "        # Position as fraction of portfolio\n",
    "        position = (self.shares_held * current_price) / portfolio_value if portfolio_value > 0 else 0\n",
    "        \n",
    "        # Unrealized P&L percentage\n",
    "        if self.shares_held > 0 and self.cost_basis > 0:\n",
    "            unrealized_pnl_pct = (current_price - self.cost_basis) / self.cost_basis\n",
    "        else:\n",
    "            unrealized_pnl_pct = 0\n",
    "        \n",
    "        # Cash ratio\n",
    "        cash_ratio = self.balance / portfolio_value if portfolio_value > 0 else 1\n",
    "        \n",
    "        # Combine all state components\n",
    "        state = np.concatenate([\n",
    "            features,\n",
    "            [position, unrealized_pnl_pct, cash_ratio]\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute action and return next state, reward, done\n",
    "        \n",
    "        Args:\n",
    "            action: 0=Hold, 1=Buy, 2=Sell\n",
    "        \n",
    "        Returns:\n",
    "            next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        current_price = self.df['Close'].iloc[self.current_step]\n",
    "        prev_portfolio_value = self.balance + self.shares_held * current_price\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 1:  # Buy\n",
    "            self._buy(current_price)\n",
    "        elif action == 2:  # Sell\n",
    "            self._sell(current_price)\n",
    "        # action == 0 is Hold, do nothing\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        \n",
    "        # Get new price and portfolio value\n",
    "        new_price = self.df['Close'].iloc[self.current_step]\n",
    "        new_portfolio_value = self.balance + self.shares_held * new_price\n",
    "        \n",
    "        # Calculate reward (log return with risk adjustment)\n",
    "        portfolio_return = (new_portfolio_value - prev_portfolio_value) / prev_portfolio_value\n",
    "        reward = self._calculate_reward(portfolio_return, action)\n",
    "        \n",
    "        # Update history\n",
    "        self.history['portfolio_value'].append(new_portfolio_value)\n",
    "        self.history['actions'].append(action)\n",
    "        self.history['positions'].append(self.shares_held * new_price / new_portfolio_value if new_portfolio_value > 0 else 0)\n",
    "        self.history['returns'].append(portfolio_return)\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': new_portfolio_value,\n",
    "            'position': self.shares_held,\n",
    "            'balance': self.balance,\n",
    "            'total_trades': self.total_trades\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def _buy(self, price):\n",
    "        \"\"\"\n",
    "        Execute buy order\n",
    "        \"\"\"\n",
    "        if self.balance > 0:\n",
    "            # Calculate max shares we can buy\n",
    "            max_investment = self.balance * self.max_position\n",
    "            transaction_fee = max_investment * self.transaction_cost\n",
    "            shares_to_buy = int((max_investment - transaction_fee) / price)\n",
    "            \n",
    "            if shares_to_buy > 0:\n",
    "                cost = shares_to_buy * price\n",
    "                fee = cost * self.transaction_cost\n",
    "                \n",
    "                self.balance -= (cost + fee)\n",
    "                self.shares_held += shares_to_buy\n",
    "                self.cost_basis = price\n",
    "                self.total_trades += 1\n",
    "    \n",
    "    def _sell(self, price):\n",
    "        \"\"\"\n",
    "        Execute sell order\n",
    "        \"\"\"\n",
    "        if self.shares_held > 0:\n",
    "            revenue = self.shares_held * price\n",
    "            fee = revenue * self.transaction_cost\n",
    "            \n",
    "            profit = revenue - (self.shares_held * self.cost_basis) - fee\n",
    "            self.total_profit += profit\n",
    "            \n",
    "            self.balance += (revenue - fee)\n",
    "            self.shares_held = 0\n",
    "            self.cost_basis = 0\n",
    "            self.total_trades += 1\n",
    "    \n",
    "    def _calculate_reward(self, portfolio_return, action):\n",
    "        \"\"\"\n",
    "        Calculate risk-adjusted reward\n",
    "        \"\"\"\n",
    "        # Base reward is the portfolio return\n",
    "        reward = portfolio_return * 100  # Scale for better learning\n",
    "        \n",
    "        # Penalty for excessive trading (transaction costs already applied)\n",
    "        if action != 0:  # If not holding\n",
    "            reward -= 0.01  # Small penalty for trading\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate performance metrics\n",
    "        \"\"\"\n",
    "        portfolio_values = np.array(self.history['portfolio_value'])\n",
    "        returns = np.array(self.history['returns']) if self.history['returns'] else np.array([0])\n",
    "        \n",
    "        # Total return\n",
    "        total_return = (portfolio_values[-1] - self.initial_balance) / self.initial_balance\n",
    "        \n",
    "        # Annualized return\n",
    "        n_days = len(portfolio_values)\n",
    "        annualized_return = (1 + total_return) ** (252 / n_days) - 1 if n_days > 0 else 0\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            sharpe = np.sqrt(252) * np.mean(returns) / np.std(returns)\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        peak = np.maximum.accumulate(portfolio_values)\n",
    "        drawdown = (peak - portfolio_values) / peak\n",
    "        max_drawdown = np.max(drawdown)\n",
    "        \n",
    "        # Win rate\n",
    "        if len(returns) > 0:\n",
    "            win_rate = np.sum(returns > 0) / len(returns)\n",
    "        else:\n",
    "            win_rate = 0\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'annualized_return': annualized_return,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'total_trades': self.total_trades,\n",
    "            'final_value': portfolio_values[-1]\n",
    "        }\n",
    "\n",
    "print(\"âœ… Trading Environment class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439f8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = TradingEnvironment(df, initial_balance=100000)\n",
    "print(f\"\\nðŸ“Š Environment Configuration:\")\n",
    "print(f\"   State dimension: {env.state_dim}\")\n",
    "print(f\"   Action space: {env.action_space} (Hold, Buy, Sell)\")\n",
    "print(f\"   Data points: {len(env.df)}\")\n",
    "print(f\"   Feature columns: {env.feature_columns}\")\n",
    "\n",
    "# Test a few random steps\n",
    "state = env.reset()\n",
    "print(f\"\\nðŸ” Initial state shape: {state.shape}\")\n",
    "print(f\"   Initial state (first 5 values): {state[:5]}\")\n",
    "\n",
    "# Take some random actions\n",
    "for i in range(5):\n",
    "    action = np.random.randint(0, 3)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    action_names = ['Hold', 'Buy', 'Sell']\n",
    "    print(f\"   Step {i+1}: Action={action_names[action]}, Reward={reward:.4f}, Portfolio=${info['portfolio_value']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd8a52",
   "metadata": {},
   "source": [
    "## Part 4: PPO Neural Network Architecture\n",
    "\n",
    "### 4.1 Actor-Critic Network\n",
    "\n",
    "PPO uses an Actor-Critic architecture:\n",
    "- **Actor (Policy Network)**: Outputs action probabilities\n",
    "- **Critic (Value Network)**: Estimates state value for advantage calculation\n",
    "\n",
    "We'll use a shared backbone with separate heads for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network for PPO\n",
    "    \n",
    "    Uses shared layers with separate heads for policy and value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights using orthogonal initialization\n",
    "        \"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Returns:\n",
    "            action_probs: Probability distribution over actions\n",
    "            value: Estimated state value\n",
    "        \"\"\"\n",
    "        shared_features = self.shared(state)\n",
    "        action_probs = self.actor(shared_features)\n",
    "        value = self.critic(shared_features)\n",
    "        return action_probs, value\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Select action based on current policy\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected action\n",
    "            log_prob: Log probability of selected action\n",
    "            value: Estimated state value\n",
    "        \"\"\"\n",
    "        action_probs, value = self.forward(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value\n",
    "    \n",
    "    def evaluate(self, states, actions):\n",
    "        \"\"\"\n",
    "        Evaluate actions for PPO update\n",
    "        \n",
    "        Returns:\n",
    "            log_probs: Log probabilities of actions\n",
    "            values: State values\n",
    "            entropy: Policy entropy\n",
    "        \"\"\"\n",
    "        action_probs, values = self.forward(states)\n",
    "        dist = Categorical(action_probs)\n",
    "        \n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return log_probs, values.squeeze(-1), entropy\n",
    "\n",
    "# Test the network\n",
    "test_model = ActorCritic(state_dim=env.state_dim, action_dim=env.action_space)\n",
    "test_state = torch.FloatTensor(env.reset()).unsqueeze(0)\n",
    "action_probs, value = test_model(test_state)\n",
    "\n",
    "print(\"âœ… Actor-Critic Network\")\n",
    "print(f\"   Input state dim: {env.state_dim}\")\n",
    "print(f\"   Action probabilities: {action_probs.detach().numpy()[0]}\")\n",
    "print(f\"   State value: {value.item():.4f}\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8adfb",
   "metadata": {},
   "source": [
    "## Part 5: PPO Agent Implementation\n",
    "\n",
    "### 5.1 Memory Buffer\n",
    "\n",
    "PPO needs to store trajectories for batch updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70571790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Buffer to store rollout data for PPO updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def add(self, state, action, reward, log_prob, value, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def get(self):\n",
    "        return (\n",
    "            np.array(self.states),\n",
    "            np.array(self.actions),\n",
    "            np.array(self.rewards),\n",
    "            np.array([lp.item() for lp in self.log_probs]),\n",
    "            np.array([v.item() for v in self.values]),\n",
    "            np.array(self.dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "print(\"âœ… RolloutBuffer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization Agent for Trading\n",
    "    \n",
    "    Key features:\n",
    "    - Clipped surrogate objective\n",
    "    - Generalized Advantage Estimation (GAE)\n",
    "    - Entropy bonus for exploration\n",
    "    - Value function clipping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, \n",
    "                 lr=3e-4,\n",
    "                 gamma=0.99,\n",
    "                 gae_lambda=0.95,\n",
    "                 clip_epsilon=0.2,\n",
    "                 c1=0.5,  # Value loss coefficient\n",
    "                 c2=0.01,  # Entropy coefficient\n",
    "                 n_epochs=10,\n",
    "                 batch_size=64,\n",
    "                 hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Initialize PPO Agent\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Number of possible actions\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            gae_lambda: GAE parameter\n",
    "            clip_epsilon: PPO clipping parameter\n",
    "            c1: Value loss coefficient\n",
    "            c2: Entropy bonus coefficient\n",
    "            n_epochs: Number of epochs per update\n",
    "            batch_size: Mini-batch size for updates\n",
    "            hidden_dim: Hidden layer dimension\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Actor-Critic network\n",
    "        self.policy = ActorCritic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr, eps=1e-5)\n",
    "        \n",
    "        # Memory buffer\n",
    "        self.buffer = RolloutBuffer()\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_stats = {\n",
    "            'policy_loss': [],\n",
    "            'value_loss': [],\n",
    "            'entropy': [],\n",
    "            'total_loss': [],\n",
    "            'clip_fraction': []\n",
    "        }\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action using current policy\n",
    "        \"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value = self.policy.act(state_tensor)\n",
    "        \n",
    "        return action, log_prob, value\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, value, done):\n",
    "        \"\"\"\n",
    "        Store transition in buffer\n",
    "        \"\"\"\n",
    "        self.buffer.add(state, action, reward, log_prob, value, done)\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones, last_value):\n",
    "        \"\"\"\n",
    "        Compute Generalized Advantage Estimation\n",
    "        \n",
    "        GAE formula:\n",
    "        A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...\n",
    "        where Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)\n",
    "        \"\"\"\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        returns = np.zeros_like(rewards)\n",
    "        \n",
    "        gae = 0\n",
    "        next_value = last_value\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if dones[t]:\n",
    "                delta = rewards[t] - values[t]\n",
    "                gae = delta\n",
    "            else:\n",
    "                delta = rewards[t] + self.gamma * next_value - values[t]\n",
    "                gae = delta + self.gamma * self.gae_lambda * gae\n",
    "            \n",
    "            advantages[t] = gae\n",
    "            returns[t] = advantages[t] + values[t]\n",
    "            next_value = values[t]\n",
    "        \n",
    "        return advantages, returns\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update policy using PPO algorithm\n",
    "        \"\"\"\n",
    "        # Get data from buffer\n",
    "        states, actions, rewards, old_log_probs, values, dones = self.buffer.get()\n",
    "        \n",
    "        # Get last value for GAE computation\n",
    "        with torch.no_grad():\n",
    "            last_state = torch.FloatTensor(states[-1]).unsqueeze(0).to(device)\n",
    "            _, last_value = self.policy(last_state)\n",
    "            last_value = last_value.item()\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_gae(rewards, values, dones, last_value)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions_tensor = torch.LongTensor(actions).to(device)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs).to(device)\n",
    "        advantages_tensor = torch.FloatTensor(advantages).to(device)\n",
    "        returns_tensor = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # PPO update epochs\n",
    "        dataset_size = len(states)\n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        total_clip_fraction = 0\n",
    "        n_updates = 0\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            # Create random indices for mini-batches\n",
    "            indices = np.random.permutation(dataset_size)\n",
    "            \n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = min(start + self.batch_size, dataset_size)\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get batch data\n",
    "                batch_states = states_tensor[batch_indices]\n",
    "                batch_actions = actions_tensor[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs_tensor[batch_indices]\n",
    "                batch_advantages = advantages_tensor[batch_indices]\n",
    "                batch_returns = returns_tensor[batch_indices]\n",
    "                \n",
    "                # Evaluate current policy\n",
    "                new_log_probs, new_values, entropy = self.policy.evaluate(\n",
    "                    batch_states, batch_actions\n",
    "                )\n",
    "                \n",
    "                # Compute probability ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Clipped surrogate objective\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss (MSE)\n",
    "                value_loss = F.mse_loss(new_values, batch_returns)\n",
    "                \n",
    "                # Entropy bonus\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.c1 * value_loss + self.c2 * entropy_loss\n",
    "                \n",
    "                # Optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Track statistics\n",
    "                clip_fraction = (torch.abs(ratio - 1) > self.clip_epsilon).float().mean().item()\n",
    "                total_policy_loss += policy_loss.item()\n",
    "                total_value_loss += value_loss.item()\n",
    "                total_entropy += entropy.mean().item()\n",
    "                total_clip_fraction += clip_fraction\n",
    "                n_updates += 1\n",
    "        \n",
    "        # Store average statistics\n",
    "        self.training_stats['policy_loss'].append(total_policy_loss / n_updates)\n",
    "        self.training_stats['value_loss'].append(total_value_loss / n_updates)\n",
    "        self.training_stats['entropy'].append(total_entropy / n_updates)\n",
    "        self.training_stats['clip_fraction'].append(total_clip_fraction / n_updates)\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer.clear()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': total_policy_loss / n_updates,\n",
    "            'value_loss': total_value_loss / n_updates,\n",
    "            'entropy': total_entropy / n_updates,\n",
    "            'clip_fraction': total_clip_fraction / n_updates\n",
    "        }\n",
    "\n",
    "print(\"âœ… PPO Agent class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7497212",
   "metadata": {},
   "source": [
    "## Part 6: Training the PPO Agent\n",
    "\n",
    "### 6.1 Training Loop\n",
    "\n",
    "We'll train the agent over multiple episodes, collecting trajectories and updating the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2bb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, n_episodes=100, update_freq=2048, verbose=True):\n",
    "    \"\"\"\n",
    "    Train PPO agent on trading environment\n",
    "    \n",
    "    Args:\n",
    "        env: Trading environment\n",
    "        agent: PPO agent\n",
    "        n_episodes: Number of training episodes\n",
    "        update_freq: Steps between policy updates\n",
    "        verbose: Print training progress\n",
    "    \n",
    "    Returns:\n",
    "        training_history: Dict of training metrics\n",
    "    \"\"\"\n",
    "    training_history = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_returns': [],\n",
    "        'sharpe_ratios': [],\n",
    "        'max_drawdowns': [],\n",
    "        'total_trades': []\n",
    "    }\n",
    "    \n",
    "    best_sharpe = -np.inf\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            agent.store_transition(state, action, reward, log_prob, value, done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Update policy\n",
    "            if total_steps % update_freq == 0:\n",
    "                update_stats = agent.update()\n",
    "        \n",
    "        # End of episode - update if buffer not empty\n",
    "        if len(agent.buffer) > 0:\n",
    "            agent.update()\n",
    "        \n",
    "        # Get episode metrics\n",
    "        metrics = env.get_metrics()\n",
    "        training_history['episode_rewards'].append(episode_reward)\n",
    "        training_history['episode_returns'].append(metrics['total_return'])\n",
    "        training_history['sharpe_ratios'].append(metrics['sharpe_ratio'])\n",
    "        training_history['max_drawdowns'].append(metrics['max_drawdown'])\n",
    "        training_history['total_trades'].append(metrics['total_trades'])\n",
    "        \n",
    "        # Track best model\n",
    "        if metrics['sharpe_ratio'] > best_sharpe:\n",
    "            best_sharpe = metrics['sharpe_ratio']\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(training_history['episode_rewards'][-10:])\n",
    "            avg_return = np.mean(training_history['episode_returns'][-10:])\n",
    "            avg_sharpe = np.mean(training_history['sharpe_ratios'][-10:])\n",
    "            \n",
    "            print(f\"Episode {episode + 1:3d}/{n_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:8.2f} | \"\n",
    "                  f\"Avg Return: {avg_return*100:6.2f}% | \"\n",
    "                  f\"Avg Sharpe: {avg_sharpe:.3f} | \"\n",
    "                  f\"Best Sharpe: {best_sharpe:.3f}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "print(\"âœ… Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df.iloc[:train_size].copy()\n",
    "test_df = df.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"ðŸ“Š Data Split:\")\n",
    "print(f\"   Training: {len(train_df)} days ({train_df.index[0].strftime('%Y-%m-%d')} to {train_df.index[-1].strftime('%Y-%m-%d')})\")\n",
    "print(f\"   Testing:  {len(test_df)} days ({test_df.index[0].strftime('%Y-%m-%d')} to {test_df.index[-1].strftime('%Y-%m-%d')})\")\n",
    "\n",
    "# Create training environment\n",
    "train_env = TradingEnvironment(train_df, initial_balance=100000)\n",
    "\n",
    "# Initialize PPO agent\n",
    "agent = PPOAgent(\n",
    "    state_dim=train_env.state_dim,\n",
    "    action_dim=train_env.action_space,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_epsilon=0.2,\n",
    "    c1=0.5,\n",
    "    c2=0.01,\n",
    "    n_epochs=10,\n",
    "    batch_size=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ¤– PPO Agent Configuration:\")\n",
    "print(f\"   Learning rate: 3e-4\")\n",
    "print(f\"   Discount factor (Î³): 0.99\")\n",
    "print(f\"   GAE Î»: 0.95\")\n",
    "print(f\"   Clip Îµ: 0.2\")\n",
    "print(f\"   Update epochs: 10\")\n",
    "print(f\"   Batch size: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "print(\"\\nðŸš€ Starting PPO Training...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_history = train_ppo(\n",
    "    env=train_env,\n",
    "    agent=agent,\n",
    "    n_episodes=50,  # Reduced for demo, increase for better results\n",
    "    update_freq=1024,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nâœ… Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Episode rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(training_history['episode_rewards'], alpha=0.6, label='Episode Reward')\n",
    "window = 10\n",
    "if len(training_history['episode_rewards']) >= window:\n",
    "    rolling_reward = pd.Series(training_history['episode_rewards']).rolling(window).mean()\n",
    "    ax1.plot(rolling_reward, linewidth=2, label=f'{window}-Episode MA')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Training Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode returns\n",
    "ax2 = axes[0, 1]\n",
    "returns_pct = [r * 100 for r in training_history['episode_returns']]\n",
    "ax2.plot(returns_pct, alpha=0.6, label='Episode Return')\n",
    "if len(returns_pct) >= window:\n",
    "    rolling_return = pd.Series(returns_pct).rolling(window).mean()\n",
    "    ax2.plot(rolling_return, linewidth=2, label=f'{window}-Episode MA')\n",
    "ax2.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Return (%)')\n",
    "ax2.set_title('Portfolio Returns')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpe ratios\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(training_history['sharpe_ratios'], alpha=0.6, label='Sharpe Ratio')\n",
    "if len(training_history['sharpe_ratios']) >= window:\n",
    "    rolling_sharpe = pd.Series(training_history['sharpe_ratios']).rolling(window).mean()\n",
    "    ax3.plot(rolling_sharpe, linewidth=2, label=f'{window}-Episode MA')\n",
    "ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax3.axhline(y=1, color='g', linestyle='--', alpha=0.5, label='Sharpe = 1')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Sharpe Ratio')\n",
    "ax3.set_title('Risk-Adjusted Performance')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# PPO-specific metrics\n",
    "ax4 = axes[1, 1]\n",
    "if agent.training_stats['policy_loss']:\n",
    "    ax4.plot(agent.training_stats['policy_loss'], label='Policy Loss', alpha=0.7)\n",
    "    ax4.plot(agent.training_stats['value_loss'], label='Value Loss', alpha=0.7)\n",
    "    ax4.set_xlabel('Update Step')\n",
    "    ax4.set_ylabel('Loss')\n",
    "    ax4.set_title('PPO Training Losses')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a880f",
   "metadata": {},
   "source": [
    "## Part 7: Evaluation and Backtesting\n",
    "\n",
    "### 7.1 Evaluate on Test Data\n",
    "\n",
    "Now let's evaluate our trained PPO agent on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, render=False):\n",
    "    \"\"\"\n",
    "    Evaluate trained agent on environment\n",
    "    \n",
    "    Args:\n",
    "        env: Trading environment\n",
    "        agent: Trained PPO agent\n",
    "        render: Whether to print step-by-step actions\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Performance metrics dict\n",
    "        history: Trading history\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    agent.policy.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_probs, _ = agent.policy(state_tensor)\n",
    "            \n",
    "            # Use greedy action selection for evaluation\n",
    "            action = torch.argmax(action_probs).item()\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if render:\n",
    "                action_names = ['Hold', 'Buy', 'Sell']\n",
    "                print(f\"Action: {action_names[action]}, Portfolio: ${info['portfolio_value']:,.2f}\")\n",
    "    \n",
    "    # Set back to training mode\n",
    "    agent.policy.train()\n",
    "    \n",
    "    metrics = env.get_metrics()\n",
    "    metrics['total_reward'] = total_reward\n",
    "    \n",
    "    return metrics, env.history\n",
    "\n",
    "print(\"âœ… Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test environment\n",
    "test_env = TradingEnvironment(test_df, initial_balance=100000)\n",
    "\n",
    "# Evaluate on test data\n",
    "print(\"ðŸ“Š Evaluating PPO Agent on Test Data...\\n\")\n",
    "\n",
    "test_metrics, test_history = evaluate_agent(test_env, agent)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PPO TRADING AGENT - TEST PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ’° Portfolio Performance:\")\n",
    "print(f\"   Initial Balance:    ${100000:>12,.2f}\")\n",
    "print(f\"   Final Value:        ${test_metrics['final_value']:>12,.2f}\")\n",
    "print(f\"   Total Return:       {test_metrics['total_return']*100:>12.2f}%\")\n",
    "print(f\"   Annualized Return:  {test_metrics['annualized_return']*100:>12.2f}%\")\n",
    "print(f\"\\nðŸ“ˆ Risk Metrics:\")\n",
    "print(f\"   Sharpe Ratio:       {test_metrics['sharpe_ratio']:>12.3f}\")\n",
    "print(f\"   Max Drawdown:       {test_metrics['max_drawdown']*100:>12.2f}%\")\n",
    "print(f\"   Win Rate:           {test_metrics['win_rate']*100:>12.2f}%\")\n",
    "print(f\"\\nðŸ“Š Trading Activity:\")\n",
    "print(f\"   Total Trades:       {test_metrics['total_trades']:>12d}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4126233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Buy & Hold benchmark\n",
    "def calculate_buy_hold(df, initial_balance=100000):\n",
    "    \"\"\"\n",
    "    Calculate Buy & Hold benchmark returns\n",
    "    \"\"\"\n",
    "    prices = df['Close'].values\n",
    "    shares = initial_balance / prices[0]\n",
    "    portfolio_values = shares * prices\n",
    "    \n",
    "    returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    total_return = (portfolio_values[-1] - initial_balance) / initial_balance\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    sharpe = np.sqrt(252) * np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0\n",
    "    \n",
    "    # Max drawdown\n",
    "    peak = np.maximum.accumulate(portfolio_values)\n",
    "    drawdown = (peak - portfolio_values) / peak\n",
    "    max_drawdown = np.max(drawdown)\n",
    "    \n",
    "    return {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'final_value': portfolio_values[-1]\n",
    "    }\n",
    "\n",
    "# Calculate benchmark\n",
    "benchmark = calculate_buy_hold(test_df)\n",
    "\n",
    "print(\"\\nðŸ“Š Comparison: PPO Agent vs Buy & Hold\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'PPO Agent':>15} {'Buy & Hold':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Total Return':<25} {test_metrics['total_return']*100:>14.2f}% {benchmark['total_return']*100:>14.2f}%\")\n",
    "print(f\"{'Sharpe Ratio':<25} {test_metrics['sharpe_ratio']:>15.3f} {benchmark['sharpe_ratio']:>15.3f}\")\n",
    "print(f\"{'Max Drawdown':<25} {test_metrics['max_drawdown']*100:>14.2f}% {benchmark['max_drawdown']*100:>14.2f}%\")\n",
    "print(f\"{'Final Value':<25} ${test_metrics['final_value']:>13,.0f} ${benchmark['final_value']:>13,.0f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b9886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test performance\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Get aligned dates for test period\n",
    "test_dates = test_df.index[len(test_df) - len(test_history['portfolio_value']):]\n",
    "\n",
    "# Portfolio value comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(test_dates, test_history['portfolio_value'], label='PPO Agent', linewidth=2, color='blue')\n",
    "benchmark_values = benchmark['portfolio_values'][-len(test_dates):]\n",
    "ax1.plot(test_dates, benchmark_values, label='Buy & Hold', linewidth=2, color='gray', alpha=0.7)\n",
    "ax1.fill_between(test_dates, test_history['portfolio_value'], benchmark_values, \n",
    "                  where=np.array(test_history['portfolio_value']) > benchmark_values,\n",
    "                  alpha=0.3, color='green', label='Outperformance')\n",
    "ax1.fill_between(test_dates, test_history['portfolio_value'], benchmark_values,\n",
    "                  where=np.array(test_history['portfolio_value']) <= benchmark_values,\n",
    "                  alpha=0.3, color='red', label='Underperformance')\n",
    "ax1.set_ylabel('Portfolio Value ($)')\n",
    "ax1.set_title('PPO Agent vs Buy & Hold - Test Period', fontsize=14)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n",
    "\n",
    "# Position over time\n",
    "ax2 = axes[1]\n",
    "positions = test_history['positions'][1:]  # Exclude initial\n",
    "colors = ['green' if p > 0 else 'gray' for p in positions]\n",
    "ax2.bar(test_dates[:len(positions)], positions, color=colors, alpha=0.7, width=1)\n",
    "ax2.set_ylabel('Position (% of Portfolio)')\n",
    "ax2.set_title('Agent Position Over Time', fontsize=14)\n",
    "ax2.set_ylim(-0.1, 1.1)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Actions taken\n",
    "ax3 = axes[2]\n",
    "actions = test_history['actions']\n",
    "action_dates = test_dates[:len(actions)]\n",
    "\n",
    "# Plot price\n",
    "price_data = test_df['Close'].values[-len(action_dates):]\n",
    "ax3.plot(action_dates, price_data, color='black', alpha=0.5, label='SPY Price')\n",
    "\n",
    "# Mark buy and sell actions\n",
    "buy_mask = np.array(actions) == 1\n",
    "sell_mask = np.array(actions) == 2\n",
    "\n",
    "if np.any(buy_mask):\n",
    "    buy_dates = action_dates[buy_mask]\n",
    "    buy_prices = price_data[buy_mask]\n",
    "    ax3.scatter(buy_dates, buy_prices, color='green', marker='^', s=100, label='Buy', zorder=5)\n",
    "\n",
    "if np.any(sell_mask):\n",
    "    sell_dates = action_dates[sell_mask]\n",
    "    sell_prices = price_data[sell_mask]\n",
    "    ax3.scatter(sell_dates, sell_prices, color='red', marker='v', s=100, label='Sell', zorder=5)\n",
    "\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Price ($)')\n",
    "ax3.set_title('Trading Actions on Price Chart', fontsize=14)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Action distribution\n",
    "action_counts = pd.Series(actions).value_counts().sort_index()\n",
    "action_names = ['Hold', 'Buy', 'Sell']\n",
    "print(f\"\\nðŸ“Š Action Distribution:\")\n",
    "for idx, count in action_counts.items():\n",
    "    pct = count / len(actions) * 100\n",
    "    print(f\"   {action_names[idx]}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d61c5",
   "metadata": {},
   "source": [
    "## Part 8: Key Takeaways and Summary\n",
    "\n",
    "### 8.1 PPO Algorithm Recap\n",
    "\n",
    "**Core Components:**\n",
    "\n",
    "1. **Clipped Surrogate Objective**: Prevents destructively large policy updates\n",
    "   $$L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "2. **Generalized Advantage Estimation (GAE)**: Reduces variance in advantage estimates\n",
    "   $$\\hat{A}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "3. **Value Function**: Provides baseline for advantage calculation\n",
    "   $$L^{VF}(\\theta) = (V_\\theta(s_t) - V_t^{target})^2$$\n",
    "\n",
    "4. **Entropy Bonus**: Encourages exploration\n",
    "   $$L^{ENT}(\\theta) = -\\mathbb{E}_t[H(\\pi_\\theta(\\cdot|s_t))]$$\n",
    "\n",
    "### 8.2 Why PPO Works Well for Trading\n",
    "\n",
    "| Advantage | Explanation |\n",
    "|-----------|-------------|\n",
    "| **Stability** | Clipping prevents catastrophic policy changes |\n",
    "| **Sample Efficiency** | Multiple epochs on same data |\n",
    "| **Robustness** | Works across different market conditions |\n",
    "| **Continuous Learning** | Can adapt to changing markets |\n",
    "\n",
    "### 8.3 Practical Considerations\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- `clip_epsilon`: 0.1-0.3 (smaller = more conservative)\n",
    "- `gamma`: 0.99 (future rewards discount)\n",
    "- `gae_lambda`: 0.95 (bias-variance tradeoff)\n",
    "- `n_epochs`: 3-10 (more = better sample efficiency)\n",
    "\n",
    "**Important Notes:**\n",
    "- Always include transaction costs\n",
    "- Test on out-of-sample data\n",
    "- Consider regime changes in markets\n",
    "- Monitor for overfitting to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"                  WEEK 16 DAY 6: PPO TRADING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“š Key Concepts Covered:\")\n",
    "print(\"   âœ“ PPO algorithm fundamentals and motivation\")\n",
    "print(\"   âœ“ Clipped surrogate objective function\")\n",
    "print(\"   âœ“ Generalized Advantage Estimation (GAE)\")\n",
    "print(\"   âœ“ Actor-Critic architecture implementation\")\n",
    "print(\"   âœ“ Full trading environment with technical indicators\")\n",
    "print(\"   âœ“ Training loop with policy updates\")\n",
    "print(\"   âœ“ Evaluation and comparison with benchmarks\")\n",
    "\n",
    "print(\"\\nðŸ”§ Technical Implementation:\")\n",
    "print(f\"   â€¢ State dimension: {train_env.state_dim} features\")\n",
    "print(f\"   â€¢ Action space: 3 discrete actions (Hold, Buy, Sell)\")\n",
    "print(f\"   â€¢ Network: Shared backbone with Actor-Critic heads\")\n",
    "print(f\"   â€¢ Training episodes: 50\")\n",
    "\n",
    "print(\"\\nðŸ“Š Test Performance:\")\n",
    "print(f\"   â€¢ PPO Return: {test_metrics['total_return']*100:.2f}%\")\n",
    "print(f\"   â€¢ Benchmark Return: {benchmark['total_return']*100:.2f}%\")\n",
    "print(f\"   â€¢ PPO Sharpe: {test_metrics['sharpe_ratio']:.3f}\")\n",
    "print(f\"   â€¢ Benchmark Sharpe: {benchmark['sharpe_ratio']:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Next Steps:\")\n",
    "print(\"   â†’ Experiment with different reward functions\")\n",
    "print(\"   â†’ Try continuous action spaces (position sizing)\")\n",
    "print(\"   â†’ Implement multi-asset trading\")\n",
    "print(\"   â†’ Add more sophisticated features (order book, sentiment)\")\n",
    "print(\"   â†’ Explore other RL algorithms (SAC, TD3)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                         ðŸŽ‰ Day 6 Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
