{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8883547",
   "metadata": {},
   "source": [
    "# Week 16 - Day 2: Multi-Armed Bandits\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the exploration vs exploitation tradeoff\n",
    "- Implement Epsilon-Greedy strategy\n",
    "- Implement Thompson Sampling with Beta distributions\n",
    "- Implement Upper Confidence Bound (UCB) algorithm\n",
    "- Apply bandits to trading strategy selection\n",
    "\n",
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "The multi-armed bandit (MAB) problem is a classic reinforcement learning problem that models the exploration-exploitation tradeoff:\n",
    "\n",
    "- **Setting**: You face K slot machines (\"arms\"), each with an unknown reward probability\n",
    "- **Goal**: Maximize cumulative reward over T time steps\n",
    "- **Challenge**: Balance exploring new arms vs exploiting known good arms\n",
    "\n",
    "### Finance Applications\n",
    "- **Strategy Selection**: Choose among multiple trading strategies\n",
    "- **Portfolio Allocation**: Dynamic allocation across assets\n",
    "- **A/B Testing**: Test different execution algorithms\n",
    "- **Market Making**: Adaptive spread setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b40c1035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2807d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Bandit Environment\n",
    "\n",
    "First, let's create a simulated bandit environment where each arm represents a trading strategy with different success probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3497216a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: BanditEnvironment(n_arms=5, best_arm=4, best_prob=0.580)\n",
      "\n",
      "Strategy Win Rates (hidden from algorithms):\n",
      "  Momentum: 52.0%\n",
      "  Mean Reversion: 48.0%\n",
      "  Breakout: 55.0%\n",
      "  Pairs Trading: 51.0%\n",
      "  Trend Following: 58.0% â† BEST\n"
     ]
    }
   ],
   "source": [
    "class BanditEnvironment:\n",
    "    \"\"\"\n",
    "    Multi-Armed Bandit environment simulating trading strategies.\n",
    "    \n",
    "    Each arm represents a strategy with:\n",
    "    - A true (hidden) probability of generating positive returns\n",
    "    - Bernoulli rewards: 1 (profitable trade) or 0 (losing trade)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, true_probabilities: List[float] = None):\n",
    "        \"\"\"\n",
    "        Initialize the bandit environment.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms (strategies)\n",
    "            true_probabilities: True win rates for each strategy\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        \n",
    "        if true_probabilities is None:\n",
    "            # Generate random probabilities\n",
    "            self.true_probabilities = np.random.uniform(0.3, 0.7, n_arms)\n",
    "        else:\n",
    "            self.true_probabilities = np.array(true_probabilities)\n",
    "        \n",
    "        self.best_arm = np.argmax(self.true_probabilities)\n",
    "        self.best_probability = self.true_probabilities[self.best_arm]\n",
    "        \n",
    "    def pull(self, arm: int) -> int:\n",
    "        \"\"\"Pull an arm and get reward (0 or 1).\"\"\"\n",
    "        return int(np.random.random() < self.true_probabilities[arm])\n",
    "    \n",
    "    def get_regret(self, arm: int) -> float:\n",
    "        \"\"\"Calculate regret for choosing a suboptimal arm.\"\"\"\n",
    "        return self.best_probability - self.true_probabilities[arm]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"BanditEnvironment(n_arms={self.n_arms}, best_arm={self.best_arm}, best_prob={self.best_probability:.3f})\"\n",
    "\n",
    "\n",
    "# Create environment with 5 trading strategies\n",
    "strategy_names = ['Momentum', 'Mean Reversion', 'Breakout', 'Pairs Trading', 'Trend Following']\n",
    "true_win_rates = [0.52, 0.48, 0.55, 0.51, 0.58]  # Realistic win rates\n",
    "\n",
    "env = BanditEnvironment(n_arms=5, true_probabilities=true_win_rates)\n",
    "print(f\"Environment: {env}\")\n",
    "print(f\"\\nStrategy Win Rates (hidden from algorithms):\")\n",
    "for i, (name, prob) in enumerate(zip(strategy_names, true_win_rates)):\n",
    "    best_marker = \" â† BEST\" if i == env.best_arm else \"\"\n",
    "    print(f\"  {name}: {prob:.1%}{best_marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ac4e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Epsilon-Greedy Strategy\n",
    "\n",
    "The simplest exploration strategy:\n",
    "- With probability Îµ: **explore** (choose random arm)\n",
    "- With probability 1-Îµ: **exploit** (choose best known arm)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\n",
    "a_t = \\begin{cases}\n",
    "\\text{random arm} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a \\hat{Q}(a) & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $\\hat{Q}(a)$ is the estimated value of arm $a$:\n",
    "\n",
    "$$\\hat{Q}(a) = \\frac{\\text{Total rewards from arm } a}{\\text{Number of times arm } a \\text{ was pulled}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09dc50a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: EpsilonGreedy(Îµ=0.1)\n",
      "Initial values: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "class BanditAlgorithm(ABC):\n",
    "    \"\"\"Abstract base class for bandit algorithms.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "        self.n_arms = n_arms\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset algorithm state.\"\"\"\n",
    "        self.counts = np.zeros(self.n_arms)  # Number of times each arm was pulled\n",
    "        self.values = np.zeros(self.n_arms)  # Estimated value of each arm\n",
    "        self.total_reward = 0\n",
    "        self.t = 0\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select which arm to pull.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update estimates after receiving reward.\"\"\"\n",
    "        self.t += 1\n",
    "        self.counts[arm] += 1\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # Incremental mean update\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "\n",
    "\n",
    "class EpsilonGreedy(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy Algorithm.\n",
    "    \n",
    "    Explores with probability epsilon, exploits otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1):\n",
    "        super().__init__(n_arms)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select arm using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Explore: choose random arm\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            # Exploit: choose best known arm (break ties randomly)\n",
    "            max_value = np.max(self.values)\n",
    "            best_arms = np.where(self.values == max_value)[0]\n",
    "            return np.random.choice(best_arms)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"EpsilonGreedy(Îµ={self.epsilon})\"\n",
    "\n",
    "\n",
    "class DecayingEpsilonGreedy(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Epsilon-Greedy with decaying epsilon.\n",
    "    \n",
    "    Epsilon decays over time: Îµ_t = Îµ_0 / (1 + decay * t)\n",
    "    This reduces exploration as we learn more.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon_start: float = 1.0, decay: float = 0.01):\n",
    "        super().__init__(n_arms)\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decay = decay\n",
    "    \n",
    "    @property\n",
    "    def epsilon(self) -> float:\n",
    "        \"\"\"Current epsilon value.\"\"\"\n",
    "        return self.epsilon_start / (1 + self.decay * self.t)\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            max_value = np.max(self.values)\n",
    "            best_arms = np.where(self.values == max_value)[0]\n",
    "            return np.random.choice(best_arms)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"DecayingEpsilonGreedy(Îµâ‚€={self.epsilon_start}, decay={self.decay})\"\n",
    "\n",
    "\n",
    "# Test epsilon-greedy\n",
    "eg = EpsilonGreedy(n_arms=5, epsilon=0.1)\n",
    "print(f\"Algorithm: {eg}\")\n",
    "print(f\"Initial values: {eg.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c5103",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Thompson Sampling\n",
    "\n",
    "Thompson Sampling is a **Bayesian** approach that maintains probability distributions over arm values.\n",
    "\n",
    "### Algorithm\n",
    "1. Maintain Beta(Î±, Î²) distribution for each arm\n",
    "2. Sample from each distribution\n",
    "3. Select arm with highest sample\n",
    "4. Update distribution with observed reward\n",
    "\n",
    "### Beta Distribution for Bernoulli Bandits\n",
    "\n",
    "For Bernoulli rewards (0 or 1):\n",
    "- Prior: Beta(1, 1) = Uniform(0, 1)\n",
    "- After s successes and f failures: Beta(1 + s, 1 + f)\n",
    "\n",
    "### Why Thompson Sampling Works\n",
    "- Arms with **uncertainty** have wider distributions â†’ more likely to be sampled\n",
    "- Arms with **high mean** are more likely to have high samples\n",
    "- Naturally balances exploration and exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5614bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ThompsonSampling' object has no attribute 'prior_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     70\u001b[39m     plt.show()\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Test Thompson Sampling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m ts = \u001b[43mThompsonSampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_arms\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlgorithm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitial alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mts.alpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mThompsonSampling.__init__\u001b[39m\u001b[34m(self, n_arms, prior_alpha, prior_beta)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_arms: \u001b[38;5;28mint\u001b[39m, prior_alpha: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m, prior_beta: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_arms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.prior_alpha = prior_alpha\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mself\u001b[39m.prior_beta = prior_beta\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mBanditAlgorithm.__init__\u001b[39m\u001b[34m(self, n_arms)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_arms: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_arms = n_arms\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mThompsonSampling.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28msuper\u001b[39m().reset()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Beta distribution parameters for each arm\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mself\u001b[39m.alpha = np.ones(\u001b[38;5;28mself\u001b[39m.n_arms) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprior_alpha\u001b[49m  \u001b[38;5;66;03m# successes + 1\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.beta = np.ones(\u001b[38;5;28mself\u001b[39m.n_arms) * \u001b[38;5;28mself\u001b[39m.prior_beta\n",
      "\u001b[31mAttributeError\u001b[39m: 'ThompsonSampling' object has no attribute 'prior_alpha'"
     ]
    }
   ],
   "source": [
    "class ThompsonSampling(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Thompson Sampling for Bernoulli bandits.\n",
    "    \n",
    "    Uses Beta distributions as conjugate priors for Bernoulli likelihood.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, prior_alpha: float = 1.0, prior_beta: float = 1.0):\n",
    "        super().__init__(n_arms)\n",
    "        self.prior_alpha = prior_alpha\n",
    "        self.prior_beta = prior_beta\n",
    "        \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        # Beta distribution parameters for each arm\n",
    "        self.alpha = np.ones(self.n_arms) * self.prior_alpha  # successes + 1\n",
    "        self.beta = np.ones(self.n_arms) * self.prior_beta    # failures + 1\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Sample from each arm's Beta distribution and select highest.\"\"\"\n",
    "        # Sample theta from Beta(alpha, beta) for each arm\n",
    "        samples = np.random.beta(self.alpha, self.beta)\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update Beta distribution parameters.\"\"\"\n",
    "        super().update(arm, reward)\n",
    "        \n",
    "        # Update Beta parameters\n",
    "        if reward == 1:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "    \n",
    "    def get_expected_values(self) -> np.ndarray:\n",
    "        \"\"\"Get expected value (mean of Beta distribution) for each arm.\"\"\"\n",
    "        return self.alpha / (self.alpha + self.beta)\n",
    "    \n",
    "    def get_uncertainty(self) -> np.ndarray:\n",
    "        \"\"\"Get uncertainty (std of Beta distribution) for each arm.\"\"\"\n",
    "        a, b = self.alpha, self.beta\n",
    "        return np.sqrt((a * b) / ((a + b)**2 * (a + b + 1)))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ThompsonSampling(prior=Beta({self.prior_alpha}, {self.prior_beta}))\"\n",
    "\n",
    "\n",
    "# Visualize Thompson Sampling distributions\n",
    "def visualize_thompson_distributions(ts: ThompsonSampling, strategy_names: List[str]):\n",
    "    \"\"\"Visualize Beta distributions for each arm.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.linspace(0, 1, 200)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, ts.n_arms))\n",
    "    \n",
    "    for i in range(ts.n_arms):\n",
    "        from scipy import stats\n",
    "        y = stats.beta.pdf(x, ts.alpha[i], ts.beta[i])\n",
    "        ax.plot(x, y, label=f\"{strategy_names[i]} (Î±={ts.alpha[i]:.0f}, Î²={ts.beta[i]:.0f})\",\n",
    "                color=colors[i], linewidth=2)\n",
    "        ax.fill_between(x, y, alpha=0.2, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Win Probability')\n",
    "    ax.set_ylabel('Probability Density')\n",
    "    ax.set_title('Thompson Sampling: Posterior Distributions')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Test Thompson Sampling\n",
    "ts = ThompsonSampling(n_arms=5)\n",
    "print(f\"Algorithm: {ts}\")\n",
    "print(f\"Initial alpha: {ts.alpha}\")\n",
    "print(f\"Initial beta: {ts.beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f72a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Upper Confidence Bound (UCB)\n",
    "\n",
    "UCB follows the principle of **\"optimism in the face of uncertainty\"**:\n",
    "- Choose the arm with highest upper confidence bound\n",
    "- UCB = estimated mean + exploration bonus\n",
    "\n",
    "### UCB1 Formula\n",
    "\n",
    "$$a_t = \\arg\\max_a \\left[ \\hat{Q}(a) + c \\cdot \\sqrt{\\frac{\\ln t}{N(a)}} \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{Q}(a)$ = estimated value of arm $a$\n",
    "- $t$ = total number of rounds\n",
    "- $N(a)$ = number of times arm $a$ was pulled\n",
    "- $c$ = exploration parameter (typically $\\sqrt{2}$)\n",
    "\n",
    "### Intuition\n",
    "- Rarely-pulled arms have **high uncertainty bonus** â†’ get explored\n",
    "- Frequently-pulled arms have **low uncertainty** â†’ selected based on mean\n",
    "- Bonus shrinks as $\\sqrt{\\ln t / N(a)}$ â†’ eventual convergence to best arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb1865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB1(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound (UCB1) Algorithm.\n",
    "    \n",
    "    Selects arm with highest UCB = mean + exploration_bonus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, c: float = np.sqrt(2)):\n",
    "        super().__init__(n_arms)\n",
    "        self.c = c  # Exploration parameter\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select arm with highest UCB.\"\"\"\n",
    "        # First, try each arm once\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # Calculate UCB for each arm\n",
    "        ucb_values = self.get_ucb_values()\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def get_ucb_values(self) -> np.ndarray:\n",
    "        \"\"\"Calculate UCB value for each arm.\"\"\"\n",
    "        # Avoid division by zero\n",
    "        counts = np.maximum(self.counts, 1e-10)\n",
    "        \n",
    "        # Exploration bonus\n",
    "        exploration_bonus = self.c * np.sqrt(np.log(self.t + 1) / counts)\n",
    "        \n",
    "        return self.values + exploration_bonus\n",
    "    \n",
    "    def get_exploration_bonus(self) -> np.ndarray:\n",
    "        \"\"\"Get current exploration bonus for each arm.\"\"\"\n",
    "        counts = np.maximum(self.counts, 1e-10)\n",
    "        return self.c * np.sqrt(np.log(self.t + 1) / counts)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"UCB1(c={self.c:.3f})\"\n",
    "\n",
    "\n",
    "class UCB_Tuned(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    UCB-Tuned: UCB with variance estimation.\n",
    "    \n",
    "    Uses observed variance to improve exploration bonus.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int):\n",
    "        super().__init__(n_arms)\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.sum_squared = np.zeros(self.n_arms)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        super().update(arm, reward)\n",
    "        self.sum_squared[arm] += reward ** 2\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # First, try each arm once\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        ucb_values = self.get_ucb_values()\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def get_ucb_values(self) -> np.ndarray:\n",
    "        \"\"\"Calculate UCB-Tuned value for each arm.\"\"\"\n",
    "        counts = np.maximum(self.counts, 1e-10)\n",
    "        \n",
    "        # Estimated variance\n",
    "        variance = self.sum_squared / counts - self.values ** 2\n",
    "        variance = np.maximum(variance, 0)  # Ensure non-negative\n",
    "        \n",
    "        # V_i bound\n",
    "        log_t = np.log(self.t + 1)\n",
    "        v = variance + np.sqrt(2 * log_t / counts)\n",
    "        \n",
    "        # UCB-Tuned bonus\n",
    "        exploration_bonus = np.sqrt(log_t / counts * np.minimum(0.25, v))\n",
    "        \n",
    "        return self.values + exploration_bonus\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"UCB_Tuned()\"\n",
    "\n",
    "\n",
    "# Test UCB\n",
    "ucb = UCB1(n_arms=5)\n",
    "print(f\"Algorithm: {ucb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46297a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Running Experiments\n",
    "\n",
    "Let's compare all algorithms on our trading strategy selection problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env: BanditEnvironment, algorithm: BanditAlgorithm, \n",
    "                   n_rounds: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run a bandit experiment.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with rewards, regrets, and arm selections history\n",
    "    \"\"\"\n",
    "    algorithm.reset()\n",
    "    \n",
    "    rewards = np.zeros(n_rounds)\n",
    "    regrets = np.zeros(n_rounds)\n",
    "    arms_selected = np.zeros(n_rounds, dtype=int)\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    cumulative_regret = 0\n",
    "    \n",
    "    for t in range(n_rounds):\n",
    "        # Select and pull arm\n",
    "        arm = algorithm.select_arm()\n",
    "        reward = env.pull(arm)\n",
    "        \n",
    "        # Update algorithm\n",
    "        algorithm.update(arm, reward)\n",
    "        \n",
    "        # Track metrics\n",
    "        cumulative_reward += reward\n",
    "        cumulative_regret += env.get_regret(arm)\n",
    "        \n",
    "        rewards[t] = cumulative_reward\n",
    "        regrets[t] = cumulative_regret\n",
    "        arms_selected[t] = arm\n",
    "    \n",
    "    return {\n",
    "        'cumulative_rewards': rewards,\n",
    "        'cumulative_regrets': regrets,\n",
    "        'arms_selected': arms_selected,\n",
    "        'final_values': algorithm.values.copy(),\n",
    "        'final_counts': algorithm.counts.copy()\n",
    "    }\n",
    "\n",
    "\n",
    "def run_multiple_experiments(env: BanditEnvironment, algorithm: BanditAlgorithm,\n",
    "                             n_rounds: int, n_experiments: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run multiple experiments and average results.\n",
    "    \"\"\"\n",
    "    all_rewards = np.zeros((n_experiments, n_rounds))\n",
    "    all_regrets = np.zeros((n_experiments, n_rounds))\n",
    "    \n",
    "    for exp in range(n_experiments):\n",
    "        results = run_experiment(env, algorithm, n_rounds)\n",
    "        all_rewards[exp] = results['cumulative_rewards']\n",
    "        all_regrets[exp] = results['cumulative_regrets']\n",
    "    \n",
    "    return {\n",
    "        'mean_rewards': all_rewards.mean(axis=0),\n",
    "        'std_rewards': all_rewards.std(axis=0),\n",
    "        'mean_regrets': all_regrets.mean(axis=0),\n",
    "        'std_regrets': all_regrets.std(axis=0)\n",
    "    }\n",
    "\n",
    "\n",
    "# Run experiments\n",
    "n_rounds = 1000\n",
    "n_experiments = 100\n",
    "\n",
    "algorithms = {\n",
    "    'Epsilon-Greedy (Îµ=0.1)': EpsilonGreedy(5, epsilon=0.1),\n",
    "    'Epsilon-Greedy (Îµ=0.01)': EpsilonGreedy(5, epsilon=0.01),\n",
    "    'Decaying Îµ-Greedy': DecayingEpsilonGreedy(5, epsilon_start=1.0, decay=0.01),\n",
    "    'Thompson Sampling': ThompsonSampling(5),\n",
    "    'UCB1': UCB1(5),\n",
    "    'UCB-Tuned': UCB_Tuned(5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "print(f\"Running {n_experiments} experiments with {n_rounds} rounds each...\\n\")\n",
    "\n",
    "for name, algo in algorithms.items():\n",
    "    print(f\"Testing {name}...\")\n",
    "    results[name] = run_multiple_experiments(env, algo, n_rounds, n_experiments)\n",
    "\n",
    "print(\"\\nExperiments complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative regret comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(algorithms)))\n",
    "\n",
    "# Cumulative Regret\n",
    "ax = axes[0]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    mean = res['mean_regrets']\n",
    "    std = res['std_regrets']\n",
    "    ax.plot(mean, label=name, color=color, linewidth=2)\n",
    "    ax.fill_between(range(n_rounds), mean - std, mean + std, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Cumulative Regret')\n",
    "ax.set_title('Cumulative Regret Over Time')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(0, n_rounds)\n",
    "\n",
    "# Cumulative Reward\n",
    "ax = axes[1]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    mean = res['mean_rewards']\n",
    "    ax.plot(mean, label=name, color=color, linewidth=2)\n",
    "\n",
    "# Add optimal line\n",
    "optimal = np.arange(1, n_rounds + 1) * env.best_probability\n",
    "ax.plot(optimal, 'k--', label='Optimal', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('Cumulative Reward Over Time')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlim(0, n_rounds)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final regrets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Cumulative Regret (lower is better):\")\n",
    "print(\"=\"*60)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_regrets'][-1])\n",
    "for name, res in sorted_results:\n",
    "    print(f\"{name:30s}: {res['mean_regrets'][-1]:.2f} Â± {res['std_regrets'][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b3ba2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Detailed Analysis of Each Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single detailed experiment for visualization\n",
    "np.random.seed(123)\n",
    "\n",
    "detailed_results = {}\n",
    "for name, algo in algorithms.items():\n",
    "    detailed_results[name] = run_experiment(env, algo, n_rounds=500)\n",
    "\n",
    "# Plot arm selection frequency over time\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, res) in enumerate(detailed_results.items()):\n",
    "    ax = axes[idx]\n",
    "    arms = res['arms_selected']\n",
    "    \n",
    "    # Calculate running selection frequency\n",
    "    window = 50\n",
    "    selection_freq = np.zeros((500 - window + 1, 5))\n",
    "    \n",
    "    for t in range(500 - window + 1):\n",
    "        window_arms = arms[t:t+window]\n",
    "        for arm in range(5):\n",
    "            selection_freq[t, arm] = np.sum(window_arms == arm) / window\n",
    "    \n",
    "    # Stack plot\n",
    "    ax.stackplot(range(window-1, 500), selection_freq.T, \n",
    "                 labels=strategy_names, alpha=0.8)\n",
    "    ax.axhline(y=1.0, color='k', linestyle='-', alpha=0.3)\n",
    "    ax.set_xlabel('Round')\n",
    "    ax.set_ylabel('Selection Frequency')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlim(window-1, 500)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "# Add legend to last plot\n",
    "axes[-1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.suptitle('Arm Selection Frequency Over Time (50-round rolling window)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze final arm counts\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(strategy_names))\n",
    "width = 0.12\n",
    "multiplier = 0\n",
    "\n",
    "for name, res in detailed_results.items():\n",
    "    counts = res['final_counts']\n",
    "    percentages = counts / counts.sum() * 100\n",
    "    offset = width * multiplier\n",
    "    ax.bar(x + offset, percentages, width, label=name)\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_xlabel('Strategy')\n",
    "ax.set_ylabel('Selection Percentage (%)')\n",
    "ax.set_title('Final Arm Selection Distribution')\n",
    "ax.set_xticks(x + width * 2.5)\n",
    "ax.set_xticklabels(strategy_names, rotation=15)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add line for best arm\n",
    "ax.axvline(x=env.best_arm + width * 2.5, color='green', linestyle='--', \n",
    "           linewidth=2, alpha=0.7, label='Best Arm')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print estimated vs true values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Estimated Win Rates vs True Win Rates:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Strategy':<20} {'True':<10} \", end=\"\")\n",
    "for name in detailed_results.keys():\n",
    "    short_name = name[:15]\n",
    "    print(f\"{short_name:<15} \", end=\"\")\n",
    "print()\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, strat in enumerate(strategy_names):\n",
    "    print(f\"{strat:<20} {true_win_rates[i]:<10.1%} \", end=\"\")\n",
    "    for name, res in detailed_results.items():\n",
    "        est = res['final_values'][i]\n",
    "        print(f\"{est:<15.1%} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2257255",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Thompson Sampling Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd21936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Thompson Sampling belief evolution\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "ts = ThompsonSampling(n_arms=5)\n",
    "\n",
    "# Record states at different time points\n",
    "checkpoints = [10, 50, 100, 500]\n",
    "states = {}\n",
    "\n",
    "for t in range(max(checkpoints) + 1):\n",
    "    arm = ts.select_arm()\n",
    "    reward = env.pull(arm)\n",
    "    ts.update(arm, reward)\n",
    "    \n",
    "    if t + 1 in checkpoints:\n",
    "        states[t + 1] = (ts.alpha.copy(), ts.beta.copy())\n",
    "\n",
    "# Plot belief evolution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x = np.linspace(0, 1, 200)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 5))\n",
    "\n",
    "for idx, checkpoint in enumerate(checkpoints):\n",
    "    ax = axes[idx]\n",
    "    alpha, beta = states[checkpoint]\n",
    "    \n",
    "    for i in range(5):\n",
    "        y = stats.beta.pdf(x, alpha[i], beta[i])\n",
    "        ax.plot(x, y, label=strategy_names[i], color=colors[i], linewidth=2)\n",
    "        ax.fill_between(x, y, alpha=0.15, color=colors[i])\n",
    "        \n",
    "        # Mark true probability\n",
    "        ax.axvline(x=true_win_rates[i], color=colors[i], linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Win Probability')\n",
    "    ax.set_ylabel('Probability Density')\n",
    "    ax.set_title(f'After {checkpoint} Rounds')\n",
    "    ax.set_xlim(0.3, 0.8)\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "plt.suptitle('Thompson Sampling: Posterior Evolution\\n(Dashed lines = true probabilities)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c86a9a",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: UCB Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96419287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UCB values over time\n",
    "np.random.seed(42)\n",
    "ucb = UCB1(n_arms=5)\n",
    "\n",
    "# Track UCB components over time\n",
    "n_track = 200\n",
    "means_history = np.zeros((n_track, 5))\n",
    "bonus_history = np.zeros((n_track, 5))\n",
    "ucb_history = np.zeros((n_track, 5))\n",
    "arms_history = np.zeros(n_track, dtype=int)\n",
    "\n",
    "for t in range(n_track):\n",
    "    arm = ucb.select_arm()\n",
    "    reward = env.pull(arm)\n",
    "    ucb.update(arm, reward)\n",
    "    \n",
    "    arms_history[t] = arm\n",
    "    means_history[t] = ucb.values.copy()\n",
    "    \n",
    "    if t >= 4:  # After initial exploration\n",
    "        bonus_history[t] = ucb.get_exploration_bonus()\n",
    "        ucb_history[t] = ucb.get_ucb_values()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Estimated means\n",
    "ax = axes[0]\n",
    "for i in range(5):\n",
    "    ax.plot(means_history[:, i], label=strategy_names[i], linewidth=2)\n",
    "    ax.axhline(y=true_win_rates[i], color=f'C{i}', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Estimated Mean')\n",
    "ax.set_title('UCB1: Estimated Mean Values')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0.3, 0.8)\n",
    "\n",
    "# Exploration bonus\n",
    "ax = axes[1]\n",
    "for i in range(5):\n",
    "    ax.plot(bonus_history[:, i], label=strategy_names[i], linewidth=2)\n",
    "ax.set_ylabel('Exploration Bonus')\n",
    "ax.set_title('UCB1: Exploration Bonus (decays with more pulls)')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# UCB values\n",
    "ax = axes[2]\n",
    "for i in range(5):\n",
    "    ax.plot(ucb_history[:, i], label=strategy_names[i], linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('UCB Value')\n",
    "ax.set_title('UCB1: Upper Confidence Bound = Mean + Bonus')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789643c6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Trading Strategy Selection Application\n",
    "\n",
    "Let's apply bandits to a more realistic trading scenario with:\n",
    "- Non-stationary rewards (regime changes)\n",
    "- Contextual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23538cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonStationaryBandit:\n",
    "    \"\"\"\n",
    "    Non-stationary bandit with regime changes.\n",
    "    \n",
    "    Simulates market regime changes where strategy effectiveness varies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, regime_length: int = 200):\n",
    "        self.n_arms = n_arms\n",
    "        self.regime_length = regime_length\n",
    "        self.t = 0\n",
    "        self._generate_regime()\n",
    "    \n",
    "    def _generate_regime(self):\n",
    "        \"\"\"Generate new regime probabilities.\"\"\"\n",
    "        self.true_probabilities = np.random.uniform(0.35, 0.65, self.n_arms)\n",
    "        self.best_arm = np.argmax(self.true_probabilities)\n",
    "        self.best_probability = self.true_probabilities[self.best_arm]\n",
    "    \n",
    "    def pull(self, arm: int) -> int:\n",
    "        \"\"\"Pull arm and potentially change regime.\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # Check for regime change\n",
    "        if self.t % self.regime_length == 0:\n",
    "            self._generate_regime()\n",
    "        \n",
    "        return int(np.random.random() < self.true_probabilities[arm])\n",
    "    \n",
    "    def get_regret(self, arm: int) -> float:\n",
    "        return self.best_probability - self.true_probabilities[arm]\n",
    "\n",
    "\n",
    "class SlidingWindowUCB(BanditAlgorithm):\n",
    "    \"\"\"\n",
    "    UCB with sliding window for non-stationary environments.\n",
    "    \n",
    "    Only considers recent observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, window_size: int = 100, c: float = np.sqrt(2)):\n",
    "        super().__init__(n_arms)\n",
    "        self.window_size = window_size\n",
    "        self.c = c\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.history = []  # List of (arm, reward) tuples\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.t += 1\n",
    "        self.history.append((arm, reward))\n",
    "        \n",
    "        # Keep only recent history\n",
    "        if len(self.history) > self.window_size:\n",
    "            self.history = self.history[-self.window_size:]\n",
    "        \n",
    "        # Recalculate counts and values from window\n",
    "        self.counts = np.zeros(self.n_arms)\n",
    "        self.values = np.zeros(self.n_arms)\n",
    "        \n",
    "        for a, r in self.history:\n",
    "            self.counts[a] += 1\n",
    "        \n",
    "        for a in range(self.n_arms):\n",
    "            if self.counts[a] > 0:\n",
    "                rewards = [r for (arm, r) in self.history if arm == a]\n",
    "                self.values[a] = np.mean(rewards)\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        # Ensure each arm tried at least once\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # UCB with window-based counts\n",
    "        n_window = len(self.history)\n",
    "        exploration_bonus = self.c * np.sqrt(np.log(n_window + 1) / np.maximum(self.counts, 1))\n",
    "        ucb_values = self.values + exploration_bonus\n",
    "        \n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"SlidingWindowUCB(window={self.window_size})\"\n",
    "\n",
    "\n",
    "# Compare algorithms in non-stationary environment\n",
    "np.random.seed(42)\n",
    "ns_env = NonStationaryBandit(n_arms=5, regime_length=200)\n",
    "\n",
    "ns_algorithms = {\n",
    "    'Epsilon-Greedy (Îµ=0.1)': EpsilonGreedy(5, epsilon=0.1),\n",
    "    'Thompson Sampling': ThompsonSampling(5),\n",
    "    'UCB1': UCB1(5),\n",
    "    'Sliding Window UCB': SlidingWindowUCB(5, window_size=100)\n",
    "}\n",
    "\n",
    "# Run experiments\n",
    "n_rounds = 1000\n",
    "ns_results = {}\n",
    "\n",
    "print(\"Running non-stationary experiments...\")\n",
    "for name, algo in ns_algorithms.items():\n",
    "    # Reset environment for fair comparison\n",
    "    np.random.seed(42)\n",
    "    ns_env = NonStationaryBandit(n_arms=5, regime_length=200)\n",
    "    algo.reset()\n",
    "    \n",
    "    rewards = np.zeros(n_rounds)\n",
    "    regrets = np.zeros(n_rounds)\n",
    "    cum_reward = 0\n",
    "    cum_regret = 0\n",
    "    \n",
    "    for t in range(n_rounds):\n",
    "        arm = algo.select_arm()\n",
    "        reward = ns_env.pull(arm)\n",
    "        algo.update(arm, reward)\n",
    "        \n",
    "        cum_reward += reward\n",
    "        cum_regret += ns_env.get_regret(arm)\n",
    "        \n",
    "        rewards[t] = cum_reward\n",
    "        regrets[t] = cum_regret\n",
    "    \n",
    "    ns_results[name] = {'rewards': rewards, 'regrets': regrets}\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31836c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-stationary results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mark regime changes\n",
    "regime_changes = [200, 400, 600, 800]\n",
    "\n",
    "# Cumulative Regret\n",
    "ax = axes[0]\n",
    "for name, res in ns_results.items():\n",
    "    ax.plot(res['regrets'], label=name, linewidth=2)\n",
    "\n",
    "for rc in regime_changes:\n",
    "    ax.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Cumulative Regret')\n",
    "ax.set_title('Non-Stationary Bandits: Cumulative Regret\\n(Red lines = regime changes)')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Cumulative Reward\n",
    "ax = axes[1]\n",
    "for name, res in ns_results.items():\n",
    "    ax.plot(res['rewards'], label=name, linewidth=2)\n",
    "\n",
    "for rc in regime_changes:\n",
    "    ax.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Cumulative Reward')\n",
    "ax.set_title('Non-Stationary Bandits: Cumulative Reward')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Non-Stationary Environment Results:\")\n",
    "print(\"=\"*50)\n",
    "for name, res in ns_results.items():\n",
    "    print(f\"{name:30s}: Regret = {res['regrets'][-1]:.1f}, Reward = {res['rewards'][-1]:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ee6de",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 10: Key Takeaways and Summary\n",
    "\n",
    "### Algorithm Comparison\n",
    "\n",
    "| Algorithm | Pros | Cons | Best For |\n",
    "|-----------|------|------|----------|\n",
    "| **Epsilon-Greedy** | Simple, easy to implement | Fixed exploration rate, no uncertainty | Quick baseline, simple problems |\n",
    "| **Decaying Îµ-Greedy** | Reduces exploration over time | Requires tuning decay rate | When confident about convergence |\n",
    "| **Thompson Sampling** | Bayesian, handles uncertainty naturally | Requires prior specification | General purpose, good default |\n",
    "| **UCB1** | No hyperparameters, proven bounds | Can over-explore initially | When theoretical guarantees needed |\n",
    "| **Sliding Window UCB** | Adapts to non-stationarity | Forgets useful information | Changing environments |\n",
    "\n",
    "### Trading Applications\n",
    "\n",
    "1. **Strategy Selection**: Use bandits to dynamically allocate capital among strategies\n",
    "2. **Execution Algorithms**: Test different execution strategies with Thompson Sampling\n",
    "3. **Market Making**: Adaptive spread setting based on market conditions\n",
    "4. **Portfolio Rebalancing**: Explore new allocations while exploiting known good ones\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with Thompson Sampling** - Good default for most problems\n",
    "2. **Consider non-stationarity** - Markets change, use sliding windows or discounting\n",
    "3. **Define rewards carefully** - Use risk-adjusted returns, not just returns\n",
    "4. **Monitor regret** - Track how much you're leaving on the table\n",
    "5. **Batch updates** - In practice, update less frequently for stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b64df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "print(\"=\"*60)\n",
    "print(\"WEEK 16 - DAY 2: MULTI-ARMED BANDITS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = \"\"\"\n",
    "ðŸ“š KEY CONCEPTS COVERED:\n",
    "\n",
    "1. EXPLORATION vs EXPLOITATION TRADEOFF\n",
    "   - Fundamental challenge in sequential decision making\n",
    "   - Must balance trying new options vs using known good ones\n",
    "\n",
    "2. EPSILON-GREEDY\n",
    "   - Explore with probability Îµ, exploit otherwise\n",
    "   - Simple but effective baseline\n",
    "   - Decaying Îµ reduces exploration over time\n",
    "\n",
    "3. THOMPSON SAMPLING\n",
    "   - Bayesian approach with posterior distributions\n",
    "   - Beta distributions for Bernoulli bandits\n",
    "   - Naturally balances exploration and exploitation\n",
    "\n",
    "4. UPPER CONFIDENCE BOUND (UCB)\n",
    "   - \"Optimism in the face of uncertainty\"\n",
    "   - UCB = mean + exploration_bonus\n",
    "   - Bonus shrinks as arm is pulled more\n",
    "\n",
    "5. NON-STATIONARY ENVIRONMENTS\n",
    "   - Real markets change over time\n",
    "   - Use sliding windows or discounting\n",
    "   - Constant exploration may be necessary\n",
    "\n",
    "ðŸŽ¯ TRADING APPLICATIONS:\n",
    "   â€¢ Strategy selection\n",
    "   â€¢ Portfolio allocation\n",
    "   â€¢ A/B testing execution algorithms\n",
    "   â€¢ Adaptive market making\n",
    "\n",
    "ðŸ“ˆ NEXT: Day 3 will cover Markov Decision Processes (MDPs)\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
