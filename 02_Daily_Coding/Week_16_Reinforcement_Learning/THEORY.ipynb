{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd97bc7",
   "metadata": {},
   "source": [
    "# Week 16: Reinforcement Learning Theory\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Markov Decision Processes (MDPs) as the mathematical framework for RL\n",
    "- Master Bellman equations for value computation\n",
    "- Implement Q-learning from scratch\n",
    "- Understand policy gradient methods\n",
    "- Learn Deep Q-Networks (DQN) and their innovations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8302893",
   "metadata": {},
   "source": [
    "## 1. Markov Decision Processes (MDPs)\n",
    "\n",
    "### What is an MDP?\n",
    "\n",
    "An MDP is a mathematical framework for modeling sequential decision-making problems. It consists of:\n",
    "\n",
    "- **States (S)**: Set of all possible situations the agent can be in\n",
    "- **Actions (A)**: Set of all possible actions the agent can take\n",
    "- **Transition Function P(s'|s,a)**: Probability of reaching state s' from state s after taking action a\n",
    "- **Reward Function R(s,a,s')**: Immediate reward received after transitioning\n",
    "- **Discount Factor γ ∈ [0,1]**: How much future rewards are valued vs immediate rewards\n",
    "\n",
    "### The Markov Property\n",
    "\n",
    "The future depends only on the current state, not the history:\n",
    "\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "### Trading as an MDP\n",
    "\n",
    "| MDP Component | Trading Context |\n",
    "|---------------|----------------|\n",
    "| State | Portfolio holdings, prices, indicators, market regime |\n",
    "| Action | Buy, sell, hold (continuous: position sizes) |\n",
    "| Transition | Market dynamics (partially observable, stochastic) |\n",
    "| Reward | PnL, risk-adjusted returns, Sharpe ratio |\n",
    "| Discount | Time preference, risk aversion |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d579b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingMDP:\n",
    "    \"\"\"\n",
    "    Simple trading environment as an MDP.\n",
    "    States: Price relative to moving average (low, neutral, high)\n",
    "    Actions: Buy (0), Hold (1), Sell (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = ['low', 'neutral', 'high']  # Price vs MA\n",
    "        self.actions = ['buy', 'hold', 'sell']\n",
    "        self.n_states = len(self.states)\n",
    "        self.n_actions = len(self.actions)\n",
    "        \n",
    "        # Transition probabilities P(s'|s,a) - simplified\n",
    "        # In reality, actions don't change market state much (price-taker)\n",
    "        self.transitions = {\n",
    "            'low': {'low': 0.3, 'neutral': 0.5, 'high': 0.2},\n",
    "            'neutral': {'low': 0.25, 'neutral': 0.5, 'high': 0.25},\n",
    "            'high': {'low': 0.2, 'neutral': 0.5, 'high': 0.3}\n",
    "        }\n",
    "        \n",
    "        # Reward structure (mean reversion strategy)\n",
    "        # Buy when low is profitable, sell when high is profitable\n",
    "        self.rewards = {\n",
    "            ('low', 'buy'): 1.0,      # Good: buy low\n",
    "            ('low', 'hold'): 0.0,\n",
    "            ('low', 'sell'): -0.5,    # Bad: sell low\n",
    "            ('neutral', 'buy'): 0.0,\n",
    "            ('neutral', 'hold'): 0.1, # Small reward for patience\n",
    "            ('neutral', 'sell'): 0.0,\n",
    "            ('high', 'buy'): -0.5,    # Bad: buy high\n",
    "            ('high', 'hold'): 0.0,\n",
    "            ('high', 'sell'): 1.0,    # Good: sell high\n",
    "        }\n",
    "        \n",
    "        self.current_state = 'neutral'\n",
    "    \n",
    "    def reset(self) -> str:\n",
    "        self.current_state = np.random.choice(self.states)\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action: str) -> Tuple[str, float, bool]:\n",
    "        \"\"\"Take action, return (next_state, reward, done)\"\"\"\n",
    "        reward = self.rewards[(self.current_state, action)]\n",
    "        \n",
    "        # Sample next state from transition probabilities\n",
    "        probs = self.transitions[self.current_state]\n",
    "        next_state = np.random.choice(\n",
    "            list(probs.keys()),\n",
    "            p=list(probs.values())\n",
    "        )\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, False\n",
    "\n",
    "# Test the MDP\n",
    "env = SimpleTradingMDP()\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "for i in range(5):\n",
    "    action = np.random.choice(env.actions)\n",
    "    next_state, reward, done = env.step(action)\n",
    "    print(f\"Action: {action:6s} -> State: {next_state:8s}, Reward: {reward:+.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32be0d78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Bellman Equations\n",
    "\n",
    "### Value Functions\n",
    "\n",
    "**State Value Function V(s)**: Expected return starting from state s, following policy π\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s\\right]$$\n",
    "\n",
    "**Action Value Function Q(s,a)**: Expected return starting from state s, taking action a, then following policy π\n",
    "\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a\\right]$$\n",
    "\n",
    "### Bellman Expectation Equation\n",
    "\n",
    "The value of a state equals immediate reward plus discounted value of next state:\n",
    "\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$$\n",
    "\n",
    "### Bellman Optimality Equation\n",
    "\n",
    "For optimal policy π*:\n",
    "\n",
    "$$V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$$\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The Bellman equations express a **recursive relationship**: the value of a state depends on the values of successor states. This enables **dynamic programming** solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env: SimpleTradingMDP, gamma: float = 0.9, \n",
    "                    theta: float = 1e-6) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Value Iteration: Find optimal value function and policy.\n",
    "    Uses Bellman optimality equation iteratively.\n",
    "    \"\"\"\n",
    "    # Initialize V(s) = 0 for all states\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "    \n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        \n",
    "        for s in env.states:\n",
    "            v = V[s]\n",
    "            \n",
    "            # Bellman optimality: V(s) = max_a [R(s,a) + γ Σ P(s'|s,a)V(s')]\n",
    "            action_values = []\n",
    "            for a in env.actions:\n",
    "                # Expected value for action a\n",
    "                expected_value = env.rewards[(s, a)]  # Immediate reward\n",
    "                for s_next, prob in env.transitions[s].items():\n",
    "                    expected_value += gamma * prob * V[s_next]\n",
    "                action_values.append(expected_value)\n",
    "            \n",
    "            V[s] = max(action_values)\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "        iteration += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Extract optimal policy\n",
    "    policy = {}\n",
    "    for s in env.states:\n",
    "        action_values = []\n",
    "        for a in env.actions:\n",
    "            expected_value = env.rewards[(s, a)]\n",
    "            for s_next, prob in env.transitions[s].items():\n",
    "                expected_value += gamma * prob * V[s_next]\n",
    "            action_values.append(expected_value)\n",
    "        policy[s] = env.actions[np.argmax(action_values)]\n",
    "    \n",
    "    print(f\"Value Iteration converged in {iteration} iterations\")\n",
    "    return V, policy\n",
    "\n",
    "# Run value iteration\n",
    "V_optimal, optimal_policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "for s, v in V_optimal.items():\n",
    "    print(f\"  V({s:8s}) = {v:.3f}\")\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for s, a in optimal_policy.items():\n",
    "    print(f\"  π({s:8s}) = {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb01aef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Q-Learning\n",
    "\n",
    "### Model-Free Learning\n",
    "\n",
    "Q-learning learns optimal Q-values **without knowing the MDP dynamics** (transition probabilities). It learns from experience (samples).\n",
    "\n",
    "### Q-Learning Update Rule\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $r + \\gamma \\max_{a'} Q(s',a')$ = TD target (bootstrapped estimate)\n",
    "- $r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ = TD error\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Off-policy**: Learns about optimal policy while following exploratory policy\n",
    "2. **Temporal Difference**: Updates based on difference between consecutive estimates\n",
    "3. **Bootstrapping**: Uses current Q estimates to update Q estimates\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "**ε-greedy policy**:\n",
    "- With probability ε: take random action (explore)\n",
    "- With probability 1-ε: take best action according to Q (exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Tabular Q-Learning Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env: SimpleTradingMDP, alpha: float = 0.1,\n",
    "                 gamma: float = 0.9, epsilon: float = 0.1):\n",
    "        self.env = env\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        self.Q = {}\n",
    "        for s in env.states:\n",
    "            for a in env.actions:\n",
    "                self.Q[(s, a)] = 0.0\n",
    "    \n",
    "    def get_action(self, state: str, training: bool = True) -> str:\n",
    "        \"\"\"ε-greedy action selection.\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            # Greedy: select action with highest Q-value\n",
    "            q_values = [self.Q[(state, a)] for a in self.env.actions]\n",
    "            return self.env.actions[np.argmax(q_values)]\n",
    "    \n",
    "    def update(self, state: str, action: str, reward: float, \n",
    "               next_state: str) -> float:\n",
    "        \"\"\"\n",
    "        Q-learning update.\n",
    "        Returns TD error for monitoring.\n",
    "        \"\"\"\n",
    "        # Current Q-value\n",
    "        current_q = self.Q[(state, action)]\n",
    "        \n",
    "        # TD target: r + γ max_a' Q(s', a')\n",
    "        max_next_q = max(self.Q[(next_state, a)] for a in self.env.actions)\n",
    "        td_target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # TD error\n",
    "        td_error = td_target - current_q\n",
    "        \n",
    "        # Update Q-value\n",
    "        self.Q[(state, action)] += self.alpha * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def get_policy(self) -> Dict[str, str]:\n",
    "        \"\"\"Extract greedy policy from Q-table.\"\"\"\n",
    "        policy = {}\n",
    "        for s in self.env.states:\n",
    "            q_values = [self.Q[(s, a)] for a in self.env.actions]\n",
    "            policy[s] = self.env.actions[np.argmax(q_values)]\n",
    "        return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d82fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env: SimpleTradingMDP, n_episodes: int = 1000,\n",
    "                     max_steps: int = 100) -> Tuple[QLearningAgent, List]:\n",
    "    \"\"\"\n",
    "    Train Q-learning agent.\n",
    "    \"\"\"\n",
    "    agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.get_action(state, training=True)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            agent.update(state, action, reward, next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
    "    \n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Train the agent\n",
    "agent, rewards = train_q_learning(env, n_episodes=1000)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (smoothed)')\n",
    "plt.title('Q-Learning Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show learned Q-values and policy\n",
    "print(\"\\nLearned Q-values:\")\n",
    "for s in env.states:\n",
    "    q_str = \", \".join([f\"{a}:{agent.Q[(s,a)]:+.2f}\" for a in env.actions])\n",
    "    print(f\"  {s:8s}: {q_str}\")\n",
    "\n",
    "print(\"\\nLearned Policy:\")\n",
    "learned_policy = agent.get_policy()\n",
    "for s, a in learned_policy.items():\n",
    "    print(f\"  π({s:8s}) = {a}\")\n",
    "\n",
    "print(\"\\nCompare with optimal policy:\")\n",
    "for s in env.states:\n",
    "    match = \"✓\" if learned_policy[s] == optimal_policy[s] else \"✗\"\n",
    "    print(f\"  {s:8s}: learned={learned_policy[s]:6s}, optimal={optimal_policy[s]:6s} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10349a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Policy Gradient Methods\n",
    "\n",
    "### Why Policy Gradients?\n",
    "\n",
    "Value-based methods (like Q-learning) have limitations:\n",
    "- Struggle with **continuous action spaces**\n",
    "- Can't represent **stochastic policies** naturally\n",
    "- Small changes in Q can cause large policy changes\n",
    "\n",
    "**Policy gradients** directly parameterize and optimize the policy.\n",
    "\n",
    "### Policy Parameterization\n",
    "\n",
    "Policy $\\pi_\\theta(a|s)$ is a neural network with parameters θ:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = P(A=a | S=s, \\theta)$$\n",
    "\n",
    "### Objective: Maximize Expected Return\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]$$\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s,a)\\right]$$\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "Monte Carlo policy gradient using returns G_t as estimate of Q:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$\n",
    "\n",
    "### Variance Reduction: Baseline\n",
    "\n",
    "Subtract a baseline b(s) to reduce variance:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (Q(s,a) - b(s))\\right]$$\n",
    "\n",
    "Common baseline: Value function V(s), giving **Advantage**: A(s,a) = Q(s,a) - V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple policy network for discrete actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 32):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"Sample action and return log probability.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action)\n",
    "\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, \n",
    "                 lr: float = 0.01, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action, log_prob = self.policy.get_action(state_tensor)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"\n",
    "        Update policy using REINFORCE.\n",
    "        Returns the loss value.\n",
    "        \"\"\"\n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        \n",
    "        # Normalize returns (baseline: mean)\n",
    "        if len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss: -log(π) * G\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"REINFORCE Agent initialized successfully\")\n",
    "print(\"\\nKey components:\")\n",
    "print(\"1. Policy network: state -> action probabilities\")\n",
    "print(\"2. Log probability tracking for gradient computation\")\n",
    "print(\"3. Return calculation with discounting\")\n",
    "print(\"4. Policy gradient: ∇θ J = -Σ log π(a|s) * G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d11b6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Deep Q-Networks (DQN)\n",
    "\n",
    "### Scaling Q-Learning with Neural Networks\n",
    "\n",
    "Tabular Q-learning doesn't scale to large/continuous state spaces. **DQN** uses a neural network to approximate Q-values:\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "\n",
    "### Challenge: Training Instability\n",
    "\n",
    "Naive neural network + Q-learning is unstable due to:\n",
    "1. **Correlated samples**: Sequential data breaks i.i.d. assumption\n",
    "2. **Non-stationary targets**: Q-targets change as network updates\n",
    "3. **Overestimation**: max operator causes positive bias\n",
    "\n",
    "### DQN Innovations\n",
    "\n",
    "#### 1. Experience Replay\n",
    "\n",
    "Store transitions $(s, a, r, s')$ in replay buffer. Sample **random mini-batches** for training:\n",
    "- Breaks correlation between consecutive samples\n",
    "- Improves sample efficiency (reuse data)\n",
    "\n",
    "#### 2. Target Network\n",
    "\n",
    "Use separate network $\\theta^-$ for computing TD targets:\n",
    "\n",
    "$$y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$$\n",
    "\n",
    "Update target network periodically: $\\theta^- \\leftarrow \\theta$\n",
    "\n",
    "This stabilizes training by keeping targets fixed between updates.\n",
    "\n",
    "### DQN Loss Function\n",
    "\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}}\\left[\\left(y - Q(s,a;\\theta)\\right)^2\\right]$$\n",
    "\n",
    "where $y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer.\n",
    "    Stores transitions and samples random mini-batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"Sample a random mini-batch.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Q-Network: maps state to Q-values for all actions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent with experience replay and target network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 1e-3,\n",
    "                 gamma: float = 0.99, epsilon: float = 1.0,\n",
    "                 epsilon_decay: float = 0.995, epsilon_min: float = 0.01,\n",
    "                 buffer_size: int = 10000, batch_size: int = 64,\n",
    "                 target_update_freq: int = 100):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Q-Networks\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"ε-greedy action selection.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer.\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"\n",
    "        Update Q-network using a mini-batch from replay buffer.\n",
    "        Returns the loss value.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample mini-batch\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "            self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q-values (using target network)\n",
    "        with torch.no_grad():\n",
    "            max_next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + self.gamma * max_next_q * (1 - dones)\n",
    "        \n",
    "        # MSE Loss\n",
    "        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"DQN Agent initialized successfully\")\n",
    "print(\"\\nKey components:\")\n",
    "print(\"1. Q-Network: state -> Q(s,a) for all actions\")\n",
    "print(\"2. Target Network: separate network for stable TD targets\")\n",
    "print(\"3. Replay Buffer: stores transitions, samples mini-batches\")\n",
    "print(\"4. ε-greedy exploration with decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate DQN training loop structure\n",
    "\n",
    "def dqn_training_pseudocode():\n",
    "    \"\"\"\n",
    "    DQN Training Algorithm (pseudocode with real structure)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DQN TRAINING ALGORITHM\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "    Initialize:\n",
    "        Q-network with random weights θ\n",
    "        Target network with weights θ⁻ = θ\n",
    "        Replay buffer D with capacity N\n",
    "    \n",
    "    For each episode:\n",
    "        Initialize state s\n",
    "        \n",
    "        For each step:\n",
    "            # Action Selection (ε-greedy)\n",
    "            With probability ε: a = random action\n",
    "            Otherwise: a = argmax_a Q(s, a; θ)\n",
    "            \n",
    "            # Environment Interaction\n",
    "            Execute action a, observe r, s'\n",
    "            Store transition (s, a, r, s') in D\n",
    "            \n",
    "            # Learning (if enough samples)\n",
    "            Sample random minibatch from D\n",
    "            \n",
    "            For each transition (sⱼ, aⱼ, rⱼ, s'ⱼ):\n",
    "                if s'ⱼ is terminal:\n",
    "                    yⱼ = rⱼ\n",
    "                else:\n",
    "                    yⱼ = rⱼ + γ max_a' Q(s'ⱼ, a'; θ⁻)  # Target network!\n",
    "            \n",
    "            # Gradient descent on (yⱼ - Q(sⱼ, aⱼ; θ))²\n",
    "            \n",
    "            # Periodic target network update\n",
    "            Every C steps: θ⁻ ← θ\n",
    "            \n",
    "            s ← s'\n",
    "    \"\"\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "dqn_training_pseudocode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6acd4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. DQN Extensions\n",
    "\n",
    "### Double DQN\n",
    "\n",
    "**Problem**: Standard DQN overestimates Q-values (max over noisy estimates is biased).\n",
    "\n",
    "**Solution**: Decouple action selection from evaluation:\n",
    "\n",
    "$$y = r + \\gamma Q(s', \\argmax_{a'} Q(s', a'; \\theta); \\theta^-)$$\n",
    "\n",
    "Use online network to **select** best action, target network to **evaluate** it.\n",
    "\n",
    "### Dueling DQN\n",
    "\n",
    "**Insight**: Some states are valuable regardless of action taken.\n",
    "\n",
    "**Architecture**: Separate streams for value and advantage:\n",
    "\n",
    "$$Q(s,a) = V(s) + A(s,a) - \\frac{1}{|A|}\\sum_{a'} A(s,a')$$\n",
    "\n",
    "### Prioritized Experience Replay\n",
    "\n",
    "**Insight**: Not all transitions are equally informative.\n",
    "\n",
    "**Solution**: Sample transitions with probability proportional to TD error:\n",
    "\n",
    "$$P(i) \\propto |\\delta_i|^\\alpha$$\n",
    "\n",
    "Higher TD error = more surprising = more to learn.\n",
    "\n",
    "### Rainbow DQN\n",
    "\n",
    "Combines all improvements:\n",
    "- Double DQN\n",
    "- Dueling architecture\n",
    "- Prioritized replay\n",
    "- Multi-step learning\n",
    "- Distributional RL\n",
    "- Noisy networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b595fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. RL in Finance: Considerations\n",
    "\n",
    "### Challenges\n",
    "\n",
    "| Challenge | Description | Mitigation |\n",
    "|-----------|-------------|------------|\n",
    "| **Non-stationarity** | Markets change over time | Continuous retraining, regime detection |\n",
    "| **Partial observability** | Can't observe all market state | Use LSTM/attention, add features |\n",
    "| **High noise** | Low signal-to-noise ratio | Robust reward design, regularization |\n",
    "| **Sample efficiency** | Limited historical data | Transfer learning, simulation |\n",
    "| **Transaction costs** | Real costs eat into profits | Include in reward, action penalties |\n",
    "| **Market impact** | Large trades move prices | Model impact, constrain actions |\n",
    "\n",
    "### Reward Design\n",
    "\n",
    "```python\n",
    "# Simple PnL\n",
    "reward = position * returns\n",
    "\n",
    "# Risk-adjusted (Sharpe-like)\n",
    "reward = position * returns - λ * position² * variance\n",
    "\n",
    "# With transaction costs\n",
    "reward = position * returns - cost * |Δposition|\n",
    "\n",
    "# Differential Sharpe ratio\n",
    "reward = (A_t * returns - 0.5 * B_t * returns²) / (B_t - A_t²)^1.5\n",
    "```\n",
    "\n",
    "### State Representation\n",
    "\n",
    "```python\n",
    "state = [\n",
    "    # Price features\n",
    "    returns_1d, returns_5d, returns_20d,\n",
    "    volatility_10d, volatility_30d,\n",
    "    \n",
    "    # Technical indicators\n",
    "    rsi, macd, bollinger_position,\n",
    "    \n",
    "    # Portfolio state\n",
    "    current_position, unrealized_pnl,\n",
    "    \n",
    "    # Market context\n",
    "    vix_level, sector_momentum\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdaae1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary: Algorithm Comparison\n",
    "\n",
    "| Method | Type | Strengths | Weaknesses | Use Case |\n",
    "|--------|------|-----------|------------|----------|\n",
    "| **Q-Learning** | Value-based, tabular | Simple, guaranteed convergence | Doesn't scale | Small discrete problems |\n",
    "| **DQN** | Value-based, deep | Handles high-dim states | Discrete actions only | Image input, discrete actions |\n",
    "| **REINFORCE** | Policy gradient | Continuous actions, stochastic | High variance | Simple continuous control |\n",
    "| **Actor-Critic** | Hybrid | Lower variance than PG | More complex | General purpose |\n",
    "| **PPO** | Policy gradient | Stable, easy to tune | Sample inefficient | General deep RL |\n",
    "| **SAC** | Off-policy AC | Sample efficient, robust | Complex | Continuous control, robotics |\n",
    "\n",
    "### Key Equations to Remember\n",
    "\n",
    "1. **Bellman Optimality**: $V^*(s) = \\max_a [R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s')]$\n",
    "\n",
    "2. **Q-Learning Update**: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "\n",
    "3. **Policy Gradient**: $\\nabla_\\theta J(\\theta) = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t]$\n",
    "\n",
    "4. **DQN Target**: $y = r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a978e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Problems\n",
    "\n",
    "### Conceptual\n",
    "\n",
    "1. Why does Q-learning converge to optimal Q* even with ε-greedy (off-policy)?\n",
    "\n",
    "2. Explain why experience replay helps stabilize DQN training.\n",
    "\n",
    "3. What's the difference between on-policy (SARSA) and off-policy (Q-learning)?\n",
    "\n",
    "4. Why do policy gradients have high variance? How does a baseline help?\n",
    "\n",
    "### Coding Challenges\n",
    "\n",
    "1. Implement Double DQN by modifying the DQNAgent class.\n",
    "\n",
    "2. Add prioritized experience replay to the ReplayBuffer.\n",
    "\n",
    "3. Implement SARSA and compare with Q-learning on the trading MDP.\n",
    "\n",
    "4. Create a simple trading environment with continuous position sizing.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Day 01-03**: Review this theory, run all code cells\n",
    "- **Day 04**: Implement DQN for simple trading (see Day_04_DQN.ipynb)\n",
    "- **Day 05**: Policy gradients and Actor-Critic methods\n",
    "- **Day 06-07**: Advanced: PPO/SAC for portfolio optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
