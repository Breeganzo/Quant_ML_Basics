{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5d24b2",
   "metadata": {},
   "source": [
    "# Week 16, Day 5: Policy Gradients for Trading\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the REINFORCE algorithm and policy gradient theorem\n",
    "- Implement Actor-Critic architecture for trading\n",
    "- Learn advantage estimation techniques (GAE)\n",
    "- Build a trading policy network with continuous actions\n",
    "\n",
    "## Why Policy Gradients for Trading?\n",
    "\n",
    "Unlike value-based methods (DQN), policy gradients:\n",
    "- **Directly optimize the policy** - Learn what action to take\n",
    "- **Handle continuous actions** - Position sizing, not just buy/sell\n",
    "- **Stochastic policies** - Natural exploration through probability distributions\n",
    "- **Better convergence** - Smoother optimization landscape\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d406f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb217e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Policy Gradient Theorem\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "The **policy gradient theorem** states:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s, a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $J(\\theta)$ = Expected cumulative reward\n",
    "- $\\pi_\\theta(a|s)$ = Policy (probability of action $a$ given state $s$)\n",
    "- $Q^{\\pi_\\theta}(s, a)$ = Action-value function\n",
    "\n",
    "### Intuition\n",
    "- If an action leads to **high reward**, increase its probability\n",
    "- If an action leads to **low reward**, decrease its probability\n",
    "- The gradient naturally does this!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc22405",
   "metadata": {},
   "source": [
    "## Part 2: REINFORCE Algorithm\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "REINFORCE uses **Monte Carlo** returns to estimate $Q(s, a)$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "Where $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ is the **return from time $t$**.\n",
    "\n",
    "### Pseudocode\n",
    "```\n",
    "1. Initialize policy network π_θ\n",
    "2. For each episode:\n",
    "   a. Generate trajectory: s₀, a₀, r₁, s₁, a₁, r₂, ..., sT\n",
    "   b. For each timestep t:\n",
    "      - Calculate return Gt\n",
    "      - Calculate loss: -log π_θ(at|st) * Gt\n",
    "   c. Update θ using gradient descent\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b438137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple policy network for discrete actions.\n",
    "    Outputs probability distribution over actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"Sample action from policy distribution.\"\"\"\n",
    "        probs = self.forward(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "\n",
    "# Test the network\n",
    "state_dim = 10\n",
    "action_dim = 3  # Buy, Hold, Sell\n",
    "\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "test_state = torch.randn(1, state_dim)\n",
    "action, log_prob = policy.get_action(test_state)\n",
    "\n",
    "print(f\"State shape: {test_state.shape}\")\n",
    "print(f\"Action probabilities: {policy(test_state).detach().numpy()}\")\n",
    "print(f\"Sampled action: {action}\")\n",
    "print(f\"Log probability: {log_prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48082bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    REINFORCE algorithm implementation.\n",
    "    \n",
    "    Key components:\n",
    "    - Policy network outputs action probabilities\n",
    "    - Monte Carlo returns for gradient estimation\n",
    "    - Optional baseline for variance reduction\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 1e-3,\n",
    "        gamma: float = 0.99,\n",
    "        baseline: bool = True\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.baseline = baseline\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Select action using current policy.\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action, log_prob = self.policy.get_action(state)\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action\n",
    "    \n",
    "    def store_reward(self, reward: float):\n",
    "        \"\"\"Store reward for current step.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_returns(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute discounted returns for each timestep.\n",
    "        G_t = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Calculate returns backwards\n",
    "        for reward in reversed(self.rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Normalize returns (baseline/variance reduction)\n",
    "        if self.baseline and len(returns) > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self) -> float:\n",
    "        \"\"\"\n",
    "        Update policy using REINFORCE gradient.\n",
    "        Loss = -Σ log π(a|s) * G_t\n",
    "        \"\"\"\n",
    "        returns = self.compute_returns()\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        loss_value = policy_loss.item()\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss_value\n",
    "\n",
    "\n",
    "print(\"REINFORCE agent created successfully!\")\n",
    "print(\"\\nKey insight: REINFORCE updates only at episode end (Monte Carlo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccf8609",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Simple Trading Environment\n",
    "\n",
    "Let's create a trading environment to test our algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingEnv:\n",
    "    \"\"\"\n",
    "    Simple trading environment for policy gradient algorithms.\n",
    "    \n",
    "    State: [price_changes, position, unrealized_pnl, volatility, momentum]\n",
    "    Actions: 0=Sell, 1=Hold, 2=Buy\n",
    "    Reward: PnL from position changes and holding\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices: np.ndarray = None,\n",
    "        lookback: int = 10,\n",
    "        transaction_cost: float = 0.001\n",
    "    ):\n",
    "        self.lookback = lookback\n",
    "        self.transaction_cost = transaction_cost\n",
    "        \n",
    "        # Generate synthetic prices if not provided\n",
    "        if prices is None:\n",
    "            self.prices = self._generate_prices(1000)\n",
    "        else:\n",
    "            self.prices = prices\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_prices(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate synthetic price series with trends and mean reversion.\"\"\"\n",
    "        prices = [100.0]\n",
    "        trend = 0\n",
    "        \n",
    "        for _ in range(n - 1):\n",
    "            # Random trend changes\n",
    "            if np.random.random() < 0.05:\n",
    "                trend = np.random.uniform(-0.002, 0.002)\n",
    "            \n",
    "            # Price change with trend and noise\n",
    "            change = trend + np.random.randn() * 0.02\n",
    "            new_price = prices[-1] * (1 + change)\n",
    "            prices.append(new_price)\n",
    "        \n",
    "        return np.array(prices)\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.current_step = self.lookback\n",
    "        self.position = 0  # -1, 0, or 1\n",
    "        self.entry_price = 0\n",
    "        self.total_pnl = 0\n",
    "        self.trades = []\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"Construct state vector.\"\"\"\n",
    "        # Price returns over lookback period\n",
    "        prices = self.prices[self.current_step - self.lookback:self.current_step + 1]\n",
    "        returns = np.diff(prices) / prices[:-1]\n",
    "        \n",
    "        # Current price and position info\n",
    "        current_price = self.prices[self.current_step]\n",
    "        \n",
    "        # Unrealized PnL (normalized)\n",
    "        if self.position != 0:\n",
    "            unrealized_pnl = self.position * (current_price - self.entry_price) / self.entry_price\n",
    "        else:\n",
    "            unrealized_pnl = 0\n",
    "        \n",
    "        # Volatility (rolling std of returns)\n",
    "        volatility = np.std(returns) if len(returns) > 1 else 0\n",
    "        \n",
    "        # Momentum (sum of recent returns)\n",
    "        momentum = np.sum(returns[-5:]) if len(returns) >= 5 else np.sum(returns)\n",
    "        \n",
    "        # Combine features\n",
    "        state = np.concatenate([\n",
    "            returns,                    # Lookback returns\n",
    "            [self.position],            # Current position\n",
    "            [unrealized_pnl],           # Unrealized PnL\n",
    "            [volatility],               # Volatility\n",
    "            [momentum]                  # Momentum\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Execute action and return new state, reward, done, info.\n",
    "        \n",
    "        Actions: 0=Sell/Short, 1=Hold, 2=Buy/Long\n",
    "        \"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # Map action to target position\n",
    "        target_position = action - 1  # 0->-1, 1->0, 2->1\n",
    "        \n",
    "        # Calculate position change\n",
    "        position_change = target_position - self.position\n",
    "        \n",
    "        # Transaction cost for position changes\n",
    "        if position_change != 0:\n",
    "            reward -= abs(position_change) * self.transaction_cost\n",
    "            \n",
    "            # Close existing position\n",
    "            if self.position != 0:\n",
    "                pnl = self.position * (current_price - self.entry_price) / self.entry_price\n",
    "                reward += pnl\n",
    "                self.total_pnl += pnl\n",
    "                self.trades.append(pnl)\n",
    "            \n",
    "            # Open new position\n",
    "            self.position = target_position\n",
    "            self.entry_price = current_price if target_position != 0 else 0\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # Add holding reward/penalty\n",
    "        if not done and self.position != 0:\n",
    "            next_price = self.prices[self.current_step]\n",
    "            holding_return = self.position * (next_price - current_price) / current_price\n",
    "            reward += holding_return * 0.1  # Scale down holding rewards\n",
    "        \n",
    "        info = {\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'position': self.position,\n",
    "            'n_trades': len(self.trades)\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    @property\n",
    "    def state_dim(self) -> int:\n",
    "        return self.lookback + 4  # returns + position + pnl + vol + momentum\n",
    "    \n",
    "    @property\n",
    "    def action_dim(self) -> int:\n",
    "        return 3  # Sell, Hold, Buy\n",
    "\n",
    "\n",
    "# Test the environment\n",
    "env = SimpleTradingEnv()\n",
    "state = env.reset()\n",
    "\n",
    "print(f\"State dimension: {env.state_dim}\")\n",
    "print(f\"Action dimension: {env.action_dim}\")\n",
    "print(f\"Initial state shape: {state.shape}\")\n",
    "print(f\"\\nSample state: {state[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(\n",
    "    env: SimpleTradingEnv,\n",
    "    agent: REINFORCE,\n",
    "    n_episodes: int = 500,\n",
    "    print_every: int = 50\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train REINFORCE agent on trading environment.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_pnls = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store reward\n",
    "            agent.store_reward(reward)\n",
    "            total_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Update policy at end of episode\n",
    "        loss = agent.update()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_pnls.append(info['total_pnl'])\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_pnl = np.mean(episode_pnls[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Reward: {avg_reward:8.4f} | \"\n",
    "                  f\"Avg PnL: {avg_pnl:8.4f} | Trades: {info['n_trades']}\")\n",
    "    \n",
    "    return episode_rewards, episode_pnls\n",
    "\n",
    "\n",
    "# Train REINFORCE agent\n",
    "print(\"Training REINFORCE Agent...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleTradingEnv()\n",
    "reinforce_agent = REINFORCE(\n",
    "    state_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    lr=1e-3,\n",
    "    gamma=0.99,\n",
    "    baseline=True\n",
    ")\n",
    "\n",
    "reinforce_rewards, reinforce_pnls = train_reinforce(env, reinforce_agent, n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702caedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize REINFORCE training\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Smoothed rewards\n",
    "window = 20\n",
    "smoothed_rewards = pd.Series(reinforce_rewards).rolling(window).mean()\n",
    "smoothed_pnls = pd.Series(reinforce_pnls).rolling(window).mean()\n",
    "\n",
    "axes[0].plot(reinforce_rewards, alpha=0.3, color='blue', label='Raw')\n",
    "axes[0].plot(smoothed_rewards, color='blue', linewidth=2, label=f'MA({window})')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward')\n",
    "axes[0].set_title('REINFORCE: Episode Rewards')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(reinforce_pnls, alpha=0.3, color='green', label='Raw')\n",
    "axes[1].plot(smoothed_pnls, color='green', linewidth=2, label=f'MA({window})')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Total PnL')\n",
    "axes[1].set_title('REINFORCE: Trading PnL')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal 50 episodes - Avg Reward: {np.mean(reinforce_rewards[-50:]):.4f}\")\n",
    "print(f\"Final 50 episodes - Avg PnL: {np.mean(reinforce_pnls[-50:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe89eb8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Actor-Critic Architecture\n",
    "\n",
    "### Motivation\n",
    "\n",
    "REINFORCE has **high variance** because:\n",
    "- Uses full episode returns (Monte Carlo)\n",
    "- Must wait until episode end to update\n",
    "\n",
    "### Actor-Critic Solution\n",
    "\n",
    "Use **two networks**:\n",
    "1. **Actor** (Policy): Decides which action to take\n",
    "2. **Critic** (Value): Estimates how good the current state is\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "**Critic Update** (TD Learning):\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "**Actor Update** (Policy Gradient):\n",
    "$$\\nabla_\\theta J(\\theta) \\approx \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot \\delta_t$$\n",
    "\n",
    "Where $\\delta_t$ is the **TD error** (temporal difference).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor network for discrete actions.\"\"\"\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network - estimates state value V(s).\"\"\"\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 128):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class ActorCritic:\n",
    "    \"\"\"\n",
    "    Actor-Critic algorithm with TD learning.\n",
    "    \n",
    "    Advantages over REINFORCE:\n",
    "    - Lower variance (bootstrapping)\n",
    "    - Online updates (no need to wait for episode end)\n",
    "    - Better sample efficiency\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        actor_lr: float = 1e-3,\n",
    "        critic_lr: float = 1e-3,\n",
    "        gamma: float = 0.99\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor (policy) network\n",
    "        self.actor = ActorNetwork(state_dim, action_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Critic (value) network\n",
    "        self.critic = CriticNetwork(state_dim).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"Select action from current policy.\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: np.ndarray,\n",
    "        done: bool,\n",
    "        log_prob: torch.Tensor\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Online update using TD error.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        reward = torch.FloatTensor([reward]).to(device)\n",
    "        \n",
    "        # Current value estimate\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        # Next value estimate (0 if terminal)\n",
    "        with torch.no_grad():\n",
    "            next_value = self.critic(next_state) if not done else torch.zeros(1).to(device)\n",
    "        \n",
    "        # TD error (advantage estimate)\n",
    "        td_target = reward + self.gamma * next_value\n",
    "        td_error = td_target - value\n",
    "        \n",
    "        # Critic loss (MSE of TD error)\n",
    "        critic_loss = td_error.pow(2).mean()\n",
    "        \n",
    "        # Actor loss (policy gradient with TD error)\n",
    "        actor_loss = -log_prob * td_error.detach()\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "\n",
    "print(\"Actor-Critic class created!\")\n",
    "print(\"\\nKey difference from REINFORCE: Updates every step, not every episode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82408414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic(\n",
    "    env: SimpleTradingEnv,\n",
    "    agent: ActorCritic,\n",
    "    n_episodes: int = 500,\n",
    "    print_every: int = 50\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train Actor-Critic agent with online updates.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_pnls = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Online update\n",
    "            agent.update(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_pnls.append(info['total_pnl'])\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_pnl = np.mean(episode_pnls[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Reward: {avg_reward:8.4f} | \"\n",
    "                  f\"Avg PnL: {avg_pnl:8.4f} | Trades: {info['n_trades']}\")\n",
    "    \n",
    "    return episode_rewards, episode_pnls\n",
    "\n",
    "\n",
    "# Train Actor-Critic agent\n",
    "print(\"Training Actor-Critic Agent...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleTradingEnv()\n",
    "ac_agent = ActorCritic(\n",
    "    state_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    actor_lr=1e-3,\n",
    "    critic_lr=1e-3,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "ac_rewards, ac_pnls = train_actor_critic(env, ac_agent, n_episodes=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6eaf8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Advantage Estimation (GAE)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Simple TD error $\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$ has:\n",
    "- **Low variance** (single-step)\n",
    "- **High bias** (depends on value function accuracy)\n",
    "\n",
    "### Generalized Advantage Estimation (GAE)\n",
    "\n",
    "GAE balances bias and variance with parameter $\\lambda$:\n",
    "\n",
    "$$\\hat{A}_t^{GAE} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda = 0$: TD(0), low variance, high bias\n",
    "- $\\lambda = 1$: Monte Carlo, high variance, low bias\n",
    "- $\\lambda \\in (0, 1)$: Trade-off between the two\n",
    "\n",
    "### Practical Computation\n",
    "\n",
    "Recursive formula:\n",
    "$$\\hat{A}_t = \\delta_t + \\gamma \\lambda \\hat{A}_{t+1}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(\n",
    "    rewards: List[float],\n",
    "    values: List[float],\n",
    "    next_value: float,\n",
    "    gamma: float = 0.99,\n",
    "    gae_lambda: float = 0.95\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation.\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards from trajectory\n",
    "        values: List of value estimates V(s_t)\n",
    "        next_value: Value estimate of final next state\n",
    "        gamma: Discount factor\n",
    "        gae_lambda: GAE parameter (0=TD(0), 1=MC)\n",
    "    \n",
    "    Returns:\n",
    "        advantages: GAE advantages\n",
    "        returns: Target returns for value function\n",
    "    \"\"\"\n",
    "    rewards = np.array(rewards)\n",
    "    values = np.array(values)\n",
    "    \n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T)\n",
    "    \n",
    "    # Compute GAE backwards\n",
    "    gae = 0\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_val = next_value\n",
    "        else:\n",
    "            next_val = values[t + 1]\n",
    "        \n",
    "        # TD error\n",
    "        delta = rewards[t] + gamma * next_val - values[t]\n",
    "        \n",
    "        # GAE recursive formula\n",
    "        gae = delta + gamma * gae_lambda * gae\n",
    "        advantages[t] = gae\n",
    "    \n",
    "    # Target returns for value function training\n",
    "    returns = advantages + values\n",
    "    \n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Demonstrate GAE with different lambda values\n",
    "print(\"GAE Example with different λ values\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample trajectory\n",
    "rewards = [0.1, -0.05, 0.2, 0.15, -0.1]\n",
    "values = [0.5, 0.45, 0.55, 0.6, 0.5]\n",
    "next_value = 0.48\n",
    "\n",
    "for lam in [0.0, 0.5, 0.95, 1.0]:\n",
    "    advantages, returns = compute_gae(rewards, values, next_value, gae_lambda=lam)\n",
    "    print(f\"\\nλ = {lam}:\")\n",
    "    print(f\"  Advantages: {advantages.round(4)}\")\n",
    "    print(f\"  Advantage std: {advantages.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience buffer for batch updates\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done', 'log_prob', 'value'])\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) with GAE.\n",
    "    \n",
    "    Improvements over basic Actor-Critic:\n",
    "    - GAE for better advantage estimation\n",
    "    - Batch updates for stability\n",
    "    - Entropy bonus for exploration\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        update_steps: int = 128\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.update_steps = update_steps\n",
    "        \n",
    "        # Shared network backbone\n",
    "        self.actor = ActorNetwork(state_dim, action_dim).to(device)\n",
    "        self.critic = CriticNetwork(state_dim).to(device)\n",
    "        \n",
    "        # Single optimizer for both networks\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.actor.parameters()) + list(self.critic.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.buffer = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Tuple[int, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Select action and get value estimate.\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        probs = self.actor(state_t)\n",
    "        value = self.critic(state_t)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value.squeeze()\n",
    "    \n",
    "    def store_experience(self, exp: Experience):\n",
    "        \"\"\"Store experience in buffer.\"\"\"\n",
    "        self.buffer.append(exp)\n",
    "    \n",
    "    def update(self, next_state: np.ndarray) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Batch update using GAE.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.update_steps:\n",
    "            return 0, 0, 0\n",
    "        \n",
    "        # Extract data from buffer\n",
    "        states = torch.FloatTensor([e.state for e in self.buffer]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in self.buffer]).to(device)\n",
    "        rewards = [e.reward for e in self.buffer]\n",
    "        dones = [e.done for e in self.buffer]\n",
    "        old_log_probs = torch.stack([e.log_prob for e in self.buffer]).to(device)\n",
    "        values = torch.stack([e.value for e in self.buffer]).detach().cpu().numpy()\n",
    "        \n",
    "        # Get next value for GAE\n",
    "        with torch.no_grad():\n",
    "            next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            next_value = self.critic(next_state_t).item()\n",
    "        \n",
    "        # Compute GAE advantages and returns\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards, values, next_value,\n",
    "            self.gamma, self.gae_lambda\n",
    "        )\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Forward pass\n",
    "        probs = self.actor(states)\n",
    "        current_values = self.critic(states).squeeze()\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Actor loss (policy gradient with advantages)\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # Critic loss (MSE with returns)\n",
    "        critic_loss = F.mse_loss(current_values, returns)\n",
    "        \n",
    "        # Total loss with entropy bonus\n",
    "        total_loss = (\n",
    "            actor_loss +\n",
    "            self.value_coef * critic_loss -\n",
    "            self.entropy_coef * entropy\n",
    "        )\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.actor.parameters()) + list(self.critic.parameters()),\n",
    "            max_norm=0.5\n",
    "        )\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.buffer = []\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), entropy.item()\n",
    "\n",
    "\n",
    "print(\"A2C Agent with GAE created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c(\n",
    "    env: SimpleTradingEnv,\n",
    "    agent: A2CAgent,\n",
    "    n_episodes: int = 500,\n",
    "    print_every: int = 50\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train A2C agent with periodic batch updates.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_pnls = []\n",
    "    total_steps = 0\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            \n",
    "            # Take step\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            exp = Experience(state, action, reward, next_state, done, log_prob, value)\n",
    "            agent.store_experience(exp)\n",
    "            \n",
    "            total_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Update when buffer is full or episode ends\n",
    "            if len(agent.buffer) >= agent.update_steps or done:\n",
    "                agent.update(next_state)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_pnls.append(info['total_pnl'])\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_pnl = np.mean(episode_pnls[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | Avg Reward: {avg_reward:8.4f} | \"\n",
    "                  f\"Avg PnL: {avg_pnl:8.4f} | Steps: {total_steps}\")\n",
    "    \n",
    "    return episode_rewards, episode_pnls\n",
    "\n",
    "\n",
    "# Train A2C agent\n",
    "print(\"Training A2C Agent with GAE...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "env = SimpleTradingEnv()\n",
    "a2c_agent = A2CAgent(\n",
    "    state_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.01,\n",
    "    update_steps=64\n",
    ")\n",
    "\n",
    "a2c_rewards, a2c_pnls = train_a2c(env, a2c_agent, n_episodes=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7c05a",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Trading Policy Network with Continuous Actions\n",
    "\n",
    "Real trading requires **continuous actions** (position sizes), not just discrete buy/sell.\n",
    "\n",
    "### Gaussian Policy\n",
    "\n",
    "For continuous actions, the policy outputs parameters of a Gaussian distribution:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s)^2)$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_\\theta(s)$ = Mean action (from neural network)\n",
    "- $\\sigma_\\theta(s)$ = Standard deviation (learned or fixed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba616463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousTradingEnv:\n",
    "    \"\"\"\n",
    "    Trading environment with continuous position sizing.\n",
    "    \n",
    "    Action: Position size in [-1, 1]\n",
    "        -1 = Full short\n",
    "         0 = No position\n",
    "        +1 = Full long\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices: np.ndarray = None,\n",
    "        lookback: int = 20,\n",
    "        transaction_cost: float = 0.001\n",
    "    ):\n",
    "        self.lookback = lookback\n",
    "        self.transaction_cost = transaction_cost\n",
    "        \n",
    "        if prices is None:\n",
    "            self.prices = self._generate_prices(1000)\n",
    "        else:\n",
    "            self.prices = prices\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_prices(self, n: int) -> np.ndarray:\n",
    "        \"\"\"Generate realistic price series with regime changes.\"\"\"\n",
    "        prices = [100.0]\n",
    "        regime = 'trending'\n",
    "        trend = 0.001\n",
    "        \n",
    "        for i in range(n - 1):\n",
    "            # Regime changes\n",
    "            if np.random.random() < 0.02:\n",
    "                regime = np.random.choice(['trending', 'mean_revert', 'volatile'])\n",
    "                trend = np.random.uniform(-0.003, 0.003)\n",
    "            \n",
    "            if regime == 'trending':\n",
    "                change = trend + np.random.randn() * 0.015\n",
    "            elif regime == 'mean_revert':\n",
    "                mean_price = np.mean(prices[-20:]) if len(prices) >= 20 else prices[0]\n",
    "                change = 0.1 * (mean_price - prices[-1]) / prices[-1] + np.random.randn() * 0.01\n",
    "            else:  # volatile\n",
    "                change = np.random.randn() * 0.03\n",
    "            \n",
    "            new_price = prices[-1] * (1 + change)\n",
    "            prices.append(max(new_price, 1.0))  # Price floor\n",
    "        \n",
    "        return np.array(prices)\n",
    "    \n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.current_step = self.lookback\n",
    "        self.position = 0.0  # Continuous position [-1, 1]\n",
    "        self.entry_price = 0\n",
    "        self.total_pnl = 0\n",
    "        self.pnl_history = []\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"Construct feature-rich state.\"\"\"\n",
    "        prices = self.prices[self.current_step - self.lookback:self.current_step + 1]\n",
    "        returns = np.diff(prices) / prices[:-1]\n",
    "        \n",
    "        # Technical features\n",
    "        sma_5 = np.mean(prices[-5:]) / prices[-1] - 1\n",
    "        sma_10 = np.mean(prices[-10:]) / prices[-1] - 1\n",
    "        sma_20 = np.mean(prices) / prices[-1] - 1\n",
    "        \n",
    "        volatility = np.std(returns)\n",
    "        momentum = np.sum(returns[-5:])\n",
    "        \n",
    "        # RSI approximation\n",
    "        gains = np.maximum(returns, 0)\n",
    "        losses = np.maximum(-returns, 0)\n",
    "        avg_gain = np.mean(gains[-14:]) if len(gains) >= 14 else np.mean(gains)\n",
    "        avg_loss = np.mean(losses[-14:]) if len(losses) >= 14 else np.mean(losses)\n",
    "        rsi = avg_gain / (avg_gain + avg_loss + 1e-8) - 0.5  # Centered at 0\n",
    "        \n",
    "        # Position features\n",
    "        unrealized_pnl = 0\n",
    "        if self.position != 0:\n",
    "            unrealized_pnl = self.position * (prices[-1] - self.entry_price) / self.entry_price\n",
    "        \n",
    "        state = np.concatenate([\n",
    "            returns[-10:],           # Recent returns\n",
    "            [sma_5, sma_10, sma_20], # Moving average features\n",
    "            [volatility],            # Volatility\n",
    "            [momentum],              # Momentum\n",
    "            [rsi],                   # RSI\n",
    "            [self.position],         # Current position\n",
    "            [unrealized_pnl]         # Unrealized PnL\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action: float) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Execute continuous action.\n",
    "        Action is clipped to [-1, 1].\n",
    "        \"\"\"\n",
    "        # Clip action to valid range\n",
    "        target_position = np.clip(action, -1, 1)\n",
    "        \n",
    "        current_price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        \n",
    "        # Position change\n",
    "        position_change = abs(target_position - self.position)\n",
    "        \n",
    "        # Transaction cost\n",
    "        reward -= position_change * self.transaction_cost\n",
    "        \n",
    "        # PnL from closing/adjusting position\n",
    "        if self.position != 0:\n",
    "            pnl = self.position * (current_price - self.entry_price) / self.entry_price\n",
    "            # Scale PnL by how much we're closing\n",
    "            close_fraction = min(position_change / (abs(self.position) + 1e-8), 1.0)\n",
    "            realized_pnl = pnl * close_fraction\n",
    "            reward += realized_pnl\n",
    "            self.total_pnl += realized_pnl\n",
    "            self.pnl_history.append(realized_pnl)\n",
    "        \n",
    "        # Update position\n",
    "        self.position = target_position\n",
    "        self.entry_price = current_price if target_position != 0 else 0\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # Holding reward\n",
    "        if not done and self.position != 0:\n",
    "            next_price = self.prices[self.current_step]\n",
    "            holding_return = self.position * (next_price - current_price) / current_price\n",
    "            reward += holding_return\n",
    "        \n",
    "        info = {\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'position': self.position,\n",
    "            'n_trades': len(self.pnl_history),\n",
    "            'sharpe': self._calculate_sharpe()\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _calculate_sharpe(self) -> float:\n",
    "        if len(self.pnl_history) < 2:\n",
    "            return 0.0\n",
    "        returns = np.array(self.pnl_history)\n",
    "        return np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
    "    \n",
    "    @property\n",
    "    def state_dim(self) -> int:\n",
    "        return 18  # 10 returns + 3 SMA + vol + mom + rsi + pos + pnl\n",
    "    \n",
    "    @property\n",
    "    def action_dim(self) -> int:\n",
    "        return 1  # Continuous position\n",
    "\n",
    "\n",
    "# Test continuous environment\n",
    "cont_env = ContinuousTradingEnv()\n",
    "state = cont_env.reset()\n",
    "print(f\"State dimension: {cont_env.state_dim}\")\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"Sample state: {state[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ca020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian policy for continuous actions.\n",
    "    Outputs mean and log_std for position sizing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int = 1,\n",
    "        hidden_dim: int = 128,\n",
    "        log_std_min: float = -20,\n",
    "        log_std_max: float = 2\n",
    "    ):\n",
    "        super(GaussianPolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Mean head (bounded by tanh)\n",
    "        self.mean_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()  # Bounds action to [-1, 1]\n",
    "        )\n",
    "        \n",
    "        # Log std head (learned)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Output mean and log_std of action distribution.\"\"\"\n",
    "        features = self.shared(state)\n",
    "        mean = self.mean_head(features)\n",
    "        log_std = self.log_std_head(features)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample action and compute log probability.\"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        # Sample from Gaussian\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.rsample()  # Reparameterization trick\n",
    "        \n",
    "        # Compute log probability\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        # Clamp action to valid range\n",
    "        action = torch.clamp(action, -1, 1)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Evaluate log probability of actions.\"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        dist = Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        return log_prob, entropy\n",
    "\n",
    "\n",
    "# Test Gaussian policy\n",
    "gaussian_policy = GaussianPolicyNetwork(state_dim=cont_env.state_dim)\n",
    "test_state = torch.randn(1, cont_env.state_dim)\n",
    "\n",
    "mean, log_std = gaussian_policy(test_state)\n",
    "action, log_prob = gaussian_policy.sample(test_state)\n",
    "\n",
    "print(f\"Mean action: {mean.item():.4f}\")\n",
    "print(f\"Std: {log_std.exp().item():.4f}\")\n",
    "print(f\"Sampled action: {action.item():.4f}\")\n",
    "print(f\"Log probability: {log_prob.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f40d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousA2C:\n",
    "    \"\"\"\n",
    "    A2C for continuous trading actions.\n",
    "    Uses Gaussian policy for position sizing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int = 1,\n",
    "        lr: float = 3e-4,\n",
    "        gamma: float = 0.99,\n",
    "        gae_lambda: float = 0.95,\n",
    "        entropy_coef: float = 0.01,\n",
    "        value_coef: float = 0.5,\n",
    "        update_steps: int = 64\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        self.update_steps = update_steps\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = GaussianPolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.critic = CriticNetwork(state_dim).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.actor.parameters()) + list(self.critic.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        self.buffer = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> Tuple[float, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Select continuous action.\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        action, log_prob = self.actor.sample(state_t)\n",
    "        value = self.critic(state_t)\n",
    "        \n",
    "        return action.item(), log_prob, value.squeeze()\n",
    "    \n",
    "    def store_experience(self, exp: Experience):\n",
    "        self.buffer.append(exp)\n",
    "    \n",
    "    def update(self, next_state: np.ndarray) -> Tuple[float, float, float]:\n",
    "        \"\"\"Update using GAE.\"\"\"\n",
    "        if len(self.buffer) < self.update_steps:\n",
    "            return 0, 0, 0\n",
    "        \n",
    "        # Extract data\n",
    "        states = torch.FloatTensor([e.state for e in self.buffer]).to(device)\n",
    "        actions = torch.FloatTensor([[e.action] for e in self.buffer]).to(device)\n",
    "        rewards = [e.reward for e in self.buffer]\n",
    "        values = torch.stack([e.value for e in self.buffer]).detach().cpu().numpy()\n",
    "        \n",
    "        # Get next value\n",
    "        with torch.no_grad():\n",
    "            next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            next_value = self.critic(next_state_t).item()\n",
    "        \n",
    "        # Compute GAE\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards, values, next_value,\n",
    "            self.gamma, self.gae_lambda\n",
    "        )\n",
    "        \n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Evaluate actions\n",
    "        log_probs, entropy = self.actor.evaluate(states, actions)\n",
    "        current_values = self.critic(states).squeeze()\n",
    "        \n",
    "        # Losses\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = F.mse_loss(current_values, returns)\n",
    "        entropy_loss = -entropy.mean()\n",
    "        \n",
    "        total_loss = (\n",
    "            actor_loss +\n",
    "            self.value_coef * critic_loss +\n",
    "            self.entropy_coef * entropy_loss\n",
    "        )\n",
    "        \n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.actor.parameters()) + list(self.critic.parameters()),\n",
    "            max_norm=0.5\n",
    "        )\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.buffer = []\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), -entropy_loss.item()\n",
    "\n",
    "\n",
    "print(\"Continuous A2C Agent created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7051ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_continuous_a2c(\n",
    "    env: ContinuousTradingEnv,\n",
    "    agent: ContinuousA2C,\n",
    "    n_episodes: int = 500,\n",
    "    print_every: int = 50\n",
    ") -> Tuple[List[float], List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train continuous A2C agent.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_pnls = []\n",
    "    episode_sharpes = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, log_prob, value = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            exp = Experience(state, action, reward, next_state, done, log_prob, value)\n",
    "            agent.store_experience(exp)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if len(agent.buffer) >= agent.update_steps or done:\n",
    "                agent.update(next_state)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_pnls.append(info['total_pnl'])\n",
    "        episode_sharpes.append(info['sharpe'])\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            avg_pnl = np.mean(episode_pnls[-print_every:])\n",
    "            avg_sharpe = np.mean(episode_sharpes[-print_every:])\n",
    "            print(f\"Episode {episode + 1:4d} | Reward: {avg_reward:8.4f} | \"\n",
    "                  f\"PnL: {avg_pnl:8.4f} | Sharpe: {avg_sharpe:6.2f}\")\n",
    "    \n",
    "    return episode_rewards, episode_pnls, episode_sharpes\n",
    "\n",
    "\n",
    "# Train Continuous A2C\n",
    "print(\"Training Continuous A2C Agent...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cont_env = ContinuousTradingEnv()\n",
    "cont_a2c_agent = ContinuousA2C(\n",
    "    state_dim=cont_env.state_dim,\n",
    "    action_dim=cont_env.action_dim,\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95\n",
    ")\n",
    "\n",
    "cont_rewards, cont_pnls, cont_sharpes = train_continuous_a2c(\n",
    "    cont_env, cont_a2c_agent, n_episodes=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# REINFORCE vs Actor-Critic rewards\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(pd.Series(reinforce_rewards).rolling(window).mean(), label='REINFORCE', alpha=0.8)\n",
    "ax1.plot(pd.Series(ac_rewards).rolling(window).mean(), label='Actor-Critic', alpha=0.8)\n",
    "ax1.plot(pd.Series(a2c_rewards).rolling(window).mean(), label='A2C (GAE)', alpha=0.8)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "ax1.set_title('Discrete Actions: Algorithm Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Continuous A2C results\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(pd.Series(cont_rewards).rolling(window).mean(), color='purple', linewidth=2)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode Reward')\n",
    "ax2.set_title('Continuous A2C: Episode Rewards')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# PnL comparison\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(pd.Series(reinforce_pnls).rolling(window).mean(), label='REINFORCE', alpha=0.8)\n",
    "ax3.plot(pd.Series(ac_pnls).rolling(window).mean(), label='Actor-Critic', alpha=0.8)\n",
    "ax3.plot(pd.Series(a2c_pnls).rolling(window).mean(), label='A2C (GAE)', alpha=0.8)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Total PnL')\n",
    "ax3.set_title('Discrete Actions: PnL Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Continuous Sharpe ratio\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(pd.Series(cont_sharpes).rolling(window).mean(), color='green', linewidth=2)\n",
    "ax4.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax4.axhline(y=1, color='blue', linestyle='--', alpha=0.5, label='Sharpe=1')\n",
    "ax4.axhline(y=2, color='green', linestyle='--', alpha=0.5, label='Sharpe=2')\n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Sharpe Ratio')\n",
    "ax4.set_title('Continuous A2C: Sharpe Ratio')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a208ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained continuous agent\n",
    "def evaluate_agent(env, agent, n_episodes: int = 10) -> dict:\n",
    "    \"\"\"Evaluate trained agent without exploration.\"\"\"\n",
    "    total_rewards = []\n",
    "    total_pnls = []\n",
    "    positions_history = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_positions = []\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                mean, _ = agent.actor(state_t)\n",
    "                action = mean.item()  # Use mean action (no sampling)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_positions.append(info['position'])\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        total_pnls.append(info['total_pnl'])\n",
    "        positions_history.append(episode_positions)\n",
    "    \n",
    "    return {\n",
    "        'mean_reward': np.mean(total_rewards),\n",
    "        'std_reward': np.std(total_rewards),\n",
    "        'mean_pnl': np.mean(total_pnls),\n",
    "        'std_pnl': np.std(total_pnls),\n",
    "        'positions': positions_history[-1]  # Last episode positions\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_agent(cont_env, cont_a2c_agent, n_episodes=10)\n",
    "\n",
    "print(\"Continuous A2C Evaluation Results\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.4f} ± {eval_results['std_reward']:.4f}\")\n",
    "print(f\"Mean PnL: {eval_results['mean_pnl']:.4f} ± {eval_results['std_pnl']:.4f}\")\n",
    "\n",
    "# Plot position distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "positions = eval_results['positions']\n",
    "axes[0].plot(positions)\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Agent Position Over Time')\n",
    "axes[0].set_ylim(-1.1, 1.1)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(positions, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Position Distribution')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21f48e",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### Algorithms Covered\n",
    "\n",
    "| Algorithm | Update | Variance | Bias | Use Case |\n",
    "|-----------|--------|----------|------|----------|\n",
    "| **REINFORCE** | Episode end | High | Low | Simple, educational |\n",
    "| **Actor-Critic** | Every step | Medium | Medium | Online learning |\n",
    "| **A2C + GAE** | Batch | Low | Adjustable | Production systems |\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Policy Gradient Theorem**: $\\nabla_\\theta J = \\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q(s,a)]$\n",
    "\n",
    "2. **Variance Reduction**:\n",
    "   - Baseline subtraction\n",
    "   - Value function (Critic)\n",
    "   - GAE ($\\lambda$ parameter)\n",
    "\n",
    "3. **Continuous Actions**:\n",
    "   - Gaussian policy: $\\pi(a|s) = \\mathcal{N}(\\mu(s), \\sigma(s)^2)$\n",
    "   - Natural for position sizing\n",
    "\n",
    "### Trading Applications\n",
    "\n",
    "- **Position sizing** → Continuous actions\n",
    "- **Risk management** → Reward shaping (Sharpe ratio)\n",
    "- **Transaction costs** → Built into reward\n",
    "- **Market regimes** → State representation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **PPO (Proximal Policy Optimization)** - More stable training\n",
    "2. **SAC (Soft Actor-Critic)** - Maximum entropy RL\n",
    "3. **Multi-asset trading** - Multiple action dimensions\n",
    "4. **Risk-adjusted rewards** - CVaR, Sortino ratio\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca22137",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Implement PPO\n",
    "Add clipped surrogate objective to A2C:\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
    "\n",
    "### Exercise 2: Risk-Adjusted Reward\n",
    "Modify the reward to include:\n",
    "- Sharpe ratio component\n",
    "- Drawdown penalty\n",
    "- Position risk penalty\n",
    "\n",
    "### Exercise 3: Multi-Asset Trading\n",
    "Extend to 2+ assets:\n",
    "- Correlation features in state\n",
    "- Portfolio-level position constraints\n",
    "- Rebalancing costs\n",
    "\n",
    "### Exercise 4: Market Regime Detection\n",
    "Add regime-aware features:\n",
    "- Hidden Markov Model states\n",
    "- Volatility regime indicators\n",
    "- Trend strength metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
