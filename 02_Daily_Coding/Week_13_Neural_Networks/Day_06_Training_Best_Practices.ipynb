{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efab649",
   "metadata": {},
   "source": [
    "# Day 6: Training Best Practices for Neural Networks in Trading\n",
    "\n",
    "## Week 13 - Neural Networks in Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "\n",
    "1. **Data Preprocessing & Standardization** - Proper scaling for financial data\n",
    "2. **Train/Validation/Test Splits for Time Series** - Avoiding look-ahead bias\n",
    "3. **Walk-Forward Training Paradigm** - Realistic backtesting methodology\n",
    "4. **Mini-batch vs Full-batch Training** - Optimization for small financial datasets\n",
    "5. **Handling Class Imbalance** - Dealing with rare trading signals\n",
    "6. **Reproducibility** - Seeds, determinism, and experiment tracking\n",
    "7. **GPU vs CPU Considerations** - Hardware optimization\n",
    "8. **Practical: Full Training Pipeline** - Production-ready implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Why Training Practices Matter in Finance\n",
    "\n",
    "**Common Pitfalls in Financial ML:**\n",
    "- Overfitting to historical data\n",
    "- Look-ahead bias from improper data splits\n",
    "- Non-reproducible results leading to false confidence\n",
    "- Class imbalance causing models to ignore rare profitable signals\n",
    "\n",
    "### European Market Considerations ðŸ‡ªðŸ‡º\n",
    "\n",
    "- **GDPR**: Data handling and storage requirements\n",
    "- **MiFID II**: Algorithm testing and validation documentation\n",
    "- **Model Governance**: Reproducibility is mandatory for regulatory audits\n",
    "- **Cross-Border Trading**: Handle different market calendars properly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b2353",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0030a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, WeightedRandomSampler\n",
    "\n",
    "# Data acquisition\n",
    "import yfinance as yf\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9211f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Reproducibility: Seeds and Determinism\n",
    "\n",
    "### Why Reproducibility Matters\n",
    "\n",
    "**Regulatory Requirements:**\n",
    "- MiFID II requires audit trails for algorithmic trading\n",
    "- Results must be reproducible for compliance reviews\n",
    "- Model validation requires consistent behavior\n",
    "\n",
    "**Practical Benefits:**\n",
    "- Debug models reliably\n",
    "- Compare experiments fairly\n",
    "- Deploy with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43654709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All seeds set to: 42\n",
      "Deterministic mode: Enabled\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def set_all_seeds(seed: int = 42, deterministic: bool = True):\n",
    "    \"\"\"\n",
    "    Set all random seeds for complete reproducibility.\n",
    "    \n",
    "    This function ensures reproducible results across:\n",
    "    - NumPy operations\n",
    "    - Python random module\n",
    "    - PyTorch CPU operations\n",
    "    - PyTorch GPU operations (if available)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    seed : int - The seed value to use\n",
    "    deterministic : bool - If True, enforce deterministic algorithms\n",
    "                          (may impact performance)\n",
    "    \n",
    "    Note: For 100% reproducibility on GPU, you may also need:\n",
    "    export CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "    \"\"\"\n",
    "    # Python's built-in random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch CPU\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # PyTorch GPU (all GPUs)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Deterministic algorithms\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # For PyTorch 1.8+\n",
    "        if hasattr(torch, 'use_deterministic_algorithms'):\n",
    "            try:\n",
    "                torch.use_deterministic_algorithms(True)\n",
    "            except Exception:\n",
    "                pass  # Some operations may not have deterministic implementations\n",
    "    else:\n",
    "        # Allow cuDNN to find optimal algorithms (faster but non-deterministic)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Set Python hash seed (for dict ordering consistency)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    return seed\n",
    "\n",
    "\n",
    "# Set seeds for this notebook\n",
    "SEED = set_all_seeds(42)\n",
    "print(f\"All seeds set to: {SEED}\")\n",
    "print(f\"Deterministic mode: Enabled\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314223c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility Test Results:\n",
      "========================================\n",
      "NumPy random reproducible: âœ“\n",
      "PyTorch random reproducible: âœ“\n",
      "Model initialization reproducible: âœ“\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify reproducibility\n",
    "def test_reproducibility(seed=42):\n",
    "    \"\"\"\n",
    "    Test that our seeding works correctly.\n",
    "    \n",
    "    Runs the same operation twice and verifies identical results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for run in range(2):\n",
    "        set_all_seeds(seed)\n",
    "        \n",
    "        # NumPy random\n",
    "        np_random = np.random.randn(5)\n",
    "        \n",
    "        # PyTorch random\n",
    "        torch_random = torch.randn(5)\n",
    "        \n",
    "        # PyTorch model initialization\n",
    "        model = nn.Linear(10, 5)\n",
    "        weights = model.weight.data.clone()\n",
    "        \n",
    "        results.append({\n",
    "            'np_random': np_random,\n",
    "            'torch_random': torch_random.numpy(),\n",
    "            'model_weights': weights.numpy()\n",
    "        })\n",
    "    \n",
    "    # Verify all match\n",
    "    print(\"Reproducibility Test Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    np_match = np.allclose(results[0]['np_random'], results[1]['np_random'])\n",
    "    torch_match = np.allclose(results[0]['torch_random'], results[1]['torch_random'])\n",
    "    weights_match = np.allclose(results[0]['model_weights'], results[1]['model_weights'])\n",
    "    \n",
    "    print(f\"NumPy random reproducible: {'âœ“' if np_match else 'âœ—'}\")\n",
    "    print(f\"PyTorch random reproducible: {'âœ“' if torch_match else 'âœ—'}\")\n",
    "    print(f\"Model initialization reproducible: {'âœ“' if weights_match else 'âœ—'}\")\n",
    "    \n",
    "    return all([np_match, torch_match, weights_match])\n",
    "\n",
    "test_reproducibility()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c21e6c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Acquisition and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee30f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 2021-01-24 to 2026-01-23\n",
      "--------------------------------------------------\n",
      "âœ“ AAPL: 1255 trading days\n",
      "âœ“ MSFT: 1255 trading days\n",
      "âœ“ GOOGL: 1255 trading days\n",
      "âœ“ AMZN: 1255 trading days\n",
      "âœ“ META: 1255 trading days\n",
      "âœ“ JPM: 1255 trading days\n",
      "âœ“ GS: 1255 trading days\n",
      "âœ“ BAC: 1255 trading days\n",
      "âœ“ SAP: 1255 trading days\n",
      "âœ“ ASML: 1255 trading days\n",
      "âœ“ NVO: 1255 trading days\n",
      "âœ“ SPY: 1255 trading days\n",
      "âœ“ QQQ: 1255 trading days\n",
      "âœ“ EWG: 1255 trading days\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ— \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Error - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Create price DataFrame (using Close column)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m prices_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mClose\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m prices_df = prices_df.ffill().dropna()\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prices_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prices_df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m assets\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    776\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    777\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    778\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:667\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[32m    670\u001b[39m     index = union_indexes(indexes)\n",
      "\u001b[31mValueError\u001b[39m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "# Download diverse market data\n",
    "# Including European stocks for market considerations\n",
    "\n",
    "tickers = [\n",
    "    # US Equities\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META',\n",
    "    'JPM', 'GS', 'BAC',\n",
    "    # European Equities (traded on US exchanges or ADRs)\n",
    "    'SAP', 'ASML', 'NVO',  # European tech/pharma\n",
    "    # Market indices\n",
    "    'SPY', 'QQQ', 'EWG',  # Germany ETF\n",
    "]\n",
    "\n",
    "# 5 years of data for robust training\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(f\"Downloading data from {start_date.date()} to {end_date.date()}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Download all data\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        if len(df) > 500:  # At least 2 years of trading days\n",
    "            data[ticker] = df\n",
    "            print(f\"âœ“ {ticker}: {len(df)} trading days\")\n",
    "        else:\n",
    "            print(f\"âœ— {ticker}: Insufficient data\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {ticker}: Error - {e}\")\n",
    "\n",
    "# Create price DataFrame (using Close column)\n",
    "prices_df = pd.DataFrame({ticker: df['Close'] for ticker, df in data.items()})\n",
    "prices_df = prices_df.ffill().dropna()\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(prices_df)} days, {len(prices_df.columns)} assets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae3b66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Preprocessing and Standardization\n",
    "\n",
    "### Why Preprocessing Matters for Neural Networks\n",
    "\n",
    "**Problem without preprocessing:**\n",
    "- Features on different scales dominate gradient updates\n",
    "- Slow convergence or failure to converge\n",
    "- Numerical instability\n",
    "\n",
    "**Common Scaling Methods:**\n",
    "\n",
    "| Method | Formula | Best For |\n",
    "|--------|---------|----------|\n",
    "| StandardScaler | (x - Î¼) / Ïƒ | Gaussian-like distributions |\n",
    "| RobustScaler | (x - median) / IQR | Outlier-heavy data |\n",
    "| MinMaxScaler | (x - min) / (max - min) | Bounded features |\n",
    "\n",
    "### Financial Data Considerations\n",
    "\n",
    "- **Returns**: Often approximately Gaussian, StandardScaler works well\n",
    "- **Volume**: Heavy tails, use RobustScaler or log transform\n",
    "- **Prices**: Non-stationary, convert to returns first\n",
    "- **Volatility**: Right-skewed, consider log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e69742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_with_target(prices_df, target_horizon=1, lookback_windows=[5, 10, 20, 60]):\n",
    "    \"\"\"\n",
    "    Create features and target for neural network training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame - Price data (Close prices)\n",
    "    target_horizon : int - Days ahead for target (default: next day)\n",
    "    lookback_windows : list - Windows for feature calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features_df : pd.DataFrame - Feature matrix\n",
    "    targets : pd.Series - Binary targets (1 = up, 0 = down)\n",
    "    feature_names : list - Names of features\n",
    "    \"\"\"\n",
    "    feature_dfs = []\n",
    "    \n",
    "    for ticker in prices_df.columns:\n",
    "        price = prices_df[ticker]\n",
    "        \n",
    "        # Create features\n",
    "        features = pd.DataFrame(index=prices_df.index)\n",
    "        \n",
    "        # Returns at different horizons\n",
    "        features[f'{ticker}_ret_1d'] = price.pct_change(1)\n",
    "        features[f'{ticker}_ret_5d'] = price.pct_change(5)\n",
    "        features[f'{ticker}_ret_20d'] = price.pct_change(20)\n",
    "        \n",
    "        # Momentum and mean reversion features\n",
    "        for window in lookback_windows:\n",
    "            # Price relative to moving average\n",
    "            sma = price.rolling(window).mean()\n",
    "            features[f'{ticker}_sma_ratio_{window}'] = price / sma - 1\n",
    "            \n",
    "            # Volatility\n",
    "            features[f'{ticker}_vol_{window}'] = price.pct_change().rolling(window).std() * np.sqrt(252)\n",
    "            \n",
    "            # Momentum\n",
    "            features[f'{ticker}_mom_{window}'] = price / price.shift(window) - 1\n",
    "        \n",
    "        # RSI\n",
    "        delta = price.diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / (loss + 1e-10)\n",
    "        features[f'{ticker}_rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        feature_dfs.append(features)\n",
    "    \n",
    "    # Combine all features\n",
    "    features_df = pd.concat(feature_dfs, axis=1)\n",
    "    \n",
    "    # Create target using SPY as market proxy\n",
    "    # Binary: 1 if market goes up, 0 if down\n",
    "    target_ticker = 'SPY' if 'SPY' in prices_df.columns else prices_df.columns[0]\n",
    "    targets = (prices_df[target_ticker].shift(-target_horizon) > prices_df[target_ticker]).astype(int)\n",
    "    targets.name = 'target'\n",
    "    \n",
    "    # Drop NaN\n",
    "    valid_idx = features_df.dropna().index.intersection(targets.dropna().index)\n",
    "    features_df = features_df.loc[valid_idx]\n",
    "    targets = targets.loc[valid_idx]\n",
    "    \n",
    "    return features_df, targets, list(features_df.columns)\n",
    "\n",
    "\n",
    "# Create features\n",
    "features_df, targets, feature_names = create_features_with_target(prices_df)\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Target distribution: Up={targets.sum()}, Down={len(targets)-targets.sum()}\")\n",
    "print(f\"Class ratio: {targets.mean():.2%} positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5261ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessor for financial features.\n",
    "    \n",
    "    Handles:\n",
    "    - Feature scaling with multiple methods\n",
    "    - Outlier handling\n",
    "    - Missing value imputation\n",
    "    - Feature-wise scaling (crucial for look-ahead bias prevention)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_method='robust', clip_outliers=True, \n",
    "                 outlier_threshold=5.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        scaling_method : str - 'standard', 'robust', or 'minmax'\n",
    "        clip_outliers : bool - Whether to clip extreme values\n",
    "        outlier_threshold : float - Number of std/IQR for clipping\n",
    "        \"\"\"\n",
    "        self.scaling_method = scaling_method\n",
    "        self.clip_outliers = clip_outliers\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        \n",
    "        # Will be fit on training data\n",
    "        self.scaler = None\n",
    "        self.feature_stats = {}\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit preprocessor on training data only.\n",
    "        \n",
    "        CRITICAL: Never fit on validation/test data to avoid look-ahead bias!\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # Store feature statistics for outlier handling\n",
    "        if self.scaling_method == 'robust':\n",
    "            self.feature_stats['median'] = np.nanmedian(X, axis=0)\n",
    "            q75 = np.nanpercentile(X, 75, axis=0)\n",
    "            q25 = np.nanpercentile(X, 25, axis=0)\n",
    "            self.feature_stats['iqr'] = q75 - q25\n",
    "        else:\n",
    "            self.feature_stats['mean'] = np.nanmean(X, axis=0)\n",
    "            self.feature_stats['std'] = np.nanstd(X, axis=0)\n",
    "        \n",
    "        # Initialize and fit scaler\n",
    "        if self.scaling_method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaling_method == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        elif self.scaling_method == 'minmax':\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "        # Handle NaN before fitting\n",
    "        X_clean = np.nan_to_num(X, nan=0.0)\n",
    "        self.scaler.fit(X_clean)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data using fitted parameters.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor not fitted. Call fit() first.\")\n",
    "        \n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        # Handle NaN\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "        \n",
    "        # Clip outliers before scaling\n",
    "        if self.clip_outliers:\n",
    "            X = self._clip_outliers(X)\n",
    "        \n",
    "        # Scale\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled.astype(np.float32)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step.\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def _clip_outliers(self, X):\n",
    "        \"\"\"Clip outliers based on fitted statistics.\"\"\"\n",
    "        if self.scaling_method == 'robust':\n",
    "            median = self.feature_stats['median']\n",
    "            iqr = self.feature_stats['iqr'] + 1e-8\n",
    "            lower = median - self.outlier_threshold * iqr\n",
    "            upper = median + self.outlier_threshold * iqr\n",
    "        else:\n",
    "            mean = self.feature_stats['mean']\n",
    "            std = self.feature_stats['std'] + 1e-8\n",
    "            lower = mean - self.outlier_threshold * std\n",
    "            upper = mean + self.outlier_threshold * std\n",
    "        \n",
    "        return np.clip(X, lower, upper)\n",
    "\n",
    "\n",
    "# Demonstrate preprocessing\n",
    "print(\"Data Preprocessing Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample_features = features_df.iloc[:1000].values\n",
    "\n",
    "for method in ['standard', 'robust', 'minmax']:\n",
    "    preprocessor = FinancialDataPreprocessor(scaling_method=method)\n",
    "    scaled = preprocessor.fit_transform(sample_features)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Scaler:\")\n",
    "    print(f\"  Original range: [{np.nanmin(sample_features):.2f}, {np.nanmax(sample_features):.2f}]\")\n",
    "    print(f\"  Scaled range: [{np.min(scaled):.2f}, {np.max(scaled):.2f}]\")\n",
    "    print(f\"  Scaled mean: {np.mean(scaled):.4f}, std: {np.std(scaled):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2d19e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Train/Validation/Test Splits for Time Series\n",
    "\n",
    "### CRITICAL: Avoiding Look-Ahead Bias\n",
    "\n",
    "**Standard ML Split (WRONG for time series):**\n",
    "```\n",
    "Random shuffle â†’ Split into train/val/test\n",
    "```\n",
    "This causes data leakage: future data influences predictions about the past.\n",
    "\n",
    "**Correct Time Series Split:**\n",
    "```\n",
    "Time-ordered data â†’ [Train] | [Validation] | [Test]\n",
    "                     Past      Present       Future\n",
    "```\n",
    "\n",
    "### Gap Period\n",
    "\n",
    "Include a gap between train and validation to:\n",
    "- Prevent target leakage from autocorrelated data\n",
    "- Simulate realistic prediction delay\n",
    "- Account for information propagation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b01eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesSplitter:\n",
    "    \"\"\"\n",
    "    Time-aware data splitter for financial ML.\n",
    "    \n",
    "    Prevents look-ahead bias by maintaining chronological order.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_ratio=0.7, val_ratio=0.15, gap_days=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_ratio : float - Proportion of data for training\n",
    "        val_ratio : float - Proportion of data for validation\n",
    "        gap_days : int - Number of days gap between splits (prevents leakage)\n",
    "        \"\"\"\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.gap_days = gap_days\n",
    "        # test_ratio is implicit: 1 - train_ratio - val_ratio\n",
    "    \n",
    "    def split(self, X, y, dates=None):\n",
    "        \"\"\"\n",
    "        Split data chronologically.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like - Features\n",
    "        y : array-like - Targets\n",
    "        dates : array-like - Optional date index for reporting\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict with train, val, test splits\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        train_end = int(n * self.train_ratio)\n",
    "        val_start = train_end + self.gap_days\n",
    "        val_end = val_start + int(n * self.val_ratio)\n",
    "        test_start = val_end + self.gap_days\n",
    "        \n",
    "        # Create splits\n",
    "        splits = {\n",
    "            'train': {\n",
    "                'X': X[:train_end],\n",
    "                'y': y[:train_end],\n",
    "                'indices': range(0, train_end)\n",
    "            },\n",
    "            'val': {\n",
    "                'X': X[val_start:val_end],\n",
    "                'y': y[val_start:val_end],\n",
    "                'indices': range(val_start, val_end)\n",
    "            },\n",
    "            'test': {\n",
    "                'X': X[test_start:],\n",
    "                'y': y[test_start:],\n",
    "                'indices': range(test_start, n)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add dates if provided\n",
    "        if dates is not None:\n",
    "            dates = np.array(dates)\n",
    "            splits['train']['dates'] = dates[:train_end]\n",
    "            splits['val']['dates'] = dates[val_start:val_end]\n",
    "            splits['test']['dates'] = dates[test_start:]\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def report(self, splits, dates=None):\n",
    "        \"\"\"Print split information.\"\"\"\n",
    "        print(\"Time Series Split Report:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for name, data in splits.items():\n",
    "            n = len(data['X'])\n",
    "            if 'dates' in data:\n",
    "                start = pd.Timestamp(data['dates'][0]).strftime('%Y-%m-%d')\n",
    "                end = pd.Timestamp(data['dates'][-1]).strftime('%Y-%m-%d')\n",
    "                print(f\"{name.upper():8s}: {n:5d} samples | {start} to {end}\")\n",
    "            else:\n",
    "                print(f\"{name.upper():8s}: {n:5d} samples\")\n",
    "\n",
    "\n",
    "# Apply time series split\n",
    "splitter = TimeSeriesSplitter(train_ratio=0.7, val_ratio=0.15, gap_days=5)\n",
    "splits = splitter.split(\n",
    "    features_df.values, \n",
    "    targets.values,\n",
    "    dates=features_df.index\n",
    ")\n",
    "\n",
    "splitter.report(splits)\n",
    "\n",
    "# Visualize the split\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "# Plot timeline\n",
    "for name, data in splits.items():\n",
    "    if 'dates' in data:\n",
    "        start_idx = data['indices'][0]\n",
    "        end_idx = data['indices'][-1]\n",
    "        color = {'train': 'blue', 'val': 'orange', 'test': 'green'}[name]\n",
    "        ax.axvspan(start_idx, end_idx, alpha=0.3, label=name.upper(), color=color)\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_title('Time Series Train/Validation/Test Split')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5b9383",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Walk-Forward Training Paradigm\n",
    "\n",
    "### Why Walk-Forward?\n",
    "\n",
    "**Standard Training:**\n",
    "- Train once on historical data\n",
    "- Test on future data\n",
    "- Problem: Model becomes stale as markets evolve\n",
    "\n",
    "**Walk-Forward Training:**\n",
    "- Train on window of historical data\n",
    "- Predict next period\n",
    "- Roll forward and repeat\n",
    "- Mimics real trading environment\n",
    "\n",
    "```\n",
    "Window 1: [Train...............] [Test]\n",
    "Window 2:     [Train...............] [Test]\n",
    "Window 3:         [Train...............] [Test]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d858305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkForwardValidator:\n",
    "    \"\"\"\n",
    "    Walk-forward validation framework for time series.\n",
    "    \n",
    "    Implements expanding or rolling window validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_splits=5, train_period=252, test_period=63,\n",
    "                 expanding=False, gap=5):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_splits : int - Number of walk-forward folds\n",
    "        train_period : int - Training window size (days)\n",
    "        test_period : int - Test window size (days)\n",
    "        expanding : bool - If True, training window expands; if False, rolls\n",
    "        gap : int - Gap between train and test to prevent leakage\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.expanding = expanding\n",
    "        self.gap = gap\n",
    "    \n",
    "    def split(self, X):\n",
    "        \"\"\"\n",
    "        Generate train/test indices for each fold.\n",
    "        \n",
    "        Yields:\n",
    "        -------\n",
    "        train_idx, test_idx for each fold\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        \n",
    "        # Calculate minimum required data\n",
    "        min_required = self.train_period + self.gap + self.test_period\n",
    "        \n",
    "        # Calculate step size between folds\n",
    "        if self.n_splits > 1:\n",
    "            # Available space for all test periods\n",
    "            available = n - min_required\n",
    "            step = max(1, available // (self.n_splits - 1))\n",
    "        else:\n",
    "            step = 0\n",
    "        \n",
    "        for fold in range(self.n_splits):\n",
    "            if self.expanding:\n",
    "                # Expanding window: train from start\n",
    "                train_start = 0\n",
    "                train_end = self.train_period + fold * step\n",
    "            else:\n",
    "                # Rolling window: fixed train size\n",
    "                train_start = fold * step\n",
    "                train_end = train_start + self.train_period\n",
    "            \n",
    "            # Test period with gap\n",
    "            test_start = train_end + self.gap\n",
    "            test_end = min(test_start + self.test_period, n)\n",
    "            \n",
    "            if test_end <= test_start:\n",
    "                break\n",
    "            \n",
    "            train_idx = np.arange(train_start, train_end)\n",
    "            test_idx = np.arange(test_start, test_end)\n",
    "            \n",
    "            yield train_idx, test_idx\n",
    "    \n",
    "    def visualize(self, X, dates=None):\n",
    "        \"\"\"Visualize walk-forward splits.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, self.n_splits))\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(self.split(X)):\n",
    "            # Plot training period\n",
    "            ax.barh(fold, len(train_idx), left=train_idx[0], \n",
    "                   color=colors[fold], alpha=0.5, label=f'Fold {fold+1} Train')\n",
    "            \n",
    "            # Plot test period\n",
    "            ax.barh(fold, len(test_idx), left=test_idx[0],\n",
    "                   color=colors[fold], alpha=1.0, edgecolor='black')\n",
    "        \n",
    "        ax.set_xlabel('Sample Index')\n",
    "        ax.set_ylabel('Fold')\n",
    "        ax.set_title('Walk-Forward Validation Splits\\n(Light = Train, Dark = Test)')\n",
    "        ax.set_yticks(range(self.n_splits))\n",
    "        ax.set_yticklabels([f'Fold {i+1}' for i in range(self.n_splits)])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Create walk-forward validator\n",
    "wf_validator = WalkForwardValidator(\n",
    "    n_splits=5,\n",
    "    train_period=500,  # ~2 years training\n",
    "    test_period=63,    # ~3 months test\n",
    "    expanding=True,    # Expanding window\n",
    "    gap=5\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "wf_validator.visualize(features_df.values)\n",
    "\n",
    "# Print fold details\n",
    "print(\"\\nWalk-Forward Fold Details:\")\n",
    "print(\"=\" * 50)\n",
    "for fold, (train_idx, test_idx) in enumerate(wf_validator.split(features_df.values)):\n",
    "    print(f\"Fold {fold+1}: Train [{train_idx[0]:4d} - {train_idx[-1]:4d}] ({len(train_idx):4d} samples), \"\n",
    "          f\"Test [{test_idx[0]:4d} - {test_idx[-1]:4d}] ({len(test_idx):3d} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975103e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Mini-batch vs Full-batch Training\n",
    "\n",
    "### Batch Size Considerations for Financial Data\n",
    "\n",
    "| Batch Size | Pros | Cons | Best For |\n",
    "|------------|------|------|----------|\n",
    "| Full batch | Stable gradients, deterministic | Memory intensive, slow updates | Very small datasets (<1000) |\n",
    "| Large batch (256-512) | Efficient GPU utilization | Less regularization | Large datasets with GPU |\n",
    "| Medium batch (32-128) | Balance of stability/noise | General purpose | Most financial datasets |\n",
    "| Small batch (8-32) | Strong regularization | Noisy gradients | Very small datasets, overfit-prone |\n",
    "\n",
    "### Financial-Specific Guidelines\n",
    "\n",
    "- **Small datasets (<5000)**: Use smaller batches (16-32) for regularization\n",
    "- **Time series**: Batch sampling should respect temporal order\n",
    "- **Class imbalance**: Consider stratified batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa251aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_size_effects(X, y, batch_sizes=[16, 32, 64, 128, 256]):\n",
    "    \"\"\"\n",
    "    Analyze the effects of different batch sizes on training.\n",
    "    \n",
    "    Demonstrates:\n",
    "    - Gradient variance at different batch sizes\n",
    "    - Training time per epoch\n",
    "    - Memory requirements\n",
    "    \"\"\"\n",
    "    print(\"Batch Size Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Batch Size':>12} | {'Batches/Epoch':>14} | {'Est. GPU Mem':>12} | {'Gradient Noise':>14}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Number of batches per epoch\n",
    "        n_batches = np.ceil(n_samples / batch_size).astype(int)\n",
    "        \n",
    "        # Estimated memory (simplified)\n",
    "        # Assumes float32, accounts for model + gradients + optimizer states\n",
    "        mem_mb = (batch_size * n_features * 4 * 3) / (1024 * 1024)\n",
    "        \n",
    "        # Gradient noise estimation (smaller batch = higher variance)\n",
    "        # Based on central limit theorem: variance ~ 1/sqrt(batch_size)\n",
    "        relative_noise = 1.0 / np.sqrt(batch_size)\n",
    "        noise_level = 'High' if relative_noise > 0.2 else 'Medium' if relative_noise > 0.1 else 'Low'\n",
    "        \n",
    "        print(f\"{batch_size:>12} | {n_batches:>14} | {mem_mb:>10.2f} MB | {noise_level:>14}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Recommendations for financial data:\")\n",
    "    if n_samples < 5000:\n",
    "        print(f\"   Dataset size: {n_samples} (small) â†’ Use batch size 16-32\")\n",
    "    elif n_samples < 50000:\n",
    "        print(f\"   Dataset size: {n_samples} (medium) â†’ Use batch size 32-64\")\n",
    "    else:\n",
    "        print(f\"   Dataset size: {n_samples} (large) â†’ Use batch size 64-256\")\n",
    "\n",
    "\n",
    "# Analyze batch sizes for our data\n",
    "analyze_batch_size_effects(features_df.values, targets.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d8ea65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Handling Class Imbalance in Trading Signals\n",
    "\n",
    "### The Imbalance Problem in Trading\n",
    "\n",
    "**Common scenarios:**\n",
    "- Rare profitable opportunities (5-10% of signals)\n",
    "- Extreme market events (crashes, rallies)\n",
    "- Asymmetric targets (large up vs small down)\n",
    "\n",
    "**Solutions:**\n",
    "1. **Class Weighting**: Penalize misclassification of minority class\n",
    "2. **Oversampling**: SMOTE, random oversampling\n",
    "3. **Undersampling**: Random undersampling (risks losing information)\n",
    "4. **Threshold Adjustment**: Modify decision threshold\n",
    "5. **Focal Loss**: Down-weight easy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d011cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbalanceHandler:\n",
    "    \"\"\"\n",
    "    Handles class imbalance for trading signal prediction.\n",
    "    \n",
    "    Provides multiple strategies:\n",
    "    - Class weights for loss function\n",
    "    - Weighted sampling for DataLoader\n",
    "    - Focal Loss implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_class_weights(y, strategy='balanced'):\n",
    "        \"\"\"\n",
    "        Compute class weights for imbalanced data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : array-like - Binary labels\n",
    "        strategy : str - 'balanced', 'sqrt', or 'none'\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        weights : dict - Class weights {0: w0, 1: w1}\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        n_samples = len(y)\n",
    "        n_classes = 2\n",
    "        \n",
    "        # Count classes\n",
    "        class_counts = np.bincount(y.astype(int))\n",
    "        \n",
    "        if strategy == 'balanced':\n",
    "            # Inverse frequency weighting\n",
    "            weights = n_samples / (n_classes * class_counts)\n",
    "        elif strategy == 'sqrt':\n",
    "            # Square root of inverse frequency (less aggressive)\n",
    "            weights = np.sqrt(n_samples / (n_classes * class_counts))\n",
    "        else:\n",
    "            weights = np.ones(n_classes)\n",
    "        \n",
    "        return {0: weights[0], 1: weights[1]}\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_weighted_sampler(y):\n",
    "        \"\"\"\n",
    "        Create WeightedRandomSampler for balanced batch sampling.\n",
    "        \n",
    "        This ensures each batch has roughly equal class representation.\n",
    "        \"\"\"\n",
    "        y = np.array(y)\n",
    "        class_counts = np.bincount(y.astype(int))\n",
    "        \n",
    "        # Weight for each sample (inverse of its class frequency)\n",
    "        weights = 1.0 / class_counts[y.astype(int)]\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=weights,\n",
    "            num_samples=len(weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        return sampler\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "    \n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    gamma > 0 reduces loss for well-classified examples,\n",
    "    focusing training on hard, misclassified examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float or list - Class weights [w0, w1]\n",
    "        gamma : float - Focusing parameter (higher = more focus on hard examples)\n",
    "        reduction : str - 'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Apply sigmoid to get probabilities\n",
    "        p = torch.sigmoid(inputs)\n",
    "        \n",
    "        # Get probability for true class\n",
    "        p_t = p * targets + (1 - p) * (1 - targets)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Binary cross-entropy\n",
    "        bce = F.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Apply focal weight\n",
    "        loss = focal_weight * bce\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        if self.alpha is not None:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"Class Distribution Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "y = targets.values\n",
    "class_counts = np.bincount(y.astype(int))\n",
    "\n",
    "print(f\"Class 0 (Down): {class_counts[0]:,} samples ({class_counts[0]/len(y)*100:.1f}%)\")\n",
    "print(f\"Class 1 (Up):   {class_counts[1]:,} samples ({class_counts[1]/len(y)*100:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {max(class_counts)/min(class_counts):.2f}:1\")\n",
    "\n",
    "# Compute class weights\n",
    "weights = ImbalanceHandler.compute_class_weights(y, strategy='balanced')\n",
    "print(f\"\\nBalanced class weights: {weights}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(['Down (0)', 'Up (1)'], class_counts, color=['red', 'green'], alpha=0.7)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99543301",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. GPU vs CPU Considerations\n",
    "\n",
    "### When to Use GPU\n",
    "\n",
    "**GPU Recommended:**\n",
    "- Large models (>100K parameters)\n",
    "- Large datasets (>50K samples)\n",
    "- Training deep networks (>5 layers)\n",
    "- Batch sizes >= 64\n",
    "\n",
    "**CPU May Be Better:**\n",
    "- Small models (<10K parameters)\n",
    "- Small datasets (<5K samples)\n",
    "- Inference with small batch sizes\n",
    "- Data loading is the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45594e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_cpu_vs_gpu(input_dim=100, hidden_dim=128, n_samples=10000, \n",
    "                         batch_size=64, n_epochs=5):\n",
    "    \"\"\"\n",
    "    Benchmark CPU vs GPU performance for neural network training.\n",
    "    \"\"\"\n",
    "    # Create dummy data\n",
    "    X = torch.randn(n_samples, input_dim)\n",
    "    y = torch.randint(0, 2, (n_samples, 1)).float()\n",
    "    \n",
    "    # Simple model\n",
    "    class BenchmarkModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    results = {}\n",
    "    devices_to_test = ['cpu']\n",
    "    if torch.cuda.is_available():\n",
    "        devices_to_test.append('cuda')\n",
    "    \n",
    "    for device_name in devices_to_test:\n",
    "        device = torch.device(device_name)\n",
    "        \n",
    "        # Create model and move to device\n",
    "        set_all_seeds(42)\n",
    "        model = BenchmarkModel().to(device)\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Warmup\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            _ = model(batch_X)\n",
    "            break\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_X)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        results[device_name] = elapsed\n",
    "    \n",
    "    # Report\n",
    "    print(\"CPU vs GPU Benchmark Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Configuration: {n_samples} samples, batch_size={batch_size}, {n_epochs} epochs\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for device_name, elapsed in results.items():\n",
    "        print(f\"{device_name.upper():6s}: {elapsed:.3f} seconds\")\n",
    "    \n",
    "    if 'cuda' in results:\n",
    "        speedup = results['cpu'] / results['cuda']\n",
    "        print(f\"\\nGPU Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_cpu_vs_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f12f1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Practical: Full Production Training Pipeline\n",
    "\n",
    "Now we'll implement a complete, production-ready training pipeline incorporating all best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for trading data with proper preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, preprocessor=None, fit_preprocessor=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like - Features\n",
    "        y : array-like - Targets\n",
    "        preprocessor : FinancialDataPreprocessor - Fitted preprocessor\n",
    "        fit_preprocessor : bool - Whether to fit preprocessor (only for training)\n",
    "        \"\"\"\n",
    "        self.y = np.array(y, dtype=np.float32)\n",
    "        \n",
    "        if fit_preprocessor:\n",
    "            self.preprocessor = FinancialDataPreprocessor(scaling_method='robust')\n",
    "            self.X = self.preprocessor.fit_transform(X)\n",
    "        elif preprocessor is not None:\n",
    "            self.preprocessor = preprocessor\n",
    "            self.X = self.preprocessor.transform(X)\n",
    "        else:\n",
    "            self.preprocessor = None\n",
    "            self.X = np.array(X, dtype=np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.X[idx]),\n",
    "            torch.FloatTensor([self.y[idx]])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready neural network for trading signals.\n",
    "    \n",
    "    Incorporates:\n",
    "    - Residual connections\n",
    "    - Batch normalization\n",
    "    - Dropout regularization\n",
    "    - GELU activation (modern alternative to ReLU)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout=0.3):\n",
    "        super(TradingNeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(prev_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier/Glorot initialization.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionTrainer:\n",
    "    \"\"\"\n",
    "    Production-ready training pipeline with all best practices.\n",
    "    \n",
    "    Features:\n",
    "    - Time series aware splits\n",
    "    - Class imbalance handling\n",
    "    - Learning rate scheduling\n",
    "    - Early stopping\n",
    "    - Gradient clipping\n",
    "    - Comprehensive logging\n",
    "    - Model checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cpu', seed=42):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "        self.best_model_state = None\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=100, lr=1e-3,\n",
    "              weight_decay=1e-5, patience=15, class_weights=None,\n",
    "              use_focal_loss=False, focal_gamma=2.0):\n",
    "        \"\"\"\n",
    "        Full training loop with all best practices.\n",
    "        \"\"\"\n",
    "        # Set seed for reproducibility\n",
    "        set_all_seeds(self.seed)\n",
    "        \n",
    "        # Setup loss function\n",
    "        if use_focal_loss:\n",
    "            alpha = class_weights[1] / (class_weights[0] + class_weights[1]) if class_weights else 0.5\n",
    "            criterion = FocalLoss(alpha=alpha, gamma=focal_gamma)\n",
    "        elif class_weights:\n",
    "            # Use weighted BCE loss\n",
    "            pos_weight = torch.FloatTensor([class_weights[1] / class_weights[0]]).to(self.device)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Optimizer with weight decay (L2 regularization)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(\"Training Configuration:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"  Device: {self.device}\")\n",
    "        print(f\"  Epochs: {num_epochs}\")\n",
    "        print(f\"  Learning Rate: {lr}\")\n",
    "        print(f\"  Weight Decay: {weight_decay}\")\n",
    "        print(f\"  Patience: {patience}\")\n",
    "        print(f\"  Loss: {'Focal Loss' if use_focal_loss else 'BCE Loss'}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # ===== TRAINING PHASE =====\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(batch_X)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # ===== VALIDATION PHASE =====\n",
    "            val_metrics = self._evaluate(val_loader, criterion)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_metrics['loss'])\n",
    "            self.history['val_acc'].append(val_metrics['accuracy'])\n",
    "            self.history['val_auc'].append(val_metrics['auc'])\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(val_metrics['loss'])\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                print(f\"Epoch {epoch+1:3d}: Train Loss = {train_loss:.4f}, \"\n",
    "                      f\"Val Loss = {val_metrics['loss']:.4f}, \"\n",
    "                      f\"Val Acc = {val_metrics['accuracy']:.4f}, \"\n",
    "                      f\"Val AUC = {val_metrics['auc']:.4f}, \"\n",
    "                      f\"LR = {current_lr:.2e}\")\n",
    "            \n",
    "            # Early stopping and checkpointing\n",
    "            if val_metrics['loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = val_metrics['loss']\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nâš¡ Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Restore best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(\"âœ“ Restored best model weights\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def _evaluate(self, loader, criterion):\n",
    "        \"\"\"Evaluate model on validation/test data.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                batch_y = batch_y.to(self.device)\n",
    "                \n",
    "                output = self.model(batch_X)\n",
    "                loss = criterion(output, batch_y)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                probs = torch.sigmoid(output)\n",
    "                preds = (probs > 0.5).float()\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy().flatten())\n",
    "                all_targets.extend(batch_y.cpu().numpy().flatten())\n",
    "                all_probs.extend(probs.cpu().numpy().flatten())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_targets, all_preds)\n",
    "        try:\n",
    "            auc = roc_auc_score(all_targets, all_probs)\n",
    "        except:\n",
    "            auc = 0.5\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / len(loader),\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc,\n",
    "            'predictions': all_preds,\n",
    "            'probabilities': all_probs\n",
    "        }\n",
    "    \n",
    "    def evaluate_trading_performance(self, test_loader, test_returns=None):\n",
    "        \"\"\"\n",
    "        Evaluate model as a trading strategy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        metrics = self._evaluate(test_loader, criterion)\n",
    "        \n",
    "        print(\"\\nTrading Strategy Performance:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"AUC-ROC: {metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        targets = [int(p) for p in metrics['predictions']]\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(\n",
    "            test_loader.dataset.y[:len(targets)].astype(int),\n",
    "            targets,\n",
    "            target_names=['Down', 'Up']\n",
    "        ))\n",
    "        \n",
    "        # Trading metrics (simplified)\n",
    "        if test_returns is not None:\n",
    "            positions = np.array([1 if p > 0.5 else -1 for p in metrics['probabilities']])\n",
    "            strategy_returns = positions * test_returns[:len(positions)]\n",
    "            \n",
    "            cum_return = (1 + strategy_returns).cumprod()[-1] - 1\n",
    "            sharpe = np.sqrt(252) * strategy_returns.mean() / (strategy_returns.std() + 1e-8)\n",
    "            \n",
    "            print(f\"\\nCumulative Return: {cum_return*100:.2f}%\")\n",
    "            print(f\"Sharpe Ratio: {sharpe:.2f}\")\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df54a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the complete training pipeline\n",
    "\n",
    "# 1. Time series split\n",
    "splitter = TimeSeriesSplitter(train_ratio=0.7, val_ratio=0.15, gap_days=5)\n",
    "splits = splitter.split(features_df.values, targets.values, dates=features_df.index)\n",
    "\n",
    "# 2. Create datasets with proper preprocessing\n",
    "train_dataset = TradingDataset(\n",
    "    splits['train']['X'], \n",
    "    splits['train']['y'],\n",
    "    fit_preprocessor=True\n",
    ")\n",
    "\n",
    "val_dataset = TradingDataset(\n",
    "    splits['val']['X'],\n",
    "    splits['val']['y'],\n",
    "    preprocessor=train_dataset.preprocessor\n",
    ")\n",
    "\n",
    "test_dataset = TradingDataset(\n",
    "    splits['test']['X'],\n",
    "    splits['test']['y'],\n",
    "    preprocessor=train_dataset.preprocessor\n",
    ")\n",
    "\n",
    "# 3. Compute class weights for imbalance handling\n",
    "class_weights = ImbalanceHandler.compute_class_weights(\n",
    "    splits['train']['y'], \n",
    "    strategy='balanced'\n",
    ")\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# 4. Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create model\n",
    "input_dim = train_dataset.X.shape[1]\n",
    "model = TradingNeuralNetwork(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[128, 64, 32],\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 6. Train\n",
    "trainer = ProductionTrainer(model, device=device, seed=SEED)\n",
    "\n",
    "history = trainer.train(\n",
    "    train_loader, val_loader,\n",
    "    num_epochs=100,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    patience=15,\n",
    "    class_weights=class_weights,\n",
    "    use_focal_loss=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['val_acc'], label='Validation', linewidth=2, color='green')\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', label='Random', alpha=0.5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[2].plot(history['val_auc'], label='Validation', linewidth=2, color='purple')\n",
    "axes[2].axhline(y=0.5, color='r', linestyle='--', label='Random', alpha=0.5)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('AUC-ROC')\n",
    "axes[2].set_title('Validation AUC-ROC')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef9fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final evaluation on test set\n",
    "test_metrics = trainer.evaluate_trading_performance(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bb441",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Key Takeaways\n",
    "\n",
    "### Training Best Practices Summary\n",
    "\n",
    "1. **Reproducibility is Non-Negotiable**\n",
    "   - Set all seeds: NumPy, Python, PyTorch\n",
    "   - Enable deterministic mode for auditable results\n",
    "   - Document all hyperparameters\n",
    "\n",
    "2. **Proper Data Preprocessing**\n",
    "   - Use RobustScaler for financial data (outlier-resistant)\n",
    "   - NEVER fit scaler on validation/test data\n",
    "   - Handle missing values explicitly\n",
    "\n",
    "3. **Time Series Splits Are Critical**\n",
    "   - Always maintain chronological order\n",
    "   - Include gap periods to prevent leakage\n",
    "   - Use walk-forward validation for realistic assessment\n",
    "\n",
    "4. **Handle Class Imbalance**\n",
    "   - Use class weights or focal loss\n",
    "   - Consider weighted sampling\n",
    "   - Monitor precision/recall, not just accuracy\n",
    "\n",
    "5. **Training Stability**\n",
    "   - Gradient clipping prevents explosions\n",
    "   - Learning rate scheduling improves convergence\n",
    "   - Early stopping prevents overfitting\n",
    "\n",
    "### European Regulatory Considerations ðŸ‡ªðŸ‡º\n",
    "\n",
    "- Document all training decisions (MiFID II)\n",
    "- Ensure reproducibility for audits\n",
    "- Maintain model versioning and governance\n",
    "- Test on data from multiple market regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8279c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Exercises\n",
    "\n",
    "### Exercise 1: Walk-Forward Validation\n",
    "Implement full walk-forward training:\n",
    "- Train separate models for each fold\n",
    "- Combine predictions across folds\n",
    "- Report aggregate performance\n",
    "\n",
    "### Exercise 2: Focal Loss Comparison\n",
    "Compare BCE loss vs Focal Loss:\n",
    "- Train identical models with each loss\n",
    "- Analyze precision/recall tradeoffs\n",
    "- Determine which works better for your data\n",
    "\n",
    "### Exercise 3: Learning Rate Finder\n",
    "Implement a learning rate finder:\n",
    "- Train with exponentially increasing LR\n",
    "- Plot loss vs learning rate\n",
    "- Find optimal starting LR\n",
    "\n",
    "### Exercise 4: European Market Strategy\n",
    "Build a model for European markets:\n",
    "- Use European stocks only\n",
    "- Account for different trading hours\n",
    "- Handle currency considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7540f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space\n",
    "print(\"Ready for exercises!\")\n",
    "\n",
    "# Example: Learning Rate Finder starter\n",
    "def lr_finder(model, train_loader, init_lr=1e-7, final_lr=1, num_iter=100):\n",
    "    \"\"\"\n",
    "    Find optimal learning rate using the LR Range Test.\n",
    "    \n",
    "    Implementation hint:\n",
    "    - Start with very low LR\n",
    "    - Exponentially increase LR each iteration\n",
    "    - Track loss at each step\n",
    "    - Plot loss vs LR (use log scale)\n",
    "    - Optimal LR is where loss decreases fastest\n",
    "    \"\"\"\n",
    "    # Your code here...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba0931",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **Smith (2018)** - \"A disciplined approach to neural network hyper-parameters\" - Learning rate, batch size\n",
    "2. **He et al. (2019)** - \"Bag of Tricks for Image Classification\" - Training best practices\n",
    "3. **Lin et al. (2017)** - \"Focal Loss for Dense Object Detection\" - Focal loss\n",
    "4. **LÃ³pez de Prado (2018)** - \"Advances in Financial Machine Learning\" - Walk-forward validation\n",
    "5. **Goyal et al. (2017)** - \"Accurate, Large Minibatch SGD\" - Batch size effects\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Day 7 - Interview Review & Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
