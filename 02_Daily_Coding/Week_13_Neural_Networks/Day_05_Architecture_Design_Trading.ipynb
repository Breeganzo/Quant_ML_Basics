{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ea3111",
   "metadata": {},
   "source": [
    "# Day 5: Neural Network Architecture Design for Trading\n",
    "\n",
    "## Week 13 - Neural Networks in Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **MLP Depth vs Width Tradeoffs** - How to balance network architecture\n",
    "2. **Skip Connections & Residual Networks** - Combating vanishing gradients\n",
    "3. **Embedding Layers** - Handling categorical features (sector, asset class)\n",
    "4. **Multi-Input Architectures** - Combining price, volume, and sentiment data\n",
    "5. **AutoML & Neural Architecture Search** - Automated architecture optimization\n",
    "6. **Practical Application** - Design optimal architecture for a factor model\n",
    "\n",
    "---\n",
    "\n",
    "## Why Architecture Matters in Trading\n",
    "\n",
    "In quantitative finance, the choice of neural network architecture can significantly impact:\n",
    "\n",
    "- **Signal-to-Noise Ratio**: Financial data is inherently noisy; architecture affects extraction of true signals\n",
    "- **Overfitting Risk**: Markets are non-stationary; simpler architectures may generalize better\n",
    "- **Inference Speed**: For real-time trading, latency matters\n",
    "- **Interpretability**: Regulatory requirements may demand model explainability\n",
    "\n",
    "### European Market Considerations ðŸ‡ªðŸ‡º\n",
    "\n",
    "- **MiFID II Compliance**: Model governance and explainability requirements\n",
    "- **Trading Hours**: European markets (LSE, Euronext, XETRA) operate 08:00-16:30 CET\n",
    "- **Multi-Currency**: Need to handle EUR, GBP, CHF denominated assets\n",
    "- **Cross-Market Correlations**: European indices often lead/lag US markets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ff18b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df6ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "# Data acquisition\n",
    "import yfinance as yf\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e85ea2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load Market Data\n",
    "\n",
    "We'll load a diverse set of assets including European stocks to build our multi-asset architecture examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f10291d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from 2023-01-24 to 2026-01-23\n",
      "--------------------------------------------------\n",
      "âœ“ AAPL: 752 days of data\n",
      "âœ“ MSFT: 752 days of data\n",
      "âœ“ GOOGL: 752 days of data\n",
      "âœ“ JPM: 752 days of data\n",
      "âœ“ GS: 752 days of data\n",
      "âœ“ SAP: 752 days of data\n",
      "âœ“ ASML: 752 days of data\n",
      "âœ“ HSBA.L: 758 days of data\n",
      "âœ“ SPY: 752 days of data\n",
      "âœ“ EWG: 752 days of data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ— \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Error - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Create DataFrames\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m prices_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m volume_df = pd.DataFrame(volume_data)\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Forward fill missing values (holidays may differ between markets)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    776\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    777\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    778\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/internals/construction.py:667\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[32m    670\u001b[39m     index = union_indexes(indexes)\n",
      "\u001b[31mValueError\u001b[39m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "# Define tickers: Mix of US and European assets\n",
    "# European tickers often have suffixes: .L (London), .PA (Paris), .DE (Germany)\n",
    "tickers = {\n",
    "    # US Tech\n",
    "    'AAPL': {'sector': 'Technology', 'asset_class': 'Equity', 'region': 'US'},\n",
    "    'MSFT': {'sector': 'Technology', 'asset_class': 'Equity', 'region': 'US'},\n",
    "    'GOOGL': {'sector': 'Technology', 'asset_class': 'Equity', 'region': 'US'},\n",
    "    # US Finance\n",
    "    'JPM': {'sector': 'Financials', 'asset_class': 'Equity', 'region': 'US'},\n",
    "    'GS': {'sector': 'Financials', 'asset_class': 'Equity', 'region': 'US'},\n",
    "    # European stocks\n",
    "    'SAP': {'sector': 'Technology', 'asset_class': 'Equity', 'region': 'EU'},  # German tech\n",
    "    'ASML': {'sector': 'Technology', 'asset_class': 'Equity', 'region': 'EU'},  # Dutch semiconductor\n",
    "    'HSBA.L': {'sector': 'Financials', 'asset_class': 'Equity', 'region': 'EU'},  # UK bank\n",
    "    # ETFs for broader market\n",
    "    'SPY': {'sector': 'Index', 'asset_class': 'ETF', 'region': 'US'},\n",
    "    'EWG': {'sector': 'Index', 'asset_class': 'ETF', 'region': 'EU'},  # Germany ETF\n",
    "}\n",
    "\n",
    "# Download data\n",
    "# Using 3 years of data for robust training\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=3*365)\n",
    "\n",
    "print(f\"Downloading data from {start_date.date()} to {end_date.date()}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Download all tickers\n",
    "price_data = {}\n",
    "volume_data = {}\n",
    "\n",
    "for ticker in tickers.keys():\n",
    "    try:\n",
    "        # Download OHLCV data\n",
    "        df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        if len(df) > 100:  # Ensure sufficient data\n",
    "            price_data[ticker] = df['Close']  # Using Close column as specified\n",
    "            volume_data[ticker] = df['Volume']\n",
    "            print(f\"âœ“ {ticker}: {len(df)} days of data\")\n",
    "        else:\n",
    "            print(f\"âœ— {ticker}: Insufficient data ({len(df)} days)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {ticker}: Error - {str(e)}\")\n",
    "\n",
    "# Create DataFrames\n",
    "prices_df = pd.DataFrame(price_data)\n",
    "volume_df = pd.DataFrame(volume_data)\n",
    "\n",
    "# Forward fill missing values (holidays may differ between markets)\n",
    "prices_df = prices_df.ffill().dropna()\n",
    "volume_df = volume_df.ffill().dropna()\n",
    "\n",
    "print(f\"\\nFinal dataset: {len(prices_df)} trading days, {len(prices_df.columns)} assets\")\n",
    "prices_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcdc74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Feature Engineering for Neural Networks\n",
    "\n",
    "We'll create a comprehensive feature set including:\n",
    "- Technical indicators (momentum, volatility)\n",
    "- Cross-asset features\n",
    "- Categorical features (sector, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4518ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(prices_df, volume_df, tickers_info, lookback_windows=[5, 10, 20, 60]):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature set for neural network training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    prices_df : pd.DataFrame - Close prices for all assets\n",
    "    volume_df : pd.DataFrame - Volume data for all assets\n",
    "    tickers_info : dict - Metadata about each ticker\n",
    "    lookback_windows : list - Windows for calculating features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features_dict : dict - Dictionary containing feature DataFrames\n",
    "    \"\"\"\n",
    "    features_dict = {}\n",
    "    \n",
    "    for ticker in prices_df.columns:\n",
    "        price = prices_df[ticker]\n",
    "        volume = volume_df[ticker] if ticker in volume_df.columns else None\n",
    "        \n",
    "        # Initialize feature DataFrame\n",
    "        features = pd.DataFrame(index=prices_df.index)\n",
    "        \n",
    "        # ===== PRICE-BASED FEATURES =====\n",
    "        # Returns at different horizons\n",
    "        features['return_1d'] = price.pct_change(1)\n",
    "        features['return_5d'] = price.pct_change(5)\n",
    "        features['return_20d'] = price.pct_change(20)\n",
    "        \n",
    "        # Momentum features\n",
    "        for window in lookback_windows:\n",
    "            # Simple moving average ratio\n",
    "            sma = price.rolling(window).mean()\n",
    "            features[f'sma_ratio_{window}'] = price / sma - 1\n",
    "            \n",
    "            # Volatility (annualized)\n",
    "            features[f'volatility_{window}'] = price.pct_change().rolling(window).std() * np.sqrt(252)\n",
    "            \n",
    "            # Price momentum\n",
    "            features[f'momentum_{window}'] = price / price.shift(window) - 1\n",
    "        \n",
    "        # RSI (Relative Strength Index)\n",
    "        delta = price.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        features['rsi'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # ===== VOLUME-BASED FEATURES =====\n",
    "        if volume is not None:\n",
    "            features['volume_sma_ratio'] = volume / volume.rolling(20).mean()\n",
    "            features['volume_change'] = volume.pct_change()\n",
    "            # On-balance volume proxy\n",
    "            features['obv_slope'] = (volume * np.sign(price.diff())).rolling(10).mean()\n",
    "        \n",
    "        # ===== CATEGORICAL FEATURES =====\n",
    "        # These will be encoded later using embeddings\n",
    "        if ticker in tickers_info:\n",
    "            features['sector'] = tickers_info[ticker]['sector']\n",
    "            features['asset_class'] = tickers_info[ticker]['asset_class']\n",
    "            features['region'] = tickers_info[ticker]['region']\n",
    "        \n",
    "        # ===== TARGET VARIABLE =====\n",
    "        # Binary classification: Will price go up in next day?\n",
    "        features['target'] = (price.shift(-1) > price).astype(int)\n",
    "        \n",
    "        # Store features\n",
    "        features_dict[ticker] = features\n",
    "    \n",
    "    return features_dict\n",
    "\n",
    "# Create features for all assets\n",
    "features_dict = create_features(prices_df, volume_df, tickers)\n",
    "\n",
    "# Display sample features\n",
    "sample_ticker = list(features_dict.keys())[0]\n",
    "print(f\"Features for {sample_ticker}:\")\n",
    "print(f\"Shape: {features_dict[sample_ticker].shape}\")\n",
    "features_dict[sample_ticker].dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c620c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. MLP Depth vs Width Tradeoffs\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Width (neurons per layer):**\n",
    "- More neurons = higher capacity to learn complex patterns\n",
    "- Risk: Overfitting, especially with limited financial data\n",
    "- Benefit: Can capture more features simultaneously\n",
    "\n",
    "**Depth (number of layers):**\n",
    "- More layers = ability to learn hierarchical features\n",
    "- Risk: Vanishing gradients, harder to train\n",
    "- Benefit: Can learn compositional representations\n",
    "\n",
    "### Trading-Specific Considerations\n",
    "\n",
    "| Scenario | Recommended Architecture |\n",
    "|----------|-------------------------|\n",
    "| Small dataset (<1000 samples) | Shallow & narrow (2 layers, 32-64 neurons) |\n",
    "| Medium dataset (1000-10000) | Moderate depth (3-4 layers, 64-128 neurons) |\n",
    "| Large dataset (>10000) | Deeper networks possible (5+ layers) |\n",
    "| High-frequency signals | Narrower networks (faster inference) |\n",
    "| Factor models | Wider first layer (capture many factors) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc407b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Configurable MLP to experiment with depth and width.\n",
    "    \n",
    "    This architecture allows systematic comparison of different\n",
    "    network configurations for trading applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1, \n",
    "                 dropout=0.3, activation='relu', batch_norm=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int - Number of input features\n",
    "        hidden_dims : list - List of hidden layer dimensions (defines depth & width)\n",
    "        output_dim : int - Output dimension (1 for binary classification)\n",
    "        dropout : float - Dropout rate for regularization\n",
    "        activation : str - Activation function ('relu', 'leaky_relu', 'elu')\n",
    "        batch_norm : bool - Whether to use batch normalization\n",
    "        \"\"\"\n",
    "        super(ConfigurableMLP, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # Select activation function\n",
    "        activation_funcs = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(0.1),\n",
    "            'elu': nn.ELU(),\n",
    "            'gelu': nn.GELU()  # Popular in transformers\n",
    "        }\n",
    "        self.activation = activation_funcs.get(activation, nn.ReLU())\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear layer\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            \n",
    "            # Batch normalization (helps with training stability)\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Activation\n",
    "            layers.append(self.activation)\n",
    "            \n",
    "            # Dropout (regularization crucial for financial data)\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Create sequential container\n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(prev_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Compare different architectures\n",
    "input_dim = 20  # Number of features\n",
    "\n",
    "architectures = {\n",
    "    'Shallow_Wide': [256, 128],           # 2 layers, wide\n",
    "    'Deep_Narrow': [64, 64, 64, 64, 64],  # 5 layers, narrow\n",
    "    'Pyramid': [256, 128, 64, 32],        # Decreasing width\n",
    "    'Hourglass': [64, 128, 64],           # Expand then contract\n",
    "    'Uniform': [128, 128, 128],           # Same width throughout\n",
    "}\n",
    "\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Name':<15} {'Layers':<8} {'Params':<12} {'Architecture'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, hidden_dims in architectures.items():\n",
    "    model = ConfigurableMLP(input_dim, hidden_dims)\n",
    "    params = model.count_parameters()\n",
    "    print(f\"{name:<15} {len(hidden_dims):<8} {params:<12,} {hidden_dims}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Insight: More parameters â‰  better performance in finance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3217836",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Skip Connections and Residual Networks\n",
    "\n",
    "### Why Skip Connections Matter in Trading\n",
    "\n",
    "**The Vanishing Gradient Problem:**\n",
    "- Deep networks struggle to propagate gradients to early layers\n",
    "- Financial signals are often weak; we can't afford to lose gradient information\n",
    "\n",
    "**Benefits of Residual Connections:**\n",
    "1. **Gradient Highway**: Direct path for gradients to flow\n",
    "2. **Identity Mapping**: Network can learn to skip unnecessary transformations\n",
    "3. **Feature Preservation**: Original features available at all depths\n",
    "4. **Ensemble Effect**: Implicitly creates an ensemble of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e64fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connection.\n",
    "    \n",
    "    Output = F(x) + x, where F is a learned transformation.\n",
    "    This allows gradients to flow directly through the skip connection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, dropout=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Two-layer transformation (typical for ResNets)\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Skip connection: add input to transformed output\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = out + residual  # The magic of residual learning!\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with residual connections for trading signal prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Projection -> [ResBlock] x N -> Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=128, num_blocks=4, \n",
    "                 output_dim=1, dropout=0.3):\n",
    "        super(ResidualMLP, self).__init__()\n",
    "        \n",
    "        # Project input to hidden dimension\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Stack of residual blocks\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim, dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project to hidden space\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Generate output\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "# Compare standard MLP vs Residual MLP\n",
    "print(\"Standard MLP vs Residual MLP:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "standard_mlp = ConfigurableMLP(input_dim=20, hidden_dims=[128, 128, 128, 128])\n",
    "residual_mlp = ResidualMLP(input_dim=20, hidden_dim=128, num_blocks=4)\n",
    "\n",
    "print(f\"Standard MLP parameters: {standard_mlp.count_parameters():,}\")\n",
    "print(f\"Residual MLP parameters: {sum(p.numel() for p in residual_mlp.parameters()):,}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Residual networks often train faster and achieve better optima!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f8940",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Embedding Layers for Categorical Features\n",
    "\n",
    "### Why Embeddings?\n",
    "\n",
    "In trading, we often have categorical features:\n",
    "- **Sector**: Technology, Financials, Healthcare, etc.\n",
    "- **Asset Class**: Equity, Fixed Income, Commodity, FX\n",
    "- **Region**: US, EU, APAC\n",
    "- **Exchange**: NYSE, NASDAQ, LSE, Euronext\n",
    "\n",
    "**One-Hot Encoding Issues:**\n",
    "- High dimensionality with many categories\n",
    "- No relationship learned between categories\n",
    "- Sparse representation is inefficient\n",
    "\n",
    "**Embedding Benefits:**\n",
    "- Dense, learned representations\n",
    "- Captures semantic similarity (Tech and Semiconductors are related)\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEmbeddingMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with embedding layers for categorical features.\n",
    "    \n",
    "    This architecture combines:\n",
    "    - Numerical features (returns, volatility, etc.)\n",
    "    - Categorical features via embeddings (sector, asset class, region)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_numerical_features, \n",
    "                 num_sectors, sector_embed_dim,\n",
    "                 num_asset_classes, asset_embed_dim,\n",
    "                 num_regions, region_embed_dim,\n",
    "                 hidden_dims=[128, 64], output_dim=1, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        num_numerical_features : int - Number of numerical input features\n",
    "        num_sectors : int - Number of unique sectors\n",
    "        sector_embed_dim : int - Embedding dimension for sectors\n",
    "        num_asset_classes : int - Number of unique asset classes\n",
    "        asset_embed_dim : int - Embedding dimension for asset classes\n",
    "        num_regions : int - Number of unique regions\n",
    "        region_embed_dim : int - Embedding dimension for regions\n",
    "        hidden_dims : list - Hidden layer dimensions\n",
    "        output_dim : int - Output dimension\n",
    "        dropout : float - Dropout rate\n",
    "        \"\"\"\n",
    "        super(TradingEmbeddingMLP, self).__init__()\n",
    "        \n",
    "        # Embedding layers for categorical features\n",
    "        self.sector_embedding = nn.Embedding(num_sectors, sector_embed_dim)\n",
    "        self.asset_embedding = nn.Embedding(num_asset_classes, asset_embed_dim)\n",
    "        self.region_embedding = nn.Embedding(num_regions, region_embed_dim)\n",
    "        \n",
    "        # Calculate total input dimension after concatenation\n",
    "        total_embed_dim = sector_embed_dim + asset_embed_dim + region_embed_dim\n",
    "        total_input_dim = num_numerical_features + total_embed_dim\n",
    "        \n",
    "        # Build MLP layers\n",
    "        layers = []\n",
    "        prev_dim = total_input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.output_layer = nn.Linear(prev_dim, output_dim)\n",
    "    \n",
    "    def forward(self, numerical_features, sector_idx, asset_idx, region_idx):\n",
    "        \"\"\"\n",
    "        Forward pass combining numerical and categorical features.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        numerical_features : tensor - Numerical features (batch_size, num_features)\n",
    "        sector_idx : tensor - Sector indices (batch_size,)\n",
    "        asset_idx : tensor - Asset class indices (batch_size,)\n",
    "        region_idx : tensor - Region indices (batch_size,)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        sector_emb = self.sector_embedding(sector_idx)\n",
    "        asset_emb = self.asset_embedding(asset_idx)\n",
    "        region_emb = self.region_embedding(region_idx)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        x = torch.cat([numerical_features, sector_emb, asset_emb, region_emb], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        x = self.mlp(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "# Example: Create embedding model for our data\n",
    "# Define categorical mappings\n",
    "sectors = ['Technology', 'Financials', 'Healthcare', 'Index', 'Energy']\n",
    "asset_classes = ['Equity', 'ETF', 'Bond', 'Commodity']\n",
    "regions = ['US', 'EU', 'APAC']\n",
    "\n",
    "# Create encoders\n",
    "sector_encoder = LabelEncoder()\n",
    "sector_encoder.fit(sectors)\n",
    "\n",
    "asset_encoder = LabelEncoder()\n",
    "asset_encoder.fit(asset_classes)\n",
    "\n",
    "region_encoder = LabelEncoder()\n",
    "region_encoder.fit(regions)\n",
    "\n",
    "# Create model\n",
    "embedding_model = TradingEmbeddingMLP(\n",
    "    num_numerical_features=15,\n",
    "    num_sectors=len(sectors),\n",
    "    sector_embed_dim=4,  # Embed 5 sectors into 4D space\n",
    "    num_asset_classes=len(asset_classes),\n",
    "    asset_embed_dim=3,\n",
    "    num_regions=len(regions),\n",
    "    region_embed_dim=2,\n",
    "    hidden_dims=[64, 32]\n",
    ")\n",
    "\n",
    "print(\"Embedding Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(embedding_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in embedding_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602015d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Multi-Input Architectures\n",
    "\n",
    "### Combining Multiple Data Sources\n",
    "\n",
    "Real-world trading systems often combine:\n",
    "1. **Price Data**: Returns, volatility, technical indicators\n",
    "2. **Volume Data**: Trading activity, liquidity metrics\n",
    "3. **Sentiment Data**: News sentiment, social media signals\n",
    "4. **Fundamental Data**: Earnings, valuations\n",
    "5. **Alternative Data**: Satellite imagery, web traffic\n",
    "\n",
    "### Multi-Head Architecture Benefits\n",
    "- Each head can specialize in processing its data type\n",
    "- Appropriate preprocessing for each modality\n",
    "- Feature extraction tailored to data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7f5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInputTradingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-input architecture combining price, volume, and sentiment data.\n",
    "    \n",
    "    Architecture:\n",
    "    - Price Branch: Processes technical indicators\n",
    "    - Volume Branch: Processes volume-based features\n",
    "    - Sentiment Branch: Processes sentiment scores\n",
    "    - Fusion Layer: Combines all branches for final prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, price_dim, volume_dim, sentiment_dim,\n",
    "                 hidden_dim=64, output_dim=1, dropout=0.3):\n",
    "        super(MultiInputTradingNetwork, self).__init__()\n",
    "        \n",
    "        # ===== PRICE BRANCH =====\n",
    "        # Deeper network for complex price patterns\n",
    "        self.price_branch = nn.Sequential(\n",
    "            nn.Linear(price_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        )\n",
    "        \n",
    "        # ===== VOLUME BRANCH =====\n",
    "        # Simpler network for volume features\n",
    "        self.volume_branch = nn.Sequential(\n",
    "            nn.Linear(volume_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        )\n",
    "        \n",
    "        # ===== SENTIMENT BRANCH =====\n",
    "        # Simple processing for sentiment scores\n",
    "        self.sentiment_branch = nn.Sequential(\n",
    "            nn.Linear(sentiment_dim, hidden_dim // 4),\n",
    "            nn.BatchNorm1d(hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 4)\n",
    "        )\n",
    "        \n",
    "        # ===== FUSION LAYERS =====\n",
    "        # Combine all branches\n",
    "        fusion_dim = hidden_dim // 2 + hidden_dim // 4 + hidden_dim // 4\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention-based weighting for branches (optional)\n",
    "        self.branch_attention = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, price_features, volume_features, sentiment_features,\n",
    "                use_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass through multi-input network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        price_features : tensor - Price-based features\n",
    "        volume_features : tensor - Volume-based features\n",
    "        sentiment_features : tensor - Sentiment scores\n",
    "        use_attention : bool - Whether to use attention weighting\n",
    "        \"\"\"\n",
    "        # Process each branch\n",
    "        price_out = self.price_branch(price_features)\n",
    "        volume_out = self.volume_branch(volume_features)\n",
    "        sentiment_out = self.sentiment_branch(sentiment_features)\n",
    "        \n",
    "        # Concatenate branch outputs\n",
    "        combined = torch.cat([price_out, volume_out, sentiment_out], dim=1)\n",
    "        \n",
    "        # Apply fusion\n",
    "        output = self.fusion(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Create and visualize multi-input model\n",
    "multi_input_model = MultiInputTradingNetwork(\n",
    "    price_dim=12,      # 12 price features\n",
    "    volume_dim=4,      # 4 volume features\n",
    "    sentiment_dim=3,   # 3 sentiment features\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "print(\"Multi-Input Trading Network:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Price branch input: 12 features\")\n",
    "print(f\"Volume branch input: 4 features\")\n",
    "print(f\"Sentiment branch input: 3 features\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in multi_input_model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 32\n",
    "test_price = torch.randn(batch_size, 12)\n",
    "test_volume = torch.randn(batch_size, 4)\n",
    "test_sentiment = torch.randn(batch_size, 3)\n",
    "\n",
    "output = multi_input_model(test_price, test_volume, test_sentiment)\n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9c3e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. AutoML and Neural Architecture Search (NAS) Basics\n",
    "\n",
    "### Why AutoML for Trading?\n",
    "\n",
    "**Challenges in Manual Architecture Design:**\n",
    "- Exponential search space (depth Ã— width Ã— activations Ã— ...)\n",
    "- Non-stationary market requires periodic re-optimization\n",
    "- Human bias towards certain architectures\n",
    "\n",
    "**AutoML Approaches:**\n",
    "1. **Grid Search**: Exhaustive but expensive\n",
    "2. **Random Search**: Often outperforms grid search\n",
    "3. **Bayesian Optimization**: Efficient exploration\n",
    "4. **Neural Architecture Search (NAS)**: Learn to design networks\n",
    "\n",
    "### European Regulatory Considerations ðŸ‡ªðŸ‡º\n",
    "\n",
    "Under MiFID II and incoming AI regulations:\n",
    "- AutoML decisions must be documented\n",
    "- Search process should be reproducible\n",
    "- Selected architecture needs justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNAS:\n",
    "    \"\"\"\n",
    "    Simple Neural Architecture Search for trading networks.\n",
    "    \n",
    "    Uses random search with early stopping for efficiency.\n",
    "    In practice, consider using libraries like:\n",
    "    - Optuna\n",
    "    - Ray Tune\n",
    "    - NNI (Neural Network Intelligence)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, search_space=None, random_state=42):\n",
    "        self.input_dim = input_dim\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Define search space\n",
    "        self.search_space = search_space or {\n",
    "            'num_layers': [2, 3, 4, 5],\n",
    "            'hidden_dim': [32, 64, 128, 256],\n",
    "            'dropout': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "            'activation': ['relu', 'leaky_relu', 'elu', 'gelu'],\n",
    "            'batch_norm': [True, False],\n",
    "            'learning_rate': [1e-4, 5e-4, 1e-3, 5e-3]\n",
    "        }\n",
    "        \n",
    "        self.results = []\n",
    "    \n",
    "    def sample_architecture(self):\n",
    "        \"\"\"Sample a random architecture from search space.\"\"\"\n",
    "        config = {}\n",
    "        \n",
    "        # Sample hyperparameters\n",
    "        num_layers = np.random.choice(self.search_space['num_layers'])\n",
    "        hidden_dim = np.random.choice(self.search_space['hidden_dim'])\n",
    "        \n",
    "        # Create hidden dimensions (can vary per layer)\n",
    "        config['hidden_dims'] = [hidden_dim] * num_layers\n",
    "        config['dropout'] = np.random.choice(self.search_space['dropout'])\n",
    "        config['activation'] = np.random.choice(self.search_space['activation'])\n",
    "        config['batch_norm'] = np.random.choice(self.search_space['batch_norm'])\n",
    "        config['learning_rate'] = np.random.choice(self.search_space['learning_rate'])\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def build_model(self, config):\n",
    "        \"\"\"Build model from configuration.\"\"\"\n",
    "        return ConfigurableMLP(\n",
    "            input_dim=self.input_dim,\n",
    "            hidden_dims=config['hidden_dims'],\n",
    "            dropout=config['dropout'],\n",
    "            activation=config['activation'],\n",
    "            batch_norm=config['batch_norm']\n",
    "        )\n",
    "    \n",
    "    def evaluate_architecture(self, model, train_loader, val_loader, \n",
    "                             config, num_epochs=10):\n",
    "        \"\"\"Train and evaluate architecture.\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Setup training\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "        \n",
    "        # Training loop with early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs.squeeze(), y_batch.float())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    val_loss += criterion(outputs.squeeze(), y_batch.float()).item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    def search(self, train_loader, val_loader, n_trials=10):\n",
    "        \"\"\"Run architecture search.\"\"\"\n",
    "        print(f\"Running NAS with {n_trials} trials...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Sample architecture\n",
    "            config = self.sample_architecture()\n",
    "            model = self.build_model(config)\n",
    "            \n",
    "            # Evaluate\n",
    "            val_loss = self.evaluate_architecture(\n",
    "                model, train_loader, val_loader, config\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'config': config,\n",
    "                'val_loss': val_loss,\n",
    "                'num_params': model.count_parameters()\n",
    "            }\n",
    "            self.results.append(result)\n",
    "            \n",
    "            print(f\"Trial {trial+1}: Val Loss = {val_loss:.4f}, \"\n",
    "                  f\"Layers = {len(config['hidden_dims'])}, \"\n",
    "                  f\"Width = {config['hidden_dims'][0]}\")\n",
    "        \n",
    "        # Find best architecture\n",
    "        best_result = min(self.results, key=lambda x: x['val_loss'])\n",
    "        return best_result\n",
    "\n",
    "\n",
    "# Demonstrate NAS setup (we'll use dummy data for quick demonstration)\n",
    "print(\"Neural Architecture Search Setup:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "nas = SimpleNAS(input_dim=20)\n",
    "\n",
    "# Show sample architectures\n",
    "print(\"\\nSample architectures from search space:\")\n",
    "for i in range(3):\n",
    "    config = nas.sample_architecture()\n",
    "    print(f\"  Config {i+1}: {config['hidden_dims']}, \"\n",
    "          f\"dropout={config['dropout']}, act={config['activation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d459c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practical: Design Optimal Architecture for Factor Model\n",
    "\n",
    "Now we'll put everything together to design an optimal neural network architecture for a multi-factor trading model.\n",
    "\n",
    "### Factor Model Objectives\n",
    "1. Predict next-day returns direction\n",
    "2. Combine multiple feature types (price, volume, categorical)\n",
    "3. Handle multiple assets simultaneously\n",
    "4. Include European market considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for factor model\n",
    "def prepare_factor_model_data(features_dict, tickers_info):\n",
    "    \"\"\"\n",
    "    Prepare data for multi-asset factor model training.\n",
    "    \n",
    "    Combines features from all assets into a unified dataset.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Encode categorical features\n",
    "    all_sectors = list(set(info['sector'] for info in tickers_info.values()))\n",
    "    all_assets = list(set(info['asset_class'] for info in tickers_info.values()))\n",
    "    all_regions = list(set(info['region'] for info in tickers_info.values()))\n",
    "    \n",
    "    sector_map = {s: i for i, s in enumerate(all_sectors)}\n",
    "    asset_map = {a: i for i, a in enumerate(all_assets)}\n",
    "    region_map = {r: i for i, r in enumerate(all_regions)}\n",
    "    \n",
    "    for ticker, features in features_dict.items():\n",
    "        if ticker not in tickers_info:\n",
    "            continue\n",
    "            \n",
    "        # Get clean data (drop NaN)\n",
    "        clean_features = features.dropna()\n",
    "        \n",
    "        if len(clean_features) < 100:\n",
    "            continue\n",
    "        \n",
    "        # Separate numerical and categorical\n",
    "        numerical_cols = [col for col in clean_features.columns \n",
    "                         if col not in ['sector', 'asset_class', 'region', 'target']]\n",
    "        \n",
    "        for idx in clean_features.index:\n",
    "            row = clean_features.loc[idx]\n",
    "            \n",
    "            # Get numerical features\n",
    "            numerical = row[numerical_cols].values.astype(np.float32)\n",
    "            \n",
    "            # Get categorical indices\n",
    "            sector_idx = sector_map.get(row['sector'], 0)\n",
    "            asset_idx = asset_map.get(row['asset_class'], 0)\n",
    "            region_idx = region_map.get(row['region'], 0)\n",
    "            \n",
    "            # Get target\n",
    "            target = row['target']\n",
    "            \n",
    "            all_data.append({\n",
    "                'numerical': numerical,\n",
    "                'sector_idx': sector_idx,\n",
    "                'asset_idx': asset_idx,\n",
    "                'region_idx': region_idx,\n",
    "                'target': target,\n",
    "                'ticker': ticker,\n",
    "                'date': idx\n",
    "            })\n",
    "    \n",
    "    return all_data, sector_map, asset_map, region_map\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "print(\"Preparing factor model data...\")\n",
    "all_data, sector_map, asset_map, region_map = prepare_factor_model_data(\n",
    "    features_dict, tickers\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(all_data):,}\")\n",
    "print(f\"Sectors: {sector_map}\")\n",
    "print(f\"Asset classes: {asset_map}\")\n",
    "print(f\"Regions: {region_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca7808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for factor model training.\n",
    "    \n",
    "    Handles numerical and categorical features separately.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, scaler=None, fit_scaler=False):\n",
    "        self.data = data\n",
    "        \n",
    "        # Extract numerical features for scaling\n",
    "        numerical_features = np.array([d['numerical'] for d in data])\n",
    "        \n",
    "        # Handle scaling\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.numerical = self.scaler.fit_transform(numerical_features)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            self.numerical = self.scaler.transform(numerical_features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            self.numerical = numerical_features\n",
    "        \n",
    "        # Extract other data\n",
    "        self.sector_idx = np.array([d['sector_idx'] for d in data])\n",
    "        self.asset_idx = np.array([d['asset_idx'] for d in data])\n",
    "        self.region_idx = np.array([d['region_idx'] for d in data])\n",
    "        self.targets = np.array([d['target'] for d in data])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.numerical[idx]),\n",
    "            torch.LongTensor([self.sector_idx[idx]]),\n",
    "            torch.LongTensor([self.asset_idx[idx]]),\n",
    "            torch.LongTensor([self.region_idx[idx]]),\n",
    "            torch.FloatTensor([self.targets[idx]])\n",
    "        )\n",
    "\n",
    "\n",
    "# Time-series aware split (no look-ahead bias)\n",
    "# Sort data by date first\n",
    "all_data_sorted = sorted(all_data, key=lambda x: x['date'])\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test\n",
    "n = len(all_data_sorted)\n",
    "train_idx = int(0.7 * n)\n",
    "val_idx = int(0.85 * n)\n",
    "\n",
    "train_data = all_data_sorted[:train_idx]\n",
    "val_data = all_data_sorted[train_idx:val_idx]\n",
    "test_data = all_data_sorted[val_idx:]\n",
    "\n",
    "print(f\"Train samples: {len(train_data):,}\")\n",
    "print(f\"Validation samples: {len(val_data):,}\")\n",
    "print(f\"Test samples: {len(test_data):,}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FactorModelDataset(train_data, fit_scaler=True)\n",
    "val_dataset = FactorModelDataset(val_data, scaler=train_dataset.scaler)\n",
    "test_dataset = FactorModelDataset(test_data, scaler=train_dataset.scaler)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalFactorModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimal neural network architecture for factor model.\n",
    "    \n",
    "    Combines:\n",
    "    - Embedding layers for categorical features\n",
    "    - Residual connections for better gradient flow\n",
    "    - Multi-branch architecture for different feature types\n",
    "    - Attention mechanism for feature importance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_numerical, num_sectors, num_assets, num_regions,\n",
    "                 hidden_dim=128, embed_dim=8, num_res_blocks=3, dropout=0.3):\n",
    "        super(OptimalFactorModel, self).__init__()\n",
    "        \n",
    "        # ===== EMBEDDING LAYERS =====\n",
    "        self.sector_embedding = nn.Embedding(num_sectors, embed_dim)\n",
    "        self.asset_embedding = nn.Embedding(num_assets, embed_dim)\n",
    "        self.region_embedding = nn.Embedding(num_regions, embed_dim)\n",
    "        \n",
    "        # ===== INPUT PROJECTION =====\n",
    "        total_input_dim = num_numerical + 3 * embed_dim\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),  # GELU often works well in financial applications\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # ===== RESIDUAL BLOCKS =====\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dim, dropout) for _ in range(num_res_blocks)\n",
    "        ])\n",
    "        \n",
    "        # ===== FEATURE IMPORTANCE ATTENTION =====\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ===== OUTPUT LAYERS =====\n",
    "        self.output_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, numerical, sector_idx, asset_idx, region_idx):\n",
    "        # Get embeddings (squeeze extra dimension)\n",
    "        sector_emb = self.sector_embedding(sector_idx.squeeze(1))\n",
    "        asset_emb = self.asset_embedding(asset_idx.squeeze(1))\n",
    "        region_emb = self.region_embedding(region_idx.squeeze(1))\n",
    "        \n",
    "        # Concatenate all features\n",
    "        x = torch.cat([numerical, sector_emb, asset_emb, region_emb], dim=1)\n",
    "        \n",
    "        # Project to hidden space\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights = self.attention(x)\n",
    "        x = x * attention_weights\n",
    "        \n",
    "        # Generate output\n",
    "        return self.output_layers(x)\n",
    "\n",
    "\n",
    "# Get numerical feature dimension\n",
    "num_numerical = train_dataset.numerical.shape[1]\n",
    "\n",
    "# Create model\n",
    "model = OptimalFactorModel(\n",
    "    num_numerical=num_numerical,\n",
    "    num_sectors=len(sector_map),\n",
    "    num_assets=len(asset_map),\n",
    "    num_regions=len(region_map),\n",
    "    hidden_dim=128,\n",
    "    embed_dim=8,\n",
    "    num_res_blocks=3,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(\"Optimal Factor Model Architecture:\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea846f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_factor_model(model, train_loader, val_loader, num_epochs=50, \n",
    "                       lr=1e-3, patience=10):\n",
    "    \"\"\"\n",
    "    Train the factor model with best practices.\n",
    "    \n",
    "    Includes:\n",
    "    - Learning rate scheduling\n",
    "    - Early stopping\n",
    "    - Gradient clipping\n",
    "    - Training history tracking\n",
    "    \"\"\"\n",
    "    # Loss function with class weighting for imbalanced data\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Optimizer with weight decay (L2 regularization)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    # Early stopping\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # ===== TRAINING =====\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            numerical, sector, asset, region, target = [\n",
    "                b.to(device) for b in batch\n",
    "            ]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(numerical, sector, asset, region)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ===== VALIDATION =====\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                numerical, sector, asset, region, target = [\n",
    "                    b.to(device) for b in batch\n",
    "                ]\n",
    "                \n",
    "                output = model(numerical, sector, asset, region)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = (torch.sigmoid(output) > 0.5).float()\n",
    "                correct += (predictions == target).sum().item()\n",
    "                total += target.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}: Train Loss = {train_loss:.4f}, \"\n",
    "                  f\"Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = train_factor_model(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=50, lr=1e-3, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99440ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', \n",
    "             linewidth=2, color='green')\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', label='Random Baseline')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trading_strategy(model, test_loader, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the model as a trading strategy.\n",
    "    \n",
    "    Calculates:\n",
    "    - Accuracy metrics\n",
    "    - Trading performance (returns, Sharpe ratio)\n",
    "    - Comparison to buy-and-hold\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            numerical, sector, asset, region, target = [\n",
    "                b.to(device) for b in batch\n",
    "            ]\n",
    "            \n",
    "            output = model(numerical, sector, asset, region)\n",
    "            probs = torch.sigmoid(output)\n",
    "            predictions = (probs > 0.5).float()\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "            all_probs.extend(probs.cpu().numpy().flatten())\n",
    "    \n",
    "    # Classification metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    \n",
    "    print(\"Model Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nClassification Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(all_targets, all_predictions, \n",
    "                                target_names=['Down', 'Up']))\n",
    "    \n",
    "    # Trading performance\n",
    "    # Convert predictions to positions: 1 = long, -1 = short (based on probability)\n",
    "    positions = np.array([1 if p > 0.5 else -1 for p in all_probs])\n",
    "    \n",
    "    # Get actual returns from test data\n",
    "    actual_returns = []\n",
    "    for i, d in enumerate(test_data[:len(all_probs)]):\n",
    "        # Use next day return (target indicates direction)\n",
    "        ret = 0.01 if d['target'] == 1 else -0.01  # Simplified return assumption\n",
    "        actual_returns.append(ret)\n",
    "    \n",
    "    actual_returns = np.array(actual_returns)\n",
    "    \n",
    "    # Strategy returns\n",
    "    strategy_returns = positions * actual_returns\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    cumulative_return = (1 + strategy_returns).cumprod()[-1] - 1\n",
    "    sharpe_ratio = np.sqrt(252) * strategy_returns.mean() / (strategy_returns.std() + 1e-8)\n",
    "    \n",
    "    # Buy and hold baseline\n",
    "    bh_return = (1 + actual_returns).cumprod()[-1] - 1\n",
    "    bh_sharpe = np.sqrt(252) * actual_returns.mean() / (actual_returns.std() + 1e-8)\n",
    "    \n",
    "    print(\"\\nTrading Strategy Performance:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Strategy Cumulative Return: {cumulative_return*100:.2f}%\")\n",
    "    print(f\"Strategy Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"\\nBuy-and-Hold Cumulative Return: {bh_return*100:.2f}%\")\n",
    "    print(f\"Buy-and-Hold Sharpe Ratio: {bh_sharpe:.2f}\")\n",
    "    print(f\"\\nExcess Return vs B&H: {(cumulative_return - bh_return)*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'probabilities': all_probs,\n",
    "        'strategy_returns': strategy_returns,\n",
    "        'cumulative_return': cumulative_return,\n",
    "        'sharpe_ratio': sharpe_ratio\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_trading_strategy(trained_model, test_loader, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc35051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cumulative returns\n",
    "cumulative_strategy = (1 + results['strategy_returns']).cumprod()\n",
    "axes[0].plot(cumulative_strategy, label='Neural Network Strategy', linewidth=2)\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Starting Capital')\n",
    "axes[0].set_xlabel('Trade Number')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].set_title('Strategy Cumulative Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction probability distribution\n",
    "axes[1].hist(results['probabilities'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0.5, color='r', linestyle='--', label='Decision Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability (Up)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Prediction Probabilities')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a119b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### Architecture Design Principles for Trading\n",
    "\n",
    "1. **Start Simple, Scale Carefully**\n",
    "   - Begin with shallow networks (2-3 layers)\n",
    "   - Add complexity only when justified by validation performance\n",
    "   - More parameters = higher overfitting risk with limited data\n",
    "\n",
    "2. **Use Residual Connections for Deeper Networks**\n",
    "   - Skip connections help preserve weak financial signals\n",
    "   - Enable training of deeper architectures\n",
    "   - Provide implicit regularization\n",
    "\n",
    "3. **Embeddings for Categorical Features**\n",
    "   - Learn relationships between sectors/assets\n",
    "   - More efficient than one-hot encoding\n",
    "   - Enables transfer learning across assets\n",
    "\n",
    "4. **Multi-Input Architectures**\n",
    "   - Combine different data types (price, volume, sentiment)\n",
    "   - Each branch can specialize in its data modality\n",
    "   - Attention mechanisms can weight branch importance\n",
    "\n",
    "5. **AutoML Considerations**\n",
    "   - Use for systematic architecture exploration\n",
    "   - Document search process for compliance\n",
    "   - Prefer Bayesian optimization over grid search\n",
    "\n",
    "### European Market Considerations ðŸ‡ªðŸ‡º\n",
    "\n",
    "- Model decisions must be explainable (MiFID II)\n",
    "- Document architecture choices and rationale\n",
    "- Consider multi-currency handling in architecture\n",
    "- Account for different trading hours and holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5161b57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exercises\n",
    "\n",
    "### Exercise 1: Architecture Comparison\n",
    "Implement and compare these architectures on the factor model:\n",
    "- Wide-shallow (2 layers, 256 neurons)\n",
    "- Deep-narrow (6 layers, 64 neurons)\n",
    "- Pyramid (256 â†’ 128 â†’ 64 â†’ 32)\n",
    "\n",
    "### Exercise 2: Custom Embeddings\n",
    "Add additional categorical features:\n",
    "- Market cap bucket (Small, Mid, Large)\n",
    "- Volatility regime (Low, Medium, High)\n",
    "- Day of week\n",
    "\n",
    "### Exercise 3: Attention Analysis\n",
    "Extract and visualize the attention weights from the model:\n",
    "- Which features does the model focus on?\n",
    "- Does attention vary by sector or region?\n",
    "\n",
    "### Exercise 4: European-Specific Model\n",
    "Build a model specifically for European equities:\n",
    "- Add currency embedding\n",
    "- Include European trading hours features\n",
    "- Account for European regulatory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4818f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise space - try implementing the exercises above!\n",
    "\n",
    "# Example: Exercise 1 starter code\n",
    "architectures_to_compare = {\n",
    "    'wide_shallow': [256, 256],\n",
    "    'deep_narrow': [64, 64, 64, 64, 64, 64],\n",
    "    'pyramid': [256, 128, 64, 32]\n",
    "}\n",
    "\n",
    "# Your code here...\n",
    "print(\"Ready for exercises!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4a387",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **He et al. (2015)** - \"Deep Residual Learning for Image Recognition\" - Original ResNet paper\n",
    "2. **Vaswani et al. (2017)** - \"Attention Is All You Need\" - Attention mechanisms\n",
    "3. **LÃ³pez de Prado (2018)** - \"Advances in Financial Machine Learning\" - ML in finance\n",
    "4. **Guo & Berkhahn (2016)** - \"Entity Embeddings of Categorical Variables\" - Embedding techniques\n",
    "5. **Elsken et al. (2019)** - \"Neural Architecture Search: A Survey\" - NAS overview\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** Day 6 - Training Best Practices for Neural Networks in Trading"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
