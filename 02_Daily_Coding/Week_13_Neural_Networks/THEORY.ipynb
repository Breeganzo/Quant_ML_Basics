{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864e7e86",
   "metadata": {},
   "source": [
    "# ðŸ“š Week 13: Neural Networks - MLP, PyTorch & Keras\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "| Day | Topic | Key Concepts |\n",
    "|-----|-------|--------------|\n",
    "| 1 | NN Fundamentals | Neurons, activation functions, forward propagation |\n",
    "| 2 | Backpropagation | Chain rule, gradient descent, learning rate |\n",
    "| 3 | MLP Architecture | Hidden layers, universal approximation |\n",
    "| 4 | PyTorch Basics | Tensors, autograd, nn.Module |\n",
    "| 5 | Keras/TensorFlow | Sequential API, Functional API |\n",
    "| 6 | Training Practices | Batch size, epochs, callbacks, regularization |\n",
    "| 7 | Interview Review | Common questions and answers |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS & SETUP\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    HAS_TF = True\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"ðŸ“¥ PyTorch version:\", torch.__version__)\n",
    "if HAS_TF:\n",
    "    print(\"ðŸ“¥ TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aad958",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 1: Neural Network Fundamentals\n",
    "\n",
    "## The Artificial Neuron\n",
    "\n",
    "A neuron computes:\n",
    "$$y = \\sigma\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = \\sigma(w^T x + b)$$\n",
    "\n",
    "Where:\n",
    "- $x$: Input vector\n",
    "- $w$: Weight vector\n",
    "- $b$: Bias\n",
    "- $\\sigma$: Activation function\n",
    "\n",
    "## Common Activation Functions\n",
    "\n",
    "| Function | Formula | Range | Use Case |\n",
    "|----------|---------|-------|----------|\n",
    "| Sigmoid | $\\sigma(z) = \\frac{1}{1+e^{-z}}$ | (0, 1) | Output layer (binary) |\n",
    "| Tanh | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | Hidden layers |\n",
    "| ReLU | $\\max(0, z)$ | [0, âˆž) | Hidden layers (most common) |\n",
    "| Leaky ReLU | $\\max(\\alpha z, z)$ | (-âˆž, âˆž) | Avoids dead neurons |\n",
    "\n",
    "## Forward Propagation\n",
    "\n",
    "For a network with L layers:\n",
    "$$a^{[l]} = \\sigma(W^{[l]} a^{[l-1]} + b^{[l]})$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 1: ACTIVATION FUNCTIONS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "z = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Activation functions\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "tanh = np.tanh(z)\n",
    "relu = np.maximum(0, z)\n",
    "leaky_relu = np.where(z > 0, z, 0.1 * z)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0, 0].plot(z, sigmoid, 'b-', linewidth=2)\n",
    "axes[0, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_title('Sigmoid: Ïƒ(z) = 1/(1+e^-z)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel('Ïƒ(z)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(z, tanh, 'g-', linewidth=2)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_title('Tanh: (e^z - e^-z)/(e^z + e^-z)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('tanh(z)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(z, relu, 'orange', linewidth=2)\n",
    "axes[1, 0].set_title('ReLU: max(0, z)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel('ReLU(z)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(z, leaky_relu, 'purple', linewidth=2)\n",
    "axes[1, 1].set_title('Leaky ReLU: max(0.1z, z)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel('LeakyReLU(z)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e8c58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 2: Backpropagation\n",
    "\n",
    "## Chain Rule for Gradients\n",
    "\n",
    "For a loss function $L$ with respect to weights $W^{[l]}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\partial L}{\\partial a^{[L]}} \\cdot \\frac{\\partial a^{[L]}}{\\partial a^{[L-1]}} \\cdots \\frac{\\partial a^{[l]}}{\\partial W^{[l]}}$$\n",
    "\n",
    "## Gradient Descent Update\n",
    "\n",
    "$$W := W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "## Vanishing/Exploding Gradients\n",
    "\n",
    "- **Vanishing:** Gradients become too small (sigmoid/tanh in deep networks)\n",
    "- **Exploding:** Gradients become too large (improper initialization)\n",
    "\n",
    "**Solutions:**\n",
    "- ReLU activation\n",
    "- Proper weight initialization (Xavier, He)\n",
    "- Batch normalization\n",
    "- Gradient clipping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72026fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 2: SIMPLE BACKPROPAGATION EXAMPLE\n",
    "# ============================================================\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"Simple 2-layer neural network from scratch.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.z2  # Linear output for regression\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.01):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz2 = self.a2 - y\n",
    "        dW2 = (1/m) * self.a1.T @ dz2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradient\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.relu_derivative(self.z1)\n",
    "        dW1 = (1/m) * X.T @ dz1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        \n",
    "        return np.mean((self.a2 - y)**2)  # MSE loss\n",
    "\n",
    "# Demo: Learn XOR\n",
    "np.random.seed(42)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = SimpleNN(2, 4, 1)\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    nn.forward(X)\n",
    "    loss = nn.backward(X, y, learning_rate=0.5)\n",
    "    losses.append(loss)\n",
    "\n",
    "print(\"ðŸ“Š XOR Learning (From Scratch)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Final Loss: {losses[-1]:.6f}\")\n",
    "print(f\"\\nPredictions:\")\n",
    "for xi, yi in zip(X, nn.forward(X)):\n",
    "    print(f\"   {xi} â†’ {yi[0]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Backpropagation Training Loss', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef449832",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 3: MLP Architecture\n",
    "\n",
    "## Multi-Layer Perceptron\n",
    "\n",
    "$$\\text{MLP}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$\n",
    "\n",
    "- **Input Layer:** Features\n",
    "- **Hidden Layers:** Non-linear transformations\n",
    "- **Output Layer:** Predictions\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "\n",
    "A feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\\mathbb{R}^n$.\n",
    "\n",
    "## Architecture Design\n",
    "\n",
    "| Problem | Output Activation | Loss Function |\n",
    "|---------|-------------------|---------------|\n",
    "| Regression | Linear | MSE |\n",
    "| Binary Classification | Sigmoid | Binary Cross-Entropy |\n",
    "| Multi-class | Softmax | Categorical Cross-Entropy |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 3: MLP ARCHITECTURE VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def draw_neural_net(ax, layer_sizes):\n",
    "    \"\"\"Draw a neural network diagram.\"\"\"\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = 1.0 / (max(layer_sizes) + 1)\n",
    "    h_spacing = 1.0 / (n_layers + 1)\n",
    "    \n",
    "    # Draw neurons\n",
    "    for i, n_neurons in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing * (n_neurons - 1) / 2 + 0.5\n",
    "        for j in range(n_neurons):\n",
    "            circle = plt.Circle((h_spacing * (i + 1), layer_top - j * v_spacing),\n",
    "                               v_spacing / 4, color='steelblue', ec='black', zorder=4)\n",
    "            ax.add_artist(circle)\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if i < n_layers - 1:\n",
    "                next_layer_top = v_spacing * (layer_sizes[i+1] - 1) / 2 + 0.5\n",
    "                for k in range(layer_sizes[i+1]):\n",
    "                    line = plt.Line2D([h_spacing * (i + 1), h_spacing * (i + 2)],\n",
    "                                      [layer_top - j * v_spacing, next_layer_top - k * v_spacing],\n",
    "                                      c='gray', alpha=0.3)\n",
    "                    ax.add_artist(line)\n",
    "    \n",
    "    # Labels\n",
    "    labels = ['Input\\n(Features)', 'Hidden 1', 'Hidden 2', 'Output\\n(Prediction)']\n",
    "    for i, label in enumerate(labels[:len(layer_sizes)]):\n",
    "        ax.text(h_spacing * (i + 1), 0.05, label, ha='center', fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "draw_neural_net(ax, [4, 8, 6, 1])\n",
    "ax.set_title('MLP Architecture: 4 â†’ 8 â†’ 6 â†’ 1', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718b8c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 4: PyTorch Basics\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "1. **Tensors:** Multi-dimensional arrays with GPU support\n",
    "2. **Autograd:** Automatic differentiation\n",
    "3. **nn.Module:** Base class for neural networks\n",
    "4. **Optimizers:** SGD, Adam, etc.\n",
    "\n",
    "## PyTorch Workflow\n",
    "\n",
    "```python\n",
    "# 1. Define model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "\n",
    "# 2. Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcecdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 4: PYTORCH NEURAL NETWORK\n",
    "# ============================================================\n",
    "\n",
    "# Get financial data\n",
    "ticker = 'SPY'\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=756)\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "returns = data['Close'].pct_change().dropna()\n",
    "\n",
    "# Create features\n",
    "df = pd.DataFrame(index=returns.index)\n",
    "df['ret'] = returns\n",
    "for lag in [1, 2, 3, 5, 10]:\n",
    "    df[f'lag_{lag}'] = returns.shift(lag)\n",
    "df['vol_10'] = returns.rolling(10).std()\n",
    "df['target'] = returns.shift(-1)\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare PyTorch data\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# Train/test split\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Standardize\n",
    "X_mean, X_std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.FloatTensor(X_train)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_size = h\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Train model\n",
    "torch.manual_seed(42)\n",
    "model = MLP(X_train.shape[1], [32, 16], 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_t)\n",
    "    loss = criterion(output, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_t)\n",
    "        test_loss = criterion(test_output, y_test_t)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "print(\"ðŸ“Š PyTorch MLP Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.6f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Train Loss', alpha=0.8)\n",
    "ax.plot(test_losses, label='Test Loss', alpha=0.8)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('PyTorch MLP Training', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb84e2d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 5: Keras/TensorFlow Basics\n",
    "\n",
    "## Keras API Styles\n",
    "\n",
    "**1. Sequential API (Simple):**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "**2. Functional API (Complex):**\n",
    "```python\n",
    "inputs = keras.Input(shape=(n_features,))\n",
    "x = keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 5: KERAS NEURAL NETWORK\n",
    "# ============================================================\n",
    "\n",
    "if HAS_TF:\n",
    "    # Build Keras model\n",
    "    keras_model = keras.Sequential([\n",
    "        keras.layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(16, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    keras_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train\n",
    "    history = keras_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ“Š Keras MLP Results\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Final Train Loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"Final Val Loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(history.history['loss'], label='Train Loss')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE Loss')\n",
    "    ax.set_title('Keras MLP Training', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ TensorFlow/Keras not installed. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2d58e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 6: Training Best Practices\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "| Parameter | Typical Values | Effect |\n",
    "|-----------|----------------|--------|\n",
    "| Learning Rate | 0.001, 0.01 | Speed vs stability |\n",
    "| Batch Size | 32, 64, 128 | Noise vs speed |\n",
    "| Hidden Units | 32, 64, 128 | Capacity |\n",
    "| Dropout | 0.1-0.5 | Regularization |\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "1. **L2 Regularization:** Weight decay\n",
    "2. **Dropout:** Random neuron deactivation\n",
    "3. **Early Stopping:** Stop when validation loss increases\n",
    "4. **Batch Normalization:** Normalize layer inputs\n",
    "\n",
    "## Callbacks\n",
    "\n",
    "- **EarlyStopping:** Prevent overfitting\n",
    "- **ReduceLROnPlateau:** Adaptive learning rate\n",
    "- **ModelCheckpoint:** Save best model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89c857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DAY 6: TRAINING WITH CALLBACKS (KERAS)\n",
    "# ============================================================\n",
    "\n",
    "if HAS_TF:\n",
    "    # Model with regularization\n",
    "    model_reg = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', \n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                          input_shape=(X_train.shape[1],)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32, activation='relu',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_reg.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                      loss='mse')\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    "    \n",
    "    history_reg = model_reg.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=200,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ“Š Regularized Model with Callbacks\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Stopped at epoch: {len(history_reg.history['loss'])}\")\n",
    "    print(f\"Best Val Loss: {min(history_reg.history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4955b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ“Š Day 7: Interview Review\n",
    "\n",
    "## Common Questions\n",
    "\n",
    "**Q1: What is the vanishing gradient problem?**\n",
    "> Gradients become very small in deep networks with sigmoid/tanh, making early layers learn slowly. Solutions: ReLU, batch normalization, residual connections.\n",
    "\n",
    "**Q2: Explain backpropagation.**\n",
    "> Algorithm to compute gradients using chain rule, propagating errors backward from output to input, updating weights via gradient descent.\n",
    "\n",
    "**Q3: When to use dropout vs L2 regularization?**\n",
    "> - **Dropout:** Better for large networks, prevents co-adaptation\n",
    "> - **L2:** Weight decay, smoother solution, always applicable\n",
    "\n",
    "**Q4: How to choose learning rate?**\n",
    "> - Start with 0.001 for Adam\n",
    "> - Use learning rate schedulers\n",
    "> - Learning rate finder technique\n",
    "\n",
    "**Q5: Batch size tradeoffs?**\n",
    "> - **Small:** More noise, better generalization, slower\n",
    "> - **Large:** Faster, may overfit, needs higher LR\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|-----------|\n",
    "| Activation | ReLU for hidden, sigmoid/softmax for output |\n",
    "| Backprop | Chain rule + gradient descent |\n",
    "| Regularization | Dropout + L2 + Early Stopping |\n",
    "| PyTorch | Research, flexibility |\n",
    "| Keras | Production, simplicity |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
