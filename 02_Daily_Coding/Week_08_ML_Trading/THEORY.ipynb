{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb39e87",
   "metadata": {},
   "source": [
    "# Week 8: Machine Learning for Trading\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Decision Trees\n",
    "2. Random Forests\n",
    "3. Gradient Boosting (XGBoost, LightGBM)\n",
    "4. Feature Engineering for Alpha\n",
    "5. Walk-Forward Validation\n",
    "6. Ensemble Methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d6687",
   "metadata": {},
   "source": [
    "## 1. Decision Trees\n",
    "\n",
    "### How Decision Trees Work\n",
    "\n",
    "A decision tree recursively splits data to minimize impurity at each node.\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "**For Regression (MSE):**\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n",
    "\n",
    "**For Classification (Gini Impurity):**\n",
    "$$\\text{Gini} = 1 - \\sum_{k=1}^{K} p_k^2$$\n",
    "\n",
    "Where $p_k$ is the proportion of class $k$ in the node.\n",
    "\n",
    "**For Classification (Entropy):**\n",
    "$$\\text{Entropy} = -\\sum_{k=1}^{K} p_k \\log_2(p_k)$$\n",
    "\n",
    "### Information Gain\n",
    "\n",
    "$$\\text{IG} = \\text{Impurity}_{parent} - \\sum_{j} \\frac{n_j}{n} \\text{Impurity}_{child_j}$$\n",
    "\n",
    "The split that maximizes information gain is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff3358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree for Direction Prediction\n",
      "==================================================\n",
      "\n",
      "Training accuracy: 72.50%\n",
      "Test accuracy: 69.00%\n",
      "\n",
      "Feature Importances:\n",
      "  momentum_5d: 0.327\n",
      "  momentum_20d: 0.650\n",
      "  volatility: 0.000\n",
      "  volume_ratio: 0.024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create trading features\n",
    "n_samples = 1000\n",
    "momentum_5d = np.random.normal(0, 0.02, n_samples)\n",
    "momentum_20d = np.random.normal(0, 0.05, n_samples)\n",
    "volatility = np.abs(np.random.normal(0.01, 0.005, n_samples))\n",
    "volume_ratio = np.random.uniform(0.5, 2.0, n_samples)\n",
    "\n",
    "# Target: next day return (with some signal)\n",
    "next_return = (0.3 * momentum_5d + 0.2 * momentum_20d - 0.1 * volatility + \n",
    "               np.random.normal(0, 0.015, n_samples))\n",
    "\n",
    "# Binary target for classification\n",
    "direction = (next_return > 0).astype(int)\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'momentum_5d': momentum_5d,\n",
    "    'momentum_20d': momentum_20d,\n",
    "    'volatility': volatility,\n",
    "    'volume_ratio': volume_ratio\n",
    "})\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, direction, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree for Direction Prediction\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nTraining accuracy: {tree.score(X_train, y_train):.2%}\")\n",
    "print(f\"Test accuracy: {tree.score(X_test, y_test):.2%}\")\n",
    "print(f\"\\nFeature Importances:\")\n",
    "for name, imp in zip(X.columns, tree.feature_importances_):\n",
    "    print(f\"  {name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b15d6",
   "metadata": {},
   "source": [
    "### Decision Tree Limitations\n",
    "\n",
    "1. **Overfitting**: Deep trees memorize training data\n",
    "2. **High variance**: Small data changes → completely different tree\n",
    "3. **Greedy**: Local optimal splits, not global optimum\n",
    "\n",
    "**Solution**: Ensemble methods (Random Forests, Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d96d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Random Forests\n",
    "\n",
    "### Bagging + Feature Randomization\n",
    "\n",
    "Random Forest combines two ideas:\n",
    "\n",
    "**1. Bootstrap Aggregating (Bagging)**:\n",
    "- Train each tree on a random sample (with replacement)\n",
    "- Reduces variance through averaging\n",
    "\n",
    "**2. Feature Randomization**:\n",
    "- Each split considers only $m$ random features\n",
    "- Typically $m = \\sqrt{p}$ for classification, $m = p/3$ for regression\n",
    "- Decorrelates trees\n",
    "\n",
    "### Prediction\n",
    "\n",
    "**Classification**: Majority vote\n",
    "$$\\hat{y} = \\text{mode}\\{\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_B\\}$$\n",
    "\n",
    "**Regression**: Average\n",
    "$$\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{y}_b$$\n",
    "\n",
    "### Out-of-Bag (OOB) Error\n",
    "\n",
    "Each tree doesn't see ~37% of data (out-of-bag samples).\n",
    "Use these for validation without a separate test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58e8d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results\n",
      "==================================================\n",
      "\n",
      "Number of trees: 100\n",
      "Max depth: 5\n",
      "\n",
      "Training accuracy: 78.12%\n",
      "OOB accuracy: 70.50%\n",
      "Test accuracy: 67.00%\n",
      "\n",
      "Feature Importances (averaged over all trees):\n",
      "  momentum_20d   : 0.530 ███████████████\n",
      "  momentum_5d    : 0.272 ████████\n",
      "  volume_ratio   : 0.100 ██\n",
      "  volatility     : 0.099 ██\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    max_features='sqrt',\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nNumber of trees: {rf.n_estimators}\")\n",
    "print(f\"Max depth: {rf.max_depth}\")\n",
    "print(f\"\\nTraining accuracy: {rf.score(X_train, y_train):.2%}\")\n",
    "print(f\"OOB accuracy: {rf.oob_score_:.2%}\")\n",
    "print(f\"Test accuracy: {rf.score(X_test, y_test):.2%}\")\n",
    "\n",
    "print(f\"\\nFeature Importances (averaged over all trees):\")\n",
    "for name, imp in sorted(zip(X.columns, rf.feature_importances_), key=lambda x: -x[1]):\n",
    "    bars = \"█\" * int(imp * 30)\n",
    "    print(f\"  {name:15s}: {imp:.3f} {bars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47076b",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "| Parameter | Effect | Typical Range |\n",
    "|-----------|--------|---------------|\n",
    "| n_estimators | More trees = better, diminishing returns | 100-500 |\n",
    "| max_depth | Deeper = more complex, risk of overfit | 3-10 |\n",
    "| max_features | Lower = more diverse trees | sqrt(p), p/3 |\n",
    "| min_samples_leaf | Higher = more regularization | 10-100 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304927c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Gradient Boosting (XGBoost, LightGBM)\n",
    "\n",
    "### Boosting vs Bagging\n",
    "\n",
    "| Bagging (RF) | Boosting |\n",
    "|--------------|----------|\n",
    "| Parallel trees | Sequential trees |\n",
    "| Reduces variance | Reduces bias |\n",
    "| Equal weights | Weighted by error |\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "1. Initialize with constant: $F_0(x) = \\bar{y}$\n",
    "2. For $m = 1$ to $M$:\n",
    "   - Compute residuals: $r_i = y_i - F_{m-1}(x_i)$\n",
    "   - Fit tree $h_m(x)$ to residuals\n",
    "   - Update: $F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "**Regression (MSE)**:\n",
    "$$L = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Classification (Log Loss)**:\n",
    "$$L = -\\sum_{i=1}^{n}[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)]$$\n",
    "\n",
    "### XGBoost Objective\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i) + \\sum_{k=1}^{K}\\Omega(f_k)$$\n",
    "\n",
    "Where regularization term:\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2$$\n",
    "\n",
    "- $T$ = number of leaves\n",
    "- $w_j$ = leaf weights\n",
    "- $\\gamma$ = complexity penalty\n",
    "- $\\lambda$ = L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbc4297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Results\n",
      "==================================================\n",
      "\n",
      "Learning rate (η): 0.1\n",
      "Number of stages: 100\n",
      "\n",
      "Training accuracy: 84.88%\n",
      "Test accuracy: 67.00%\n",
      "\n",
      "==================================================\n",
      "Model Comparison (Test Accuracy):\n",
      "  Decision Tree:      69.00%\n",
      "  Random Forest:      67.00%\n",
      "  Gradient Boosting:  67.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting (sklearn version)\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Gradient Boosting Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nLearning rate (η): {gb.learning_rate}\")\n",
    "print(f\"Number of stages: {gb.n_estimators}\")\n",
    "print(f\"\\nTraining accuracy: {gb.score(X_train, y_train):.2%}\")\n",
    "print(f\"Test accuracy: {gb.score(X_test, y_test):.2%}\")\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison (Test Accuracy):\")\n",
    "print(f\"  Decision Tree:      {tree.score(X_test, y_test):.2%}\")\n",
    "print(f\"  Random Forest:      {rf.score(X_test, y_test):.2%}\")\n",
    "print(f\"  Gradient Boosting:  {gb.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472b897",
   "metadata": {},
   "source": [
    "### XGBoost Key Parameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| learning_rate (η) | Shrinkage per tree | 0.01-0.3 |\n",
    "| n_estimators | Number of trees | 100-1000 |\n",
    "| max_depth | Tree depth | 3-8 |\n",
    "| subsample | Row sampling | 0.7-1.0 |\n",
    "| colsample_bytree | Feature sampling | 0.7-1.0 |\n",
    "| reg_lambda | L2 regularization | 0-10 |\n",
    "| reg_alpha | L1 regularization | 0-10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e73427",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Engineering for Alpha\n",
    "\n",
    "### Categories of Features\n",
    "\n",
    "**1. Price-Based**\n",
    "- Returns (1d, 5d, 20d, 60d)\n",
    "- Moving averages (SMA, EMA)\n",
    "- Price relative to MA\n",
    "\n",
    "**2. Technical Indicators**\n",
    "- RSI, MACD, Bollinger Bands\n",
    "- ATR (volatility)\n",
    "- Volume indicators (OBV)\n",
    "\n",
    "**3. Cross-Sectional**\n",
    "- Sector-relative momentum\n",
    "- Rank within universe\n",
    "- Z-score vs peers\n",
    "\n",
    "**4. Fundamental (if available)**\n",
    "- P/E, P/B ratios\n",
    "- Earnings surprise\n",
    "- Analyst revisions\n",
    "\n",
    "### Feature Engineering Best Practices\n",
    "\n",
    "1. **Normalize/Standardize**: Z-score or rank\n",
    "2. **Handle outliers**: Winsorize at 1st/99th percentile\n",
    "3. **Lag appropriately**: No look-ahead bias!\n",
    "4. **Cross-sectional**: Compare within universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf19874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading Features Example\n",
      "==================================================\n",
      "    ret_1d  ret_5d  ret_20d   sma_20   sma_50  price_to_sma20  volatility_20d  \\\n",
      "50 -0.0259 -0.0210  -0.0739  90.4739  96.3760         -0.0479          0.0150   \n",
      "51  0.0054 -0.0055  -0.0609  90.1931  96.0922         -0.0398          0.0151   \n",
      "52 -0.0053 -0.0044  -0.0915  89.7590  95.8025         -0.0402          0.0132   \n",
      "53 -0.0097 -0.0298  -0.1006  89.2820  95.4756         -0.0444          0.0132   \n",
      "54  0.0097 -0.0260  -0.0777  88.9190  95.1177         -0.0312          0.0134   \n",
      "55  0.0160  0.0159  -0.0749  88.5649  94.7936         -0.0118          0.0136   \n",
      "56  0.0145  0.0252  -0.0445  88.3583  94.5010          0.0048          0.0138   \n",
      "57 -0.0121  0.0181  -0.0594  88.0812  94.1370         -0.0042          0.0139   \n",
      "58 -0.0041  0.0238  -0.0354  87.9207  93.7402         -0.0065          0.0125   \n",
      "59  0.0055  0.0195  -0.0110  87.8721  93.3671         -0.0005          0.0118   \n",
      "\n",
      "        rsi  \n",
      "50  26.6530  \n",
      "51  27.2763  \n",
      "52  32.2030  \n",
      "53  34.8084  \n",
      "54  37.6318  \n",
      "55  39.3712  \n",
      "56  43.8777  \n",
      "57  40.9219  \n",
      "58  40.9211  \n",
      "59  49.3573  \n"
     ]
    }
   ],
   "source": [
    "def create_trading_features(prices, volume=None):\n",
    "    \"\"\"\n",
    "    Create common trading features from price data.\n",
    "    All features are lagged to avoid look-ahead bias.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Returns at different horizons\n",
    "    df['ret_1d'] = prices.pct_change(1).shift(1)\n",
    "    df['ret_5d'] = prices.pct_change(5).shift(1)\n",
    "    df['ret_20d'] = prices.pct_change(20).shift(1)\n",
    "    \n",
    "    # Moving averages\n",
    "    df['sma_20'] = prices.rolling(20).mean().shift(1)\n",
    "    df['sma_50'] = prices.rolling(50).mean().shift(1)\n",
    "    \n",
    "    # Price relative to MA\n",
    "    df['price_to_sma20'] = (prices.shift(1) / df['sma_20']) - 1\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility_20d'] = prices.pct_change().rolling(20).std().shift(1)\n",
    "    \n",
    "    # RSI\n",
    "    delta = prices.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / loss\n",
    "    df['rsi'] = (100 - 100 / (1 + rs)).shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example\n",
    "np.random.seed(42)\n",
    "prices = pd.Series(100 * np.cumprod(1 + np.random.normal(0.0005, 0.015, 500)))\n",
    "features = create_trading_features(prices)\n",
    "\n",
    "print(\"Trading Features Example\")\n",
    "print(\"=\"*50)\n",
    "print(features.dropna().head(10).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2f127",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Walk-Forward Validation\n",
    "\n",
    "### Why Not Standard Cross-Validation?\n",
    "\n",
    "**Problem**: Standard k-fold mixes past and future data → look-ahead bias!\n",
    "\n",
    "**Solution**: Walk-forward (rolling or expanding window)\n",
    "\n",
    "### Walk-Forward Scheme\n",
    "\n",
    "```\n",
    "Expanding Window:\n",
    "Period 1: [===Train===][Test]\n",
    "Period 2: [====Train====][Test]\n",
    "Period 3: [=====Train=====][Test]\n",
    "\n",
    "Rolling Window:\n",
    "Period 1: [===Train===][Test]\n",
    "Period 2:    [===Train===][Test]\n",
    "Period 3:       [===Train===][Test]\n",
    "```\n",
    "\n",
    "### Purging and Embargo\n",
    "\n",
    "**Purging**: Remove samples around test set to avoid data leakage\n",
    "\n",
    "**Embargo**: Gap between train and test (e.g., 5 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdae37c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walk-Forward Validation Results\n",
      "============================================================\n",
      " fold  train_size  test_start  test_end  train_acc  test_acc\n",
      "    1         500         500       599   0.812000      0.75\n",
      "    2         600         600       699   0.796667      0.66\n",
      "    3         700         700       799   0.790000      0.71\n",
      "    4         800         800       899   0.781250      0.68\n",
      "    5         900         900       999   0.767778      0.67\n",
      "\n",
      "Mean Test Accuracy: 69.40%\n",
      "Std Test Accuracy: 3.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def walk_forward_validation(X, y, model, n_splits=5, test_size=50):\n",
    "    \"\"\"\n",
    "    Perform walk-forward validation for time series.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "        X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Clone and fit model\n",
    "        from sklearn.base import clone\n",
    "        m = clone(model)\n",
    "        m.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Score\n",
    "        train_score = m.score(X_tr, y_tr)\n",
    "        test_score = m.score(X_te, y_te)\n",
    "        \n",
    "        results.append({\n",
    "            'fold': fold,\n",
    "            'train_size': len(train_idx),\n",
    "            'test_start': test_idx[0],\n",
    "            'test_end': test_idx[-1],\n",
    "            'train_acc': train_score,\n",
    "            'test_acc': test_score\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run walk-forward validation\n",
    "y_series = pd.Series(direction)\n",
    "wf_results = walk_forward_validation(X, y_series, rf, n_splits=5, test_size=100)\n",
    "\n",
    "print(\"Walk-Forward Validation Results\")\n",
    "print(\"=\"*60)\n",
    "print(wf_results.to_string(index=False))\n",
    "print(f\"\\nMean Test Accuracy: {wf_results['test_acc'].mean():.2%}\")\n",
    "print(f\"Std Test Accuracy: {wf_results['test_acc'].std():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c9617",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Ensemble Methods\n",
    "\n",
    "### Types of Ensembles\n",
    "\n",
    "**1. Averaging**: Simple mean of predictions\n",
    "$$\\hat{y} = \\frac{1}{M}\\sum_{m=1}^{M}\\hat{y}_m$$\n",
    "\n",
    "**2. Weighted Averaging**: \n",
    "$$\\hat{y} = \\sum_{m=1}^{M}w_m\\hat{y}_m, \\quad \\sum w_m = 1$$\n",
    "\n",
    "**3. Stacking**: Train meta-model on base model predictions\n",
    "\n",
    "### Why Ensembles Work\n",
    "\n",
    "**Variance Reduction**: If models have uncorrelated errors:\n",
    "\n",
    "$$\\text{Var}\\left(\\frac{1}{M}\\sum_{m=1}^{M}\\hat{y}_m\\right) = \\frac{\\sigma^2}{M}$$\n",
    "\n",
    "Variance decreases with more models!\n",
    "\n",
    "### Diversity is Key\n",
    "\n",
    "Ensemble models should be **diverse**:\n",
    "- Different algorithms (RF, XGBoost, Linear)\n",
    "- Different features\n",
    "- Different time windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "749df506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Comparison\n",
      "==================================================\n",
      "\n",
      "Model                     |  Train Acc |   Test Acc\n",
      "--------------------------------------------------\n",
      "rf                        |     75.25% |     69.00%\n",
      "gb                        |     78.62% |     68.50%\n",
      "lr                        |     66.38% |     63.50%\n",
      "--------------------------------------------------\n",
      "ENSEMBLE                  |     77.12% |     68.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create diverse ensemble\n",
    "model1 = RandomForestClassifier(n_estimators=50, max_depth=4, random_state=42)\n",
    "model2 = GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "model3 = LogisticRegression(random_state=42)\n",
    "\n",
    "# Voting ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', model1),\n",
    "        ('gb', model2),\n",
    "        ('lr', model3)\n",
    "    ],\n",
    "    voting='soft'  # Use predicted probabilities\n",
    ")\n",
    "\n",
    "# Fit all models\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "print(\"Ensemble Model Comparison\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'Model':<25} | {'Train Acc':>10} | {'Test Acc':>10}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for name, model in ensemble.named_estimators_.items():\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"{name:<25} | {train_acc:>10.2%} | {test_acc:>10.2%}\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(f\"{'ENSEMBLE':<25} | {ensemble.score(X_train, y_train):>10.2%} | {ensemble.score(X_test, y_test):>10.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdb5fd",
   "metadata": {},
   "source": [
    "### Stacking Example\n",
    "\n",
    "**Level 1**: Base models make predictions\n",
    "**Level 2**: Meta-model learns to combine them\n",
    "\n",
    "Use out-of-fold predictions to avoid overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b001ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking Ensemble\n",
      "==================================================\n",
      "Base models: Random Forest, Gradient Boosting\n",
      "Meta-model: Logistic Regression\n",
      "\n",
      "Test Accuracy: 67.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Stacking ensemble\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, max_depth=4, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nStacking Ensemble\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Base models: Random Forest, Gradient Boosting\")\n",
    "print(f\"Meta-model: Logistic Regression\")\n",
    "print(f\"\\nTest Accuracy: {stacking.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72810b30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Week 8 Key Concepts\n",
    "\n",
    "| Method | Key Idea | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| Decision Tree | Recursive splitting | Interpretable | Overfits |\n",
    "| Random Forest | Bagging + feature randomization | Robust, parallel | Memory intensive |\n",
    "| Gradient Boosting | Sequential error correction | Often best accuracy | Slower, overfits |\n",
    "| Ensemble | Combine diverse models | Reduces variance | Complexity |\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "| Concept | Formula |\n",
    "|---------|--------|\n",
    "| Gini Impurity | $G = 1 - \\sum p_k^2$ |\n",
    "| Information Gain | $IG = H_{parent} - \\sum \\frac{n_j}{n} H_{child_j}$ |\n",
    "| Boosting Update | $F_m = F_{m-1} + \\eta \\cdot h_m$ |\n",
    "| Ensemble Variance | $Var(\\bar{y}) = \\sigma^2/M$ |\n",
    "\n",
    "### Trading-Specific Tips\n",
    "\n",
    "1. **Always use walk-forward validation** (never random k-fold)\n",
    "2. **Lag all features** to avoid look-ahead bias\n",
    "3. **Feature importance** helps with interpretability\n",
    "4. **Ensemble diverse models** for robustness\n",
    "5. **Tune for stability**, not just accuracy\n",
    "\n",
    "---\n",
    "\n",
    "*Next Week: Deep Learning Fundamentals*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
