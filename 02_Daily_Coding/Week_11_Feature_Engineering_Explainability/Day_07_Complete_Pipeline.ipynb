{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11db98d",
   "metadata": {},
   "source": [
    "# Day 7: Complete Interpretable ML Pipeline\n",
    "\n",
    "## Learning Objectives\n",
    "- Build end-to-end interpretable ML system\n",
    "- Combine feature engineering, selection, and explainability\n",
    "- Production-ready documentation and reporting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93566e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ğŸ“š Day 7: Complete Interpretable ML Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9f7a7",
   "metadata": {},
   "source": [
    "## Part 1: Feature Engineering Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING CLASS\n",
    "# ============================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Comprehensive feature engineering for financial data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Generate all features.\"\"\"\n",
    "        result = df.copy()\n",
    "        \n",
    "        # Return features\n",
    "        for h in [1, 5, 10, 20]:\n",
    "            result[f'ret_{h}d'] = df['price'].pct_change(h)\n",
    "        \n",
    "        # Volatility features\n",
    "        for h in [5, 10, 20]:\n",
    "            result[f'vol_{h}d'] = df['returns'].rolling(h).std()\n",
    "        \n",
    "        # Momentum features\n",
    "        for h in [5, 10, 20]:\n",
    "            result[f'mom_{h}d'] = df['returns'].rolling(h).sum()\n",
    "        \n",
    "        # RSI\n",
    "        gains = df['returns'].where(df['returns'] > 0, 0).rolling(14).mean()\n",
    "        losses = (-df['returns'].where(df['returns'] < 0, 0)).rolling(14).mean()\n",
    "        result['rsi'] = 100 - (100 / (1 + gains / (losses + 1e-10)))\n",
    "        result['rsi_norm'] = (result['rsi'] - 50) / 50\n",
    "        \n",
    "        # MA ratios\n",
    "        result['ma_ratio_5_20'] = df['price'].rolling(5).mean() / df['price'].rolling(20).mean()\n",
    "        \n",
    "        # Volatility ratio\n",
    "        result['vol_ratio'] = result['vol_5d'] / result['vol_20d']\n",
    "        \n",
    "        # Momentum acceleration\n",
    "        result['mom_accel'] = result['mom_5d'] - result['mom_5d'].shift(5)\n",
    "        \n",
    "        self.feature_names = [c for c in result.columns if c not in ['price', 'returns']]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "\n",
    "print(\"âœ… FeatureEngineer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9819edf",
   "metadata": {},
   "source": [
    "## Part 2: Feature Selection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ba453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTOR CLASS\n",
    "# ============================================================\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"Feature selection with multiple methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=10, correlation_threshold=0.85):\n",
    "        self.n_features = n_features\n",
    "        self.corr_threshold = correlation_threshold\n",
    "        self.selected_features = []\n",
    "        self.selection_report = {}\n",
    "        \n",
    "    def fit(self, X, y, feature_names):\n",
    "        \"\"\"Select features using multiple methods.\"\"\"\n",
    "        \n",
    "        # 1. Remove highly correlated features\n",
    "        df_temp = pd.DataFrame(X, columns=feature_names)\n",
    "        corr_matrix = df_temp.corr().abs()\n",
    "        \n",
    "        to_drop = set()\n",
    "        for i in range(len(feature_names)):\n",
    "            for j in range(i+1, len(feature_names)):\n",
    "                if corr_matrix.iloc[i, j] > self.corr_threshold:\n",
    "                    to_drop.add(feature_names[j])\n",
    "        \n",
    "        remaining = [f for f in feature_names if f not in to_drop]\n",
    "        X_remaining = df_temp[remaining].values\n",
    "        \n",
    "        # 2. Mutual Information\n",
    "        mi_scores = mutual_info_classif(X_remaining, y, random_state=42)\n",
    "        mi_ranking = sorted(zip(remaining, mi_scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 3. Select top features\n",
    "        self.selected_features = [f for f, _ in mi_ranking[:self.n_features]]\n",
    "        \n",
    "        # Store report\n",
    "        self.selection_report = {\n",
    "            'original_features': len(feature_names),\n",
    "            'after_corr_filter': len(remaining),\n",
    "            'final_selected': len(self.selected_features),\n",
    "            'dropped_correlated': list(to_drop),\n",
    "            'mi_ranking': mi_ranking\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, feature_names):\n",
    "        \"\"\"Select only chosen features.\"\"\"\n",
    "        df_temp = pd.DataFrame(X, columns=feature_names)\n",
    "        return df_temp[self.selected_features].values\n",
    "    \n",
    "    def get_selected_features(self):\n",
    "        return self.selected_features\n",
    "    \n",
    "    def print_report(self):\n",
    "        print(\"\\nFEATURE SELECTION REPORT\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Original features: {self.selection_report['original_features']}\")\n",
    "        print(f\"After correlation filter: {self.selection_report['after_corr_filter']}\")\n",
    "        print(f\"Final selected: {self.selection_report['final_selected']}\")\n",
    "        print(f\"\\nSelected features: {self.selected_features}\")\n",
    "\n",
    "print(\"âœ… FeatureSelector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24ba54",
   "metadata": {},
   "source": [
    "## Part 3: Model Explainer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6a72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL EXPLAINER CLASS\n",
    "# ============================================================\n",
    "\n",
    "class ModelExplainer:\n",
    "    \"\"\"Model explainability toolkit.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_names):\n",
    "        self.model = model\n",
    "        self.feature_names = feature_names\n",
    "        self.shap_values = None\n",
    "        self.permutation_importance = None\n",
    "        \n",
    "    def calculate_shap(self, X_train, X_test, n_samples=100):\n",
    "        \"\"\"Calculate SHAP-like values.\"\"\"\n",
    "        n_explain = min(n_samples, len(X_test))\n",
    "        n_features = X_test.shape[1]\n",
    "        \n",
    "        shap_values = np.zeros((n_explain, n_features))\n",
    "        \n",
    "        for i in range(n_explain):\n",
    "            x = X_test[i:i+1]\n",
    "            original_pred = self.model.predict_proba(x)[0, 1]\n",
    "            \n",
    "            for j in range(n_features):\n",
    "                x_modified = x.copy()\n",
    "                x_modified[0, j] = X_train[:, j].mean()\n",
    "                modified_pred = self.model.predict_proba(x_modified)[0, 1]\n",
    "                shap_values[i, j] = original_pred - modified_pred\n",
    "        \n",
    "        self.shap_values = shap_values\n",
    "        return shap_values\n",
    "    \n",
    "    def calculate_permutation_importance(self, X_test, y_test):\n",
    "        \"\"\"Calculate permutation importance.\"\"\"\n",
    "        result = permutation_importance(self.model, X_test, y_test, n_repeats=20, random_state=42)\n",
    "        self.permutation_importance = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': result.importances_mean,\n",
    "            'std': result.importances_std\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        return self.permutation_importance\n",
    "    \n",
    "    def explain_prediction(self, idx, X_test, y_test):\n",
    "        \"\"\"Explain a single prediction.\"\"\"\n",
    "        sample = X_test[idx:idx+1]\n",
    "        pred_prob = self.model.predict_proba(sample)[0, 1]\n",
    "        actual = y_test[idx]\n",
    "        \n",
    "        print(f\"\\nPREDICTION EXPLANATION (Sample {idx})\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Prediction: {pred_prob:.2%} probability of UP\")\n",
    "        print(f\"Actual: {'UP âœ“' if actual == 1 else 'DOWN âœ—'}\")\n",
    "        \n",
    "        if self.shap_values is not None and idx < len(self.shap_values):\n",
    "            contrib = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'value': sample[0],\n",
    "                'contribution': self.shap_values[idx]\n",
    "            }).sort_values('contribution', key=abs, ascending=False)\n",
    "            \n",
    "            print(\"\\nTop contributors:\")\n",
    "            for _, row in contrib.head(5).iterrows():\n",
    "                sign = '+' if row['contribution'] > 0 else '-'\n",
    "                print(f\"  {row['feature']:<15} = {row['value']:>7.3f}  {sign}{abs(row['contribution']):.4f}\")\n",
    "    \n",
    "    def plot_summary(self, X_test):\n",
    "        \"\"\"Create summary visualizations.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Permutation importance\n",
    "        if self.permutation_importance is not None:\n",
    "            self.permutation_importance.plot(x='feature', y='importance', kind='barh',\n",
    "                                             xerr=self.permutation_importance['std'],\n",
    "                                             ax=axes[0], color='steelblue', alpha=0.7, legend=False)\n",
    "            axes[0].set_xlabel('Mean Accuracy Decrease')\n",
    "            axes[0].set_title('Permutation Importance', fontweight='bold')\n",
    "            axes[0].invert_yaxis()\n",
    "        \n",
    "        # SHAP summary\n",
    "        if self.shap_values is not None:\n",
    "            mean_shap = np.abs(self.shap_values).mean(axis=0)\n",
    "            sorted_idx = np.argsort(mean_shap)[::-1]\n",
    "            \n",
    "            for i, feat_idx in enumerate(sorted_idx[:8]):\n",
    "                shap_vals = self.shap_values[:, feat_idx]\n",
    "                feat_vals = X_test[:len(shap_vals), feat_idx]\n",
    "                norm_feat = (feat_vals - feat_vals.min()) / (feat_vals.max() - feat_vals.min() + 1e-10)\n",
    "                \n",
    "                y_jitter = i + np.random.randn(len(shap_vals)) * 0.15\n",
    "                axes[1].scatter(shap_vals, y_jitter, c=norm_feat, cmap='coolwarm', alpha=0.5, s=10)\n",
    "            \n",
    "            axes[1].set_yticks(range(min(8, len(self.feature_names))))\n",
    "            axes[1].set_yticklabels([self.feature_names[i] for i in sorted_idx[:8]])\n",
    "            axes[1].axvline(0, color='gray', linewidth=0.5)\n",
    "            axes[1].set_xlabel('SHAP Value')\n",
    "            axes[1].set_title('SHAP Summary', fontweight='bold')\n",
    "            axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"âœ… ModelExplainer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ed91b",
   "metadata": {},
   "source": [
    "## Part 4: Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERATE DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"RUNNING COMPLETE PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "n_days = 2000\n",
    "\n",
    "returns = np.random.normal(0.0002, 0.015, n_days)\n",
    "for i in range(1, len(returns)):\n",
    "    returns[i] += 0.05 * returns[i-1]\n",
    "\n",
    "prices = 100 * np.cumprod(1 + returns)\n",
    "df = pd.DataFrame({'price': prices, 'returns': returns})\n",
    "\n",
    "print(f\"Generated {n_days} days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be75cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 1: FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSTEP 1: FEATURE ENGINEERING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "fe = FeatureEngineer()\n",
    "df_features = fe.fit_transform(df)\n",
    "\n",
    "# Add target\n",
    "df_features['target'] = (df['returns'].shift(-1) > 0).astype(int)\n",
    "df_features = df_features.dropna()\n",
    "\n",
    "feature_names = fe.get_feature_names()\n",
    "X = df_features[feature_names].values\n",
    "y = df_features['target'].values\n",
    "\n",
    "print(f\"Created {len(feature_names)} features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2: FEATURE SELECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSTEP 2: FEATURE SELECTION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "selector = FeatureSelector(n_features=8, correlation_threshold=0.85)\n",
    "selector.fit(X, y, feature_names)\n",
    "\n",
    "X_selected = selector.transform(X, feature_names)\n",
    "selected_features = selector.get_selected_features()\n",
    "\n",
    "selector.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSTEP 3: MODEL TRAINING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Scale and split\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "\n",
    "split = int(len(X_scaled) * 0.7)\n",
    "X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Train model\n",
    "model = GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  F1:        {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"  AUC-ROC:   {roc_auc_score(y_test, y_proba):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af192d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4: MODEL EXPLAINABILITY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSTEP 4: MODEL EXPLAINABILITY\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "explainer = ModelExplainer(model, selected_features)\n",
    "\n",
    "# Calculate importance metrics\n",
    "perm_imp = explainer.calculate_permutation_importance(X_test, y_test)\n",
    "shap_vals = explainer.calculate_shap(X_train, X_test, n_samples=200)\n",
    "\n",
    "print(\"\\nPermutation Importance:\")\n",
    "print(perm_imp.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8589ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 5: VISUALIZE & EXPLAIN\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nSTEP 5: VISUALIZATIONS\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "explainer.plot_summary(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INDIVIDUAL PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nEXPLAINING INDIVIDUAL PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "explainer.explain_prediction(50, X_test, y_test)\n",
    "explainer.explain_prediction(100, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRADING SIMULATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nTRADING SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get actual returns for test period\n",
    "test_returns = returns[split + 60:split + 60 + len(y_test)]  # Offset for feature lag\n",
    "\n",
    "min_len = min(len(test_returns), len(y_pred))\n",
    "test_returns = test_returns[:min_len]\n",
    "predictions = y_pred[:min_len]\n",
    "\n",
    "# Strategy returns\n",
    "strategy_returns = test_returns * (2 * predictions - 1)\n",
    "\n",
    "# Metrics\n",
    "cum_strategy = np.cumprod(1 + strategy_returns)\n",
    "cum_market = np.cumprod(1 + test_returns)\n",
    "\n",
    "strategy_sharpe = np.sqrt(252) * strategy_returns.mean() / strategy_returns.std()\n",
    "market_sharpe = np.sqrt(252) * test_returns.mean() / test_returns.std()\n",
    "\n",
    "print(f\"Strategy Total Return: {(cum_strategy[-1]-1)*100:.1f}%\")\n",
    "print(f\"Market Total Return:   {(cum_market[-1]-1)*100:.1f}%\")\n",
    "print(f\"Strategy Sharpe:       {strategy_sharpe:.2f}\")\n",
    "print(f\"Market Sharpe:         {market_sharpe:.2f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(cum_strategy, label=f'Strategy ({(cum_strategy[-1]-1)*100:.0f}%)', linewidth=2)\n",
    "ax.plot(cum_market, label=f'Market ({(cum_market[-1]-1)*100:.0f}%)', alpha=0.7)\n",
    "ax.set_xlabel('Trading Days')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Strategy vs Market Performance', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘      DAY 7 COMPLETE: INTERPRETABLE ML PIPELINE                   â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  âœ“ FeatureEngineer: Comprehensive feature creation              â•‘\n",
    "â•‘  âœ“ FeatureSelector: Correlation + MI-based selection            â•‘\n",
    "â•‘  âœ“ Model Training: Gradient Boosting with evaluation            â•‘\n",
    "â•‘  âœ“ ModelExplainer: SHAP + Permutation importance                â•‘\n",
    "â•‘  âœ“ Individual prediction explanations                           â•‘\n",
    "â•‘  âœ“ Trading simulation with performance metrics                  â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ‰ WEEK 11 COMPLETE! You've mastered Feature Engineering & Explainability!\n",
    "\n",
    "Next: Week 12 - Advanced Backtesting & Deployment\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
