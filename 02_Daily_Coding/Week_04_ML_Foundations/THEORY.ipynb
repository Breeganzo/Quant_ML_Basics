{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacf5a00",
   "metadata": {},
   "source": [
    "# Week 4: Machine Learning Basics for Trading\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Supervised vs Unsupervised Learning\n",
    "2. Train-Test Split\n",
    "3. Cross-Validation\n",
    "4. Bias-Variance Tradeoff\n",
    "5. Feature Engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports and data loading",
    "import numpy as np",
    "import pandas as pd",
    "import yfinance as yf",
    "from datetime import datetime, timedelta",
    "",
    "# Standard 5 equities for analysis",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'GS']",
    "",
    "# Fetch 5 years of data",
    "end_date = datetime.now()",
    "start_date = end_date - timedelta(days=5*365)",
    "",
    "print(\"\ud83d\udce5 Downloading market data...\")",
    "data = yf.download(tickers, start=start_date, end=end_date, progress=False, auto_adjust=True)",
    "prices = data['Close'].dropna()",
    "returns = prices.pct_change().dropna()",
    "print(f\"\u2705 Loaded {len(prices)} days of data for {len(tickers)} tickers\")",
    "print(f\"\ud83d\udcc5 Date range: {prices.index[0].strftime('%Y-%m-%d')} to {prices.index[-1].strftime('%Y-%m-%d')}\")",
    "print(prices.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58804d4e",
   "metadata": {},
   "source": [
    "## 1. Supervised vs Unsupervised Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "Learn from labeled data to predict outcomes.\n",
    "\n",
    "**Goal**: Learn function $f$ such that $Y = f(X) + \\epsilon$\n",
    "\n",
    "**Types**:\n",
    "- **Regression**: Predict continuous values (e.g., next day's return)\n",
    "- **Classification**: Predict categories (e.g., up/down/neutral)\n",
    "\n",
    "**Trading Applications**:\n",
    "- Return prediction\n",
    "- Direction forecasting\n",
    "- Credit default prediction\n",
    "- Trade execution optimization\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Find patterns in data without labels.\n",
    "\n",
    "**Types**:\n",
    "- **Clustering**: Group similar assets\n",
    "- **Dimensionality Reduction**: Find hidden factors\n",
    "- **Anomaly Detection**: Identify unusual market events\n",
    "\n",
    "**Trading Applications**:\n",
    "- Regime detection\n",
    "- Asset clustering for diversification\n",
    "- Factor discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf190f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create sample data: 3 features predicting returns\n",
    "n_samples = 1000\n",
    "momentum = np.random.normal(0, 1, n_samples)     # Momentum signal\n",
    "value = np.random.normal(0, 1, n_samples)        # Value signal\n",
    "volatility = np.abs(np.random.normal(0, 1, n_samples))  # Volatility\n",
    "\n",
    "# True relationship (with noise)\n",
    "true_returns = 0.3 * momentum + 0.2 * value - 0.1 * volatility + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'momentum': momentum,\n",
    "    'value': value,\n",
    "    'volatility': volatility\n",
    "})\n",
    "y = true_returns\n",
    "\n",
    "print(\"SUPERVISED LEARNING EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Features (X): {list(X.columns)}\")\n",
    "print(f\"Target (y): Stock returns\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"\\nWe want to learn: Return = f(momentum, value, volatility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500e445",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Train-Test Split\n",
    "\n",
    "### Why Split Data?\n",
    "\n",
    "**Overfitting**: Model memorizes training data but fails on new data.\n",
    "\n",
    "**Solution**: Hold out data to test generalization.\n",
    "\n",
    "### Standard Split\n",
    "\n",
    "$$\\text{Data} = \\text{Training Set (70-80%)} + \\text{Test Set (20-30%)}$$\n",
    "\n",
    "### Time Series Split (Critical for Finance!)\n",
    "\n",
    "**WRONG**: Random split (future data in training set = look-ahead bias)\n",
    "\n",
    "**RIGHT**: Chronological split\n",
    "\n",
    "```\n",
    "Timeline: ---|----Training----|----Test----|->\n",
    "             Start           Split        End\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b43d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "# WRONG: Random split (introduces look-ahead bias)\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# RIGHT: Chronological split for time series\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(\"Train-Test Split for Time Series\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n\u274c WRONG (Random Split):\")\n",
    "print(f\"   Training indices include: {sorted(X_train_wrong.index[:5].tolist())}... (mixed!)\")\n",
    "print(f\"   This causes look-ahead bias!\")\n",
    "\n",
    "print(f\"\\n\u2713 CORRECT (Chronological Split):\")\n",
    "print(f\"   Training: indices 0 to {split_idx-1} ({len(X_train)} samples)\")\n",
    "print(f\"   Testing: indices {split_idx} to {len(X)-1} ({len(X_test)} samples)\")\n",
    "print(f\"   No future information leaks into training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685ec0d",
   "metadata": {},
   "source": [
    "### Training and Evaluating\n",
    "\n",
    "**Process**:\n",
    "1. Train model on training data only\n",
    "2. Make predictions on test data\n",
    "3. Compare predictions to actual values\n",
    "4. Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Model Performance\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nCoefficients learned:\")\n",
    "for name, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\n           | Training | Test     |\")\n",
    "print(f\"  RMSE     | {train_rmse:.4f}   | {test_rmse:.4f}   |\")\n",
    "print(f\"  R\u00b2       | {train_r2:.4f}   | {test_r2:.4f}   |\")\n",
    "\n",
    "if test_r2 < train_r2 * 0.8:\n",
    "    print(\"\\n\u26a0\ufe0f Warning: Possible overfitting (test R\u00b2 << train R\u00b2)\")\n",
    "else:\n",
    "    print(\"\\n\u2713 Model generalizes well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a3c79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Cross-Validation\n",
    "\n",
    "### Why Cross-Validation?\n",
    "\n",
    "Single train-test split is unreliable:\n",
    "- What if test period was unusual?\n",
    "- We \"waste\" data (only train on 80%)\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "Split data into $K$ folds, train $K$ times:\n",
    "\n",
    "```\n",
    "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
    "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
    "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
    "...\n",
    "```\n",
    "\n",
    "**Final score** = Average of all folds\n",
    "\n",
    "### Time Series Cross-Validation (Walk-Forward)\n",
    "\n",
    "For time series, use **expanding window**:\n",
    "\n",
    "```\n",
    "Fold 1: [Train    ] [Test]\n",
    "Fold 2: [Train         ] [Test]\n",
    "Fold 3: [Train              ] [Test]\n",
    "Fold 4: [Train                   ] [Test]\n",
    "```\n",
    "\n",
    "This mimics real trading: train on history, test on future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7221e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Time Series Cross-Validation (Walk-Forward)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cv_scores = []\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    # Split data\n",
    "    X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_tr, y_te = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    score = model.score(X_te, y_te)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Train[{train_idx[0]:3d}-{train_idx[-1]:3d}] \u2192 \"\n",
    "          f\"Test[{test_idx[0]:3d}-{test_idx[-1]:3d}] | R\u00b2 = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean R\u00b2: {np.mean(cv_scores):.4f} (\u00b1{np.std(cv_scores):.4f})\")\n",
    "print(\"\\n\u2713 This tests model across different market periods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2c36c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bias-Variance Tradeoff\n",
    "\n",
    "### The Fundamental Tradeoff\n",
    "\n",
    "**Total Error** = Bias\u00b2 + Variance + Irreducible Noise\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "### Definitions\n",
    "\n",
    "**Bias**: Error from wrong assumptions (underfitting)\n",
    "$$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$$\n",
    "\n",
    "**Variance**: Error from sensitivity to training data (overfitting)\n",
    "$$\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "### The Tradeoff\n",
    "\n",
    "| Model Complexity | Bias | Variance | Typical Result |\n",
    "|-----------------|------|----------|----------------|\n",
    "| Too Simple | High | Low | Underfitting |\n",
    "| Just Right | Balanced | Balanced | Good Generalization |\n",
    "| Too Complex | Low | High | Overfitting |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ea02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Demonstrate bias-variance with polynomial regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# True signal: simple linear relationship\n",
    "X_simple = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_true = 2 * X_simple.ravel() + np.random.normal(0, 0.3, 100)\n",
    "\n",
    "# Split\n",
    "X_tr, X_te = X_simple[:70], X_simple[70:]\n",
    "y_tr, y_te = y_true[:70], y_true[70:]\n",
    "\n",
    "print(\"Bias-Variance Tradeoff Demo\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTrue relationship: y = 2x + noise\")\n",
    "print(\"\\nModel Complexity Comparison:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for degree in [1, 3, 15]:\n",
    "    # Create polynomial features\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    train_err = mean_squared_error(y_tr, model.predict(X_tr))\n",
    "    test_err = mean_squared_error(y_te, model.predict(X_te))\n",
    "    \n",
    "    status = \"Good\" if abs(train_err - test_err) < 0.1 else \\\n",
    "             (\"Underfit\" if train_err > 0.1 else \"Overfit\")\n",
    "    \n",
    "    print(f\"Degree {degree:2d}: Train MSE={train_err:.4f}, Test MSE={test_err:.4f} \u2192 {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431822b",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Add penalty for complexity to prevent overfitting:\n",
    "\n",
    "**Ridge (L2)**: $\\text{Loss} = MSE + \\lambda \\sum \\beta_j^2$\n",
    "\n",
    "**Lasso (L1)**: $\\text{Loss} = MSE + \\lambda \\sum |\\beta_j|$\n",
    "\n",
    "- Larger $\\lambda$ = more regularization = simpler model\n",
    "- Lasso can shrink coefficients to exactly zero (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Compare regularization strengths\n",
    "print(\"Effect of Regularization (Ridge)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use original data with many features\n",
    "for alpha in [0.01, 0.1, 1.0, 10.0]:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"\u03bb = {alpha:5.2f}: Train R\u00b2={train_r2:.4f}, Test R\u00b2={test_r2:.4f}, \"\n",
    "          f\"Coefs magnitude: {np.sum(model.coef_**2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c2e0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feature Engineering\n",
    "\n",
    "### Why Feature Engineering?\n",
    "\n",
    "**\"Garbage in, garbage out\"**\n",
    "\n",
    "Raw data is rarely suitable for ML. We must create meaningful features.\n",
    "\n",
    "### Common Trading Features\n",
    "\n",
    "**Price-Based**:\n",
    "- Returns: $r_t = \\frac{P_t - P_{t-1}}{P_{t-1}}$\n",
    "- Log returns: $r_t = \\ln(P_t / P_{t-1})$\n",
    "- Moving averages: $MA_n = \\frac{1}{n}\\sum_{i=0}^{n-1} P_{t-i}$\n",
    "\n",
    "**Momentum**:\n",
    "- RSI: $RSI = 100 - \\frac{100}{1 + RS}$ where $RS = \\frac{\\text{Avg Gain}}{\\text{Avg Loss}}$\n",
    "- MACD: $EMA_{12} - EMA_{26}$\n",
    "\n",
    "**Volatility**:\n",
    "- Rolling std dev\n",
    "- True Range\n",
    "- Bollinger Band width\n",
    "\n",
    "**Volume**:\n",
    "- Volume moving average\n",
    "- On-balance volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa947d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate price data\n",
    "n_days = 500\n",
    "returns = np.random.normal(0.0005, 0.015, n_days)\n",
    "prices = 100 * np.cumprod(1 + returns)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'price': prices,\n",
    "    'return': returns\n",
    "})\n",
    "\n",
    "# Create features\n",
    "df['return_1d'] = df['price'].pct_change()           # 1-day return\n",
    "df['return_5d'] = df['price'].pct_change(5)          # 5-day return (momentum)\n",
    "df['ma_20'] = df['price'].rolling(20).mean()         # 20-day moving average\n",
    "df['ma_50'] = df['price'].rolling(50).mean()         # 50-day moving average\n",
    "df['ma_ratio'] = df['ma_20'] / df['ma_50']           # MA crossover signal\n",
    "df['volatility_20'] = df['return_1d'].rolling(20).std()  # 20-day volatility\n",
    "\n",
    "# Price relative to MA (mean reversion signal)\n",
    "df['price_ma_ratio'] = df['price'] / df['ma_20']\n",
    "\n",
    "print(\"Feature Engineering Example\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nCreated features from raw price data:\")\n",
    "print(df[['price', 'return_1d', 'return_5d', 'ma_ratio', 'volatility_20']].dropna().head(10).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9452e",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Many ML algorithms require scaled features:\n",
    "\n",
    "**Standardization** (Z-score):\n",
    "$$X_{scaled} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "**Min-Max Scaling**:\n",
    "$$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "**Important**: Fit scaler on training data only, transform both train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['return_5d', 'ma_ratio', 'volatility_20', 'price_ma_ratio']\n",
    "df_clean = df.dropna().copy()\n",
    "\n",
    "# Target: Next day's return\n",
    "df_clean['target'] = df_clean['return_1d'].shift(-1)\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Split chronologically\n",
    "split_idx = int(len(df_clean) * 0.8)\n",
    "train_df = df_clean.iloc[:split_idx]\n",
    "test_df = df_clean.iloc[split_idx:]\n",
    "\n",
    "# Fit scaler on TRAINING DATA ONLY\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_df[features])\n",
    "X_test_scaled = scaler.transform(test_df[features])  # Use same params!\n",
    "\n",
    "print(\"Feature Scaling\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n\u26a0\ufe0f Key: Fit scaler on training data, transform both!\")\n",
    "print(\"\\nBefore scaling (training data):\")\n",
    "print(train_df[features].describe().loc[['mean', 'std']].round(4))\n",
    "\n",
    "print(\"\\nAfter scaling (training data):\")\n",
    "print(f\"Mean: ~0, Std: ~1 for all features\")\n",
    "print(f\"Actual - Mean: {X_train_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"         Std:  {X_train_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac82e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Week 4 Key Concepts\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Supervised Learning | Predict from labeled data (regression/classification) |\n",
    "| Train-Test Split | Chronological for time series (no look-ahead!) |\n",
    "| Cross-Validation | Walk-forward for robust evaluation |\n",
    "| Bias-Variance | Simple=high bias, Complex=high variance |\n",
    "| Regularization | Ridge (L2), Lasso (L1) prevent overfitting |\n",
    "| Feature Engineering | Create meaningful inputs from raw data |\n",
    "| Feature Scaling | Fit on train, transform both |\n",
    "\n",
    "---\n",
    "\n",
    "*Next Week: Portfolio Theory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd34 PROS & CONS: THEORY\n",
    "\n",
    "### \u2705 PROS (Advantages)\n",
    "\n",
    "| Advantage | Description | Real-World Application |\n",
    "|-----------|-------------|----------------------|\n",
    "| **Industry Standard** | Widely adopted in quantitative finance | Used by major hedge funds and banks |\n",
    "| **Well-Documented** | Extensive research and documentation | Easy to find resources and support |\n",
    "| **Proven Track Record** | Years of practical application | Validated in real market conditions |\n",
    "| **Interpretable** | Results can be explained to stakeholders | Important for risk management and compliance |\n",
    "\n",
    "### \u274c CONS (Limitations)\n",
    "\n",
    "| Limitation | Description | How to Mitigate |\n",
    "|------------|-------------|-----------------|\n",
    "| **Assumptions** | May not hold in all market conditions | Validate assumptions with data |\n",
    "| **Historical Bias** | Based on past data patterns | Use rolling windows and regime detection |\n",
    "| **Overfitting Risk** | May fit noise rather than signal | Use proper cross-validation |\n",
    "| **Computational Cost** | Can be resource-intensive | Optimize code and use appropriate hardware |\n",
    "\n",
    "### \ud83c\udfaf Real-World Usage\n",
    "\n",
    "**WHERE THIS IS USED:**\n",
    "- \u2705 Quantitative hedge funds (Two Sigma, Renaissance, Citadel)\n",
    "- \u2705 Investment banks (Goldman Sachs, JP Morgan, Morgan Stanley)\n",
    "- \u2705 Asset management firms\n",
    "- \u2705 Risk management departments\n",
    "- \u2705 Algorithmic trading desks\n",
    "\n",
    "**NOT JUST THEORY - THIS IS PRODUCTION CODE:**\n",
    "The techniques in this notebook are used daily by professionals managing billions of dollars."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}