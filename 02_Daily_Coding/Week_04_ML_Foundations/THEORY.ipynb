{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacf5a00",
   "metadata": {},
   "source": [
    "# Week 4: Machine Learning Basics for Trading\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. Supervised vs Unsupervised Learning\n",
    "2. Train-Test Split\n",
    "3. Cross-Validation\n",
    "4. Bias-Variance Tradeoff\n",
    "5. Feature Engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading market data...\n",
      "‚úÖ Loaded 1255 days of data for 5 tickers\n",
      "üìÖ Date range: 2021-01-25 to 2026-01-22\n",
      "Ticker            AAPL       GOOGL          GS         JPM        MSFT\n",
      "Date                                                                  \n",
      "2026-01-15  258.209991  332.779999  975.859985  309.260010  456.660004\n",
      "2026-01-16  255.529999  330.000000  962.000000  312.470001  459.859985\n",
      "2026-01-20  246.699997  322.000000  943.369995  302.739990  454.519989\n",
      "2026-01-21  247.649994  328.380005  953.010010  302.040009  444.109985\n",
      "2026-01-22  249.695007  331.475006  965.546692  306.709991  449.884491\n"
     ]
    }
   ],
   "source": [
    "# Standard imports and data loading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Standard 5 equities for analysis\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'GS']\n",
    "\n",
    "# Fetch 5 years of data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(\"üì• Downloading market data...\")\n",
    "data = yf.download(tickers, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "prices = data['Close'].dropna()\n",
    "returns = prices.pct_change().dropna()\n",
    "print(f\"‚úÖ Loaded {len(prices)} days of data for {len(tickers)} tickers\")\n",
    "print(f\"üìÖ Date range: {prices.index[0].strftime('%Y-%m-%d')} to {prices.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(prices.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58804d4e",
   "metadata": {},
   "source": [
    "## 1. Supervised vs Unsupervised Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "Learn from labeled data to predict outcomes.\n",
    "\n",
    "**Goal**: Learn function $f$ such that $Y = f(X) + \\epsilon$\n",
    "\n",
    "**Types**:\n",
    "- **Regression**: Predict continuous values (e.g., next day's return)\n",
    "- **Classification**: Predict categories (e.g., up/down/neutral)\n",
    "\n",
    "**Trading Applications**:\n",
    "- Return prediction\n",
    "- Direction forecasting\n",
    "- Credit default prediction\n",
    "- Trade execution optimization\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "Find patterns in data without labels.\n",
    "\n",
    "**Types**:\n",
    "- **Clustering**: Group similar assets\n",
    "- **Dimensionality Reduction**: Find hidden factors\n",
    "- **Anomaly Detection**: Identify unusual market events\n",
    "\n",
    "**Trading Applications**:\n",
    "- Regime detection\n",
    "- Asset clustering for diversification\n",
    "- Factor discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf190f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPERVISED LEARNING EXAMPLE\n",
      "==================================================\n",
      "Features (X): ['momentum', 'value', 'volatility']\n",
      "Target (y): Stock returns\n",
      "Samples: 1000\n",
      "\n",
      "We want to learn: Return = f(momentum, value, volatility)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create sample data: 3 features predicting returns\n",
    "n_samples = 1000\n",
    "momentum = np.random.normal(0, 1, n_samples)     # Momentum signal\n",
    "value = np.random.normal(0, 1, n_samples)        # Value signal\n",
    "volatility = np.abs(np.random.normal(0, 1, n_samples))  # Volatility\n",
    "\n",
    "# True relationship (with noise)\n",
    "true_returns = 0.3 * momentum + 0.2 * value - 0.1 * volatility + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'momentum': momentum,\n",
    "    'value': value,\n",
    "    'volatility': volatility\n",
    "})\n",
    "y = true_returns\n",
    "\n",
    "print(\"SUPERVISED LEARNING EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Features (X): {list(X.columns)}\")\n",
    "print(f\"Target (y): Stock returns\")\n",
    "print(f\"Samples: {n_samples}\")\n",
    "print(f\"\\nWe want to learn: Return = f(momentum, value, volatility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500e445",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Train-Test Split\n",
    "\n",
    "### Why Split Data?\n",
    "\n",
    "**Overfitting**: Model memorizes training data but fails on new data.\n",
    "\n",
    "**Solution**: Hold out data to test generalization.\n",
    "\n",
    "### Standard Split\n",
    "\n",
    "$$\\text{Data} = \\text{Training Set (70-80%)} + \\text{Test Set (20-30%)}$$\n",
    "\n",
    "### Time Series Split (Critical for Finance!)\n",
    "\n",
    "**WRONG**: Random split (future data in training set = look-ahead bias)\n",
    "\n",
    "**RIGHT**: Chronological split\n",
    "\n",
    "```\n",
    "Timeline: ---|----Training----|----Test----|->\n",
    "             Start           Split        End\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b43d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Split for Time Series\n",
      "==================================================\n",
      "\n",
      "‚ùå WRONG (Random Split):\n",
      "   Training indices include: [29, 535, 557, 695, 836]... (mixed!)\n",
      "   This causes look-ahead bias!\n",
      "\n",
      "‚úì CORRECT (Chronological Split):\n",
      "   Training: indices 0 to 799 (800 samples)\n",
      "   Testing: indices 800 to 999 (200 samples)\n",
      "   No future information leaks into training!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "\n",
    "# WRONG: Random split (introduces look-ahead bias)\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# RIGHT: Chronological split for time series\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(\"Train-Test Split for Time Series\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n‚ùå WRONG (Random Split):\")\n",
    "print(f\"   Training indices include: {sorted(X_train_wrong.index[:5].tolist())}... (mixed!)\")\n",
    "print(f\"   This causes look-ahead bias!\")\n",
    "\n",
    "print(f\"\\n‚úì CORRECT (Chronological Split):\")\n",
    "print(f\"   Training: indices 0 to {split_idx-1} ({len(X_train)} samples)\")\n",
    "print(f\"   Testing: indices {split_idx} to {len(X)-1} ({len(X_test)} samples)\")\n",
    "print(f\"   No future information leaks into training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685ec0d",
   "metadata": {},
   "source": [
    "### Training and Evaluating\n",
    "\n",
    "**Process**:\n",
    "1. Train model on training data only\n",
    "2. Make predictions on test data\n",
    "3. Compare predictions to actual values\n",
    "4. Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b59aabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "==================================================\n",
      "\n",
      "Coefficients learned:\n",
      "  momentum: 0.2991\n",
      "  value: 0.1687\n",
      "  volatility: -0.1961\n",
      "\n",
      "           | Training | Test     |\n",
      "  RMSE     | 0.5115   | 0.5150   |\n",
      "  R¬≤       | 0.3192   | 0.2319   |\n",
      "\n",
      "‚ö†Ô∏è Warning: Possible overfitting (test R¬≤ << train R¬≤)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Model Performance\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nCoefficients learned:\")\n",
    "for name, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\n           | Training | Test     |\")\n",
    "print(f\"  RMSE     | {train_rmse:.4f}   | {test_rmse:.4f}   |\")\n",
    "print(f\"  R¬≤       | {train_r2:.4f}   | {test_r2:.4f}   |\")\n",
    "\n",
    "if test_r2 < train_r2 * 0.8:\n",
    "    print(\"\\n‚ö†Ô∏è Warning: Possible overfitting (test R¬≤ << train R¬≤)\")\n",
    "else:\n",
    "    print(\"\\n‚úì Model generalizes well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a3c79",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Cross-Validation\n",
    "\n",
    "### Why Cross-Validation?\n",
    "\n",
    "Single train-test split is unreliable:\n",
    "- What if test period was unusual?\n",
    "- We \"waste\" data (only train on 80%)\n",
    "\n",
    "### K-Fold Cross-Validation\n",
    "\n",
    "Split data into $K$ folds, train $K$ times:\n",
    "\n",
    "```\n",
    "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
    "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
    "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
    "...\n",
    "```\n",
    "\n",
    "**Final score** = Average of all folds\n",
    "\n",
    "### Time Series Cross-Validation (Walk-Forward)\n",
    "\n",
    "For time series, use **expanding window**:\n",
    "\n",
    "```\n",
    "Fold 1: [Train    ] [Test]\n",
    "Fold 2: [Train         ] [Test]\n",
    "Fold 3: [Train              ] [Test]\n",
    "Fold 4: [Train                   ] [Test]\n",
    "```\n",
    "\n",
    "This mimics real trading: train on history, test on future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7221e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Cross-Validation (Walk-Forward)\n",
      "==================================================\n",
      "Fold 1: Train[  0-169] ‚Üí Test[170-335] | R¬≤ = 0.2796\n",
      "Fold 2: Train[  0-335] ‚Üí Test[336-501] | R¬≤ = 0.2712\n",
      "Fold 3: Train[  0-501] ‚Üí Test[502-667] | R¬≤ = 0.3181\n",
      "Fold 4: Train[  0-667] ‚Üí Test[668-833] | R¬≤ = 0.3009\n",
      "Fold 5: Train[  0-833] ‚Üí Test[834-999] | R¬≤ = 0.2004\n",
      "\n",
      "Mean R¬≤: 0.2741 (¬±0.0403)\n",
      "\n",
      "‚úì This tests model across different market periods!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Time Series Cross-Validation (Walk-Forward)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "cv_scores = []\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "    # Split data\n",
    "    X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_tr, y_te = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    score = model.score(X_te, y_te)\n",
    "    cv_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold}: Train[{train_idx[0]:3d}-{train_idx[-1]:3d}] ‚Üí \"\n",
    "          f\"Test[{test_idx[0]:3d}-{test_idx[-1]:3d}] | R¬≤ = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nMean R¬≤: {np.mean(cv_scores):.4f} (¬±{np.std(cv_scores):.4f})\")\n",
    "print(\"\\n‚úì This tests model across different market periods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa2c36c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bias-Variance Tradeoff\n",
    "\n",
    "### The Fundamental Tradeoff\n",
    "\n",
    "**Total Error** = Bias¬≤ + Variance + Irreducible Noise\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "### Definitions\n",
    "\n",
    "**Bias**: Error from wrong assumptions (underfitting)\n",
    "$$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$$\n",
    "\n",
    "**Variance**: Error from sensitivity to training data (overfitting)\n",
    "$$\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "### The Tradeoff\n",
    "\n",
    "| Model Complexity | Bias | Variance | Typical Result |\n",
    "|-----------------|------|----------|----------------|\n",
    "| Too Simple | High | Low | Underfitting |\n",
    "| Just Right | Balanced | Balanced | Good Generalization |\n",
    "| Too Complex | Low | High | Overfitting |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3ea02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias-Variance Tradeoff Demo\n",
      "==================================================\n",
      "\n",
      "True relationship: y = 2x + noise\n",
      "\n",
      "Model Complexity Comparison:\n",
      "--------------------------------------------------\n",
      "Degree  1: Train MSE=0.0717, Test MSE=0.0771 ‚Üí Good\n",
      "Degree  3: Train MSE=0.0658, Test MSE=0.1448 ‚Üí Good\n",
      "Degree 15: Train MSE=0.0558, Test MSE=895361478770.1882 ‚Üí Overfit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Demonstrate bias-variance with polynomial regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# True signal: simple linear relationship\n",
    "X_simple = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_true = 2 * X_simple.ravel() + np.random.normal(0, 0.3, 100)\n",
    "\n",
    "# Split\n",
    "X_tr, X_te = X_simple[:70], X_simple[70:]\n",
    "y_tr, y_te = y_true[:70], y_true[70:]\n",
    "\n",
    "print(\"Bias-Variance Tradeoff Demo\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTrue relationship: y = 2x + noise\")\n",
    "print(\"\\nModel Complexity Comparison:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for degree in [1, 3, 15]:\n",
    "    # Create polynomial features\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    train_err = mean_squared_error(y_tr, model.predict(X_tr))\n",
    "    test_err = mean_squared_error(y_te, model.predict(X_te))\n",
    "    \n",
    "    status = \"Good\" if abs(train_err - test_err) < 0.1 else \\\n",
    "             (\"Underfit\" if train_err > 0.1 else \"Overfit\")\n",
    "    \n",
    "    print(f\"Degree {degree:2d}: Train MSE={train_err:.4f}, Test MSE={test_err:.4f} ‚Üí {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431822b",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Add penalty for complexity to prevent overfitting:\n",
    "\n",
    "**Ridge (L2)**: $\\text{Loss} = MSE + \\lambda \\sum \\beta_j^2$\n",
    "\n",
    "**Lasso (L1)**: $\\text{Loss} = MSE + \\lambda \\sum |\\beta_j|$\n",
    "\n",
    "- Larger $\\lambda$ = more regularization = simpler model\n",
    "- Lasso can shrink coefficients to exactly zero (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d46574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of Regularization (Ridge)\n",
      "==================================================\n",
      "Œª =  0.01: Train R¬≤=0.3192, Test R¬≤=0.2319, Coefs magnitude: 0.1564\n",
      "Œª =  0.10: Train R¬≤=0.3192, Test R¬≤=0.2319, Coefs magnitude: 0.1563\n",
      "Œª =  1.00: Train R¬≤=0.3192, Test R¬≤=0.2323, Coefs magnitude: 0.1558\n",
      "Œª = 10.00: Train R¬≤=0.3192, Test R¬≤=0.2354, Coefs magnitude: 0.1505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Compare regularization strengths\n",
    "print(\"Effect of Regularization (Ridge)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use original data with many features\n",
    "for alpha in [0.01, 0.1, 1.0, 10.0]:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Œª = {alpha:5.2f}: Train R¬≤={train_r2:.4f}, Test R¬≤={test_r2:.4f}, \"\n",
    "          f\"Coefs magnitude: {np.sum(model.coef_**2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c2e0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Feature Engineering\n",
    "\n",
    "### Why Feature Engineering?\n",
    "\n",
    "**\"Garbage in, garbage out\"**\n",
    "\n",
    "Raw data is rarely suitable for ML. We must create meaningful features.\n",
    "\n",
    "### Common Trading Features\n",
    "\n",
    "**Price-Based**:\n",
    "- Returns: $r_t = \\frac{P_t - P_{t-1}}{P_{t-1}}$\n",
    "- Log returns: $r_t = \\ln(P_t / P_{t-1})$\n",
    "- Moving averages: $MA_n = \\frac{1}{n}\\sum_{i=0}^{n-1} P_{t-i}$\n",
    "\n",
    "**Momentum**:\n",
    "- RSI: $RSI = 100 - \\frac{100}{1 + RS}$ where $RS = \\frac{\\text{Avg Gain}}{\\text{Avg Loss}}$\n",
    "- MACD: $EMA_{12} - EMA_{26}$\n",
    "\n",
    "**Volatility**:\n",
    "- Rolling std dev\n",
    "- True Range\n",
    "- Bollinger Band width\n",
    "\n",
    "**Volume**:\n",
    "- Volume moving average\n",
    "- On-balance volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa947d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Example\n",
      "==================================================\n",
      "\n",
      "Created features from raw price data:\n",
      "      price  return_1d  return_5d  ma_ratio  volatility_20\n",
      "49  86.1447    -0.0259    -0.0210    0.9388         0.0150\n",
      "50  86.6065     0.0054    -0.0055    0.9386         0.0151\n",
      "51  86.1496    -0.0053    -0.0044    0.9369         0.0132\n",
      "52  85.3179    -0.0097    -0.0298    0.9351         0.0132\n",
      "53  86.1434     0.0097    -0.0260    0.9348         0.0134\n",
      "54  87.5186     0.0160     0.0159    0.9343         0.0136\n",
      "55  88.7850     0.0145     0.0252    0.9350         0.0138\n",
      "56  87.7117    -0.0121     0.0181    0.9357         0.0139\n",
      "57  87.3487    -0.0041     0.0238    0.9379         0.0125\n",
      "58  87.8265     0.0055     0.0195    0.9411         0.0118\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate price data\n",
    "n_days = 500\n",
    "returns = np.random.normal(0.0005, 0.015, n_days)\n",
    "prices = 100 * np.cumprod(1 + returns)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'price': prices,\n",
    "    'return': returns\n",
    "})\n",
    "\n",
    "# Create features\n",
    "df['return_1d'] = df['price'].pct_change()           # 1-day return\n",
    "df['return_5d'] = df['price'].pct_change(5)          # 5-day return (momentum)\n",
    "df['ma_20'] = df['price'].rolling(20).mean()         # 20-day moving average\n",
    "df['ma_50'] = df['price'].rolling(50).mean()         # 50-day moving average\n",
    "df['ma_ratio'] = df['ma_20'] / df['ma_50']           # MA crossover signal\n",
    "df['volatility_20'] = df['return_1d'].rolling(20).std()  # 20-day volatility\n",
    "\n",
    "# Price relative to MA (mean reversion signal)\n",
    "df['price_ma_ratio'] = df['price'] / df['ma_20']\n",
    "\n",
    "print(\"Feature Engineering Example\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nCreated features from raw price data:\")\n",
    "print(df[['price', 'return_1d', 'return_5d', 'ma_ratio', 'volatility_20']].dropna().head(10).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9452e",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Many ML algorithms require scaled features:\n",
    "\n",
    "**Standardization** (Z-score):\n",
    "$$X_{scaled} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "**Min-Max Scaling**:\n",
    "$$X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "**Important**: Fit scaler on training data only, transform both train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4abe9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Scaling\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è Key: Fit scaler on training data, transform both!\n",
      "\n",
      "Before scaling (training data):\n",
      "      return_5d  ma_ratio  volatility_20  price_ma_ratio\n",
      "mean     0.0058    1.0125         0.0142          1.0101\n",
      "std      0.0293    0.0294         0.0026          0.0310\n",
      "\n",
      "After scaling (training data):\n",
      "Mean: ~0, Std: ~1 for all features\n",
      "Actual - Mean: [0. 0. 0. 0.]\n",
      "         Std:  [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['return_5d', 'ma_ratio', 'volatility_20', 'price_ma_ratio']\n",
    "df_clean = df.dropna().copy()\n",
    "\n",
    "# Target: Next day's return\n",
    "df_clean['target'] = df_clean['return_1d'].shift(-1)\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Split chronologically\n",
    "split_idx = int(len(df_clean) * 0.8)\n",
    "train_df = df_clean.iloc[:split_idx]\n",
    "test_df = df_clean.iloc[split_idx:]\n",
    "\n",
    "# Fit scaler on TRAINING DATA ONLY\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_df[features])\n",
    "X_test_scaled = scaler.transform(test_df[features])  # Use same params!\n",
    "\n",
    "print(\"Feature Scaling\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n‚ö†Ô∏è Key: Fit scaler on training data, transform both!\")\n",
    "print(\"\\nBefore scaling (training data):\")\n",
    "print(train_df[features].describe().loc[['mean', 'std']].round(4))\n",
    "\n",
    "print(\"\\nAfter scaling (training data):\")\n",
    "print(f\"Mean: ~0, Std: ~1 for all features\")\n",
    "print(f\"Actual - Mean: {X_train_scaled.mean(axis=0).round(4)}\")\n",
    "print(f\"         Std:  {X_train_scaled.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac82e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Week 4 Key Concepts\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Supervised Learning | Predict from labeled data (regression/classification) |\n",
    "| Train-Test Split | Chronological for time series (no look-ahead!) |\n",
    "| Cross-Validation | Walk-forward for robust evaluation |\n",
    "| Bias-Variance | Simple=high bias, Complex=high variance |\n",
    "| Regularization | Ridge (L2), Lasso (L1) prevent overfitting |\n",
    "| Feature Engineering | Create meaningful inputs from raw data |\n",
    "| Feature Scaling | Fit on train, transform both |\n",
    "\n",
    "---\n",
    "\n",
    "*Next Week: Portfolio Theory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¥ PROS & CONS: THEORY\n",
    "\n",
    "### ‚úÖ PROS (Advantages)\n",
    "\n",
    "| Advantage | Description | Real-World Application |\n",
    "|-----------|-------------|----------------------|\n",
    "| **Industry Standard** | Widely adopted in quantitative finance | Used by major hedge funds and banks |\n",
    "| **Well-Documented** | Extensive research and documentation | Easy to find resources and support |\n",
    "| **Proven Track Record** | Years of practical application | Validated in real market conditions |\n",
    "| **Interpretable** | Results can be explained to stakeholders | Important for risk management and compliance |\n",
    "\n",
    "### ‚ùå CONS (Limitations)\n",
    "\n",
    "| Limitation | Description | How to Mitigate |\n",
    "|------------|-------------|-----------------|\n",
    "| **Assumptions** | May not hold in all market conditions | Validate assumptions with data |\n",
    "| **Historical Bias** | Based on past data patterns | Use rolling windows and regime detection |\n",
    "| **Overfitting Risk** | May fit noise rather than signal | Use proper cross-validation |\n",
    "| **Computational Cost** | Can be resource-intensive | Optimize code and use appropriate hardware |\n",
    "\n",
    "### üéØ Real-World Usage\n",
    "\n",
    "**WHERE THIS IS USED:**\n",
    "- ‚úÖ Quantitative hedge funds (Two Sigma, Renaissance, Citadel)\n",
    "- ‚úÖ Investment banks (Goldman Sachs, JP Morgan, Morgan Stanley)\n",
    "- ‚úÖ Asset management firms\n",
    "- ‚úÖ Risk management departments\n",
    "- ‚úÖ Algorithmic trading desks\n",
    "\n",
    "**NOT JUST THEORY - THIS IS PRODUCTION CODE:**\n",
    "The techniques in this notebook are used daily by professionals managing billions of dollars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
