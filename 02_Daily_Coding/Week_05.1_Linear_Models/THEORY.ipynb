{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e034f7ee",
   "metadata": {},
   "source": [
    "# Week 5.1: Linear Regression Models for Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Linear regression is the cornerstone of quantitative finance and forms the foundation for:\n",
    "- **Factor Models**: CAPM, Fama-French, APT\n",
    "- **Return Prediction**: Cross-sectional and time-series forecasting\n",
    "- **Risk Decomposition**: Attribution analysis\n",
    "- **Portfolio Construction**: Optimal weight estimation\n",
    "\n",
    "This week covers:\n",
    "| Day | Topic | Key Concepts |\n",
    "|-----|-------|-------------|\n",
    "| 1 | OLS Regression | Assumptions, MLE, Statistical Inference |\n",
    "| 2 | Ridge Regression | L2 Regularization, Bias-Variance Tradeoff |\n",
    "| 3 | Lasso Regression | L1 Regularization, Feature Selection |\n",
    "| 4 | Elastic Net | Combined Regularization, Grouped Selection |\n",
    "| 5 | Fama-MacBeth | Cross-Sectional Regression, Risk Premia |\n",
    "| 6 | WLS & Robust | Heteroskedasticity, Outlier Resistance |\n",
    "| 7 | Interview Review | Key Questions & Applications |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee26717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26064442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  10 of 10 completed\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Adj Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Adj Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m end_date = \u001b[33m'\u001b[39m\u001b[33m2024-01-01\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Download adjusted close prices\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m prices = \u001b[43myf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdj Close\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m returns = prices.pct_change().dropna()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Download Fama-French factors proxy (using ETFs)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4112\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4113\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.columns.get_loc(key)\n\u001b[32m   4114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4170\u001b[39m, in \u001b[36mDataFrame._getitem_multilevel\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m   4169\u001b[39m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4170\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np.ndarray)):\n\u001b[32m   4172\u001b[39m         new_columns = \u001b[38;5;28mself\u001b[39m.columns[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:3059\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[32m   3058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[32m   3062\u001b[39m keylen = \u001b[38;5;28mlen\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:3410\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3407\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[32m   3409\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3410\u001b[39m     idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_loc_single_level_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m level > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lexsort_depth == \u001b[32m0\u001b[39m:\n\u001b[32m   3413\u001b[39m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[32m   3414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   3415\u001b[39m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/multi.py:2999\u001b[39m, in \u001b[36mMultiIndex._get_loc_single_level_index\u001b[39m\u001b[34m(self, level_index, key)\u001b[39m\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[32m1\u001b[39m\n\u001b[32m   2998\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlevel_index\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learning_Trading_ML/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Adj Close'"
     ]
    }
   ],
   "source": [
    "# Download Financial Data\n",
    "# We'll use a diversified set of assets for our examples\n",
    "\n",
    "tickers = ['SPY', 'QQQ', 'IWM', 'EFA', 'TLT', 'GLD', 'VNQ', 'XLF', 'XLE', 'XLK']\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "# Download adjusted close prices\n",
    "prices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "# Download Fama-French factors proxy (using ETFs)\n",
    "ff_tickers = ['SPY', 'IWM', 'IWD', 'IWF']  # Market, Small Cap, Value, Growth\n",
    "ff_prices = yf.download(ff_tickers, start=start_date, end=end_date)['Adj Close']\n",
    "ff_returns = ff_prices.pct_change().dropna()\n",
    "\n",
    "print(f\"Data downloaded: {len(returns)} trading days\")\n",
    "print(f\"Date range: {returns.index[0].strftime('%Y-%m-%d')} to {returns.index[-1].strftime('%Y-%m-%d')}\")\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8718f5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 1: Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "## 1.1 The Linear Regression Model\n",
    "\n",
    "The fundamental linear regression model assumes:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $Y \\in \\mathbb{R}^{n}$: Vector of dependent variable observations\n",
    "- $X \\in \\mathbb{R}^{n \\times p}$: Design matrix of independent variables\n",
    "- $\\beta \\in \\mathbb{R}^{p}$: Vector of coefficients to estimate\n",
    "- $\\epsilon \\in \\mathbb{R}^{n}$: Vector of error terms\n",
    "\n",
    "### In Finance: The CAPM Example\n",
    "\n",
    "The Capital Asset Pricing Model is a single-factor regression:\n",
    "\n",
    "$$R_i - R_f = \\alpha_i + \\beta_i(R_m - R_f) + \\epsilon_i$$\n",
    "\n",
    "Where:\n",
    "- $R_i - R_f$: Excess return of asset $i$\n",
    "- $\\alpha_i$: Jensen's alpha (risk-adjusted excess return)\n",
    "- $\\beta_i$: Market beta (systematic risk exposure)\n",
    "- $R_m - R_f$: Market risk premium\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 OLS Estimator Derivation\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "OLS minimizes the sum of squared residuals:\n",
    "\n",
    "$$\\hat{\\beta}_{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} (y_i - x_i'\\beta)^2 = \\arg\\min_{\\beta} ||Y - X\\beta||_2^2$$\n",
    "\n",
    "### Closed-Form Solution\n",
    "\n",
    "Taking the derivative and setting to zero:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta} (Y - X\\beta)'(Y - X\\beta) = -2X'Y + 2X'X\\beta = 0$$\n",
    "\n",
    "Solving for $\\beta$:\n",
    "\n",
    "$$\\boxed{\\hat{\\beta}_{OLS} = (X'X)^{-1}X'Y}$$\n",
    "\n",
    "This is the **Normal Equation** - the most important formula in regression analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6029558",
   "metadata": {},
   "source": [
    "## 1.3 Gauss-Markov Assumptions\n",
    "\n",
    "For OLS to be the **Best Linear Unbiased Estimator (BLUE)**, we need:\n",
    "\n",
    "| Assumption | Mathematical Form | Financial Implication |\n",
    "|------------|-------------------|----------------------|\n",
    "| **Linearity** | $E[Y|X] = X\\beta$ | Returns are linear in factors |\n",
    "| **No Perfect Multicollinearity** | $\\text{rank}(X) = p$ | Factors are not redundant |\n",
    "| **Exogeneity** | $E[\\epsilon|X] = 0$ | No omitted variables, no simultaneity |\n",
    "| **Homoskedasticity** | $\\text{Var}(\\epsilon|X) = \\sigma^2 I$ | Constant volatility (rarely true!) |\n",
    "| **No Autocorrelation** | $\\text{Cov}(\\epsilon_i, \\epsilon_j) = 0$ for $i \\neq j$ | No serial correlation in residuals |\n",
    "\n",
    "### Why These Matter in Finance\n",
    "\n",
    "1. **Homoskedasticity Violation**: Volatility clustering → Use WLS or GARCH\n",
    "2. **Autocorrelation Violation**: Momentum/mean-reversion → Use Newey-West standard errors\n",
    "3. **Exogeneity Violation**: Endogenous regressors → Use instrumental variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1: OLS Implementation - CAPM Beta Estimation\n",
    "\n",
    "# Prepare data for CAPM regression\n",
    "# Using SPY as market proxy, XLK (Tech) as the asset\n",
    "market_returns = returns['SPY'].values\n",
    "asset_returns = returns['XLK'].values\n",
    "\n",
    "# Method 1: Manual OLS using Normal Equation\n",
    "def ols_manual(X, y):\n",
    "    \"\"\"\n",
    "    Implement OLS using the normal equation: β = (X'X)^(-1)X'y\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "    y : array-like, shape (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    beta : array, OLS coefficients\n",
    "    \"\"\"\n",
    "    X = np.column_stack([np.ones(len(X)), X])  # Add intercept\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return beta\n",
    "\n",
    "# Estimate CAPM beta manually\n",
    "beta_manual = ols_manual(market_returns, asset_returns)\n",
    "print(\"=\" * 60)\n",
    "print(\"CAPM REGRESSION: XLK ~ SPY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nManual OLS Results:\")\n",
    "print(f\"  Alpha (Intercept): {beta_manual[0]:.6f} ({beta_manual[0]*252:.4f} annualized)\")\n",
    "print(f\"  Beta (Market):     {beta_manual[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e122573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using statsmodels for full statistical inference\n",
    "X_sm = sm.add_constant(market_returns)\n",
    "model_ols = sm.OLS(asset_returns, X_sm).fit()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATSMODELS OLS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(model_ols.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CAPM Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Scatter plot with regression line\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(market_returns * 100, asset_returns * 100, alpha=0.4, s=10, label='Daily Returns')\n",
    "x_line = np.linspace(market_returns.min(), market_returns.max(), 100)\n",
    "y_line = beta_manual[0] + beta_manual[1] * x_line\n",
    "ax1.plot(x_line * 100, y_line * 100, 'r-', linewidth=2, \n",
    "         label=f'CAPM: α={beta_manual[0]*252:.2%}, β={beta_manual[1]:.2f}')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Market Return (SPY) %')\n",
    "ax1.set_ylabel('Asset Return (XLK) %')\n",
    "ax1.set_title('CAPM Regression: XLK vs SPY')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Residual diagnostics\n",
    "residuals = asset_returns - (beta_manual[0] + beta_manual[1] * market_returns)\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(model_ols.fittedvalues * 100, residuals * 100, alpha=0.4, s=10)\n",
    "ax2.axhline(y=0, color='r', linestyle='-', linewidth=2)\n",
    "ax2.set_xlabel('Fitted Values (%)')\n",
    "ax2.set_ylabel('Residuals (%)')\n",
    "ax2.set_title('Residual Plot (Checking Homoskedasticity)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test OLS Assumptions\n",
    "print(\"=\" * 60)\n",
    "print(\"OLS ASSUMPTION TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Normality Test (Jarque-Bera)\n",
    "jb_stat, jb_pvalue = stats.jarque_bera(residuals)\n",
    "print(f\"\\n1. Normality Test (Jarque-Bera):\")\n",
    "print(f\"   Statistic: {jb_stat:.2f}, p-value: {jb_pvalue:.4f}\")\n",
    "print(f\"   Result: {'Residuals are normal' if jb_pvalue > 0.05 else 'Residuals NOT normal (common in finance)'}\")\n",
    "\n",
    "# 2. Homoskedasticity Test (Breusch-Pagan)\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "bp_stat, bp_pvalue, _, _ = het_breuschpagan(residuals, X_sm)\n",
    "print(f\"\\n2. Homoskedasticity Test (Breusch-Pagan):\")\n",
    "print(f\"   Statistic: {bp_stat:.2f}, p-value: {bp_pvalue:.4f}\")\n",
    "print(f\"   Result: {'Homoskedastic' if bp_pvalue > 0.05 else 'Heteroskedastic - consider WLS/robust SE'}\")\n",
    "\n",
    "# 3. Autocorrelation Test (Durbin-Watson)\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"\\n3. Autocorrelation Test (Durbin-Watson):\")\n",
    "print(f\"   Statistic: {dw_stat:.2f} (ideal = 2.0)\")\n",
    "print(f\"   Result: {'No autocorrelation' if 1.5 < dw_stat < 2.5 else 'Possible autocorrelation'}\")\n",
    "\n",
    "# 4. VIF for multicollinearity (if multiple regressors)\n",
    "print(f\"\\n4. Model Statistics:\")\n",
    "print(f\"   R-squared: {model_ols.rsquared:.4f}\")\n",
    "print(f\"   Adjusted R-squared: {model_ols.rsquared_adj:.4f}\")\n",
    "print(f\"   F-statistic: {model_ols.fvalue:.2f} (p-value: {model_ols.f_pvalue:.2e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74997b87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 2: Ridge Regression (L2 Regularization)\n",
    "\n",
    "## 2.1 The Bias-Variance Tradeoff\n",
    "\n",
    "OLS is **unbiased** but can have **high variance** when:\n",
    "- Features are highly correlated (multicollinearity)\n",
    "- Number of features is large relative to observations\n",
    "- Model is overfitting to noise\n",
    "\n",
    "The expected prediction error decomposes as:\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\underbrace{\\text{Bias}^2[\\hat{f}(x)]}_{\\text{Error from simplification}} + \\underbrace{\\text{Var}[\\hat{f}(x)]}_{\\text{Error from estimation}} + \\underbrace{\\sigma^2_{\\epsilon}}_{\\text{Irreducible noise}}$$\n",
    "\n",
    "**Ridge regression introduces bias to reduce variance**, potentially lowering overall prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Ridge Regression Formulation\n",
    "\n",
    "Ridge adds an **L2 penalty** on coefficient magnitudes:\n",
    "\n",
    "$$\\hat{\\beta}_{Ridge} = \\arg\\min_{\\beta} \\left\\{ \\sum_{i=1}^{n} (y_i - x_i'\\beta)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\}$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$\\hat{\\beta}_{Ridge} = \\arg\\min_{\\beta} ||Y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2$$\n",
    "\n",
    "### Closed-Form Solution\n",
    "\n",
    "$$\\boxed{\\hat{\\beta}_{Ridge} = (X'X + \\lambda I)^{-1}X'Y}$$\n",
    "\n",
    "**Key insight**: Adding $\\lambda I$ to $X'X$ ensures invertibility even when $X'X$ is singular!\n",
    "\n",
    "---\n",
    "\n",
    "## 2.3 Properties of Ridge Regression\n",
    "\n",
    "### Shrinkage Behavior\n",
    "\n",
    "Using SVD decomposition $X = UDV'$, the Ridge solution can be written as:\n",
    "\n",
    "$$\\hat{\\beta}_{Ridge} = \\sum_{j=1}^{p} \\frac{d_j^2}{d_j^2 + \\lambda} \\cdot \\frac{u_j'y}{d_j} \\cdot v_j$$\n",
    "\n",
    "Where:\n",
    "- $d_j$: Singular values of $X$\n",
    "- $\\frac{d_j^2}{d_j^2 + \\lambda}$: Shrinkage factor (0 to 1)\n",
    "\n",
    "**Coefficients associated with small singular values are shrunk more heavily.**\n",
    "\n",
    "### Financial Application\n",
    "\n",
    "In multi-factor models, Ridge helps when:\n",
    "- Factors are correlated (e.g., Value and Quality overlap)\n",
    "- Estimating time-varying betas with limited data\n",
    "- Preventing extreme factor exposures in portfolio construction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 2: Ridge Regression Implementation\n",
    "\n",
    "# Create multi-factor model features\n",
    "# Using multiple ETFs as factor proxies\n",
    "factor_tickers = ['SPY', 'IWM', 'TLT', 'GLD', 'VNQ']\n",
    "target_ticker = 'XLF'  # Financial sector\n",
    "\n",
    "X_factors = returns[factor_tickers].values\n",
    "y_target = returns[target_ticker].values\n",
    "\n",
    "# Standardize features (important for regularization!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_factors)\n",
    "\n",
    "print(\"Multi-Factor Model Setup:\")\n",
    "print(f\"Target: {target_ticker}\")\n",
    "print(f\"Factors: {factor_tickers}\")\n",
    "print(f\"Observations: {len(y_target)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee89a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS vs Ridge across different lambda values\n",
    "lambdas = np.logspace(-4, 4, 50)\n",
    "ridge_coefs = []\n",
    "\n",
    "# OLS coefficients (lambda = 0)\n",
    "ols_model = LinearRegression()\n",
    "ols_model.fit(X_scaled, y_target)\n",
    "ols_coefs = ols_model.coef_\n",
    "\n",
    "# Ridge coefficients for each lambda\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam)\n",
    "    ridge.fit(X_scaled, y_target)\n",
    "    ridge_coefs.append(ridge.coef_)\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "\n",
    "# Plot Ridge Path (coefficient shrinkage)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Ridge coefficient paths\n",
    "ax1 = axes[0]\n",
    "for i, factor in enumerate(factor_tickers):\n",
    "    ax1.semilogx(lambdas, ridge_coefs[:, i], label=factor, linewidth=2)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('Lambda (Regularization Strength)')\n",
    "ax1.set_ylabel('Coefficient Value')\n",
    "ax1.set_title('Ridge Regression Path: Coefficient Shrinkage')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# Plot 2: Coefficient comparison at specific lambda\n",
    "optimal_lambda = 0.1\n",
    "ridge_optimal = Ridge(alpha=optimal_lambda)\n",
    "ridge_optimal.fit(X_scaled, y_target)\n",
    "\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(len(factor_tickers))\n",
    "width = 0.35\n",
    "bars1 = ax2.bar(x_pos - width/2, ols_coefs, width, label='OLS', color='steelblue')\n",
    "bars2 = ax2.bar(x_pos + width/2, ridge_optimal.coef_, width, label=f'Ridge (λ={optimal_lambda})', color='coral')\n",
    "ax2.set_xlabel('Factor')\n",
    "ax2.set_ylabel('Coefficient')\n",
    "ax2.set_title(f'OLS vs Ridge Coefficients')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(factor_tickers)\n",
    "ax2.legend()\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for optimal lambda selection\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Time Series Cross-Validation (important for financial data!)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Ridge CV with time series splits\n",
    "ridge_cv_scores = []\n",
    "for lam in lambdas:\n",
    "    ridge = Ridge(alpha=lam)\n",
    "    scores = cross_val_score(ridge, X_scaled, y_target, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    ridge_cv_scores.append(-scores.mean())\n",
    "\n",
    "optimal_idx = np.argmin(ridge_cv_scores)\n",
    "optimal_lambda_cv = lambdas[optimal_idx]\n",
    "\n",
    "# Plot CV results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogx(lambdas, ridge_cv_scores, 'b-', linewidth=2)\n",
    "plt.axvline(x=optimal_lambda_cv, color='r', linestyle='--', \n",
    "            label=f'Optimal λ = {optimal_lambda_cv:.4f}')\n",
    "plt.xlabel('Lambda (Regularization Strength)')\n",
    "plt.ylabel('Mean Squared Error (CV)')\n",
    "plt.title('Ridge Regression: Cross-Validation for Lambda Selection')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal Lambda (Time Series CV): {optimal_lambda_cv:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59591790",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 3: Lasso Regression (L1 Regularization)\n",
    "\n",
    "## 3.1 Lasso Formulation\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) uses an **L1 penalty**:\n",
    "\n",
    "$$\\hat{\\beta}_{Lasso} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - x_i'\\beta)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\}$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$\\hat{\\beta}_{Lasso} = \\arg\\min_{\\beta} \\frac{1}{2n}||Y - X\\beta||_2^2 + \\lambda ||\\beta||_1$$\n",
    "\n",
    "### Key Difference from Ridge\n",
    "\n",
    "| Property | Ridge (L2) | Lasso (L1) |\n",
    "|----------|-----------|------------|\n",
    "| Penalty | $\\lambda\\sum\\beta_j^2$ | $\\lambda\\sum|\\beta_j|$ |\n",
    "| Solution | Closed-form | Requires optimization |\n",
    "| Sparsity | Shrinks toward zero | **Sets coefficients exactly to zero** |\n",
    "| Feature Selection | No | **Yes** |\n",
    "| Correlated features | Keeps all | Tends to pick one |\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Why L1 Induces Sparsity\n",
    "\n",
    "### Geometric Intuition\n",
    "\n",
    "The constraint regions have different shapes:\n",
    "- **Ridge**: $||\\beta||_2 \\leq t$ is a **sphere** (smooth, no corners)\n",
    "- **Lasso**: $||\\beta||_1 \\leq t$ is a **diamond** (corners at axes)\n",
    "\n",
    "The OLS solution contours are ellipses. With Lasso, these contours are more likely to intersect the constraint region at a corner, where some coordinates are exactly zero.\n",
    "\n",
    "### Soft Thresholding\n",
    "\n",
    "For orthonormal $X$, Lasso has a closed-form solution:\n",
    "\n",
    "$$\\hat{\\beta}_j^{Lasso} = \\text{sign}(\\hat{\\beta}_j^{OLS}) \\cdot \\max(|\\hat{\\beta}_j^{OLS}| - \\lambda, 0)$$\n",
    "\n",
    "This is the **soft-thresholding operator** - coefficients smaller than $\\lambda$ become exactly zero.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Financial Applications of Lasso\n",
    "\n",
    "1. **Factor Selection**: Which factors actually contribute to returns?\n",
    "2. **Sparse Portfolios**: Limit number of holdings\n",
    "3. **Signal Selection**: Identify relevant predictive signals from many candidates\n",
    "4. **High-Dimensional Regression**: When $p > n$ (more features than observations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 3: Lasso Regression Implementation\n",
    "\n",
    "# Create a high-dimensional factor model\n",
    "# Using all available ETFs plus lagged returns\n",
    "all_factor_tickers = ['SPY', 'QQQ', 'IWM', 'EFA', 'TLT', 'GLD', 'VNQ']\n",
    "target = 'XLK'\n",
    "\n",
    "# Create feature matrix with current and lagged returns\n",
    "feature_df = returns[all_factor_tickers].copy()\n",
    "\n",
    "# Add lagged features (1-5 days)\n",
    "for lag in range(1, 6):\n",
    "    for ticker in all_factor_tickers:\n",
    "        feature_df[f'{ticker}_lag{lag}'] = returns[ticker].shift(lag)\n",
    "\n",
    "# Add rolling volatility features\n",
    "for ticker in all_factor_tickers:\n",
    "    feature_df[f'{ticker}_vol20'] = returns[ticker].rolling(20).std()\n",
    "\n",
    "# Prepare data\n",
    "feature_df = feature_df.dropna()\n",
    "y_lasso = returns.loc[feature_df.index, target].values\n",
    "X_lasso = feature_df.values\n",
    "feature_names = feature_df.columns.tolist()\n",
    "\n",
    "# Standardize\n",
    "scaler_lasso = StandardScaler()\n",
    "X_lasso_scaled = scaler_lasso.fit_transform(X_lasso)\n",
    "\n",
    "print(f\"Feature Matrix Shape: {X_lasso.shape}\")\n",
    "print(f\"Number of Features: {len(feature_names)}\")\n",
    "print(f\"\\nFeature Categories:\")\n",
    "print(f\"  - Current returns: {len(all_factor_tickers)}\")\n",
    "print(f\"  - Lagged returns: {len(all_factor_tickers) * 5}\")\n",
    "print(f\"  - Volatility features: {len(all_factor_tickers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd787dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Path - see which features get selected at different lambda values\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "# Compute Lasso path\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_lasso_scaled, y_lasso, alphas=np.logspace(-6, -2, 100))\n",
    "\n",
    "# Plot Lasso Path\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, name in enumerate(feature_names):\n",
    "    if 'lag' not in name and 'vol' not in name:  # Only plot current returns for clarity\n",
    "        plt.semilogx(alphas_lasso, coefs_lasso[i, :], label=name, linewidth=2)\n",
    "    else:\n",
    "        plt.semilogx(alphas_lasso, coefs_lasso[i, :], alpha=0.2, linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Lambda (Regularization Strength)')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Lasso Regression Path: Coefficient Shrinkage & Selection')\n",
    "plt.legend(loc='upper right')\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.xlim(alphas_lasso.max(), alphas_lasso.min())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal Lasso lambda using Cross-Validation\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Lasso with Time Series CV\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "lasso_cv = LassoCV(alphas=np.logspace(-6, -2, 100), cv=tscv, max_iter=10000)\n",
    "lasso_cv.fit(X_lasso_scaled, y_lasso)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LASSO REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOptimal Lambda: {lasso_cv.alpha_:.6f}\")\n",
    "print(f\"\\nSelected Features ({np.sum(lasso_cv.coef_ != 0)} of {len(feature_names)}):\")\n",
    "\n",
    "# Show non-zero coefficients\n",
    "selected_features = [(name, coef) for name, coef in zip(feature_names, lasso_cv.coef_) if coef != 0]\n",
    "selected_features.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\n{:<20} {:>15}\".format(\"Feature\", \"Coefficient\"))\n",
    "print(\"-\" * 35)\n",
    "for name, coef in selected_features[:15]:  # Top 15\n",
    "    print(f\"{name:<20} {coef:>15.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6965a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance from Lasso\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: All coefficients (many are zero)\n",
    "ax1 = axes[0]\n",
    "colors = ['coral' if c != 0 else 'lightgray' for c in lasso_cv.coef_]\n",
    "ax1.bar(range(len(lasso_cv.coef_)), np.abs(lasso_cv.coef_), color=colors)\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('|Coefficient|')\n",
    "ax1.set_title(f'Lasso Feature Selection: {np.sum(lasso_cv.coef_ != 0)} Features Selected')\n",
    "\n",
    "# Plot 2: Top selected features\n",
    "ax2 = axes[1]\n",
    "if selected_features:\n",
    "    names, coefs = zip(*selected_features[:10])\n",
    "    colors = ['green' if c > 0 else 'red' for c in coefs]\n",
    "    ax2.barh(range(len(names)), coefs, color=colors)\n",
    "    ax2.set_yticks(range(len(names)))\n",
    "    ax2.set_yticklabels(names)\n",
    "    ax2.set_xlabel('Coefficient Value')\n",
    "    ax2.set_title('Top 10 Selected Features')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0559ef4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 4: Elastic Net (Combined L1 + L2)\n",
    "\n",
    "## 4.1 Elastic Net Formulation\n",
    "\n",
    "Elastic Net combines both L1 and L2 penalties:\n",
    "\n",
    "$$\\hat{\\beta}_{EN} = \\arg\\min_{\\beta} \\left\\{ \\frac{1}{2n}||Y - X\\beta||_2^2 + \\lambda \\left[ \\frac{1-\\alpha}{2}||\\beta||_2^2 + \\alpha||\\beta||_1 \\right] \\right\\}$$\n",
    "\n",
    "Where:\n",
    "- $\\lambda$: Overall regularization strength\n",
    "- $\\alpha \\in [0, 1]$: Mixing parameter\n",
    "  - $\\alpha = 0$: Pure Ridge\n",
    "  - $\\alpha = 1$: Pure Lasso\n",
    "  - $\\alpha = 0.5$: Equal mix\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Advantages of Elastic Net\n",
    "\n",
    "### Grouped Selection\n",
    "\n",
    "When features are highly correlated:\n",
    "- **Lasso**: Tends to select one feature arbitrarily, ignoring others\n",
    "- **Elastic Net**: Tends to select/deselect correlated features together\n",
    "\n",
    "### Example in Finance\n",
    "\n",
    "Consider correlated factors like:\n",
    "- `MSCI_Value` and `FF_HML` (both measure value)\n",
    "- `MSCI_Momentum` and `FF_UMD` (both measure momentum)\n",
    "\n",
    "Elastic Net will either include both or exclude both, providing more stable factor selection.\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "For two perfectly correlated features $x_1 = x_2$:\n",
    "- **Lasso solution**: $\\beta_1 + \\beta_2 = c$ (infinite solutions along a line)\n",
    "- **Elastic Net solution**: $\\beta_1 = \\beta_2 = c/2$ (unique solution due to L2)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 When to Use Each Method\n",
    "\n",
    "| Scenario | Recommended Method |\n",
    "|----------|-------------------|\n",
    "| Few features, no multicollinearity | OLS |\n",
    "| Many correlated features, keep all | Ridge |\n",
    "| Many features, need sparsity | Lasso |\n",
    "| Correlated features, need sparsity | **Elastic Net** |\n",
    "| $p >> n$ (more features than samples) | Elastic Net or Lasso |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb825a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 4: Elastic Net Implementation\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Test different alpha values (L1 ratio)\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "\n",
    "elastic_results = []\n",
    "for l1_ratio in l1_ratios:\n",
    "    en_cv = ElasticNetCV(\n",
    "        l1_ratio=l1_ratio, \n",
    "        alphas=np.logspace(-6, -2, 50),\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        max_iter=10000\n",
    "    )\n",
    "    en_cv.fit(X_lasso_scaled, y_lasso)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    predictions = en_cv.predict(X_lasso_scaled)\n",
    "    mse = mean_squared_error(y_lasso, predictions)\n",
    "    n_selected = np.sum(en_cv.coef_ != 0)\n",
    "    \n",
    "    elastic_results.append({\n",
    "        'l1_ratio': l1_ratio,\n",
    "        'alpha': en_cv.alpha_,\n",
    "        'mse': mse,\n",
    "        'n_features': n_selected,\n",
    "        'coef': en_cv.coef_.copy()\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(elastic_results)\n",
    "print(\"=\" * 70)\n",
    "print(\"ELASTIC NET: VARYING L1 RATIO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'L1 Ratio':<12} {'Alpha (λ)':<15} {'MSE':<15} {'# Features':<12}\")\n",
    "print(\"-\" * 54)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['l1_ratio']:<12.2f} {row['alpha']:<15.6f} {row['mse']:<15.8f} {row['n_features']:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9fc4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Elastic Net results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Features selected vs L1 ratio\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(results_df['l1_ratio'], results_df['n_features'], 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('L1 Ratio (α)')\n",
    "ax1.set_ylabel('Number of Selected Features')\n",
    "ax1.set_title('Feature Sparsity vs L1 Ratio')\n",
    "ax1.axhline(y=len(feature_names), color='gray', linestyle='--', alpha=0.5, label=f'Total: {len(feature_names)}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: MSE vs L1 ratio\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(results_df['l1_ratio'], results_df['mse'], 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('L1 Ratio (α)')\n",
    "ax2.set_ylabel('Mean Squared Error')\n",
    "ax2.set_title('Prediction Error vs L1 Ratio')\n",
    "\n",
    "# Plot 3: Compare coefficients - Ridge vs Lasso vs Elastic Net\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "# Get models\n",
    "ridge_model = Ridge(alpha=optimal_lambda_cv)\n",
    "ridge_model.fit(X_lasso_scaled, y_lasso)\n",
    "\n",
    "lasso_model = Lasso(alpha=lasso_cv.alpha_, max_iter=10000)\n",
    "lasso_model.fit(X_lasso_scaled, y_lasso)\n",
    "\n",
    "en_model = ElasticNet(alpha=results_df.loc[results_df['l1_ratio']==0.5, 'alpha'].values[0], \n",
    "                      l1_ratio=0.5, max_iter=10000)\n",
    "en_model.fit(X_lasso_scaled, y_lasso)\n",
    "\n",
    "# Only plot first 7 (current returns) for clarity\n",
    "n_show = 7\n",
    "x_pos = np.arange(n_show)\n",
    "width = 0.25\n",
    "\n",
    "ax3.bar(x_pos - width, ridge_model.coef_[:n_show], width, label='Ridge', color='steelblue')\n",
    "ax3.bar(x_pos, en_model.coef_[:n_show], width, label='Elastic Net', color='green')\n",
    "ax3.bar(x_pos + width, lasso_model.coef_[:n_show], width, label='Lasso', color='coral')\n",
    "ax3.set_xlabel('Factor')\n",
    "ax3.set_ylabel('Coefficient')\n",
    "ax3.set_title('Coefficient Comparison: Current Returns')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(feature_names[:n_show], rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot 4: Coefficient magnitude comparison (L2 norm)\n",
    "ax4 = axes[1, 1]\n",
    "norms = {\n",
    "    'OLS': np.linalg.norm(LinearRegression().fit(X_lasso_scaled, y_lasso).coef_),\n",
    "    'Ridge': np.linalg.norm(ridge_model.coef_),\n",
    "    'Elastic Net': np.linalg.norm(en_model.coef_),\n",
    "    'Lasso': np.linalg.norm(lasso_model.coef_)\n",
    "}\n",
    "ax4.bar(norms.keys(), norms.values(), color=['gray', 'steelblue', 'green', 'coral'])\n",
    "ax4.set_ylabel('L2 Norm of Coefficients')\n",
    "ax4.set_title('Coefficient Shrinkage Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636b9be4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 5: Fama-MacBeth Cross-Sectional Regression\n",
    "\n",
    "## 5.1 The Cross-Sectional Approach\n",
    "\n",
    "While time-series regression estimates how one asset responds to factors over time, **cross-sectional regression** estimates how factor exposures explain return differences **across assets** at each point in time.\n",
    "\n",
    "### Two-Pass Methodology\n",
    "\n",
    "**Pass 1: Time-Series Regression (Estimate Betas)**\n",
    "\n",
    "For each asset $i$, run time-series regression:\n",
    "\n",
    "$$R_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,1}F_{1,t} + ... + \\beta_{i,K}F_{K,t} + \\epsilon_{i,t}$$\n",
    "\n",
    "This gives us $\\hat{\\beta}_i$ for each asset.\n",
    "\n",
    "**Pass 2: Cross-Sectional Regression (Estimate Risk Premia)**\n",
    "\n",
    "At each time $t$, run cross-sectional regression:\n",
    "\n",
    "$$R_{i,t} = \\gamma_{0,t} + \\gamma_{1,t}\\hat{\\beta}_{i,1} + ... + \\gamma_{K,t}\\hat{\\beta}_{i,K} + \\eta_{i,t}$$\n",
    "\n",
    "Average the $\\gamma$ estimates over time:\n",
    "\n",
    "$$\\hat{\\gamma}_k = \\frac{1}{T}\\sum_{t=1}^{T} \\hat{\\gamma}_{k,t}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Why Fama-MacBeth?\n",
    "\n",
    "### Handles Panel Data Issues\n",
    "\n",
    "1. **Cross-sectional correlation**: Asset returns are correlated at each $t$\n",
    "2. **Time-varying coefficients**: Risk premia change over time\n",
    "3. **Standard errors**: Fama-MacBeth provides valid standard errors\n",
    "\n",
    "### Standard Error Calculation\n",
    "\n",
    "$$SE(\\hat{\\gamma}_k) = \\frac{\\sigma(\\hat{\\gamma}_{k,t})}{\\sqrt{T}}$$\n",
    "\n",
    "Where $\\sigma(\\hat{\\gamma}_{k,t})$ is the time-series standard deviation of the monthly $\\gamma$ estimates.\n",
    "\n",
    "**Key advantage**: This automatically accounts for cross-sectional correlation!\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Financial Interpretation\n",
    "\n",
    "- $\\gamma_0$: **Zero-beta rate** (return of a portfolio with zero factor exposure)\n",
    "- $\\gamma_k$: **Factor risk premium** (expected return per unit of factor exposure)\n",
    "\n",
    "If CAPM holds:\n",
    "- $\\gamma_0 = R_f$ (zero-beta rate equals risk-free rate)\n",
    "- $\\gamma_1 = E[R_m - R_f]$ (market risk premium)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6177c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 5: Fama-MacBeth Cross-Sectional Regression\n",
    "\n",
    "# Use sector ETFs as our cross-section of assets\n",
    "sector_tickers = ['XLK', 'XLF', 'XLE', 'XLV', 'XLI', 'XLP', 'XLY', 'XLU', 'XLB']\n",
    "factor_ticker = 'SPY'  # Market factor\n",
    "\n",
    "# Download data\n",
    "all_tickers = sector_tickers + [factor_ticker]\n",
    "sector_prices = yf.download(all_tickers, start='2015-01-01', end='2024-01-01')['Adj Close']\n",
    "sector_returns = sector_prices.pct_change().dropna()\n",
    "\n",
    "# Convert to monthly returns for Fama-MacBeth\n",
    "monthly_returns = sector_returns.resample('M').apply(lambda x: (1 + x).prod() - 1)\n",
    "\n",
    "print(f\"Monthly data: {len(monthly_returns)} months\")\n",
    "print(f\"Assets: {sector_tickers}\")\n",
    "monthly_returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 1: Estimate betas for each sector using rolling window\n",
    "window = 36  # 3-year rolling window\n",
    "\n",
    "# Store rolling betas\n",
    "betas_df = pd.DataFrame(index=monthly_returns.index, columns=sector_tickers)\n",
    "\n",
    "for ticker in sector_tickers:\n",
    "    # Rolling regression to estimate time-varying betas\n",
    "    y = monthly_returns[ticker]\n",
    "    X = sm.add_constant(monthly_returns[factor_ticker])\n",
    "    \n",
    "    try:\n",
    "        rolling_model = RollingOLS(y, X, window=window)\n",
    "        rolling_results = rolling_model.fit()\n",
    "        betas_df[ticker] = rolling_results.params[factor_ticker]\n",
    "    except:\n",
    "        # Fallback to simple rolling calculation\n",
    "        for i in range(window, len(monthly_returns)):\n",
    "            y_window = monthly_returns[ticker].iloc[i-window:i]\n",
    "            X_window = monthly_returns[factor_ticker].iloc[i-window:i]\n",
    "            model = sm.OLS(y_window, sm.add_constant(X_window)).fit()\n",
    "            betas_df.loc[monthly_returns.index[i], ticker] = model.params.iloc[1]\n",
    "\n",
    "betas_df = betas_df.dropna()\n",
    "print(f\"Rolling betas calculated for {len(betas_df)} months\")\n",
    "betas_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass 2: Cross-Sectional Regressions\n",
    "# At each month, regress returns on lagged betas\n",
    "\n",
    "# Align returns with lagged betas\n",
    "returns_aligned = monthly_returns[sector_tickers].loc[betas_df.index]\n",
    "betas_lagged = betas_df.shift(1).dropna()\n",
    "returns_aligned = returns_aligned.loc[betas_lagged.index]\n",
    "\n",
    "# Store cross-sectional gamma estimates\n",
    "gamma_0 = []  # Intercepts\n",
    "gamma_1 = []  # Market risk premium estimates\n",
    "dates = []\n",
    "\n",
    "for date in betas_lagged.index:\n",
    "    # Get returns and betas for this month\n",
    "    y_cs = returns_aligned.loc[date].values\n",
    "    X_cs = betas_lagged.loc[date].values.astype(float)\n",
    "    \n",
    "    # Skip if any NaN\n",
    "    if np.any(np.isnan(y_cs)) or np.any(np.isnan(X_cs)):\n",
    "        continue\n",
    "    \n",
    "    # Cross-sectional regression\n",
    "    X_cs_const = sm.add_constant(X_cs)\n",
    "    model_cs = sm.OLS(y_cs, X_cs_const).fit()\n",
    "    \n",
    "    gamma_0.append(model_cs.params[0])\n",
    "    gamma_1.append(model_cs.params[1])\n",
    "    dates.append(date)\n",
    "\n",
    "# Convert to arrays\n",
    "gamma_0 = np.array(gamma_0)\n",
    "gamma_1 = np.array(gamma_1)\n",
    "\n",
    "print(f\"Cross-sectional regressions: {len(gamma_1)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-MacBeth Risk Premium Estimates\n",
    "print(\"=\" * 60)\n",
    "print(\"FAMA-MACBETH REGRESSION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate means and standard errors\n",
    "gamma_0_mean = np.mean(gamma_0)\n",
    "gamma_0_se = np.std(gamma_0) / np.sqrt(len(gamma_0))\n",
    "gamma_0_t = gamma_0_mean / gamma_0_se\n",
    "\n",
    "gamma_1_mean = np.mean(gamma_1)\n",
    "gamma_1_se = np.std(gamma_1) / np.sqrt(len(gamma_1))\n",
    "gamma_1_t = gamma_1_mean / gamma_1_se\n",
    "\n",
    "print(f\"\\n{'Parameter':<20} {'Estimate':<12} {'Std Error':<12} {'t-stat':<10} {'Annualized':<12}\")\n",
    "print(\"-\" * 66)\n",
    "print(f\"{'γ₀ (Zero-Beta)':<20} {gamma_0_mean:>11.4f} {gamma_0_se:>11.4f} {gamma_0_t:>9.2f} {gamma_0_mean*12:>11.2%}\")\n",
    "print(f\"{'γ₁ (Market Premium)':<20} {gamma_1_mean:>11.4f} {gamma_1_se:>11.4f} {gamma_1_t:>9.2f} {gamma_1_mean*12:>11.2%}\")\n",
    "\n",
    "print(f\"\\n\\nInterpretation:\")\n",
    "print(f\"  - Zero-beta return: {gamma_0_mean*12:.2%} per year\")\n",
    "print(f\"  - Market risk premium: {gamma_1_mean*12:.2%} per year per unit of beta\")\n",
    "print(f\"  - Market premium significant: {'Yes' if abs(gamma_1_t) > 1.96 else 'No'} (t={gamma_1_t:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b93d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Fama-MacBeth Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Time series of gamma estimates\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(dates, gamma_1, 'b-', alpha=0.5, label='Monthly γ₁')\n",
    "ax1.axhline(y=gamma_1_mean, color='r', linestyle='--', linewidth=2, \n",
    "            label=f'Mean = {gamma_1_mean*12:.1%} p.a.')\n",
    "ax1.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('γ₁ (Market Premium)')\n",
    "ax1.set_title('Time-Varying Market Risk Premium Estimates')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Distribution of gamma estimates\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(gamma_1 * 12, bins=30, density=True, alpha=0.7, color='steelblue', edgecolor='white')\n",
    "ax2.axvline(x=gamma_1_mean * 12, color='r', linestyle='--', linewidth=2, \n",
    "            label=f'Mean = {gamma_1_mean*12:.1%}')\n",
    "ax2.set_xlabel('Annualized Market Premium')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution of Monthly Risk Premium Estimates')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Security Market Line (final month)\n",
    "ax3 = axes[1, 0]\n",
    "final_betas = betas_df.iloc[-1].values.astype(float)\n",
    "avg_returns = returns_aligned.mean() * 12  # Annualized\n",
    "\n",
    "ax3.scatter(final_betas, avg_returns.values * 100, s=100, c='steelblue', edgecolor='white', linewidth=2)\n",
    "for i, ticker in enumerate(sector_tickers):\n",
    "    ax3.annotate(ticker, (final_betas[i], avg_returns.values[i] * 100), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "# Plot SML\n",
    "beta_range = np.linspace(0, 2, 100)\n",
    "sml = (gamma_0_mean + gamma_1_mean * beta_range) * 12 * 100\n",
    "ax3.plot(beta_range, sml, 'r--', linewidth=2, label='Security Market Line')\n",
    "ax3.set_xlabel('Beta')\n",
    "ax3.set_ylabel('Average Annual Return (%)')\n",
    "ax3.set_title('Security Market Line (Cross-Sectional View)')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Rolling betas over time\n",
    "ax4 = axes[1, 1]\n",
    "for ticker in sector_tickers:\n",
    "    ax4.plot(betas_df.index, betas_df[ticker], label=ticker, alpha=0.7)\n",
    "ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Beta')\n",
    "ax4.set_title('Rolling 36-Month Betas by Sector')\n",
    "ax4.legend(loc='upper left', ncol=3, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b5d74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 6: WLS and Robust Regression\n",
    "\n",
    "## 6.1 Heteroskedasticity in Finance\n",
    "\n",
    "Financial returns are almost always **heteroskedastic** - variance changes over time:\n",
    "- Volatility clustering (GARCH effects)\n",
    "- Higher volatility during market stress\n",
    "- Different volatility across assets\n",
    "\n",
    "When $Var(\\epsilon_i) = \\sigma_i^2 \\neq \\sigma^2$:\n",
    "- OLS estimates are still **unbiased**\n",
    "- But standard errors are **wrong** (usually too small)\n",
    "- OLS is no longer **efficient** (not minimum variance)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2 Weighted Least Squares (WLS)\n",
    "\n",
    "### Formulation\n",
    "\n",
    "If $Var(\\epsilon) = \\Omega$ (non-scalar covariance matrix), the GLS estimator is:\n",
    "\n",
    "$$\\hat{\\beta}_{GLS} = (X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}Y$$\n",
    "\n",
    "For **WLS** (diagonal $\\Omega$), with weights $w_i = 1/\\sigma_i^2$:\n",
    "\n",
    "$$\\hat{\\beta}_{WLS} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} w_i(y_i - x_i'\\beta)^2$$\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Observations with lower variance get **higher weights**\n",
    "- High-volatility observations contribute less to the fit\n",
    "- Makes regression more stable across different volatility regimes\n",
    "\n",
    "### Common Weight Specifications in Finance\n",
    "\n",
    "1. **Inverse variance**: $w_i = 1/\\hat{\\sigma}_i^2$ (from GARCH)\n",
    "2. **Inverse squared return**: $w_i = 1/r_i^2$ (simple approximation)\n",
    "3. **Time decay**: $w_i = \\lambda^{T-t}$ (exponential decay)\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3 Robust Standard Errors (White/Newey-West)\n",
    "\n",
    "### Heteroskedasticity-Consistent (White) Standard Errors\n",
    "\n",
    "Instead of transforming the regression, adjust the standard errors:\n",
    "\n",
    "$$\\widehat{Var}(\\hat{\\beta}) = (X'X)^{-1} \\left( \\sum_{i=1}^{n} \\hat{\\epsilon}_i^2 x_i x_i' \\right) (X'X)^{-1}$$\n",
    "\n",
    "This is the **sandwich estimator** - valid under heteroskedasticity.\n",
    "\n",
    "### HAC (Newey-West) Standard Errors\n",
    "\n",
    "Also accounts for autocorrelation:\n",
    "\n",
    "$$\\widehat{Var}_{HAC}(\\hat{\\beta}) = (X'X)^{-1} \\hat{\\Sigma}_{HAC} (X'X)^{-1}$$\n",
    "\n",
    "Where $\\hat{\\Sigma}_{HAC}$ includes autocovariance terms up to lag $L$.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.4 Robust Regression (M-Estimation)\n",
    "\n",
    "When outliers are present, OLS can be severely affected. Robust regression uses different loss functions:\n",
    "\n",
    "| Method | Loss Function | Property |\n",
    "|--------|--------------|----------|\n",
    "| OLS | $\\rho(r) = r^2$ | Sensitive to outliers |\n",
    "| Huber | $\\rho(r) = r^2$ if $|r| \\leq k$, else $2k|r| - k^2$ | Bounded influence |\n",
    "| Bisquare | $\\rho(r) = 1 - (1 - (r/k)^2)^3$ if $|r| \\leq k$, else 1 | Downweights outliers |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce99767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6: WLS and Robust Regression\n",
    "\n",
    "# Prepare data - CAPM regression with heteroskedasticity\n",
    "market_ret = returns['SPY'].values\n",
    "asset_ret = returns['XLE'].values  # Energy sector - highly volatile\n",
    "\n",
    "# Calculate rolling volatility for weights\n",
    "rolling_vol = returns['XLE'].rolling(20).std().dropna()\n",
    "start_idx = len(returns) - len(rolling_vol)\n",
    "\n",
    "market_ret_wls = market_ret[start_idx:]\n",
    "asset_ret_wls = asset_ret[start_idx:]\n",
    "weights = 1 / (rolling_vol.values ** 2)  # Inverse variance weights\n",
    "weights = weights / weights.mean()  # Normalize\n",
    "\n",
    "print(f\"Data points: {len(asset_ret_wls)}\")\n",
    "print(f\"Weight range: {weights.min():.2f} to {weights.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OLS, WLS, and Robust regression\n",
    "X_reg = sm.add_constant(market_ret_wls)\n",
    "\n",
    "# 1. OLS with regular standard errors\n",
    "model_ols = sm.OLS(asset_ret_wls, X_reg).fit()\n",
    "\n",
    "# 2. OLS with White (heteroskedasticity-robust) standard errors\n",
    "model_hc = sm.OLS(asset_ret_wls, X_reg).fit(cov_type='HC1')\n",
    "\n",
    "# 3. OLS with Newey-West (HAC) standard errors\n",
    "model_hac = sm.OLS(asset_ret_wls, X_reg).fit(cov_type='HAC', cov_kwds={'maxlags': 5})\n",
    "\n",
    "# 4. WLS with inverse volatility weights\n",
    "model_wls = sm.WLS(asset_ret_wls, X_reg, weights=weights).fit()\n",
    "\n",
    "# 5. Robust regression (Huber)\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "model_robust = RLM(asset_ret_wls, X_reg, M=sm.robust.norms.HuberT()).fit()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REGRESSION COMPARISON: XLE ~ SPY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Model':<25} {'Alpha':<12} {'SE(α)':<10} {'Beta':<12} {'SE(β)':<10} {'R²':<8}\")\n",
    "print(\"-\" * 77)\n",
    "print(f\"{'OLS (Standard)':<25} {model_ols.params[0]:>11.6f} {model_ols.bse[0]:>9.6f} {model_ols.params[1]:>11.4f} {model_ols.bse[1]:>9.4f} {model_ols.rsquared:>7.4f}\")\n",
    "print(f\"{'OLS (White HC1)':<25} {model_hc.params[0]:>11.6f} {model_hc.bse[0]:>9.6f} {model_hc.params[1]:>11.4f} {model_hc.bse[1]:>9.4f} {model_hc.rsquared:>7.4f}\")\n",
    "print(f\"{'OLS (Newey-West)':<25} {model_hac.params[0]:>11.6f} {model_hac.bse[0]:>9.6f} {model_hac.params[1]:>11.4f} {model_hac.bse[1]:>9.4f} {model_hac.rsquared:>7.4f}\")\n",
    "print(f\"{'WLS (Inv. Volatility)':<25} {model_wls.params[0]:>11.6f} {model_wls.bse[0]:>9.6f} {model_wls.params[1]:>11.4f} {model_wls.bse[1]:>9.4f} {model_wls.rsquared:>7.4f}\")\n",
    "print(f\"{'Robust (Huber)':<25} {model_robust.params[0]:>11.6f} {model_robust.bse[0]:>9.6f} {model_robust.params[1]:>11.4f} {model_robust.bse[1]:>9.4f} {'N/A':>7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize heteroskedasticity and model fits\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Scatter with OLS vs WLS fits\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(market_ret_wls * 100, asset_ret_wls * 100, alpha=0.3, s=10, c='gray')\n",
    "\n",
    "x_line = np.linspace(market_ret_wls.min(), market_ret_wls.max(), 100)\n",
    "ax1.plot(x_line * 100, (model_ols.params[0] + model_ols.params[1] * x_line) * 100, \n",
    "         'b-', linewidth=2, label=f'OLS (β={model_ols.params[1]:.3f})')\n",
    "ax1.plot(x_line * 100, (model_wls.params[0] + model_wls.params[1] * x_line) * 100, \n",
    "         'r--', linewidth=2, label=f'WLS (β={model_wls.params[1]:.3f})')\n",
    "ax1.plot(x_line * 100, (model_robust.params[0] + model_robust.params[1] * x_line) * 100, \n",
    "         'g:', linewidth=2, label=f'Robust (β={model_robust.params[1]:.3f})')\n",
    "ax1.set_xlabel('Market Return (SPY) %')\n",
    "ax1.set_ylabel('Asset Return (XLE) %')\n",
    "ax1.set_title('OLS vs WLS vs Robust Regression')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: WLS weights over time\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(rolling_vol.index, weights, 'b-', alpha=0.7)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Weight (1/σ²)')\n",
    "ax2.set_title('WLS Weights Over Time (Inverse Variance)')\n",
    "ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Residuals vs fitted (heteroskedasticity check)\n",
    "ax3 = axes[1, 0]\n",
    "residuals_ols = model_ols.resid\n",
    "ax3.scatter(model_ols.fittedvalues * 100, residuals_ols * 100, alpha=0.3, s=10)\n",
    "ax3.axhline(y=0, color='r', linestyle='-', linewidth=2)\n",
    "ax3.set_xlabel('Fitted Values (%)')\n",
    "ax3.set_ylabel('OLS Residuals (%)')\n",
    "ax3.set_title('OLS Residuals (Evidence of Heteroskedasticity)')\n",
    "\n",
    "# Plot 4: Standard error comparison\n",
    "ax4 = axes[1, 1]\n",
    "models = ['OLS\\n(Standard)', 'OLS\\n(White)', 'OLS\\n(Newey-West)', 'WLS', 'Robust']\n",
    "se_beta = [model_ols.bse[1], model_hc.bse[1], model_hac.bse[1], model_wls.bse[1], model_robust.bse[1]]\n",
    "colors = ['steelblue', 'coral', 'green', 'purple', 'orange']\n",
    "ax4.bar(models, se_beta, color=colors)\n",
    "ax4.set_ylabel('Standard Error of Beta')\n",
    "ax4.set_title('Standard Error Comparison')\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334092e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate impact of outliers on different methods\n",
    "\n",
    "# Add synthetic outliers to demonstrate robust regression\n",
    "np.random.seed(42)\n",
    "n_outliers = 10\n",
    "outlier_idx = np.random.choice(len(market_ret_wls), n_outliers, replace=False)\n",
    "\n",
    "asset_ret_outliers = asset_ret_wls.copy()\n",
    "asset_ret_outliers[outlier_idx] = asset_ret_outliers[outlier_idx] * 5  # Amplify outliers\n",
    "\n",
    "# Fit models with outliers\n",
    "model_ols_out = sm.OLS(asset_ret_outliers, X_reg).fit()\n",
    "model_robust_out = RLM(asset_ret_outliers, X_reg, M=sm.robust.norms.HuberT()).fit()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPACT OF OUTLIERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal Data:\")\n",
    "print(f\"  OLS Beta: {model_ols.params[1]:.4f}\")\n",
    "print(f\"  Robust Beta: {model_robust.params[1]:.4f}\")\n",
    "print(f\"\\nWith {n_outliers} Amplified Outliers:\")\n",
    "print(f\"  OLS Beta: {model_ols_out.params[1]:.4f} (change: {model_ols_out.params[1] - model_ols.params[1]:+.4f})\")\n",
    "print(f\"  Robust Beta: {model_robust_out.params[1]:.4f} (change: {model_robust_out.params[1] - model_robust.params[1]:+.4f})\")\n",
    "print(f\"\\n→ Robust regression is {abs(model_ols_out.params[1] - model_ols.params[1])/abs(model_robust_out.params[1] - model_robust.params[1]):.1f}x more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf876f00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Day 7: Interview Review & Key Takeaways\n",
    "\n",
    "## 7.1 Summary: When to Use Each Method\n",
    "\n",
    "| Scenario | Recommended Method | Reason |\n",
    "|----------|-------------------|--------|\n",
    "| Simple factor regression | OLS | Baseline, interpretable |\n",
    "| Multicollinear factors | Ridge | Stabilizes coefficients |\n",
    "| Many candidate factors | Lasso | Automatic selection |\n",
    "| Correlated factors + sparsity | Elastic Net | Grouped selection |\n",
    "| Cross-sectional asset pricing | Fama-MacBeth | Handles correlation |\n",
    "| Changing volatility | WLS | Efficient estimation |\n",
    "| Potential outliers | Robust regression | Bounded influence |\n",
    "| Serial correlation | Newey-West SE | Valid inference |\n",
    "\n",
    "---\n",
    "\n",
    "## 7.2 Common Interview Questions\n",
    "\n",
    "### Q1: What are the Gauss-Markov assumptions?\n",
    "\n",
    "**Answer**: The five assumptions for OLS to be BLUE:\n",
    "1. Linearity in parameters\n",
    "2. Random sampling / Full rank of X\n",
    "3. Zero conditional mean of errors (exogeneity)\n",
    "4. Homoskedasticity (constant error variance)\n",
    "5. No autocorrelation\n",
    "\n",
    "If (4) or (5) fail → OLS is still unbiased but inefficient, use robust SE or WLS.\n",
    "\n",
    "### Q2: Explain the bias-variance tradeoff\n",
    "\n",
    "**Answer**: \n",
    "- **Bias**: Error from wrong model assumptions (underfitting)\n",
    "- **Variance**: Error from sensitivity to training data (overfitting)\n",
    "- **Tradeoff**: Reducing one typically increases the other\n",
    "- **Regularization**: Intentionally adds bias to reduce variance\n",
    "\n",
    "### Q3: When would you use Lasso vs Ridge?\n",
    "\n",
    "**Answer**:\n",
    "- **Lasso**: When you believe only a subset of features are relevant (sparse model). Good for feature selection.\n",
    "- **Ridge**: When most features are relevant but potentially correlated. Never sets coefficients exactly to zero.\n",
    "- **Elastic Net**: When features are correlated AND you want sparsity.\n",
    "\n",
    "### Q4: What is the Fama-MacBeth procedure?\n",
    "\n",
    "**Answer**: A two-pass methodology to estimate factor risk premia:\n",
    "1. **Time-series pass**: Estimate factor betas for each asset\n",
    "2. **Cross-sectional pass**: At each time, regress returns on betas\n",
    "3. **Average**: Take time-series average of cross-sectional coefficients\n",
    "\n",
    "Advantage: Standard errors automatically account for cross-sectional correlation.\n",
    "\n",
    "### Q5: How do you handle heteroskedasticity?\n",
    "\n",
    "**Answer**: Several approaches:\n",
    "1. **Robust SE (White)**: Adjust standard errors, keep OLS estimates\n",
    "2. **HAC SE (Newey-West)**: Also handles autocorrelation\n",
    "3. **WLS**: Weight observations by inverse variance\n",
    "4. **GARCH**: Model time-varying volatility explicitly\n",
    "\n",
    "---\n",
    "\n",
    "## 7.3 Mathematical Formulas Summary\n",
    "\n",
    "### Estimators\n",
    "\n",
    "$$\\hat{\\beta}_{OLS} = (X'X)^{-1}X'Y$$\n",
    "\n",
    "$$\\hat{\\beta}_{Ridge} = (X'X + \\lambda I)^{-1}X'Y$$\n",
    "\n",
    "$$\\hat{\\beta}_{Lasso} = \\arg\\min_\\beta \\left\\{ ||Y - X\\beta||_2^2 + \\lambda||\\beta||_1 \\right\\}$$\n",
    "\n",
    "$$\\hat{\\beta}_{EN} = \\arg\\min_\\beta \\left\\{ ||Y - X\\beta||_2^2 + \\lambda\\alpha||\\beta||_1 + \\frac{\\lambda(1-\\alpha)}{2}||\\beta||_2^2 \\right\\}$$\n",
    "\n",
    "$$\\hat{\\beta}_{WLS} = (X'WX)^{-1}X'WY$$\n",
    "\n",
    "### CAPM\n",
    "\n",
    "$$E[R_i] - R_f = \\beta_i(E[R_m] - R_f)$$\n",
    "\n",
    "$$\\beta_i = \\frac{Cov(R_i, R_m)}{Var(R_m)}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9801cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary: Model Performance Comparison\n",
    "\n",
    "# Use the same data for fair comparison\n",
    "X_final = X_lasso_scaled\n",
    "y_final = y_lasso\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=optimal_lambda_cv),\n",
    "    'Lasso': Lasso(alpha=lasso_cv.alpha_, max_iter=10000),\n",
    "    'Elastic Net': ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_final, y_final, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    # Fit full model for coefficient count\n",
    "    model.fit(X_final, y_final)\n",
    "    n_coef = np.sum(model.coef_ != 0)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'CV MSE': -cv_scores.mean(),\n",
    "        'CV MSE Std': cv_scores.std(),\n",
    "        'Non-zero Coef': n_coef,\n",
    "        'Coef Norm': np.linalg.norm(model.coef_)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL MODEL COMPARISON (Time Series CV)\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76acf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Final comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: CV MSE Comparison\n",
    "ax1 = axes[0]\n",
    "colors = ['gray', 'steelblue', 'coral', 'green']\n",
    "bars = ax1.bar(results_df['Model'], results_df['CV MSE'], color=colors, \n",
    "               yerr=results_df['CV MSE Std'], capsize=5)\n",
    "ax1.set_ylabel('Cross-Validation MSE')\n",
    "ax1.set_title('Prediction Error Comparison')\n",
    "ax1.tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Plot 2: Number of Features\n",
    "ax2 = axes[1]\n",
    "ax2.bar(results_df['Model'], results_df['Non-zero Coef'], color=colors)\n",
    "ax2.axhline(y=len(feature_names), color='gray', linestyle='--', alpha=0.5, label=f'Total: {len(feature_names)}')\n",
    "ax2.set_ylabel('Number of Non-zero Coefficients')\n",
    "ax2.set_title('Model Sparsity')\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Coefficient Shrinkage\n",
    "ax3 = axes[2]\n",
    "ax3.bar(results_df['Model'], results_df['Coef Norm'], color=colors)\n",
    "ax3.set_ylabel('L2 Norm of Coefficients')\n",
    "ax3.set_title('Coefficient Magnitude')\n",
    "ax3.tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEK 5.1 COMPLETE: Linear Models for Finance\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
