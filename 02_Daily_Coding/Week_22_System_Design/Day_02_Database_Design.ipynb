{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d321540",
   "metadata": {},
   "source": [
    "# Day 02: Database Design for Tick Data and Time Series\n",
    "\n",
    "## Week 22: System Design for Quantitative Finance\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand database architecture choices for financial time series data\n",
    "- Design efficient schemas for tick data (trades, quotes, order book)\n",
    "- Implement indexing strategies for time-based queries\n",
    "- Master partitioning and data retention policies\n",
    "- Optimize queries for backtesting and analytics workloads\n",
    "\n",
    "### Why Database Design Matters in Quant Finance\n",
    "- **Data Volume**: A single liquid stock can generate 100K+ ticks per day\n",
    "- **Query Patterns**: Backtests require fast range queries over years of data\n",
    "- **Real-time Ingestion**: Production systems must handle thousands of updates/second\n",
    "- **Cost Efficiency**: Proper design can reduce storage costs by 10x or more\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Import Required Libraries](#1-import-required-libraries)\n",
    "2. [Database Options for Financial Data](#2-database-options)\n",
    "3. [Tick Data Schema Design](#3-tick-data-schema)\n",
    "4. [OHLCV Time Series Schema](#4-ohlcv-schema)\n",
    "5. [Database Connection with Connection Pooling](#5-database-connection)\n",
    "6. [SQLAlchemy ORM Models](#6-orm-models)\n",
    "7. [Indexing Strategy for Time Series](#7-indexing-strategy)\n",
    "8. [Data Partitioning by Date](#8-partitioning)\n",
    "9. [High-Throughput Data Insertion](#9-data-insertion)\n",
    "10. [Query Optimization Patterns](#10-query-optimization)\n",
    "11. [Data Retention Policies](#11-retention-policies)\n",
    "12. [Best Practices Summary](#12-best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e372a48",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries <a id=\"1-import-required-libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8029bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# SQLAlchemy for ORM and database operations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlalchemy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     create_engine, MetaData, Table, Column, Index,\n\u001b[32m     20\u001b[39m     Integer, BigInteger, String, Float, DateTime, Date,\n\u001b[32m     21\u001b[39m     Numeric, Boolean, Text, JSON, Enum \u001b[38;5;28;01mas\u001b[39;00m SQLEnum,\n\u001b[32m     22\u001b[39m     ForeignKey, UniqueConstraint, CheckConstraint,\n\u001b[32m     23\u001b[39m     PrimaryKeyConstraint, func, text, event, inspect\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlalchemy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01morm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     declarative_base, sessionmaker, Session, relationship,\n\u001b[32m     27\u001b[39m     scoped_session\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlalchemy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdialects\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpostgresql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     TIMESTAMP, INTERVAL, ARRAY, JSONB, UUID, BYTEA,\n\u001b[32m     31\u001b[39m     insert \u001b[38;5;28;01mas\u001b[39;00m pg_insert\n\u001b[32m     32\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta, date\n",
    "from decimal import Decimal\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SQLAlchemy for ORM and database operations\n",
    "from sqlalchemy import (\n",
    "    create_engine, MetaData, Table, Column, Index,\n",
    "    Integer, BigInteger, String, Float, DateTime, Date,\n",
    "    Numeric, Boolean, Text, JSON, Enum as SQLEnum,\n",
    "    ForeignKey, UniqueConstraint, CheckConstraint,\n",
    "    PrimaryKeyConstraint, func, text, event, inspect\n",
    ")\n",
    "from sqlalchemy.orm import (\n",
    "    declarative_base, sessionmaker, Session, relationship,\n",
    "    scoped_session\n",
    ")\n",
    "from sqlalchemy.dialects.postgresql import (\n",
    "    TIMESTAMP, INTERVAL, ARRAY, JSONB, UUID, BYTEA,\n",
    "    insert as pg_insert\n",
    ")\n",
    "from sqlalchemy.pool import QueuePool\n",
    "from sqlalchemy.exc import IntegrityError, OperationalError\n",
    "\n",
    "# For connection pooling and async operations\n",
    "from contextlib import contextmanager\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.6f}')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“… Notebook Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa670c80",
   "metadata": {},
   "source": [
    "## 2. Database Options for Financial Data <a id=\"2-database-options\"></a>\n",
    "\n",
    "### Overview of Database Technologies for Time Series\n",
    "\n",
    "| Database | Type | Best For | Throughput | Compression | Notes |\n",
    "|----------|------|----------|------------|-------------|-------|\n",
    "| **PostgreSQL** | Relational | General purpose, joins | Medium | Low | Most versatile |\n",
    "| **TimescaleDB** | Time-series (PG extension) | Time-series with SQL | High | High | Best balance |\n",
    "| **InfluxDB** | Time-series native | Metrics, IoT | Very High | Very High | No joins |\n",
    "| **QuestDB** | Time-series native | Ultra-low latency | Extreme | High | SQL-like |\n",
    "| **kdb+/q** | Column-store | HFT, derivatives | Extreme | Custom | Industry standard, expensive |\n",
    "| **Arctic (MongoDB)** | Document + versioned | Research, versioned data | Medium | High | Python-native |\n",
    "| **ClickHouse** | Column-store OLAP | Analytics, aggregations | Very High | Very High | Great for reads |\n",
    "| **DuckDB** | Embedded OLAP | Local analytics | High | Medium | In-process |\n",
    "\n",
    "### Key Selection Criteria\n",
    "1. **Query patterns**: OLTP (many small queries) vs OLAP (few large queries)\n",
    "2. **Write patterns**: Batch vs streaming ingestion\n",
    "3. **Data volume**: GB vs TB vs PB scale\n",
    "4. **Latency requirements**: Milliseconds vs seconds acceptable\n",
    "5. **SQL compatibility**: Full SQL vs limited query language\n",
    "6. **Operational complexity**: Managed vs self-hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Comparison Helper Class\n",
    "@dataclass\n",
    "class DatabaseOption:\n",
    "    \"\"\"Compare database options for financial data storage.\"\"\"\n",
    "    name: str\n",
    "    db_type: str\n",
    "    write_throughput: str  # rows/second\n",
    "    query_latency: str\n",
    "    compression_ratio: str\n",
    "    sql_support: str\n",
    "    best_use_case: str\n",
    "    cost: str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.best_use_case}\"\n",
    "\n",
    "# Define database options for quant finance\n",
    "DATABASE_OPTIONS = [\n",
    "    DatabaseOption(\n",
    "        name=\"PostgreSQL + TimescaleDB\",\n",
    "        db_type=\"Time-series (SQL)\",\n",
    "        write_throughput=\"100K-500K rows/sec\",\n",
    "        query_latency=\"10-100ms\",\n",
    "        compression_ratio=\"10-20x\",\n",
    "        sql_support=\"Full SQL\",\n",
    "        best_use_case=\"General quant research, backtesting\",\n",
    "        cost=\"Free / Enterprise\"\n",
    "    ),\n",
    "    DatabaseOption(\n",
    "        name=\"QuestDB\",\n",
    "        db_type=\"Time-series native\",\n",
    "        write_throughput=\"1M+ rows/sec\",\n",
    "        query_latency=\"<10ms\",\n",
    "        compression_ratio=\"10-15x\",\n",
    "        sql_support=\"SQL-like (partial)\",\n",
    "        best_use_case=\"Real-time analytics, HFT monitoring\",\n",
    "        cost=\"Free / Enterprise\"\n",
    "    ),\n",
    "    DatabaseOption(\n",
    "        name=\"InfluxDB\",\n",
    "        db_type=\"Time-series native\",\n",
    "        write_throughput=\"500K rows/sec\",\n",
    "        query_latency=\"10-50ms\",\n",
    "        compression_ratio=\"20-50x\",\n",
    "        sql_support=\"InfluxQL / Flux\",\n",
    "        best_use_case=\"Metrics, monitoring, simple time-series\",\n",
    "        cost=\"Free / Cloud pricing\"\n",
    "    ),\n",
    "    DatabaseOption(\n",
    "        name=\"kdb+/q\",\n",
    "        db_type=\"Column-store\",\n",
    "        write_throughput=\"5M+ rows/sec\",\n",
    "        query_latency=\"<1ms\",\n",
    "        compression_ratio=\"5-10x\",\n",
    "        sql_support=\"q language\",\n",
    "        best_use_case=\"HFT, derivatives, real-time trading\",\n",
    "        cost=\"$$$$ (very expensive)\"\n",
    "    ),\n",
    "    DatabaseOption(\n",
    "        name=\"Arctic (MongoDB)\",\n",
    "        db_type=\"Document store\",\n",
    "        write_throughput=\"50K-100K rows/sec\",\n",
    "        query_latency=\"50-200ms\",\n",
    "        compression_ratio=\"5-15x\",\n",
    "        sql_support=\"Python API\",\n",
    "        best_use_case=\"Research, versioned datasets\",\n",
    "        cost=\"Free\"\n",
    "    ),\n",
    "    DatabaseOption(\n",
    "        name=\"ClickHouse\",\n",
    "        db_type=\"Column-store OLAP\",\n",
    "        write_throughput=\"1M+ rows/sec\",\n",
    "        query_latency=\"50-500ms\",\n",
    "        compression_ratio=\"15-30x\",\n",
    "        sql_support=\"SQL\",\n",
    "        best_use_case=\"Large-scale analytics, batch processing\",\n",
    "        cost=\"Free / Cloud pricing\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Display comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"DATABASE OPTIONS FOR QUANTITATIVE FINANCE\")\n",
    "print(\"=\" * 80)\n",
    "for db in DATABASE_OPTIONS:\n",
    "    print(f\"\\nðŸ“Š {db.name}\")\n",
    "    print(f\"   Type: {db.db_type}\")\n",
    "    print(f\"   Write: {db.write_throughput}\")\n",
    "    print(f\"   Latency: {db.query_latency}\")\n",
    "    print(f\"   Compression: {db.compression_ratio}\")\n",
    "    print(f\"   Best for: {db.best_use_case}\")\n",
    "    print(f\"   Cost: {db.cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625c021",
   "metadata": {},
   "source": [
    "## 3. Tick Data Schema Design <a id=\"3-tick-data-schema\"></a>\n",
    "\n",
    "### What is Tick Data?\n",
    "Tick data represents the most granular level of market data - every individual trade and quote update.\n",
    "\n",
    "### Types of Tick Data\n",
    "1. **Trade Ticks**: Actual executed transactions (price, volume, timestamp)\n",
    "2. **Quote Ticks**: Best bid/ask updates (bid price, ask price, sizes)\n",
    "3. **Order Book Snapshots**: Full depth of book at specific intervals\n",
    "4. **Order Book Deltas**: Changes to the order book (adds, modifies, deletes)\n",
    "\n",
    "### Key Schema Considerations\n",
    "- **Timestamp precision**: Nanosecond for HFT, microsecond for most use cases\n",
    "- **Price representation**: Decimal vs float (Decimal for accuracy)\n",
    "- **Symbol normalization**: Consistent ticker format across exchanges\n",
    "- **Exchange identification**: Multiple venues for same security\n",
    "- **Sequence numbers**: Order preservation and gap detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8522e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base for ORM models\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enums for tick data\n",
    "class TickType(str, Enum):\n",
    "    \"\"\"Type of tick event.\"\"\"\n",
    "    TRADE = \"trade\"\n",
    "    QUOTE = \"quote\"\n",
    "    ORDER_BOOK = \"order_book\"\n",
    "\n",
    "class TradeSide(str, Enum):\n",
    "    \"\"\"Side of a trade (aggressor side).\"\"\"\n",
    "    BUY = \"buy\"   # Buyer was aggressor (hit the ask)\n",
    "    SELL = \"sell\" # Seller was aggressor (hit the bid)\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "class Exchange(str, Enum):\n",
    "    \"\"\"Trading venue.\"\"\"\n",
    "    NYSE = \"NYSE\"\n",
    "    NASDAQ = \"NASDAQ\"\n",
    "    ARCA = \"ARCA\"\n",
    "    BATS = \"BATS\"\n",
    "    IEX = \"IEX\"\n",
    "    CME = \"CME\"\n",
    "    CBOE = \"CBOE\"\n",
    "    CRYPTO = \"CRYPTO\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRADE TICK SCHEMA\n",
    "# ============================================================================\n",
    "class TradeTick(Base):\n",
    "    \"\"\"\n",
    "    Schema for trade tick data - individual executed transactions.\n",
    "    \n",
    "    Storage estimate: ~100 bytes per row\n",
    "    For 1000 liquid stocks @ 50K trades/day each = 5GB/day uncompressed\n",
    "    \"\"\"\n",
    "    __tablename__ = 'trade_ticks'\n",
    "    \n",
    "    # Primary identification (composite primary key for efficiency)\n",
    "    id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Timestamp with nanosecond precision (stored as bigint nanoseconds from epoch)\n",
    "    # Using BigInteger for nanosecond precision - PostgreSQL TIMESTAMP only supports microseconds\n",
    "    timestamp_ns = Column(BigInteger, nullable=False, index=True)\n",
    "    \n",
    "    # Alternative: Timestamp with microsecond precision (native PostgreSQL)\n",
    "    timestamp = Column(TIMESTAMP(timezone=True, precision=6), nullable=False)\n",
    "    \n",
    "    # Symbol identification\n",
    "    symbol = Column(String(20), nullable=False, index=True)\n",
    "    \n",
    "    # Price as NUMERIC for exact decimal representation (avoiding float precision issues)\n",
    "    # Precision 18, Scale 8 supports most asset classes (crypto needs more decimals)\n",
    "    price = Column(Numeric(18, 8), nullable=False)\n",
    "    \n",
    "    # Volume/Size\n",
    "    volume = Column(BigInteger, nullable=False)  # BigInt for large volumes\n",
    "    \n",
    "    # Trade metadata\n",
    "    trade_id = Column(String(50), nullable=True)  # Exchange trade ID if available\n",
    "    exchange = Column(String(20), nullable=False, default='UNKNOWN')\n",
    "    \n",
    "    # Aggressor side (who initiated the trade)\n",
    "    side = Column(String(10), nullable=True)\n",
    "    \n",
    "    # Conditions/flags (e.g., odd lot, out of sequence)\n",
    "    conditions = Column(ARRAY(String(10)), nullable=True)\n",
    "    \n",
    "    # Sequence number for ordering within same timestamp\n",
    "    sequence_num = Column(BigInteger, nullable=True)\n",
    "    \n",
    "    # Value in quote currency (price * volume) - denormalized for query efficiency\n",
    "    notional_value = Column(Numeric(24, 8), nullable=True)\n",
    "    \n",
    "    # Data ingestion metadata\n",
    "    received_at = Column(TIMESTAMP(timezone=True), default=func.now())\n",
    "    \n",
    "    # Composite index for most common query pattern\n",
    "    __table_args__ = (\n",
    "        Index('ix_trade_symbol_timestamp', 'symbol', 'timestamp'),\n",
    "        Index('ix_trade_timestamp_symbol', 'timestamp', 'symbol'),\n",
    "        Index('ix_trade_exchange_symbol_time', 'exchange', 'symbol', 'timestamp'),\n",
    "        # Unique constraint to prevent duplicates\n",
    "        UniqueConstraint('symbol', 'timestamp_ns', 'exchange', 'trade_id', \n",
    "                        name='uq_trade_tick'),\n",
    "        {'schema': 'market_data'}  # Use schema for organization\n",
    "    )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"<TradeTick(symbol={self.symbol}, time={self.timestamp}, \"\n",
    "                f\"price={self.price}, volume={self.volume})>\")\n",
    "\n",
    "\n",
    "# Display schema details\n",
    "print(\"=\" * 80)\n",
    "print(\"TRADE TICK SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTable: trade_ticks\")\n",
    "print(\"\\nColumns:\")\n",
    "for column in TradeTick.__table__.columns:\n",
    "    nullable = \"NULL\" if column.nullable else \"NOT NULL\"\n",
    "    print(f\"  {column.name:20} {str(column.type):25} {nullable}\")\n",
    "print(\"\\nIndexes:\")\n",
    "for idx in TradeTick.__table__.indexes:\n",
    "    cols = \", \".join([c.name for c in idx.columns])\n",
    "    print(f\"  {idx.name}: ({cols})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QUOTE TICK SCHEMA\n",
    "# ============================================================================\n",
    "class QuoteTick(Base):\n",
    "    \"\"\"\n",
    "    Schema for quote tick data - best bid/ask updates (NBBO or exchange-level).\n",
    "    \n",
    "    Storage estimate: ~120 bytes per row\n",
    "    Quotes update more frequently than trades - can be 10x volume\n",
    "    \"\"\"\n",
    "    __tablename__ = 'quote_ticks'\n",
    "    \n",
    "    id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Timestamps\n",
    "    timestamp_ns = Column(BigInteger, nullable=False)\n",
    "    timestamp = Column(TIMESTAMP(timezone=True, precision=6), nullable=False)\n",
    "    \n",
    "    # Symbol\n",
    "    symbol = Column(String(20), nullable=False)\n",
    "    \n",
    "    # Bid side\n",
    "    bid_price = Column(Numeric(18, 8), nullable=True)\n",
    "    bid_size = Column(BigInteger, nullable=True)\n",
    "    bid_exchange = Column(String(20), nullable=True)\n",
    "    \n",
    "    # Ask side\n",
    "    ask_price = Column(Numeric(18, 8), nullable=True)\n",
    "    ask_size = Column(BigInteger, nullable=True)\n",
    "    ask_exchange = Column(String(20), nullable=True)\n",
    "    \n",
    "    # Derived fields (denormalized for query efficiency)\n",
    "    spread = Column(Numeric(18, 8), nullable=True)  # ask - bid\n",
    "    spread_bps = Column(Numeric(10, 4), nullable=True)  # spread in basis points\n",
    "    mid_price = Column(Numeric(18, 8), nullable=True)  # (bid + ask) / 2\n",
    "    \n",
    "    # Quote type (NBBO, exchange-specific, etc.)\n",
    "    quote_type = Column(String(20), nullable=True)\n",
    "    \n",
    "    # Sequence and conditions\n",
    "    sequence_num = Column(BigInteger, nullable=True)\n",
    "    conditions = Column(ARRAY(String(10)), nullable=True)\n",
    "    \n",
    "    # Metadata\n",
    "    received_at = Column(TIMESTAMP(timezone=True), default=func.now())\n",
    "    \n",
    "    __table_args__ = (\n",
    "        Index('ix_quote_symbol_timestamp', 'symbol', 'timestamp'),\n",
    "        Index('ix_quote_timestamp', 'timestamp'),\n",
    "        UniqueConstraint('symbol', 'timestamp_ns', 'quote_type', name='uq_quote_tick'),\n",
    "        {'schema': 'market_data'}\n",
    "    )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"<QuoteTick(symbol={self.symbol}, bid={self.bid_price}@{self.bid_size}, \"\n",
    "                f\"ask={self.ask_price}@{self.ask_size})>\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ORDER BOOK SNAPSHOT SCHEMA\n",
    "# ============================================================================\n",
    "class OrderBookSnapshot(Base):\n",
    "    \"\"\"\n",
    "    Schema for full order book snapshots at a point in time.\n",
    "    \n",
    "    Two approaches:\n",
    "    1. Store as JSON blob (flexible, slower queries)\n",
    "    2. Store as separate rows per level (normalized, faster aggregations)\n",
    "    \n",
    "    This uses the JSON approach for flexibility.\n",
    "    \"\"\"\n",
    "    __tablename__ = 'order_book_snapshots'\n",
    "    \n",
    "    id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    timestamp_ns = Column(BigInteger, nullable=False)\n",
    "    timestamp = Column(TIMESTAMP(timezone=True, precision=6), nullable=False)\n",
    "    \n",
    "    symbol = Column(String(20), nullable=False)\n",
    "    exchange = Column(String(20), nullable=False)\n",
    "    \n",
    "    # Number of levels captured\n",
    "    depth = Column(Integer, nullable=False, default=10)\n",
    "    \n",
    "    # Order book data as JSONB for flexible querying\n",
    "    # Structure: {\"bids\": [[price, size], ...], \"asks\": [[price, size], ...]}\n",
    "    book_data = Column(JSONB, nullable=False)\n",
    "    \n",
    "    # Summary statistics (denormalized for common queries)\n",
    "    best_bid = Column(Numeric(18, 8), nullable=True)\n",
    "    best_ask = Column(Numeric(18, 8), nullable=True)\n",
    "    total_bid_volume = Column(BigInteger, nullable=True)\n",
    "    total_ask_volume = Column(BigInteger, nullable=True)\n",
    "    \n",
    "    # Imbalance = (bid_vol - ask_vol) / (bid_vol + ask_vol)\n",
    "    book_imbalance = Column(Numeric(6, 4), nullable=True)\n",
    "    \n",
    "    received_at = Column(TIMESTAMP(timezone=True), default=func.now())\n",
    "    \n",
    "    __table_args__ = (\n",
    "        Index('ix_ob_symbol_timestamp', 'symbol', 'timestamp'),\n",
    "        Index('ix_ob_exchange_symbol_time', 'exchange', 'symbol', 'timestamp'),\n",
    "        # GIN index for JSONB queries\n",
    "        Index('ix_ob_book_data', 'book_data', postgresql_using='gin'),\n",
    "        {'schema': 'market_data'}\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QUOTE TICK SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTable: quote_ticks\")\n",
    "print(\"\\nKey columns for quotes:\")\n",
    "print(\"  - bid_price/bid_size: Best bid\")\n",
    "print(\"  - ask_price/ask_size: Best ask\")\n",
    "print(\"  - spread_bps: Spread in basis points (pre-calculated)\")\n",
    "print(\"  - mid_price: (bid + ask) / 2\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ORDER BOOK SNAPSHOT SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTable: order_book_snapshots\")\n",
    "print(\"\\nStores full depth of book as JSONB:\")\n",
    "print(\"  book_data: {\")\n",
    "print('    \"bids\": [[100.50, 1000], [100.49, 2000], ...],')\n",
    "print('    \"asks\": [[100.51, 500], [100.52, 1500], ...]')\n",
    "print(\"  }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0389766",
   "metadata": {},
   "source": [
    "## 4. OHLCV Time Series Schema <a id=\"4-ohlcv-schema\"></a>\n",
    "\n",
    "### OHLCV (Candlestick) Data\n",
    "Aggregated price data over fixed time intervals:\n",
    "- **O**pen: First price in the interval\n",
    "- **H**igh: Highest price in the interval\n",
    "- **L**ow: Lowest price in the interval\n",
    "- **C**lose: Last price in the interval\n",
    "- **V**olume: Total volume traded in the interval\n",
    "\n",
    "### Common Timeframes\n",
    "- **Tick-based**: 1 tick, 10 ticks, 100 ticks\n",
    "- **Time-based**: 1s, 1m, 5m, 15m, 30m, 1h, 4h, 1d, 1w, 1M\n",
    "- **Volume-based**: 1K volume, 10K volume (equal volume bars)\n",
    "- **Dollar-based**: $1M notional (equal dollar bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bca3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OHLCV CANDLE SCHEMA\n",
    "# ============================================================================\n",
    "class Timeframe(str, Enum):\n",
    "    \"\"\"Supported timeframes for OHLCV data.\"\"\"\n",
    "    SECOND_1 = \"1s\"\n",
    "    MINUTE_1 = \"1m\"\n",
    "    MINUTE_5 = \"5m\"\n",
    "    MINUTE_15 = \"15m\"\n",
    "    MINUTE_30 = \"30m\"\n",
    "    HOUR_1 = \"1h\"\n",
    "    HOUR_4 = \"4h\"\n",
    "    DAY_1 = \"1d\"\n",
    "    WEEK_1 = \"1w\"\n",
    "    MONTH_1 = \"1M\"\n",
    "\n",
    "TIMEFRAME_SECONDS = {\n",
    "    \"1s\": 1,\n",
    "    \"1m\": 60,\n",
    "    \"5m\": 300,\n",
    "    \"15m\": 900,\n",
    "    \"30m\": 1800,\n",
    "    \"1h\": 3600,\n",
    "    \"4h\": 14400,\n",
    "    \"1d\": 86400,\n",
    "    \"1w\": 604800,\n",
    "    \"1M\": 2592000,  # Approximate\n",
    "}\n",
    "\n",
    "\n",
    "class OHLCVCandle(Base):\n",
    "    \"\"\"\n",
    "    Schema for OHLCV candlestick data.\n",
    "    \n",
    "    Design choices:\n",
    "    1. Single table with timeframe column vs separate tables per timeframe\n",
    "       - Single table: Simpler, but larger indexes\n",
    "       - Separate tables: More complex, but faster queries per timeframe\n",
    "    \n",
    "    2. This design uses a single table with partitioning by timeframe\n",
    "    \n",
    "    Storage estimate: ~80 bytes per row\n",
    "    For 5000 symbols @ 1m bars = 5000 * 1440 * 80 = 576MB/day for 1m data\n",
    "    \"\"\"\n",
    "    __tablename__ = 'ohlcv_candles'\n",
    "    \n",
    "    id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Candle period start time (ALWAYS use the START of the period)\n",
    "    timestamp = Column(TIMESTAMP(timezone=True), nullable=False)\n",
    "    \n",
    "    # Candle period end time (optional, can be derived from timeframe)\n",
    "    timestamp_end = Column(TIMESTAMP(timezone=True), nullable=True)\n",
    "    \n",
    "    # Symbol\n",
    "    symbol = Column(String(20), nullable=False)\n",
    "    \n",
    "    # Timeframe (1m, 5m, 1h, 1d, etc.)\n",
    "    timeframe = Column(String(10), nullable=False)\n",
    "    \n",
    "    # OHLCV core fields\n",
    "    open = Column(Numeric(18, 8), nullable=False)\n",
    "    high = Column(Numeric(18, 8), nullable=False)\n",
    "    low = Column(Numeric(18, 8), nullable=False)\n",
    "    close = Column(Numeric(18, 8), nullable=False)\n",
    "    volume = Column(BigInteger, nullable=False)\n",
    "    \n",
    "    # Extended fields for richer analysis\n",
    "    vwap = Column(Numeric(18, 8), nullable=True)  # Volume-weighted average price\n",
    "    trade_count = Column(Integer, nullable=True)  # Number of trades\n",
    "    notional_volume = Column(Numeric(24, 8), nullable=True)  # Dollar volume\n",
    "    \n",
    "    # Quote data aggregates (if available)\n",
    "    avg_spread_bps = Column(Numeric(10, 4), nullable=True)\n",
    "    avg_bid_size = Column(BigInteger, nullable=True)\n",
    "    avg_ask_size = Column(BigInteger, nullable=True)\n",
    "    \n",
    "    # Return calculation (denormalized for efficiency)\n",
    "    # return = (close - prev_close) / prev_close\n",
    "    return_pct = Column(Numeric(12, 8), nullable=True)\n",
    "    log_return = Column(Numeric(12, 8), nullable=True)\n",
    "    \n",
    "    # Volatility measures\n",
    "    intrabar_range = Column(Numeric(12, 8), nullable=True)  # (high - low) / open\n",
    "    close_location = Column(Numeric(6, 4), nullable=True)  # (close - low) / (high - low)\n",
    "    \n",
    "    # Data quality flags\n",
    "    is_complete = Column(Boolean, default=True)  # False if partial candle\n",
    "    data_source = Column(String(50), nullable=True)\n",
    "    \n",
    "    # Timestamps\n",
    "    created_at = Column(TIMESTAMP(timezone=True), default=func.now())\n",
    "    updated_at = Column(TIMESTAMP(timezone=True), onupdate=func.now())\n",
    "    \n",
    "    __table_args__ = (\n",
    "        # Primary lookup pattern: symbol + timeframe + timestamp\n",
    "        Index('ix_ohlcv_symbol_tf_time', 'symbol', 'timeframe', 'timestamp'),\n",
    "        # For time-range queries across all symbols\n",
    "        Index('ix_ohlcv_tf_time_symbol', 'timeframe', 'timestamp', 'symbol'),\n",
    "        # Unique constraint\n",
    "        UniqueConstraint('symbol', 'timeframe', 'timestamp', name='uq_ohlcv_candle'),\n",
    "        # Check constraints for data quality\n",
    "        CheckConstraint('high >= low', name='check_high_gte_low'),\n",
    "        CheckConstraint('high >= open AND high >= close', name='check_high_is_max'),\n",
    "        CheckConstraint('low <= open AND low <= close', name='check_low_is_min'),\n",
    "        CheckConstraint('volume >= 0', name='check_volume_positive'),\n",
    "        {'schema': 'market_data'}\n",
    "    )\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"<OHLCVCandle(symbol={self.symbol}, tf={self.timeframe}, \"\n",
    "                f\"time={self.timestamp}, O={self.open}, H={self.high}, \"\n",
    "                f\"L={self.low}, C={self.close}, V={self.volume})>\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DAILY AGGREGATES TABLE (Optimized for daily queries)\n",
    "# ============================================================================\n",
    "class DailyBar(Base):\n",
    "    \"\"\"\n",
    "    Separate table for daily data with additional fields.\n",
    "    Daily data is queried so frequently it deserves its own optimized table.\n",
    "    \"\"\"\n",
    "    __tablename__ = 'daily_bars'\n",
    "    \n",
    "    id = Column(BigInteger, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Date (not timestamp - daily data is date-based)\n",
    "    date = Column(Date, nullable=False)\n",
    "    symbol = Column(String(20), nullable=False)\n",
    "    \n",
    "    # OHLCV\n",
    "    open = Column(Numeric(18, 8), nullable=False)\n",
    "    high = Column(Numeric(18, 8), nullable=False)\n",
    "    low = Column(Numeric(18, 8), nullable=False)\n",
    "    close = Column(Numeric(18, 8), nullable=False)\n",
    "    volume = Column(BigInteger, nullable=False)\n",
    "    \n",
    "    # Adjusted prices (for splits and dividends)\n",
    "    adj_open = Column(Numeric(18, 8), nullable=True)\n",
    "    adj_high = Column(Numeric(18, 8), nullable=True)\n",
    "    adj_low = Column(Numeric(18, 8), nullable=True)\n",
    "    adj_close = Column(Numeric(18, 8), nullable=True)\n",
    "    adj_volume = Column(BigInteger, nullable=True)\n",
    "    \n",
    "    # Adjustment factor (multiply raw by factor to get adjusted)\n",
    "    adjustment_factor = Column(Numeric(18, 10), default=1.0)\n",
    "    \n",
    "    # Additional daily metrics\n",
    "    vwap = Column(Numeric(18, 8), nullable=True)\n",
    "    trade_count = Column(Integer, nullable=True)\n",
    "    notional_volume = Column(Numeric(24, 8), nullable=True)\n",
    "    \n",
    "    # Returns\n",
    "    return_pct = Column(Numeric(12, 8), nullable=True)\n",
    "    log_return = Column(Numeric(12, 8), nullable=True)\n",
    "    \n",
    "    # Corporate actions\n",
    "    dividend = Column(Numeric(12, 8), nullable=True)\n",
    "    split_ratio = Column(Numeric(12, 8), nullable=True)\n",
    "    \n",
    "    __table_args__ = (\n",
    "        Index('ix_daily_symbol_date', 'symbol', 'date'),\n",
    "        Index('ix_daily_date_symbol', 'date', 'symbol'),\n",
    "        UniqueConstraint('symbol', 'date', name='uq_daily_bar'),\n",
    "        {'schema': 'market_data'}\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OHLCV CANDLE SCHEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTable: ohlcv_candles\")\n",
    "print(\"\\nSupported timeframes:\", list(TIMEFRAME_SECONDS.keys()))\n",
    "print(\"\\nCore fields: open, high, low, close, volume\")\n",
    "print(\"\\nExtended fields:\")\n",
    "print(\"  - vwap: Volume-weighted average price\")\n",
    "print(\"  - trade_count: Number of trades in period\")\n",
    "print(\"  - return_pct: Period return (denormalized)\")\n",
    "print(\"  - avg_spread_bps: Average bid-ask spread\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DAILY BAR SCHEMA (Optimized)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTable: daily_bars\")\n",
    "print(\"\\nAdditional fields for daily data:\")\n",
    "print(\"  - Adjusted prices (adj_open, adj_close, etc.)\")\n",
    "print(\"  - Adjustment factor for splits/dividends\")\n",
    "print(\"  - Corporate action fields (dividend, split_ratio)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0336f",
   "metadata": {},
   "source": [
    "## 5. Database Connection with Connection Pooling <a id=\"5-database-connection\"></a>\n",
    "\n",
    "### Connection Pooling Best Practices\n",
    "- **Pool Size**: Start with 5-10 connections per worker\n",
    "- **Max Overflow**: Allow 20-50 additional connections for bursts\n",
    "- **Connection Timeout**: 30 seconds typical\n",
    "- **Recycle**: Recycle connections every 1-2 hours to avoid stale connections\n",
    "- **Pre-ping**: Enable connection validation before use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE CONNECTION MANAGER\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DatabaseConfig:\n",
    "    \"\"\"Database connection configuration.\"\"\"\n",
    "    host: str = \"localhost\"\n",
    "    port: int = 5432\n",
    "    database: str = \"market_data\"\n",
    "    user: str = \"trader\"\n",
    "    password: str = \"password\"\n",
    "    \n",
    "    # Connection pool settings\n",
    "    pool_size: int = 10\n",
    "    max_overflow: int = 20\n",
    "    pool_timeout: int = 30\n",
    "    pool_recycle: int = 3600  # 1 hour\n",
    "    pool_pre_ping: bool = True\n",
    "    \n",
    "    # SSL settings\n",
    "    ssl_mode: str = \"prefer\"\n",
    "    \n",
    "    def get_connection_string(self, include_password: bool = True) -> str:\n",
    "        \"\"\"Generate SQLAlchemy connection string.\"\"\"\n",
    "        pwd = self.password if include_password else \"****\"\n",
    "        return (\n",
    "            f\"postgresql+psycopg2://{self.user}:{pwd}@\"\n",
    "            f\"{self.host}:{self.port}/{self.database}\"\n",
    "            f\"?sslmode={self.ssl_mode}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class DatabaseConnectionManager:\n",
    "    \"\"\"\n",
    "    Manages database connections with connection pooling.\n",
    "    \n",
    "    Usage:\n",
    "        db_manager = DatabaseConnectionManager(config)\n",
    "        with db_manager.get_session() as session:\n",
    "            session.query(TradeTick).filter(...)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DatabaseConfig):\n",
    "        self.config = config\n",
    "        self._engine = None\n",
    "        self._session_factory = None\n",
    "        self._scoped_session = None\n",
    "        \n",
    "    def _create_engine(self):\n",
    "        \"\"\"Create SQLAlchemy engine with connection pooling.\"\"\"\n",
    "        return create_engine(\n",
    "            self.config.get_connection_string(),\n",
    "            poolclass=QueuePool,\n",
    "            pool_size=self.config.pool_size,\n",
    "            max_overflow=self.config.max_overflow,\n",
    "            pool_timeout=self.config.pool_timeout,\n",
    "            pool_recycle=self.config.pool_recycle,\n",
    "            pool_pre_ping=self.config.pool_pre_ping,\n",
    "            # Echo SQL for debugging (disable in production)\n",
    "            echo=False,\n",
    "            # Future-proof settings\n",
    "            future=True,\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def engine(self):\n",
    "        \"\"\"Get or create the database engine.\"\"\"\n",
    "        if self._engine is None:\n",
    "            self._engine = self._create_engine()\n",
    "        return self._engine\n",
    "    \n",
    "    @property\n",
    "    def session_factory(self):\n",
    "        \"\"\"Get or create session factory.\"\"\"\n",
    "        if self._session_factory is None:\n",
    "            self._session_factory = sessionmaker(\n",
    "                bind=self.engine,\n",
    "                autocommit=False,\n",
    "                autoflush=False,\n",
    "                expire_on_commit=False\n",
    "            )\n",
    "        return self._session_factory\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_session(self) -> Session:\n",
    "        \"\"\"\n",
    "        Get a database session with automatic cleanup.\n",
    "        \n",
    "        Usage:\n",
    "            with db_manager.get_session() as session:\n",
    "                results = session.query(TradeTick).all()\n",
    "        \"\"\"\n",
    "        session = self.session_factory()\n",
    "        try:\n",
    "            yield session\n",
    "            session.commit()\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            raise e\n",
    "        finally:\n",
    "            session.close()\n",
    "    \n",
    "    def get_scoped_session(self):\n",
    "        \"\"\"Get thread-local scoped session for multi-threaded apps.\"\"\"\n",
    "        if self._scoped_session is None:\n",
    "            self._scoped_session = scoped_session(self.session_factory)\n",
    "        return self._scoped_session\n",
    "    \n",
    "    def execute_raw(self, sql: str, params: dict = None) -> List[Dict]:\n",
    "        \"\"\"Execute raw SQL and return results as list of dicts.\"\"\"\n",
    "        with self.engine.connect() as conn:\n",
    "            result = conn.execute(text(sql), params or {})\n",
    "            if result.returns_rows:\n",
    "                return [dict(row._mapping) for row in result]\n",
    "            return []\n",
    "    \n",
    "    def health_check(self) -> bool:\n",
    "        \"\"\"Check if database connection is healthy.\"\"\"\n",
    "        try:\n",
    "            self.execute_raw(\"SELECT 1\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Database health check failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_pool_status(self) -> Dict[str, int]:\n",
    "        \"\"\"Get connection pool statistics.\"\"\"\n",
    "        pool = self.engine.pool\n",
    "        return {\n",
    "            \"pool_size\": pool.size(),\n",
    "            \"checked_in\": pool.checkedin(),\n",
    "            \"checked_out\": pool.checkedout(),\n",
    "            \"overflow\": pool.overflow(),\n",
    "            \"invalid\": pool.invalidatedcount() if hasattr(pool, 'invalidatedcount') else 0\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close all connections and dispose of the engine.\"\"\"\n",
    "        if self._scoped_session:\n",
    "            self._scoped_session.remove()\n",
    "        if self._engine:\n",
    "            self._engine.dispose()\n",
    "            self._engine = None\n",
    "\n",
    "\n",
    "# Example configuration\n",
    "example_config = DatabaseConfig(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    database=\"quant_trading\",\n",
    "    user=\"trader\",\n",
    "    password=\"secure_password\",\n",
    "    pool_size=10,\n",
    "    max_overflow=20\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATABASE CONNECTION MANAGER\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nConnection string (sanitized):\")\n",
    "print(f\"  {example_config.get_connection_string(include_password=False)}\")\n",
    "print(f\"\\nPool settings:\")\n",
    "print(f\"  Pool size: {example_config.pool_size}\")\n",
    "print(f\"  Max overflow: {example_config.max_overflow}\")\n",
    "print(f\"  Pool timeout: {example_config.pool_timeout}s\")\n",
    "print(f\"  Pool recycle: {example_config.pool_recycle}s\")\n",
    "print(f\"  Pre-ping: {example_config.pool_pre_ping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e1b09",
   "metadata": {},
   "source": [
    "## 6. SQLAlchemy ORM Models - Complete Implementation <a id=\"6-orm-models\"></a>\n",
    "\n",
    "### ORM vs Raw SQL Trade-offs\n",
    "\n",
    "| Aspect | ORM | Raw SQL |\n",
    "|--------|-----|---------|\n",
    "| Development speed | Fast | Slower |\n",
    "| Type safety | High | Low |\n",
    "| Query flexibility | Medium | High |\n",
    "| Performance | ~10-20% overhead | Optimal |\n",
    "| Maintenance | Easier | Harder |\n",
    "\n",
    "**Recommendation**: Use ORM for CRUD operations, raw SQL for complex analytics queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52477735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE TABLE CREATION SCRIPT\n",
    "# ============================================================================\n",
    "\n",
    "def create_market_data_schema(engine, drop_existing: bool = False):\n",
    "    \"\"\"\n",
    "    Create all market data tables.\n",
    "    \n",
    "    Args:\n",
    "        engine: SQLAlchemy engine\n",
    "        drop_existing: If True, drops existing tables first (USE WITH CAUTION!)\n",
    "    \"\"\"\n",
    "    # Create schema if it doesn't exist\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS market_data\"))\n",
    "        conn.commit()\n",
    "    \n",
    "    if drop_existing:\n",
    "        Base.metadata.drop_all(engine)\n",
    "        print(\"âš ï¸ Dropped existing tables\")\n",
    "    \n",
    "    # Create all tables\n",
    "    Base.metadata.create_all(engine)\n",
    "    print(\"âœ… Created all tables\")\n",
    "\n",
    "\n",
    "def get_table_ddl(table_class) -> str:\n",
    "    \"\"\"Get the DDL (CREATE TABLE statement) for a table.\"\"\"\n",
    "    from sqlalchemy.schema import CreateTable\n",
    "    from sqlalchemy.dialects import postgresql\n",
    "    \n",
    "    create_stmt = CreateTable(table_class.__table__)\n",
    "    return str(create_stmt.compile(dialect=postgresql.dialect()))\n",
    "\n",
    "\n",
    "# Display DDL for each table\n",
    "print(\"=\" * 80)\n",
    "print(\"TABLE DDL STATEMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tables_to_show = [\n",
    "    (\"TradeTick\", TradeTick),\n",
    "    (\"QuoteTick\", QuoteTick),\n",
    "    (\"OrderBookSnapshot\", OrderBookSnapshot),\n",
    "    (\"OHLCVCandle\", OHLCVCandle),\n",
    "    (\"DailyBar\", DailyBar),\n",
    "]\n",
    "\n",
    "for name, table_class in tables_to_show:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"-- {name}\")\n",
    "    print(\"=\"*40)\n",
    "    ddl = get_table_ddl(table_class)\n",
    "    # Truncate for display\n",
    "    print(ddl[:1500] + \"...\" if len(ddl) > 1500 else ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a571b5",
   "metadata": {},
   "source": [
    "## 7. Indexing Strategy for Time Series Queries <a id=\"7-indexing-strategy\"></a>\n",
    "\n",
    "### Index Types for Time Series\n",
    "\n",
    "1. **B-tree Index** (default): Best for range queries, equality, ordering\n",
    "   - Use for: `timestamp`, `symbol`, `exchange`\n",
    "   \n",
    "2. **Hash Index**: Fast equality lookups only\n",
    "   - Use for: exact symbol lookups (rarely better than B-tree)\n",
    "   \n",
    "3. **BRIN Index** (Block Range Index): For naturally ordered data\n",
    "   - Use for: append-only time series (10-100x smaller than B-tree)\n",
    "   \n",
    "4. **GIN Index**: For array/JSONB columns\n",
    "   - Use for: `conditions` array, `book_data` JSONB\n",
    "\n",
    "### Index Design Principles\n",
    "- **Leading column**: Most selective or most filtered column first\n",
    "- **Include columns**: Add frequently selected columns to avoid table lookups\n",
    "- **Partial indexes**: Index only relevant rows (e.g., last 30 days)\n",
    "- **Covering indexes**: Include all columns needed by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e81ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INDEX CREATION STRATEGIES\n",
    "# ============================================================================\n",
    "\n",
    "def get_index_creation_sql() -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate SQL for creating optimized indexes.\n",
    "    \n",
    "    Returns dict of table_name -> list of index creation statements\n",
    "    \"\"\"\n",
    "    indexes = {}\n",
    "    \n",
    "    # Trade ticks indexes\n",
    "    indexes['trade_ticks'] = [\n",
    "        # Primary access pattern: get trades for a symbol in a time range\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_trade_symbol_time_covering\n",
    "        ON market_data.trade_ticks (symbol, timestamp)\n",
    "        INCLUDE (price, volume, side)\n",
    "        \"\"\",\n",
    "        \n",
    "        # For time-ordered scans across all symbols\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_trade_time_symbol\n",
    "        ON market_data.trade_ticks (timestamp, symbol)\n",
    "        \"\"\",\n",
    "        \n",
    "        # BRIN index for append-only data (very small, fast scans)\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_trade_time_brin\n",
    "        ON market_data.trade_ticks USING BRIN (timestamp)\n",
    "        WITH (pages_per_range = 128)\n",
    "        \"\"\",\n",
    "        \n",
    "        # Partial index for recent data (last 30 days)\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_trade_recent\n",
    "        ON market_data.trade_ticks (symbol, timestamp)\n",
    "        WHERE timestamp > CURRENT_TIMESTAMP - INTERVAL '30 days'\n",
    "        \"\"\",\n",
    "        \n",
    "        # Index for exchange-specific queries\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_trade_exchange_symbol_time\n",
    "        ON market_data.trade_ticks (exchange, symbol, timestamp)\n",
    "        WHERE exchange IS NOT NULL\n",
    "        \"\"\",\n",
    "    ]\n",
    "    \n",
    "    # OHLCV candles indexes\n",
    "    indexes['ohlcv_candles'] = [\n",
    "        # Primary access: symbol + timeframe + time range\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_ohlcv_primary\n",
    "        ON market_data.ohlcv_candles (symbol, timeframe, timestamp DESC)\n",
    "        INCLUDE (open, high, low, close, volume)\n",
    "        \"\"\",\n",
    "        \n",
    "        # For cross-sectional queries (all symbols at a time)\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_ohlcv_time_tf_symbol\n",
    "        ON market_data.ohlcv_candles (timestamp, timeframe, symbol)\n",
    "        \"\"\",\n",
    "        \n",
    "        # Partial index for 1-minute data (most common)\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_ohlcv_1m_only\n",
    "        ON market_data.ohlcv_candles (symbol, timestamp DESC)\n",
    "        INCLUDE (open, high, low, close, volume)\n",
    "        WHERE timeframe = '1m'\n",
    "        \"\"\",\n",
    "        \n",
    "        # BRIN for time-ordered scans\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_ohlcv_time_brin\n",
    "        ON market_data.ohlcv_candles USING BRIN (timestamp)\n",
    "        WITH (pages_per_range = 64)\n",
    "        \"\"\",\n",
    "    ]\n",
    "    \n",
    "    # Daily bars indexes\n",
    "    indexes['daily_bars'] = [\n",
    "        # Primary access pattern\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_daily_symbol_date\n",
    "        ON market_data.daily_bars (symbol, date DESC)\n",
    "        INCLUDE (open, high, low, close, volume, adj_close)\n",
    "        \"\"\",\n",
    "        \n",
    "        # Cross-sectional (all symbols on a date)\n",
    "        \"\"\"\n",
    "        CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_daily_date_symbol\n",
    "        ON market_data.daily_bars (date DESC, symbol)\n",
    "        \"\"\",\n",
    "    ]\n",
    "    \n",
    "    return indexes\n",
    "\n",
    "\n",
    "def estimate_index_size(\n",
    "    table_rows: int,\n",
    "    key_columns_bytes: int,\n",
    "    include_columns_bytes: int = 0,\n",
    "    fill_factor: float = 0.7\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate B-tree index size in MB.\n",
    "    \n",
    "    Args:\n",
    "        table_rows: Number of rows in table\n",
    "        key_columns_bytes: Size of key columns in bytes\n",
    "        include_columns_bytes: Size of INCLUDE columns in bytes\n",
    "        fill_factor: Fill factor (default 70%)\n",
    "    \n",
    "    Returns:\n",
    "        Estimated size in MB\n",
    "    \"\"\"\n",
    "    # Each leaf node: 8 bytes pointer + key + include data\n",
    "    leaf_entry_size = 8 + key_columns_bytes + include_columns_bytes\n",
    "    \n",
    "    # Entries per page (8KB page)\n",
    "    entries_per_page = int((8192 * fill_factor) / leaf_entry_size)\n",
    "    \n",
    "    # Number of leaf pages\n",
    "    leaf_pages = table_rows / entries_per_page\n",
    "    \n",
    "    # Internal pages (approximately 1% of leaf pages)\n",
    "    internal_pages = leaf_pages * 0.01\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_pages = leaf_pages + internal_pages\n",
    "    size_mb = (total_pages * 8192) / (1024 * 1024)\n",
    "    \n",
    "    return size_mb\n",
    "\n",
    "\n",
    "# Display index strategy\n",
    "print(\"=\" * 80)\n",
    "print(\"INDEX CREATION STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "indexes = get_index_creation_sql()\n",
    "for table, index_sqls in indexes.items():\n",
    "    print(f\"\\nðŸ“Š Table: {table}\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, sql in enumerate(index_sqls, 1):\n",
    "        # Extract index name from SQL\n",
    "        name = sql.split(\"IF NOT EXISTS \")[1].split(\"\\n\")[0].strip()\n",
    "        print(f\"  {i}. {name}\")\n",
    "\n",
    "# Estimate index sizes for a realistic workload\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INDEX SIZE ESTIMATES (1 year of data)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "estimates = [\n",
    "    (\"trade_ticks\", 500_000_000, 28, 24),  # 500M rows, symbol(8)+timestamp(8)+exchange(12), price+vol+side\n",
    "    (\"ohlcv_candles (1m)\", 2_000_000_000, 32, 40),  # 2B rows for 1m data\n",
    "    (\"daily_bars\", 5_000_000, 14, 48),  # 5M rows (5000 symbols * 1000 days)\n",
    "]\n",
    "\n",
    "for table, rows, key_bytes, include_bytes in estimates:\n",
    "    size = estimate_index_size(rows, key_bytes, include_bytes)\n",
    "    print(f\"\\n{table}:\")\n",
    "    print(f\"  Rows: {rows:,}\")\n",
    "    print(f\"  Estimated covering index size: {size:,.0f} MB ({size/1024:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f3fae",
   "metadata": {},
   "source": [
    "## 8. Data Partitioning by Date <a id=\"8-partitioning\"></a>\n",
    "\n",
    "### Why Partition Time Series Data?\n",
    "\n",
    "1. **Query Performance**: Partition pruning eliminates scanning irrelevant data\n",
    "2. **Data Management**: Easy to drop old partitions (instant vs slow DELETE)\n",
    "3. **Maintenance**: VACUUM/ANALYZE can run on individual partitions\n",
    "4. **Storage Tiering**: Move old partitions to cheaper storage\n",
    "\n",
    "### Partitioning Strategies\n",
    "\n",
    "| Strategy | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **Daily** | Tick data, high-volume | Fine-grained, easy cleanup | Many partitions |\n",
    "| **Weekly** | Medium volume | Good balance | Week boundaries |\n",
    "| **Monthly** | OHLCV, moderate volume | Fewer partitions | Coarser cleanup |\n",
    "| **Yearly** | Daily bars, low volume | Minimal overhead | Large partitions |\n",
    "\n",
    "### TimescaleDB Hypertables\n",
    "TimescaleDB automates partitioning with \"chunks\" (partitions) and provides:\n",
    "- Automatic time-based partitioning\n",
    "- Efficient chunk pruning\n",
    "- Native compression (10-20x)\n",
    "- Continuous aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# POSTGRESQL NATIVE PARTITIONING\n",
    "# ============================================================================\n",
    "\n",
    "def generate_partition_ddl(\n",
    "    table_name: str,\n",
    "    schema: str,\n",
    "    partition_column: str,\n",
    "    partition_type: str = \"RANGE\",\n",
    "    start_date: date = None,\n",
    "    end_date: date = None,\n",
    "    interval: str = \"monthly\"  # daily, weekly, monthly\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate DDL for creating partitioned table and partitions.\n",
    "    \n",
    "    Args:\n",
    "        table_name: Name of the table\n",
    "        schema: Schema name\n",
    "        partition_column: Column to partition by (usually timestamp)\n",
    "        partition_type: RANGE, LIST, or HASH\n",
    "        start_date: Start date for partitions\n",
    "        end_date: End date for partitions\n",
    "        interval: Partition interval (daily, weekly, monthly)\n",
    "    \n",
    "    Returns:\n",
    "        List of SQL statements\n",
    "    \"\"\"\n",
    "    if start_date is None:\n",
    "        start_date = date(2020, 1, 1)\n",
    "    if end_date is None:\n",
    "        end_date = date(2026, 12, 31)\n",
    "    \n",
    "    statements = []\n",
    "    \n",
    "    # Example for trade_ticks partitioned table\n",
    "    parent_ddl = f\"\"\"\n",
    "-- Create partitioned parent table\n",
    "CREATE TABLE IF NOT EXISTS {schema}.{table_name} (\n",
    "    id BIGSERIAL,\n",
    "    timestamp_ns BIGINT NOT NULL,\n",
    "    timestamp TIMESTAMPTZ NOT NULL,\n",
    "    symbol VARCHAR(20) NOT NULL,\n",
    "    price NUMERIC(18, 8) NOT NULL,\n",
    "    volume BIGINT NOT NULL,\n",
    "    trade_id VARCHAR(50),\n",
    "    exchange VARCHAR(20) NOT NULL DEFAULT 'UNKNOWN',\n",
    "    side VARCHAR(10),\n",
    "    conditions TEXT[],\n",
    "    sequence_num BIGINT,\n",
    "    notional_value NUMERIC(24, 8),\n",
    "    received_at TIMESTAMPTZ DEFAULT NOW(),\n",
    "    PRIMARY KEY (id, timestamp)\n",
    ") PARTITION BY {partition_type} ({partition_column});\n",
    "    \"\"\"\n",
    "    statements.append(parent_ddl)\n",
    "    \n",
    "    # Generate partition statements based on interval\n",
    "    current = start_date\n",
    "    \n",
    "    if interval == \"daily\":\n",
    "        delta = timedelta(days=1)\n",
    "        date_format = \"%Y%m%d\"\n",
    "    elif interval == \"weekly\":\n",
    "        delta = timedelta(weeks=1)\n",
    "        date_format = \"%Y_w%W\"\n",
    "    else:  # monthly\n",
    "        delta = None  # Handle month increments specially\n",
    "        date_format = \"%Y%m\"\n",
    "    \n",
    "    while current < end_date:\n",
    "        if interval == \"monthly\":\n",
    "            # Calculate next month\n",
    "            if current.month == 12:\n",
    "                next_date = date(current.year + 1, 1, 1)\n",
    "            else:\n",
    "                next_date = date(current.year, current.month + 1, 1)\n",
    "            partition_suffix = current.strftime(date_format)\n",
    "        else:\n",
    "            next_date = current + delta\n",
    "            partition_suffix = current.strftime(date_format)\n",
    "        \n",
    "        partition_name = f\"{table_name}_{partition_suffix}\"\n",
    "        \n",
    "        partition_ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {schema}.{partition_name}\n",
    "PARTITION OF {schema}.{table_name}\n",
    "FOR VALUES FROM ('{current.isoformat()}') TO ('{next_date.isoformat()}');\n",
    "        \"\"\"\n",
    "        statements.append(partition_ddl)\n",
    "        \n",
    "        current = next_date\n",
    "    \n",
    "    # Create default partition for out-of-range data\n",
    "    default_ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {schema}.{table_name}_default\n",
    "PARTITION OF {schema}.{table_name}\n",
    "DEFAULT;\n",
    "    \"\"\"\n",
    "    statements.append(default_ddl)\n",
    "    \n",
    "    return statements\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TIMESCALEDB HYPERTABLE SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def get_timescaledb_setup_sql(table_name: str, schema: str = \"market_data\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate SQL for setting up TimescaleDB hypertables.\n",
    "    \n",
    "    TimescaleDB provides:\n",
    "    - Automatic chunk management\n",
    "    - Native compression\n",
    "    - Continuous aggregates\n",
    "    - Data retention policies\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Enable TimescaleDB extension\n",
    "        \"CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;\",\n",
    "        \n",
    "        # Convert regular table to hypertable\n",
    "        # chunk_time_interval: how much time data per chunk (partition)\n",
    "        f\"\"\"\n",
    "        SELECT create_hypertable(\n",
    "            '{schema}.{table_name}',\n",
    "            'timestamp',\n",
    "            chunk_time_interval => INTERVAL '1 day',\n",
    "            if_not_exists => TRUE,\n",
    "            migrate_data => TRUE\n",
    "        );\n",
    "        \"\"\",\n",
    "        \n",
    "        # Add space partitioning by symbol for better parallelism\n",
    "        f\"\"\"\n",
    "        SELECT add_dimension(\n",
    "            '{schema}.{table_name}',\n",
    "            'symbol',\n",
    "            number_partitions => 16,\n",
    "            if_not_exists => TRUE\n",
    "        );\n",
    "        \"\"\",\n",
    "        \n",
    "        # Enable compression (TimescaleDB 2.0+)\n",
    "        f\"\"\"\n",
    "        ALTER TABLE {schema}.{table_name} SET (\n",
    "            timescaledb.compress,\n",
    "            timescaledb.compress_segmentby = 'symbol',\n",
    "            timescaledb.compress_orderby = 'timestamp DESC'\n",
    "        );\n",
    "        \"\"\",\n",
    "        \n",
    "        # Create compression policy (compress chunks older than 7 days)\n",
    "        f\"\"\"\n",
    "        SELECT add_compression_policy(\n",
    "            '{schema}.{table_name}',\n",
    "            INTERVAL '7 days',\n",
    "            if_not_exists => TRUE\n",
    "        );\n",
    "        \"\"\",\n",
    "        \n",
    "        # Create data retention policy (drop chunks older than 2 years)\n",
    "        f\"\"\"\n",
    "        SELECT add_retention_policy(\n",
    "            '{schema}.{table_name}',\n",
    "            INTERVAL '2 years',\n",
    "            if_not_exists => TRUE\n",
    "        );\n",
    "        \"\"\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Display partitioning examples\n",
    "print(\"=\" * 80)\n",
    "print(\"POSTGRESQL NATIVE PARTITIONING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_ddl = generate_partition_ddl(\n",
    "    \"trade_ticks_partitioned\",\n",
    "    \"market_data\",\n",
    "    \"timestamp\",\n",
    "    start_date=date(2024, 1, 1),\n",
    "    end_date=date(2024, 4, 1),\n",
    "    interval=\"monthly\"\n",
    ")\n",
    "\n",
    "print(\"\\nSample DDL for monthly partitioned trade_ticks:\")\n",
    "for stmt in sample_ddl[:3]:  # Show first 3 statements\n",
    "    print(stmt)\n",
    "print(f\"\\n... and {len(sample_ddl) - 3} more partition statements\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TIMESCALEDB HYPERTABLE SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ts_sql = get_timescaledb_setup_sql(\"trade_ticks\")\n",
    "for i, stmt in enumerate(ts_sql, 1):\n",
    "    print(f\"\\n-- Step {i}\")\n",
    "    print(stmt.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49fbde",
   "metadata": {},
   "source": [
    "## 9. High-Throughput Data Insertion <a id=\"9-data-insertion\"></a>\n",
    "\n",
    "### Batch Insert Strategies\n",
    "\n",
    "| Method | Throughput | Use Case |\n",
    "|--------|------------|----------|\n",
    "| Individual INSERTs | ~1K rows/sec | Debugging only |\n",
    "| executemany() | ~10K rows/sec | Small batches |\n",
    "| execute_values() (psycopg2) | ~100K rows/sec | Production |\n",
    "| COPY command | ~500K rows/sec | Bulk loads |\n",
    "| Multi-value INSERT | ~50K rows/sec | Medium batches |\n",
    "\n",
    "### Key Techniques\n",
    "1. **Batch size**: 1000-10000 rows per batch\n",
    "2. **Disable indexes during load**: Add indexes after bulk insert\n",
    "3. **Unlogged tables**: For temporary staging data\n",
    "4. **Async writes**: Don't wait for WAL sync\n",
    "5. **Upsert (ON CONFLICT)**: Handle duplicates efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HIGH-THROUGHPUT DATA INSERTION\n",
    "# ============================================================================\n",
    "\n",
    "class TickDataIngestor:\n",
    "    \"\"\"\n",
    "    High-performance tick data ingestion with batching and upserts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_manager: DatabaseConnectionManager,\n",
    "        batch_size: int = 5000,\n",
    "        flush_interval_seconds: float = 1.0\n",
    "    ):\n",
    "        self.db_manager = db_manager\n",
    "        self.batch_size = batch_size\n",
    "        self.flush_interval = flush_interval_seconds\n",
    "        \n",
    "        self._buffer = []\n",
    "        self._last_flush = time.time()\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        # Statistics\n",
    "        self.total_inserted = 0\n",
    "        self.total_batches = 0\n",
    "        self.total_errors = 0\n",
    "    \n",
    "    def add_tick(self, tick: Dict[str, Any]):\n",
    "        \"\"\"Add a tick to the buffer. Flushes when batch is full.\"\"\"\n",
    "        with self._lock:\n",
    "            self._buffer.append(tick)\n",
    "            \n",
    "            # Flush if batch size reached or time interval exceeded\n",
    "            if (len(self._buffer) >= self.batch_size or \n",
    "                time.time() - self._last_flush > self.flush_interval):\n",
    "                self._flush_buffer()\n",
    "    \n",
    "    def add_ticks(self, ticks: List[Dict[str, Any]]):\n",
    "        \"\"\"Add multiple ticks at once.\"\"\"\n",
    "        with self._lock:\n",
    "            self._buffer.extend(ticks)\n",
    "            \n",
    "            while len(self._buffer) >= self.batch_size:\n",
    "                self._flush_buffer()\n",
    "    \n",
    "    def _flush_buffer(self):\n",
    "        \"\"\"Flush the buffer to the database.\"\"\"\n",
    "        if not self._buffer:\n",
    "            return\n",
    "        \n",
    "        # Take batch_size items from buffer\n",
    "        batch = self._buffer[:self.batch_size]\n",
    "        self._buffer = self._buffer[self.batch_size:]\n",
    "        \n",
    "        try:\n",
    "            self._insert_batch(batch)\n",
    "            self.total_inserted += len(batch)\n",
    "            self.total_batches += 1\n",
    "        except Exception as e:\n",
    "            self.total_errors += 1\n",
    "            print(f\"Error inserting batch: {e}\")\n",
    "            # Could implement retry logic or dead letter queue here\n",
    "        \n",
    "        self._last_flush = time.time()\n",
    "    \n",
    "    def _insert_batch(self, batch: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Insert a batch of ticks using efficient upsert.\n",
    "        \n",
    "        Uses PostgreSQL ON CONFLICT DO UPDATE for deduplication.\n",
    "        \"\"\"\n",
    "        if not batch:\n",
    "            return\n",
    "        \n",
    "        # SQL for upsert (INSERT ... ON CONFLICT)\n",
    "        upsert_sql = \"\"\"\n",
    "        INSERT INTO market_data.trade_ticks (\n",
    "            timestamp_ns, timestamp, symbol, price, volume,\n",
    "            trade_id, exchange, side, conditions, sequence_num, notional_value\n",
    "        ) VALUES (\n",
    "            :timestamp_ns, :timestamp, :symbol, :price, :volume,\n",
    "            :trade_id, :exchange, :side, :conditions, :sequence_num, :notional_value\n",
    "        )\n",
    "        ON CONFLICT (symbol, timestamp_ns, exchange, trade_id) \n",
    "        DO UPDATE SET\n",
    "            price = EXCLUDED.price,\n",
    "            volume = EXCLUDED.volume,\n",
    "            side = EXCLUDED.side,\n",
    "            notional_value = EXCLUDED.notional_value\n",
    "        \"\"\"\n",
    "        \n",
    "        # Using SQLAlchemy's execute with parameter binding\n",
    "        # In production, use psycopg2.extras.execute_values for better performance\n",
    "        with self.db_manager.get_session() as session:\n",
    "            session.execute(text(upsert_sql), batch)\n",
    "    \n",
    "    def flush(self):\n",
    "        \"\"\"Force flush any remaining items in buffer.\"\"\"\n",
    "        with self._lock:\n",
    "            while self._buffer:\n",
    "                self._flush_buffer()\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get ingestion statistics.\"\"\"\n",
    "        return {\n",
    "            \"total_inserted\": self.total_inserted,\n",
    "            \"total_batches\": self.total_batches,\n",
    "            \"total_errors\": self.total_errors,\n",
    "            \"buffer_size\": len(self._buffer),\n",
    "            \"avg_batch_size\": self.total_inserted / max(1, self.total_batches),\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BULK INSERT METHODS COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def generate_sample_ticks(n: int, symbol: str = \"AAPL\") -> List[Dict]:\n",
    "    \"\"\"Generate sample tick data for testing.\"\"\"\n",
    "    base_time = datetime(2024, 1, 15, 9, 30, 0)\n",
    "    base_price = 150.0\n",
    "    \n",
    "    ticks = []\n",
    "    for i in range(n):\n",
    "        timestamp = base_time + timedelta(microseconds=i * 100)\n",
    "        price = base_price + np.random.randn() * 0.01\n",
    "        volume = np.random.randint(100, 10000)\n",
    "        \n",
    "        ticks.append({\n",
    "            \"timestamp_ns\": int(timestamp.timestamp() * 1e9),\n",
    "            \"timestamp\": timestamp,\n",
    "            \"symbol\": symbol,\n",
    "            \"price\": round(price, 4),\n",
    "            \"volume\": volume,\n",
    "            \"trade_id\": f\"T{i:010d}\",\n",
    "            \"exchange\": \"NASDAQ\",\n",
    "            \"side\": np.random.choice([\"buy\", \"sell\"]),\n",
    "            \"conditions\": None,\n",
    "            \"sequence_num\": i,\n",
    "            \"notional_value\": round(price * volume, 2),\n",
    "        })\n",
    "    \n",
    "    return ticks\n",
    "\n",
    "\n",
    "def get_bulk_insert_sql_examples() -> Dict[str, str]:\n",
    "    \"\"\"Return different SQL patterns for bulk inserts.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"multi_value_insert\": \"\"\"\n",
    "-- Multi-value INSERT (moderate performance)\n",
    "INSERT INTO market_data.trade_ticks (timestamp, symbol, price, volume)\n",
    "VALUES \n",
    "    ('2024-01-15 09:30:00', 'AAPL', 150.25, 100),\n",
    "    ('2024-01-15 09:30:01', 'AAPL', 150.26, 200),\n",
    "    ('2024-01-15 09:30:02', 'AAPL', 150.24, 150);\n",
    "        \"\"\",\n",
    "        \n",
    "        \"copy_command\": \"\"\"\n",
    "-- COPY command (fastest for bulk loads)\n",
    "COPY market_data.trade_ticks (timestamp, symbol, price, volume)\n",
    "FROM '/path/to/data.csv'\n",
    "WITH (FORMAT csv, HEADER true, DELIMITER ',');\n",
    "        \"\"\",\n",
    "        \n",
    "        \"copy_from_stdin\": \"\"\"\n",
    "-- COPY from STDIN (for programmatic access)\n",
    "-- Python: cursor.copy_expert(\"COPY table FROM STDIN CSV\", file_obj)\n",
    "COPY market_data.trade_ticks (timestamp, symbol, price, volume)\n",
    "FROM STDIN WITH (FORMAT csv);\n",
    "        \"\"\",\n",
    "        \n",
    "        \"upsert_on_conflict\": \"\"\"\n",
    "-- Upsert with ON CONFLICT (for deduplication)\n",
    "INSERT INTO market_data.trade_ticks (timestamp, symbol, price, volume, trade_id)\n",
    "VALUES ('2024-01-15 09:30:00', 'AAPL', 150.25, 100, 'T001')\n",
    "ON CONFLICT (symbol, timestamp, trade_id) \n",
    "DO UPDATE SET \n",
    "    price = EXCLUDED.price,\n",
    "    volume = EXCLUDED.volume;\n",
    "        \"\"\",\n",
    "        \n",
    "        \"staging_table\": \"\"\"\n",
    "-- Staging table pattern (for complex ETL)\n",
    "-- Step 1: Load into unlogged staging table (fast)\n",
    "CREATE UNLOGGED TABLE staging_ticks (LIKE market_data.trade_ticks);\n",
    "COPY staging_ticks FROM '/path/to/data.csv' CSV;\n",
    "\n",
    "-- Step 2: Insert with deduplication\n",
    "INSERT INTO market_data.trade_ticks\n",
    "SELECT DISTINCT ON (symbol, timestamp, trade_id) *\n",
    "FROM staging_ticks\n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- Step 3: Cleanup\n",
    "DROP TABLE staging_ticks;\n",
    "        \"\"\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Display insertion methods\n",
    "print(\"=\" * 80)\n",
    "print(\"BULK INSERT METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "examples = get_bulk_insert_sql_examples()\n",
    "for name, sql in examples.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Method: {name}\")\n",
    "    print(\"=\"*40)\n",
    "    print(sql)\n",
    "\n",
    "# Sample tick generation demo\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE TICK DATA STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_ticks = generate_sample_ticks(3)\n",
    "for tick in sample_ticks:\n",
    "    print(f\"\\n{tick}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805d4ff",
   "metadata": {},
   "source": [
    "## 10. Query Optimization Patterns <a id=\"10-query-optimization\"></a>\n",
    "\n",
    "### Common Query Patterns in Quant Finance\n",
    "\n",
    "1. **Time Range Query**: Get all ticks for a symbol between two timestamps\n",
    "2. **Cross-Sectional Query**: Get latest price for all symbols at a point in time\n",
    "3. **Resampling**: Aggregate tick data to OHLCV bars\n",
    "4. **Rolling Window**: Calculate rolling statistics (SMA, volatility)\n",
    "5. **ASOF Join**: Match trades to quotes at the time of trade\n",
    "6. **Gap Detection**: Find missing data periods\n",
    "\n",
    "### Query Optimization Techniques\n",
    "- Use EXPLAIN ANALYZE to understand query plans\n",
    "- Leverage covering indexes (INCLUDE clause)\n",
    "- Use window functions instead of self-joins\n",
    "- Prefer CTEs for readability, but inline for performance\n",
    "- Use materialized views for expensive aggregations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
