{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7851b2a2",
   "metadata": {},
   "source": [
    "# Day 04: Low-Latency System Design for Trading\n",
    "\n",
    "## Week 22 - System Design\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand latency fundamentals and measurement techniques\n",
    "- Master high-resolution timing and profiling in Python\n",
    "- Implement lock-free data structures and memory-efficient patterns\n",
    "- Compare synchronous vs asynchronous processing\n",
    "- Build low-latency components for trading systems\n",
    "\n",
    "### Why Latency Matters in Trading\n",
    "- **Microseconds matter**: In HFT, a 1μs advantage can mean winning or losing trades\n",
    "- **Competitive edge**: Lower latency = better fills, tighter spreads, more alpha\n",
    "- **Risk management**: Delayed signals can lead to significant losses\n",
    "- **Regulatory requirements**: MiFID II requires timestamp accuracy to microseconds\n",
    "\n",
    "### Latency Budget Example (Typical HFT System)\n",
    "| Component | Target Latency |\n",
    "|-----------|----------------|\n",
    "| Market Data Parsing | < 1 μs |\n",
    "| Signal Generation | < 5 μs |\n",
    "| Risk Check | < 2 μs |\n",
    "| Order Encoding | < 1 μs |\n",
    "| Network (colocation) | < 10 μs |\n",
    "| **Total Tick-to-Trade** | **< 20 μs** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb46e4f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2288ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "Python version: 3.14.2 (main, Dec  5 2025, 16:49:16) [Clang 17.0.0 (clang-1700.4.4.1)]\n",
      "NumPy version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Queue as MPQueue\n",
    "from collections import deque\n",
    "from queue import Queue, PriorityQueue\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from functools import wraps\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, List, Optional, Dict\n",
    "import statistics\n",
    "import random\n",
    "import socket\n",
    "import struct\n",
    "import heapq\n",
    "from contextlib import contextmanager\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"Python version: {__import__('sys').version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9566f9",
   "metadata": {},
   "source": [
    "## 2. Measuring Latency Basics\n",
    "\n",
    "### High-Resolution Timing in Python\n",
    "\n",
    "Python offers several timing mechanisms with different resolutions:\n",
    "- `time.time()` - Wall clock time (microsecond resolution)\n",
    "- `time.perf_counter()` - Performance counter (nanosecond resolution)\n",
    "- `time.perf_counter_ns()` - Performance counter in nanoseconds (best for latency)\n",
    "- `time.monotonic()` - Monotonic clock (cannot go backwards)\n",
    "\n",
    "**For trading systems, always use `time.perf_counter_ns()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd46db88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing Method Overhead Comparison\n",
      "==================================================\n",
      "time.time()              :    51.45 ns/call\n",
      "time.perf_counter()      :    51.46 ns/call\n",
      "time.perf_counter_ns()   :    56.65 ns/call\n",
      "time.monotonic()         :    51.21 ns/call\n",
      "time.monotonic_ns()      :    65.32 ns/call\n"
     ]
    }
   ],
   "source": [
    "# Compare different timing methods\n",
    "def compare_timing_methods():\n",
    "    \"\"\"Compare resolution and overhead of different timing methods.\"\"\"\n",
    "    methods = {\n",
    "        'time.time()': time.time,\n",
    "        'time.perf_counter()': time.perf_counter,\n",
    "        'time.perf_counter_ns()': time.perf_counter_ns,\n",
    "        'time.monotonic()': time.monotonic,\n",
    "        'time.monotonic_ns()': time.monotonic_ns\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    n_samples = 10000\n",
    "    \n",
    "    for name, method in methods.items():\n",
    "        # Measure overhead of calling the timing function itself\n",
    "        start = time.perf_counter_ns()\n",
    "        for _ in range(n_samples):\n",
    "            _ = method()\n",
    "        end = time.perf_counter_ns()\n",
    "        \n",
    "        overhead_ns = (end - start) / n_samples\n",
    "        results[name] = overhead_ns\n",
    "    \n",
    "    print(\"Timing Method Overhead Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    for name, overhead in results.items():\n",
    "        print(f\"{name:25s}: {overhead:8.2f} ns/call\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "overhead_results = compare_timing_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb40e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Latency Report: demo_operation\n",
      "============================================================\n",
      "Samples: 9,950\n",
      "\n",
      "Statistic           Nanoseconds    Microseconds\n",
      "-----------------------------------------------\n",
      "MEAN                        548            0.55\n",
      "MEDIAN                      500            0.50\n",
      "STD                       3,222            3.22\n",
      "MIN                         417            0.42\n",
      "MAX                     227,416          227.42\n",
      "P50                         500            0.50\n",
      "P90                         500            0.50\n",
      "P99                         625            0.62\n",
      "P999                      4,333            4.33\n"
     ]
    }
   ],
   "source": [
    "class LatencyTracker:\n",
    "    \"\"\"High-precision latency tracking for trading systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, warmup_iterations: int = 100):\n",
    "        self.name = name\n",
    "        self.warmup_iterations = warmup_iterations\n",
    "        self.latencies_ns: List[int] = []\n",
    "        self._warmup_done = False\n",
    "        self._warmup_count = 0\n",
    "    \n",
    "    @contextmanager\n",
    "    def measure(self):\n",
    "        \"\"\"Context manager for measuring latency.\"\"\"\n",
    "        start = time.perf_counter_ns()\n",
    "        yield\n",
    "        end = time.perf_counter_ns()\n",
    "        latency = end - start\n",
    "        \n",
    "        # Skip warmup iterations (JIT, cache warming)\n",
    "        if self._warmup_count < self.warmup_iterations:\n",
    "            self._warmup_count += 1\n",
    "        else:\n",
    "            self.latencies_ns.append(latency)\n",
    "    \n",
    "    def record(self, latency_ns: int):\n",
    "        \"\"\"Manually record a latency measurement.\"\"\"\n",
    "        if self._warmup_count < self.warmup_iterations:\n",
    "            self._warmup_count += 1\n",
    "        else:\n",
    "            self.latencies_ns.append(latency_ns)\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, float]:\n",
    "        \"\"\"Calculate latency statistics.\"\"\"\n",
    "        if not self.latencies_ns:\n",
    "            return {}\n",
    "        \n",
    "        sorted_latencies = sorted(self.latencies_ns)\n",
    "        n = len(sorted_latencies)\n",
    "        \n",
    "        return {\n",
    "            'name': self.name,\n",
    "            'count': n,\n",
    "            'mean_ns': statistics.mean(sorted_latencies),\n",
    "            'median_ns': statistics.median(sorted_latencies),\n",
    "            'std_ns': statistics.stdev(sorted_latencies) if n > 1 else 0,\n",
    "            'min_ns': sorted_latencies[0],\n",
    "            'max_ns': sorted_latencies[-1],\n",
    "            'p50_ns': sorted_latencies[int(n * 0.50)],\n",
    "            'p90_ns': sorted_latencies[int(n * 0.90)],\n",
    "            'p99_ns': sorted_latencies[int(n * 0.99)],\n",
    "            'p999_ns': sorted_latencies[int(n * 0.999)] if n >= 1000 else sorted_latencies[-1],\n",
    "        }\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print a formatted latency report.\"\"\"\n",
    "        stats = self.get_statistics()\n",
    "        if not stats:\n",
    "            print(f\"No measurements for {self.name}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Latency Report: {stats['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Samples: {stats['count']:,}\")\n",
    "        print(f\"\\n{'Statistic':<15} {'Nanoseconds':>15} {'Microseconds':>15}\")\n",
    "        print(\"-\" * 47)\n",
    "        \n",
    "        for key in ['mean', 'median', 'std', 'min', 'max', 'p50', 'p90', 'p99', 'p999']:\n",
    "            ns_key = f'{key}_ns'\n",
    "            if ns_key in stats:\n",
    "                ns_val = stats[ns_key]\n",
    "                us_val = ns_val / 1000\n",
    "                print(f\"{key.upper():<15} {ns_val:>15,.0f} {us_val:>15,.2f}\")\n",
    "\n",
    "# Demonstrate the latency tracker\n",
    "tracker = LatencyTracker(\"demo_operation\", warmup_iterations=50)\n",
    "\n",
    "# Simulate some operations\n",
    "for i in range(10000):\n",
    "    with tracker.measure():\n",
    "        # Simulate work\n",
    "        _ = sum(range(100))\n",
    "\n",
    "tracker.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffbf5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Latency Report: simulated_order_validation\n",
      "============================================================\n",
      "Samples: 5,000\n",
      "\n",
      "Statistic           Nanoseconds    Microseconds\n",
      "-----------------------------------------------\n",
      "MEAN                        270            0.27\n",
      "MEDIAN                      209            0.21\n",
      "STD                         658            0.66\n",
      "MIN                         166            0.17\n",
      "MAX                      28,458           28.46\n",
      "P50                         209            0.21\n",
      "P90                         250            0.25\n",
      "P99                         708            0.71\n",
      "P999                     12,208           12.21\n"
     ]
    }
   ],
   "source": [
    "def latency_benchmark(warmup: int = 100, iterations: int = 10000):\n",
    "    \"\"\"Decorator to benchmark function latency.\"\"\"\n",
    "    def decorator(func: Callable):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            tracker = LatencyTracker(func.__name__, warmup_iterations=warmup)\n",
    "            result = None\n",
    "            \n",
    "            for _ in range(warmup + iterations):\n",
    "                with tracker.measure():\n",
    "                    result = func(*args, **kwargs)\n",
    "            \n",
    "            tracker.print_report()\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage\n",
    "@latency_benchmark(warmup=100, iterations=5000)\n",
    "def simulated_order_validation(order_size: float, max_size: float = 1_000_000) -> bool:\n",
    "    \"\"\"Simulate order validation logic.\"\"\"\n",
    "    # Bounds check\n",
    "    if order_size <= 0 or order_size > max_size:\n",
    "        return False\n",
    "    # Simulate additional checks\n",
    "    _ = order_size * 1.001  # Apply some calculation\n",
    "    return True\n",
    "\n",
    "# Run the benchmark\n",
    "_ = simulated_order_validation(1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b7a013",
   "metadata": {},
   "source": [
    "## 3. Memory Access Patterns and Cache Optimization\n",
    "\n",
    "### Why Cache Matters\n",
    "\n",
    "Modern CPUs have a memory hierarchy:\n",
    "- **L1 Cache**: ~1-4 cycles, ~32-64 KB\n",
    "- **L2 Cache**: ~10-20 cycles, ~256-512 KB  \n",
    "- **L3 Cache**: ~40-75 cycles, ~8-32 MB\n",
    "- **Main Memory (RAM)**: ~100-300 cycles\n",
    "\n",
    "**Cache-friendly code can be 10-100x faster!**\n",
    "\n",
    "Key principles:\n",
    "1. **Spatial locality**: Access memory sequentially\n",
    "2. **Temporal locality**: Reuse recently accessed data\n",
    "3. **Avoid cache thrashing**: Don't exceed cache size in tight loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cab233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Access Pattern Comparison\n",
      "============================================================\n",
      "Matrix size: 10,000 x 10,000 = 100,000,000 elements\n",
      "Memory size: 762.9 MB\n",
      "\n",
      "row_major           :   12551.09 ms  (speedup:    1.0x)\n",
      "col_major           :   11548.30 ms  (speedup:    1.1x)\n",
      "vectorized          :      19.56 ms  (speedup:  641.8x)\n"
     ]
    }
   ],
   "source": [
    "def benchmark_memory_access_patterns():\n",
    "    \"\"\"\n",
    "    Demonstrate the massive performance difference between \n",
    "    cache-friendly and cache-unfriendly memory access.\n",
    "    \"\"\"\n",
    "    # Create a large 2D array (row-major storage in NumPy/C)\n",
    "    rows, cols = 10000, 10000\n",
    "    matrix = np.random.randn(rows, cols)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Row-major access (cache-friendly for C-contiguous arrays)\n",
    "    tracker_row = LatencyTracker(\"row_major_access\", warmup_iterations=2)\n",
    "    for _ in range(5):\n",
    "        with tracker_row.measure():\n",
    "            total = 0.0\n",
    "            for i in range(rows):\n",
    "                for j in range(cols):\n",
    "                    total += matrix[i, j]\n",
    "    results['row_major'] = tracker_row.get_statistics()\n",
    "    \n",
    "    # Column-major access (cache-unfriendly for C-contiguous arrays)\n",
    "    tracker_col = LatencyTracker(\"col_major_access\", warmup_iterations=2)\n",
    "    for _ in range(5):\n",
    "        with tracker_col.measure():\n",
    "            total = 0.0\n",
    "            for j in range(cols):\n",
    "                for i in range(rows):\n",
    "                    total += matrix[i, j]\n",
    "    results['col_major'] = tracker_col.get_statistics()\n",
    "    \n",
    "    # NumPy vectorized (best - uses SIMD and optimal memory access)\n",
    "    tracker_vec = LatencyTracker(\"numpy_vectorized\", warmup_iterations=2)\n",
    "    for _ in range(5):\n",
    "        with tracker_vec.measure():\n",
    "            total = np.sum(matrix)\n",
    "    results['vectorized'] = tracker_vec.get_statistics()\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nMemory Access Pattern Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Matrix size: {rows:,} x {cols:,} = {rows*cols:,} elements\")\n",
    "    print(f\"Memory size: {matrix.nbytes / 1024 / 1024:.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    baseline = results['row_major']['mean_ns']\n",
    "    for pattern, stats in results.items():\n",
    "        mean_ms = stats['mean_ns'] / 1_000_000\n",
    "        speedup = baseline / stats['mean_ns']\n",
    "        print(f\"{pattern:20s}: {mean_ms:10.2f} ms  (speedup: {speedup:6.1f}x)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "cache_results = benchmark_memory_access_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718990d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Struct of Arrays vs Array of Structs\n",
      "============================================================\n",
      "Orders: 100,000\n",
      "\n",
      "AoS (list of objects):\n",
      "  Mean: 4375.86 μs, P99: 5758.00 μs\n",
      "\n",
      "SoA (columnar NumPy):\n",
      "  Mean: 1024.28 μs, P99: 1129.58 μs\n",
      "\n",
      "Speedup: 4.3x\n"
     ]
    }
   ],
   "source": [
    "def benchmark_struct_of_arrays_vs_array_of_structs():\n",
    "    \"\"\"\n",
    "    Compare SoA (Struct of Arrays) vs AoS (Array of Structs) patterns.\n",
    "    SoA is typically more cache-efficient for columnar operations.\n",
    "    \"\"\"\n",
    "    n_orders = 100000\n",
    "    \n",
    "    # Array of Structs (AoS) - like a list of dictionaries\n",
    "    @dataclass\n",
    "    class Order:\n",
    "        order_id: int\n",
    "        price: float\n",
    "        quantity: int\n",
    "        side: int  # 0=buy, 1=sell\n",
    "    \n",
    "    # Create AoS\n",
    "    orders_aos = [\n",
    "        Order(\n",
    "            order_id=i,\n",
    "            price=100.0 + random.random(),\n",
    "            quantity=random.randint(1, 1000),\n",
    "            side=random.randint(0, 1)\n",
    "        )\n",
    "        for i in range(n_orders)\n",
    "    ]\n",
    "    \n",
    "    # Struct of Arrays (SoA) - columnar layout\n",
    "    orders_soa = {\n",
    "        'order_id': np.arange(n_orders, dtype=np.int64),\n",
    "        'price': 100.0 + np.random.random(n_orders),\n",
    "        'quantity': np.random.randint(1, 1000, n_orders),\n",
    "        'side': np.random.randint(0, 2, n_orders)\n",
    "    }\n",
    "    \n",
    "    # Benchmark: Calculate total buy value (price * quantity for buy orders)\n",
    "    \n",
    "    # AoS approach\n",
    "    tracker_aos = LatencyTracker(\"AoS_calculation\", warmup_iterations=10)\n",
    "    for _ in range(100):\n",
    "        with tracker_aos.measure():\n",
    "            total_buy_value = sum(\n",
    "                o.price * o.quantity \n",
    "                for o in orders_aos \n",
    "                if o.side == 0\n",
    "            )\n",
    "    \n",
    "    # SoA approach with NumPy\n",
    "    tracker_soa = LatencyTracker(\"SoA_calculation\", warmup_iterations=10)\n",
    "    for _ in range(100):\n",
    "        with tracker_soa.measure():\n",
    "            buy_mask = orders_soa['side'] == 0\n",
    "            total_buy_value = np.sum(\n",
    "                orders_soa['price'][buy_mask] * orders_soa['quantity'][buy_mask]\n",
    "            )\n",
    "    \n",
    "    aos_stats = tracker_aos.get_statistics()\n",
    "    soa_stats = tracker_soa.get_statistics()\n",
    "    \n",
    "    print(\"\\nStruct of Arrays vs Array of Structs\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Orders: {n_orders:,}\")\n",
    "    print(f\"\\nAoS (list of objects):\")\n",
    "    print(f\"  Mean: {aos_stats['mean_ns']/1000:.2f} μs, P99: {aos_stats['p99_ns']/1000:.2f} μs\")\n",
    "    print(f\"\\nSoA (columnar NumPy):\")\n",
    "    print(f\"  Mean: {soa_stats['mean_ns']/1000:.2f} μs, P99: {soa_stats['p99_ns']/1000:.2f} μs\")\n",
    "    print(f\"\\nSpeedup: {aos_stats['mean_ns']/soa_stats['mean_ns']:.1f}x\")\n",
    "\n",
    "benchmark_struct_of_arrays_vs_array_of_structs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761fa6a",
   "metadata": {},
   "source": [
    "## 4. Lock-Free Data Structures\n",
    "\n",
    "### Why Lock-Free?\n",
    "- **Contention**: Locks create serialization points that increase latency\n",
    "- **Priority inversion**: High-priority threads blocked by low-priority lock holders\n",
    "- **Deadlocks**: Lock ordering bugs can halt the system\n",
    "- **Context switches**: Lock contention causes expensive OS context switches\n",
    "\n",
    "### Lock-Free Alternatives\n",
    "- `collections.deque` - Thread-safe for append/pop operations\n",
    "- Atomic operations via `threading.Lock` with minimal hold time\n",
    "- Ring buffers with atomic indices\n",
    "- Compare-and-swap (CAS) patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17694571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Queue Implementation Comparison\n",
      "============================================================\n",
      "Operations: 100,000 push+pop pairs\n",
      "\n",
      "threading.Queue          \n",
      "  Mean: 1,042 ns, P99: 1,166 ns\n",
      "collections.deque        \n",
      "  Mean: 138 ns, P99: 167 ns\n",
      "LockFreeRingBuffer       \n",
      "  Mean: 277 ns, P99: 333 ns\n"
     ]
    }
   ],
   "source": [
    "class LockFreeRingBuffer:\n",
    "    \"\"\"\n",
    "    A simple lock-free single-producer single-consumer (SPSC) ring buffer.\n",
    "    Uses atomic-like operations with memory barriers implied by Python's GIL.\n",
    "    \n",
    "    In production C++/Rust, you'd use std::atomic with proper memory ordering.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        # Power of 2 for fast modulo via bitwise AND\n",
    "        self.capacity = 1 << (capacity - 1).bit_length()\n",
    "        self.mask = self.capacity - 1\n",
    "        self.buffer = [None] * self.capacity\n",
    "        self.head = 0  # Write position (producer)\n",
    "        self.tail = 0  # Read position (consumer)\n",
    "    \n",
    "    def push(self, item) -> bool:\n",
    "        \"\"\"Add item to buffer. Returns False if full.\"\"\"\n",
    "        next_head = (self.head + 1) & self.mask\n",
    "        if next_head == self.tail:\n",
    "            return False  # Buffer full\n",
    "        self.buffer[self.head] = item\n",
    "        self.head = next_head\n",
    "        return True\n",
    "    \n",
    "    def pop(self):\n",
    "        \"\"\"Remove and return item. Returns None if empty.\"\"\"\n",
    "        if self.tail == self.head:\n",
    "            return None  # Buffer empty\n",
    "        item = self.buffer[self.tail]\n",
    "        self.tail = (self.tail + 1) & self.mask\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (self.head - self.tail) & self.mask\n",
    "\n",
    "\n",
    "class PreallocatedObjectPool:\n",
    "    \"\"\"\n",
    "    Object pool to avoid allocation latency in hot paths.\n",
    "    Pre-allocate objects and reuse them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, factory: Callable, initial_size: int = 1000):\n",
    "        self.factory = factory\n",
    "        self.pool = deque([factory() for _ in range(initial_size)])\n",
    "        self.allocated = 0\n",
    "    \n",
    "    def acquire(self):\n",
    "        \"\"\"Get an object from the pool.\"\"\"\n",
    "        if self.pool:\n",
    "            return self.pool.pop()\n",
    "        self.allocated += 1\n",
    "        return self.factory()\n",
    "    \n",
    "    def release(self, obj):\n",
    "        \"\"\"Return an object to the pool.\"\"\"\n",
    "        self.pool.append(obj)\n",
    "    \n",
    "    @contextmanager\n",
    "    def borrow(self):\n",
    "        \"\"\"Context manager for automatic release.\"\"\"\n",
    "        obj = self.acquire()\n",
    "        try:\n",
    "            yield obj\n",
    "        finally:\n",
    "            self.release(obj)\n",
    "\n",
    "\n",
    "# Demonstrate ring buffer vs standard queue\n",
    "def benchmark_queue_implementations():\n",
    "    \"\"\"Compare different queue implementations.\"\"\"\n",
    "    n_operations = 100000\n",
    "    \n",
    "    # Standard Queue (thread-safe with locks)\n",
    "    std_queue = Queue()\n",
    "    tracker_std = LatencyTracker(\"threading.Queue\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_operations):\n",
    "        with tracker_std.measure():\n",
    "            std_queue.put(i)\n",
    "            _ = std_queue.get()\n",
    "    \n",
    "    # collections.deque (lock-free for single operations)\n",
    "    dq = deque()\n",
    "    tracker_deque = LatencyTracker(\"collections.deque\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_operations):\n",
    "        with tracker_deque.measure():\n",
    "            dq.append(i)\n",
    "            _ = dq.popleft()\n",
    "    \n",
    "    # Custom ring buffer\n",
    "    ring = LockFreeRingBuffer(1024)\n",
    "    tracker_ring = LatencyTracker(\"LockFreeRingBuffer\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_operations):\n",
    "        with tracker_ring.measure():\n",
    "            ring.push(i)\n",
    "            _ = ring.pop()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nQueue Implementation Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Operations: {n_operations:,} push+pop pairs\\n\")\n",
    "    \n",
    "    for tracker in [tracker_std, tracker_deque, tracker_ring]:\n",
    "        stats = tracker.get_statistics()\n",
    "        print(f\"{stats['name']:25s}\")\n",
    "        print(f\"  Mean: {stats['mean_ns']:,.0f} ns, P99: {stats['p99_ns']:,.0f} ns\")\n",
    "\n",
    "benchmark_queue_implementations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55bd7ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Object Pool vs Fresh Allocation\n",
      "============================================================\n",
      "Operations: 50,000\n",
      "\n",
      "Fresh allocation:\n",
      "  Mean: 656 ns, P99: 750 ns\n",
      "\n",
      "Object pool:\n",
      "  Mean: 1,136 ns, P99: 1,250 ns\n",
      "\n",
      "Speedup: 0.58x\n",
      "P99 improvement: 0.60x\n"
     ]
    }
   ],
   "source": [
    "# Benchmark object pool vs fresh allocation\n",
    "def benchmark_object_pool():\n",
    "    \"\"\"Compare allocation latency with and without object pooling.\"\"\"\n",
    "    \n",
    "    @dataclass\n",
    "    class OrderMessage:\n",
    "        \"\"\"Represents an order message.\"\"\"\n",
    "        order_id: int = 0\n",
    "        symbol: str = \"\"\n",
    "        price: float = 0.0\n",
    "        quantity: int = 0\n",
    "        side: str = \"BUY\"\n",
    "        timestamp: int = 0\n",
    "    \n",
    "    n_operations = 50000\n",
    "    \n",
    "    # Without pool - fresh allocation each time\n",
    "    tracker_alloc = LatencyTracker(\"fresh_allocation\", warmup_iterations=1000)\n",
    "    for i in range(n_operations):\n",
    "        with tracker_alloc.measure():\n",
    "            msg = OrderMessage(\n",
    "                order_id=i,\n",
    "                symbol=\"AAPL\",\n",
    "                price=150.0,\n",
    "                quantity=100,\n",
    "                side=\"BUY\",\n",
    "                timestamp=time.perf_counter_ns()\n",
    "            )\n",
    "            # Simulate using the object\n",
    "            _ = msg.price * msg.quantity\n",
    "            # Object goes out of scope (GC pressure)\n",
    "    \n",
    "    # With pool - reuse pre-allocated objects\n",
    "    pool = PreallocatedObjectPool(OrderMessage, initial_size=100)\n",
    "    tracker_pool = LatencyTracker(\"object_pool\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_operations):\n",
    "        with tracker_pool.measure():\n",
    "            with pool.borrow() as msg:\n",
    "                msg.order_id = i\n",
    "                msg.symbol = \"AAPL\"\n",
    "                msg.price = 150.0\n",
    "                msg.quantity = 100\n",
    "                msg.side = \"BUY\"\n",
    "                msg.timestamp = time.perf_counter_ns()\n",
    "                _ = msg.price * msg.quantity\n",
    "    \n",
    "    alloc_stats = tracker_alloc.get_statistics()\n",
    "    pool_stats = tracker_pool.get_statistics()\n",
    "    \n",
    "    print(\"\\nObject Pool vs Fresh Allocation\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Operations: {n_operations:,}\\n\")\n",
    "    print(f\"Fresh allocation:\")\n",
    "    print(f\"  Mean: {alloc_stats['mean_ns']:,.0f} ns, P99: {alloc_stats['p99_ns']:,.0f} ns\")\n",
    "    print(f\"\\nObject pool:\")\n",
    "    print(f\"  Mean: {pool_stats['mean_ns']:,.0f} ns, P99: {pool_stats['p99_ns']:,.0f} ns\")\n",
    "    print(f\"\\nSpeedup: {alloc_stats['mean_ns']/pool_stats['mean_ns']:.2f}x\")\n",
    "    print(f\"P99 improvement: {alloc_stats['p99_ns']/pool_stats['p99_ns']:.2f}x\")\n",
    "\n",
    "benchmark_object_pool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd631f",
   "metadata": {},
   "source": [
    "## 5. Async vs Sync Processing Comparison\n",
    "\n",
    "### When to Use Async\n",
    "- **I/O bound workloads**: Network calls, file I/O, database queries\n",
    "- **Many concurrent connections**: Thousands of market data feeds\n",
    "- **Non-blocking requirements**: Don't want to block main event loop\n",
    "\n",
    "### When to Use Sync\n",
    "- **CPU bound workloads**: Signal computation, risk calculations\n",
    "- **Low latency requirements**: Async adds ~1-5 μs overhead\n",
    "- **Simple sequential logic**: Avoid complexity where not needed\n",
    "\n",
    "### Trading System Pattern\n",
    "Most HFT systems use a **hybrid approach**:\n",
    "- Async for market data ingestion (many feeds)\n",
    "- Sync for the hot path (signal → order)\n",
    "- Separate threads for different concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330823f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous sequential benchmark...\n",
      "Running asynchronous concurrent benchmark...\n",
      "\n",
      "Async vs Sync I/O Comparison\n",
      "============================================================\n",
      "Operations: 100, Simulated I/O delay: 0.5 ms each\n",
      "\n",
      "Synchronous (sequential):\n",
      "  Total time: 63.70 ms\n",
      "  Per operation: 0.64 ms\n",
      "\n",
      "Asynchronous (concurrent):\n",
      "  Total time: 1.20 ms\n",
      "  Per operation: 0.01 ms\n",
      "\n",
      "Speedup: 53.0x\n",
      "Theoretical max: 100x (fully parallel)\n"
     ]
    }
   ],
   "source": [
    "async def async_io_simulation(delay_ms: float = 0.1):\n",
    "    \"\"\"Simulate an async I/O operation.\"\"\"\n",
    "    await asyncio.sleep(delay_ms / 1000)\n",
    "    return time.perf_counter_ns()\n",
    "\n",
    "def sync_io_simulation(delay_ms: float = 0.1):\n",
    "    \"\"\"Simulate a sync I/O operation.\"\"\"\n",
    "    time.sleep(delay_ms / 1000)\n",
    "    return time.perf_counter_ns()\n",
    "\n",
    "async def benchmark_async_vs_sync():\n",
    "    \"\"\"Compare async and sync processing patterns.\"\"\"\n",
    "    n_operations = 100\n",
    "    delay_ms = 0.5\n",
    "    \n",
    "    # Synchronous sequential processing\n",
    "    print(\"Running synchronous sequential benchmark...\")\n",
    "    start_sync = time.perf_counter_ns()\n",
    "    sync_results = []\n",
    "    for _ in range(n_operations):\n",
    "        result = sync_io_simulation(delay_ms)\n",
    "        sync_results.append(result)\n",
    "    end_sync = time.perf_counter_ns()\n",
    "    sync_total_ms = (end_sync - start_sync) / 1_000_000\n",
    "    \n",
    "    # Asynchronous concurrent processing\n",
    "    print(\"Running asynchronous concurrent benchmark...\")\n",
    "    start_async = time.perf_counter_ns()\n",
    "    tasks = [async_io_simulation(delay_ms) for _ in range(n_operations)]\n",
    "    async_results = await asyncio.gather(*tasks)\n",
    "    end_async = time.perf_counter_ns()\n",
    "    async_total_ms = (end_async - start_async) / 1_000_000\n",
    "    \n",
    "    print(\"\\nAsync vs Sync I/O Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Operations: {n_operations}, Simulated I/O delay: {delay_ms} ms each\")\n",
    "    print(f\"\\nSynchronous (sequential):\")\n",
    "    print(f\"  Total time: {sync_total_ms:.2f} ms\")\n",
    "    print(f\"  Per operation: {sync_total_ms/n_operations:.2f} ms\")\n",
    "    print(f\"\\nAsynchronous (concurrent):\")\n",
    "    print(f\"  Total time: {async_total_ms:.2f} ms\")\n",
    "    print(f\"  Per operation: {async_total_ms/n_operations:.2f} ms\")\n",
    "    print(f\"\\nSpeedup: {sync_total_ms/async_total_ms:.1f}x\")\n",
    "    print(f\"Theoretical max: {n_operations}x (fully parallel)\")\n",
    "\n",
    "# Run async benchmark\n",
    "await benchmark_async_vs_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a3ec29",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  P99:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masync_stats[\u001b[33m'\u001b[39m\u001b[33mp99_ns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAsync overhead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masync_stats[\u001b[33m'\u001b[39m\u001b[33mmean_ns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39msync_stats[\u001b[33m'\u001b[39m\u001b[33mmean_ns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(async_stats[\u001b[33m'\u001b[39m\u001b[33mmean_ns\u001b[39m\u001b[33m'\u001b[39m]/sync_stats[\u001b[33m'\u001b[39m\u001b[33mmean_ns\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m)*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mbenchmark_async_overhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mbenchmark_async_overhead\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m             _ = \u001b[38;5;28;01mawait\u001b[39;00m async_cpu_work()\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tracker_async\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m tracker_async = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_async_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m sync_stats = tracker_sync.get_statistics()\n\u001b[32m     37\u001b[39m async_stats = tracker_async.get_statistics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.2/Frameworks/Python.framework/Versions/3.14/lib/python3.14/asyncio/base_events.py:695\u001b[39m, in \u001b[36mBaseEventLoop.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    684\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run until the Future is done.\u001b[39;00m\n\u001b[32m    685\u001b[39m \n\u001b[32m    686\u001b[39m \u001b[33;03mIf the argument is a coroutine, it is wrapped in a Task.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m \u001b[33;03mReturn the Future's result, or raise its exception.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m new_task = \u001b[38;5;129;01mnot\u001b[39;00m futures.isfuture(future)\n\u001b[32m    698\u001b[39m future = tasks.ensure_future(future, loop=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.2/Frameworks/Python.framework/Versions/3.14/lib/python3.14/asyncio/base_events.py:631\u001b[39m, in \u001b[36mBaseEventLoop._check_running\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_running():\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThis event loop is already running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    633\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    634\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mCannot run the event loop while another loop is running\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "# Measure async overhead for CPU-bound work\n",
    "def benchmark_async_overhead():\n",
    "    \"\"\"\n",
    "    Measure the overhead of async for CPU-bound work.\n",
    "    Shows why sync is preferred for the hot path.\n",
    "    \"\"\"\n",
    "    def cpu_work():\n",
    "        \"\"\"Simulate CPU-bound work.\"\"\"\n",
    "        total = 0\n",
    "        for i in range(1000):\n",
    "            total += i * i\n",
    "        return total\n",
    "    \n",
    "    async def async_cpu_work():\n",
    "        \"\"\"Same work wrapped in async.\"\"\"\n",
    "        return cpu_work()\n",
    "    \n",
    "    n_iterations = 10000\n",
    "    \n",
    "    # Pure sync\n",
    "    tracker_sync = LatencyTracker(\"sync_cpu_work\", warmup_iterations=100)\n",
    "    for _ in range(n_iterations):\n",
    "        with tracker_sync.measure():\n",
    "            _ = cpu_work()\n",
    "    \n",
    "    # Async with event loop overhead\n",
    "    async def run_async_benchmark():\n",
    "        tracker_async = LatencyTracker(\"async_cpu_work\", warmup_iterations=100)\n",
    "        for _ in range(n_iterations):\n",
    "            with tracker_async.measure():\n",
    "                _ = await async_cpu_work()\n",
    "        return tracker_async\n",
    "    \n",
    "    tracker_async = asyncio.get_event_loop().run_until_complete(run_async_benchmark())\n",
    "    \n",
    "    sync_stats = tracker_sync.get_statistics()\n",
    "    async_stats = tracker_async.get_statistics()\n",
    "    \n",
    "    print(\"\\nAsync Overhead for CPU-bound Work\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Iterations: {n_iterations:,}\")\n",
    "    print(f\"\\nSynchronous:\")\n",
    "    print(f\"  Mean: {sync_stats['mean_ns']:,.0f} ns\")\n",
    "    print(f\"  P99:  {sync_stats['p99_ns']:,.0f} ns\")\n",
    "    print(f\"\\nAsynchronous:\")\n",
    "    print(f\"  Mean: {async_stats['mean_ns']:,.0f} ns\")\n",
    "    print(f\"  P99:  {async_stats['p99_ns']:,.0f} ns\")\n",
    "    print(f\"\\nAsync overhead: {async_stats['mean_ns'] - sync_stats['mean_ns']:,.0f} ns ({(async_stats['mean_ns']/sync_stats['mean_ns'] - 1)*100:.1f}%)\")\n",
    "\n",
    "benchmark_async_overhead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e22d61d",
   "metadata": {},
   "source": [
    "## 6. Message Queue Latency Benchmarking\n",
    "\n",
    "### Message Queue Design Considerations\n",
    "- **In-memory vs Persistent**: In-memory for latency, persistent for reliability\n",
    "- **FIFO vs Priority**: Priority queues add log(n) overhead\n",
    "- **Bounded vs Unbounded**: Bounded prevents memory exhaustion\n",
    "- **Single vs Multi-producer/consumer**: Affects locking requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLatencyMessageQueue:\n",
    "    \"\"\"\n",
    "    A low-latency in-memory message queue for trading systems.\n",
    "    Optimized for single-producer single-consumer pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 65536):\n",
    "        self.buffer = LockFreeRingBuffer(capacity)\n",
    "        self.sequence_number = 0\n",
    "        self.stats = {\n",
    "            'enqueue_count': 0,\n",
    "            'dequeue_count': 0,\n",
    "            'drops': 0\n",
    "        }\n",
    "    \n",
    "    def enqueue(self, message: Any, timestamp_ns: Optional[int] = None) -> bool:\n",
    "        \"\"\"Add message to queue with optional timestamp.\"\"\"\n",
    "        if timestamp_ns is None:\n",
    "            timestamp_ns = time.perf_counter_ns()\n",
    "        \n",
    "        envelope = (self.sequence_number, timestamp_ns, message)\n",
    "        success = self.buffer.push(envelope)\n",
    "        \n",
    "        if success:\n",
    "            self.sequence_number += 1\n",
    "            self.stats['enqueue_count'] += 1\n",
    "        else:\n",
    "            self.stats['drops'] += 1\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def dequeue(self) -> Optional[tuple]:\n",
    "        \"\"\"Remove and return message with metadata.\"\"\"\n",
    "        result = self.buffer.pop()\n",
    "        if result is not None:\n",
    "            self.stats['dequeue_count'] += 1\n",
    "            seq, enqueue_time, message = result\n",
    "            dequeue_time = time.perf_counter_ns()\n",
    "            queue_latency_ns = dequeue_time - enqueue_time\n",
    "            return (seq, message, queue_latency_ns)\n",
    "        return None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class PriorityMessageQueue:\n",
    "    \"\"\"\n",
    "    Priority queue for messages (e.g., cancel orders before new orders).\n",
    "    Uses heapq which has O(log n) insert/extract.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.heap = []\n",
    "        self.sequence = 0\n",
    "    \n",
    "    def enqueue(self, priority: int, message: Any) -> None:\n",
    "        \"\"\"Add message with priority (lower = higher priority).\"\"\"\n",
    "        timestamp = time.perf_counter_ns()\n",
    "        # (priority, sequence, timestamp, message)\n",
    "        heapq.heappush(self.heap, (priority, self.sequence, timestamp, message))\n",
    "        self.sequence += 1\n",
    "    \n",
    "    def dequeue(self) -> Optional[tuple]:\n",
    "        \"\"\"Get highest priority message.\"\"\"\n",
    "        if not self.heap:\n",
    "            return None\n",
    "        priority, seq, enqueue_time, message = heapq.heappop(self.heap)\n",
    "        dequeue_time = time.perf_counter_ns()\n",
    "        return (priority, message, dequeue_time - enqueue_time)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.heap)\n",
    "\n",
    "\n",
    "def benchmark_message_queues():\n",
    "    \"\"\"Compare different message queue implementations.\"\"\"\n",
    "    n_messages = 50000\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Simple deque (baseline)\n",
    "    dq = deque()\n",
    "    tracker_deque = LatencyTracker(\"deque_fifo\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_messages):\n",
    "        enqueue_time = time.perf_counter_ns()\n",
    "        with tracker_deque.measure():\n",
    "            dq.append((i, enqueue_time))\n",
    "            seq, ts = dq.popleft()\n",
    "    results['deque'] = tracker_deque.get_statistics()\n",
    "    \n",
    "    # 2. Low-latency queue with timestamps\n",
    "    llq = LowLatencyMessageQueue(capacity=4096)\n",
    "    tracker_llq = LatencyTracker(\"LowLatencyQueue\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_messages):\n",
    "        with tracker_llq.measure():\n",
    "            llq.enqueue(f\"message_{i}\")\n",
    "            _ = llq.dequeue()\n",
    "    results['low_latency'] = tracker_llq.get_statistics()\n",
    "    \n",
    "    # 3. Priority queue\n",
    "    pq = PriorityMessageQueue()\n",
    "    tracker_pq = LatencyTracker(\"PriorityQueue\", warmup_iterations=1000)\n",
    "    \n",
    "    for i in range(n_messages):\n",
    "        priority = random.randint(0, 10)\n",
    "        with tracker_pq.measure():\n",
    "            pq.enqueue(priority, f\"message_{i}\")\n",
    "            _ = pq.dequeue()\n",
    "    results['priority'] = tracker_pq.get_statistics()\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nMessage Queue Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Queue Type':<25} {'Mean (ns)':<15} {'P99 (ns)':<15} {'P999 (ns)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, stats in results.items():\n",
    "        print(f\"{name:<25} {stats['mean_ns']:<15,.0f} {stats['p99_ns']:<15,.0f} {stats['p999_ns']:<15,.0f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "queue_results = benchmark_message_queues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cb988",
   "metadata": {},
   "source": [
    "## 7. Connection Pooling Implementation\n",
    "\n",
    "### Why Connection Pooling?\n",
    "- **TCP handshake latency**: ~1-2 RTT (round trip time)\n",
    "- **TLS handshake**: Additional ~2-3 RTT\n",
    "- **Connection setup**: Memory allocation, socket buffer setup\n",
    "- **Keep-alive**: Reuse established connections\n",
    "\n",
    "### Trading System Connections\n",
    "- Exchange gateways (FIX, Binary protocols)\n",
    "- Market data feeds  \n",
    "- Database connections\n",
    "- Internal service mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9565e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatedConnection:\n",
    "    \"\"\"Simulates a network connection with setup latency.\"\"\"\n",
    "    \n",
    "    _connection_id = 0\n",
    "    \n",
    "    def __init__(self, host: str, port: int, setup_latency_us: float = 100):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.setup_latency_us = setup_latency_us\n",
    "        self.id = SimulatedConnection._connection_id\n",
    "        SimulatedConnection._connection_id += 1\n",
    "        self.is_connected = False\n",
    "        self.request_count = 0\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Simulate connection establishment.\"\"\"\n",
    "        # Simulate TCP/TLS handshake latency\n",
    "        time.sleep(self.setup_latency_us / 1_000_000)\n",
    "        self.is_connected = True\n",
    "    \n",
    "    def send(self, data: bytes) -> bytes:\n",
    "        \"\"\"Simulate sending data and receiving response.\"\"\"\n",
    "        if not self.is_connected:\n",
    "            self.connect()\n",
    "        self.request_count += 1\n",
    "        # Simulate some processing time (1 μs)\n",
    "        time.sleep(0.000001)\n",
    "        return b\"OK\"\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the connection.\"\"\"\n",
    "        self.is_connected = False\n",
    "\n",
    "\n",
    "class ConnectionPool:\n",
    "    \"\"\"\n",
    "    Connection pool for low-latency request handling.\n",
    "    Pre-establishes and maintains connections.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host: str, port: int, pool_size: int = 10,\n",
    "                 connection_factory: Callable = None):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.pool_size = pool_size\n",
    "        self.factory = connection_factory or (\n",
    "            lambda: SimulatedConnection(host, port)\n",
    "        )\n",
    "        \n",
    "        # Available connections\n",
    "        self.available = deque()\n",
    "        # All connections (for cleanup)\n",
    "        self.all_connections = []\n",
    "        # Lock for thread safety\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Pre-warm the pool\n",
    "        self._initialize_pool()\n",
    "    \n",
    "    def _initialize_pool(self):\n",
    "        \"\"\"Pre-create and connect all connections.\"\"\"\n",
    "        print(f\"Initializing connection pool with {self.pool_size} connections...\")\n",
    "        start = time.perf_counter_ns()\n",
    "        \n",
    "        for _ in range(self.pool_size):\n",
    "            conn = self.factory()\n",
    "            conn.connect()\n",
    "            self.available.append(conn)\n",
    "            self.all_connections.append(conn)\n",
    "        \n",
    "        init_time_ms = (time.perf_counter_ns() - start) / 1_000_000\n",
    "        print(f\"Pool initialized in {init_time_ms:.2f} ms\")\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self):\n",
    "        \"\"\"Get a connection from the pool.\"\"\"\n",
    "        conn = None\n",
    "        with self.lock:\n",
    "            if self.available:\n",
    "                conn = self.available.popleft()\n",
    "        \n",
    "        if conn is None:\n",
    "            # Pool exhausted, create new connection (slow path)\n",
    "            conn = self.factory()\n",
    "            conn.connect()\n",
    "            self.all_connections.append(conn)\n",
    "        \n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            # Return to pool\n",
    "            with self.lock:\n",
    "                self.available.append(conn)\n",
    "    \n",
    "    def close_all(self):\n",
    "        \"\"\"Close all connections.\"\"\"\n",
    "        for conn in self.all_connections:\n",
    "            conn.close()\n",
    "\n",
    "\n",
    "def benchmark_connection_pooling():\n",
    "    \"\"\"Compare pooled vs non-pooled connection performance.\"\"\"\n",
    "    n_requests = 1000\n",
    "    \n",
    "    # Without pooling - new connection per request\n",
    "    tracker_no_pool = LatencyTracker(\"no_pooling\", warmup_iterations=10)\n",
    "    \n",
    "    for _ in range(n_requests):\n",
    "        with tracker_no_pool.measure():\n",
    "            conn = SimulatedConnection(\"localhost\", 8080, setup_latency_us=50)\n",
    "            conn.connect()\n",
    "            _ = conn.send(b\"test\")\n",
    "            conn.close()\n",
    "    \n",
    "    # With pooling - reuse connections\n",
    "    pool = ConnectionPool(\"localhost\", 8080, pool_size=5,\n",
    "                         connection_factory=lambda: SimulatedConnection(\"localhost\", 8080, setup_latency_us=50))\n",
    "    \n",
    "    tracker_pool = LatencyTracker(\"with_pooling\", warmup_iterations=10)\n",
    "    \n",
    "    for _ in range(n_requests):\n",
    "        with tracker_pool.measure():\n",
    "            with pool.get_connection() as conn:\n",
    "                _ = conn.send(b\"test\")\n",
    "    \n",
    "    pool.close_all()\n",
    "    \n",
    "    no_pool_stats = tracker_no_pool.get_statistics()\n",
    "    pool_stats = tracker_pool.get_statistics()\n",
    "    \n",
    "    print(\"\\nConnection Pooling Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Requests: {n_requests:,}\")\n",
    "    print(f\"\\nWithout pooling (new connection each time):\")\n",
    "    print(f\"  Mean: {no_pool_stats['mean_ns']/1000:.2f} μs\")\n",
    "    print(f\"  P99:  {no_pool_stats['p99_ns']/1000:.2f} μs\")\n",
    "    print(f\"\\nWith pooling (connection reuse):\")\n",
    "    print(f\"  Mean: {pool_stats['mean_ns']/1000:.2f} μs\")\n",
    "    print(f\"  P99:  {pool_stats['p99_ns']/1000:.2f} μs\")\n",
    "    print(f\"\\nLatency reduction: {(1 - pool_stats['mean_ns']/no_pool_stats['mean_ns'])*100:.1f}%\")\n",
    "    print(f\"Speedup: {no_pool_stats['mean_ns']/pool_stats['mean_ns']:.1f}x\")\n",
    "\n",
    "benchmark_connection_pooling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d067e",
   "metadata": {},
   "source": [
    "## 8. Batch Processing vs Single Processing\n",
    "\n",
    "### The Latency-Throughput Tradeoff\n",
    "- **Single processing**: Lowest latency per item, lower throughput\n",
    "- **Batch processing**: Higher latency per item, higher throughput\n",
    "- **Adaptive batching**: Balance based on current load\n",
    "\n",
    "### When to Batch in Trading\n",
    "- **Market data normalization**: Combine ticks before processing\n",
    "- **Risk aggregation**: Batch position updates\n",
    "- **Order submission**: Some exchanges accept batch orders\n",
    "- **Logging/Analytics**: Always batch non-critical writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single(item: float) -> float:\n",
    "    \"\"\"Process a single item (simulate computation).\"\"\"\n",
    "    return item * 1.001 + 0.01\n",
    "\n",
    "def process_batch(items: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Process items in batch (vectorized).\"\"\"\n",
    "    return items * 1.001 + 0.01\n",
    "\n",
    "def benchmark_batch_vs_single():\n",
    "    \"\"\"Compare single vs batch processing.\"\"\"\n",
    "    n_items = 100000\n",
    "    data = np.random.randn(n_items)\n",
    "    \n",
    "    # Single item processing\n",
    "    tracker_single = LatencyTracker(\"single_processing\", warmup_iterations=100)\n",
    "    results_single = []\n",
    "    \n",
    "    for item in data[:10000]:  # Subset for single processing\n",
    "        with tracker_single.measure():\n",
    "            result = process_single(item)\n",
    "        results_single.append(result)\n",
    "    \n",
    "    # Batch processing - various batch sizes\n",
    "    batch_results = {}\n",
    "    \n",
    "    for batch_size in [10, 100, 1000, 10000]:\n",
    "        tracker_batch = LatencyTracker(f\"batch_{batch_size}\", warmup_iterations=10)\n",
    "        \n",
    "        for i in range(0, n_items, batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            with tracker_batch.measure():\n",
    "                _ = process_batch(batch)\n",
    "        \n",
    "        stats = tracker_batch.get_statistics()\n",
    "        # Calculate per-item latency\n",
    "        stats['per_item_ns'] = stats['mean_ns'] / batch_size\n",
    "        batch_results[batch_size] = stats\n",
    "    \n",
    "    single_stats = tracker_single.get_statistics()\n",
    "    \n",
    "    print(\"\\nBatch vs Single Processing Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nSingle item processing:\")\n",
    "    print(f\"  Per-item latency: {single_stats['mean_ns']:.0f} ns\")\n",
    "    \n",
    "    print(f\"\\nBatch processing:\")\n",
    "    print(f\"{'Batch Size':<15} {'Batch (ns)':<15} {'Per-Item (ns)':<15} {'Speedup':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for batch_size, stats in batch_results.items():\n",
    "        speedup = single_stats['mean_ns'] / stats['per_item_ns']\n",
    "        print(f\"{batch_size:<15} {stats['mean_ns']:<15,.0f} {stats['per_item_ns']:<15,.1f} {speedup:<10.1f}x\")\n",
    "\n",
    "benchmark_batch_vs_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveBatcher:\n",
    "    \"\"\"\n",
    "    Adaptive batching based on queue depth and time.\n",
    "    Balances latency and throughput dynamically.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 min_batch_size: int = 1,\n",
    "                 max_batch_size: int = 100,\n",
    "                 max_wait_us: float = 100,\n",
    "                 processor: Callable = None):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_wait_ns = int(max_wait_us * 1000)\n",
    "        self.processor = processor or (lambda x: x)\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.last_flush_time = time.perf_counter_ns()\n",
    "        self.stats = {\n",
    "            'total_items': 0,\n",
    "            'total_batches': 0,\n",
    "            'batch_sizes': []\n",
    "        }\n",
    "    \n",
    "    def add(self, item) -> Optional[List]:\n",
    "        \"\"\"Add item, return processed batch if ready.\"\"\"\n",
    "        self.buffer.append(item)\n",
    "        \n",
    "        current_time = time.perf_counter_ns()\n",
    "        time_since_flush = current_time - self.last_flush_time\n",
    "        \n",
    "        # Flush conditions\n",
    "        should_flush = (\n",
    "            len(self.buffer) >= self.max_batch_size or\n",
    "            (len(self.buffer) >= self.min_batch_size and \n",
    "             time_since_flush >= self.max_wait_ns)\n",
    "        )\n",
    "        \n",
    "        if should_flush:\n",
    "            return self.flush()\n",
    "        return None\n",
    "    \n",
    "    def flush(self) -> List:\n",
    "        \"\"\"Process and return current batch.\"\"\"\n",
    "        if not self.buffer:\n",
    "            return []\n",
    "        \n",
    "        batch = self.buffer\n",
    "        self.buffer = []\n",
    "        self.last_flush_time = time.perf_counter_ns()\n",
    "        \n",
    "        self.stats['total_items'] += len(batch)\n",
    "        self.stats['total_batches'] += 1\n",
    "        self.stats['batch_sizes'].append(len(batch))\n",
    "        \n",
    "        return self.processor(batch)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get batching statistics.\"\"\"\n",
    "        if self.stats['batch_sizes']:\n",
    "            return {\n",
    "                **self.stats,\n",
    "                'avg_batch_size': statistics.mean(self.stats['batch_sizes']),\n",
    "                'min_batch_size_seen': min(self.stats['batch_sizes']),\n",
    "                'max_batch_size_seen': max(self.stats['batch_sizes'])\n",
    "            }\n",
    "        return self.stats\n",
    "\n",
    "\n",
    "def demo_adaptive_batching():\n",
    "    \"\"\"Demonstrate adaptive batching behavior.\"\"\"\n",
    "    \n",
    "    def batch_processor(items):\n",
    "        \"\"\"Simulate batch processing.\"\"\"\n",
    "        return [item * 2 for item in items]\n",
    "    \n",
    "    batcher = AdaptiveBatcher(\n",
    "        min_batch_size=5,\n",
    "        max_batch_size=50,\n",
    "        max_wait_us=500,\n",
    "        processor=batch_processor\n",
    "    )\n",
    "    \n",
    "    # Simulate varying load\n",
    "    print(\"Simulating adaptive batching with varying load...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # High load period - should batch to max_batch_size\n",
    "    print(\"\\nHigh load period (items arriving rapidly):\")\n",
    "    for i in range(200):\n",
    "        result = batcher.add(i)\n",
    "        if result:\n",
    "            print(f\"  Batch processed: {len(result)} items\")\n",
    "    \n",
    "    # Low load period - should flush on timeout\n",
    "    print(\"\\nLow load period (items arriving slowly):\")\n",
    "    for i in range(10):\n",
    "        result = batcher.add(i)\n",
    "        if result:\n",
    "            print(f\"  Batch processed: {len(result)} items\")\n",
    "        time.sleep(0.0001)  # 100 μs between items\n",
    "    \n",
    "    # Flush remaining\n",
    "    remaining = batcher.flush()\n",
    "    if remaining:\n",
    "        print(f\"  Final flush: {len(remaining)} items\")\n",
    "    \n",
    "    stats = batcher.get_stats()\n",
    "    print(f\"\\nBatching Statistics:\")\n",
    "    print(f\"  Total items: {stats['total_items']}\")\n",
    "    print(f\"  Total batches: {stats['total_batches']}\")\n",
    "    print(f\"  Avg batch size: {stats['avg_batch_size']:.1f}\")\n",
    "\n",
    "demo_adaptive_batching()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
