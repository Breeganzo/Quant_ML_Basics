{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0abdb1",
   "metadata": {},
   "source": [
    "# Day 3: Returns, Volatility & Risk Metrics\n",
    "\n",
    "## Week 1 - Python for Quantitative Finance\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "- Master all types of return calculations\n",
    "- Understand volatility estimation methods\n",
    "- Implement industry-standard risk metrics (VaR, ES, Sharpe, Sortino)\n",
    "- Calculate and analyze drawdowns\n",
    "\n",
    "### â±ï¸ Time Allocation\n",
    "- Theory review: 30 min\n",
    "- Guided exercises: 90 min\n",
    "- Practice problems: 60 min\n",
    "- Interview prep: 30 min\n",
    "\n",
    "---\n",
    "\n",
    "**Key Interview Topics**: Sharpe ratio, VaR confidence levels, realized vs implied volatility, maximum drawdown\n",
    "\n",
    "**Author**: ML Quant Finance Mastery  \n",
    "**Difficulty**: Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f930785f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded: 1771 days, 5 stocks\n",
      "ðŸ“… Range: 2019-01-02 to 2026-01-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "DATA_DIR = Path(\"../datasets/raw_data\")\n",
    "prices = pd.read_csv(DATA_DIR / \"combined_adjusted_close.csv\", index_col=0, parse_dates=True)\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'SPY', 'JPM']\n",
    "df = prices[tickers].dropna()\n",
    "\n",
    "print(f\"âœ… Data loaded: {df.shape[0]} days, {len(tickers)} stocks\")\n",
    "print(f\"ðŸ“… Range: {df.index[0].strftime('%Y-%m-%d')} to {df.index[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402d772",
   "metadata": {},
   "source": [
    "## 1. Types of Returns\n",
    "\n",
    "| Type | Formula | When to Use |\n",
    "|------|---------|-------------|\n",
    "| Simple | $(P_t - P_{t-1}) / P_{t-1}$ | Portfolio aggregation |\n",
    "| Log | $\\ln(P_t / P_{t-1})$ | Time series analysis |\n",
    "| Excess | $R_t - R_f$ | Risk-adjusted metrics |\n",
    "| Relative | $R_t - R_{benchmark}$ | Alpha calculation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc63cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RETURN TYPE COMPARISON (AAPL, last 5 days)\n",
      "============================================================\n",
      "            Simple     Log  Excess  vs SPY\n",
      "Date                                      \n",
      "2026-01-12  0.0034  0.0034  0.0032  0.0018\n",
      "2026-01-13  0.0031  0.0031  0.0029  0.0051\n",
      "2026-01-14 -0.0042 -0.0042 -0.0044  0.0007\n",
      "2026-01-15 -0.0067 -0.0068 -0.0069 -0.0095\n",
      "2026-01-16 -0.0104 -0.0104 -0.0106 -0.0095\n"
     ]
    }
   ],
   "source": [
    "TRADING_DAYS = 252\n",
    "RISK_FREE_RATE = 0.05 / TRADING_DAYS  # Daily risk-free rate\n",
    "\n",
    "# Calculate all return types\n",
    "simple_returns = df.pct_change().dropna()\n",
    "log_returns = np.log(df / df.shift(1)).dropna()\n",
    "excess_returns = simple_returns - RISK_FREE_RATE\n",
    "relative_returns = simple_returns.sub(simple_returns['SPY'], axis=0)\n",
    "\n",
    "print(\"ðŸ“Š RETURN TYPE COMPARISON (AAPL, last 5 days)\")\n",
    "print(\"=\" * 60)\n",
    "comparison = pd.DataFrame({\n",
    "    'Simple': simple_returns['AAPL'].tail(),\n",
    "    'Log': log_returns['AAPL'].tail(),\n",
    "    'Excess': excess_returns['AAPL'].tail(),\n",
    "    'vs SPY': relative_returns['AAPL'].tail()\n",
    "})\n",
    "print(comparison.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10752121",
   "metadata": {},
   "source": [
    "## 2. Volatility Estimation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec6fb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š VOLATILITY ESTIMATION\n",
      "==================================================\n",
      "Historical (full period): 31.00%\n",
      "Rolling 20-day (latest):  10.55%\n",
      "EWM 20-day (latest):      10.90%\n",
      "\n",
      "ðŸ“ˆ Volatility range over period:\n",
      "   Min:  9.46%\n",
      "   Max:  107.95%\n"
     ]
    }
   ],
   "source": [
    "# Different volatility estimators\n",
    "aapl_ret = simple_returns['AAPL']\n",
    "\n",
    "# 1. Historical (realized) volatility\n",
    "hist_vol = aapl_ret.std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "# 2. Rolling volatility (20-day)\n",
    "rolling_vol = aapl_ret.rolling(20).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "# 3. Exponentially weighted volatility\n",
    "ewm_vol = aapl_ret.ewm(span=20).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "# 4. Parkinson volatility (if you have High/Low data)\n",
    "# Uses price range instead of close-to-close\n",
    "# Ïƒ_parkinson = sqrt(1/(4*ln(2)) * ln(H/L)^2)\n",
    "\n",
    "print(\"ðŸ“Š VOLATILITY ESTIMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Historical (full period): {hist_vol*100:.2f}%\")\n",
    "print(f\"Rolling 20-day (latest):  {rolling_vol.iloc[-1]*100:.2f}%\")\n",
    "print(f\"EWM 20-day (latest):      {ewm_vol.iloc[-1]*100:.2f}%\")\n",
    "print(f\"\\nðŸ“ˆ Volatility range over period:\")\n",
    "print(f\"   Min:  {rolling_vol.min()*100:.2f}%\")\n",
    "print(f\"   Max:  {rolling_vol.max()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f9b02",
   "metadata": {},
   "source": [
    "## 3. Value at Risk (VaR) and Expected Shortfall (ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a44accf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š VALUE AT RISK & EXPECTED SHORTFALL (95%)\n",
      "============================================================\n",
      "\n",
      "Method               VaR (daily)     ES (daily)     \n",
      "--------------------------------------------------\n",
      "Historical            -2.98%         -4.41%\n",
      "Parametric            -3.08%         -3.90%\n",
      "\n",
      "ðŸ’¡ Interpretation:\n",
      "   95% VaR of -2.98% means:\n",
      "   'On 95% of days, loss will be less than 2.98%'\n"
     ]
    }
   ],
   "source": [
    "def calculate_var_es(returns: pd.Series, confidence: float = 0.95) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate VaR and Expected Shortfall using multiple methods.\n",
    "    \n",
    "    VaR: \"With X% confidence, loss won't exceed this amount\"\n",
    "    ES:  \"When loss exceeds VaR, the average loss will be this amount\"\n",
    "    \"\"\"\n",
    "    alpha = 1 - confidence\n",
    "    \n",
    "    # Method 1: Historical (non-parametric)\n",
    "    var_historical = returns.quantile(alpha)\n",
    "    es_historical = returns[returns <= var_historical].mean()\n",
    "    \n",
    "    # Method 2: Parametric (assumes normal distribution)\n",
    "    mu = returns.mean()\n",
    "    sigma = returns.std()\n",
    "    var_parametric = mu + sigma * stats.norm.ppf(alpha)\n",
    "    # ES for normal: Î¼ - Ïƒ * Ï†(Î¦^(-1)(Î±)) / Î±\n",
    "    es_parametric = mu - sigma * stats.norm.pdf(stats.norm.ppf(alpha)) / alpha\n",
    "    \n",
    "    return {\n",
    "        'var_historical': var_historical,\n",
    "        'es_historical': es_historical,\n",
    "        'var_parametric': var_parametric,\n",
    "        'es_parametric': es_parametric\n",
    "    }\n",
    "\n",
    "# Calculate for AAPL\n",
    "var_es = calculate_var_es(aapl_ret, confidence=0.95)\n",
    "\n",
    "print(\"ðŸ“Š VALUE AT RISK & EXPECTED SHORTFALL (95%)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Method':<20} {'VaR (daily)':<15} {'ES (daily)':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Historical':<20} {var_es['var_historical']*100:>6.2f}%        {var_es['es_historical']*100:>6.2f}%\")\n",
    "print(f\"{'Parametric':<20} {var_es['var_parametric']*100:>6.2f}%        {var_es['es_parametric']*100:>6.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Interpretation:\")\n",
    "print(f\"   95% VaR of {var_es['var_historical']*100:.2f}% means:\")\n",
    "print(f\"   'On 95% of days, loss will be less than {abs(var_es['var_historical'])*100:.2f}%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c0fd2",
   "metadata": {},
   "source": [
    "## 4. Risk-Adjusted Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e16c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RISK-ADJUSTED PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n",
      "Ticker   Ann Ret    Ann Vol    Sharpe     Sortino    Max DD    \n",
      "----------------------------------------------------------------------\n",
      "AAPL      32.12%    31.00%      0.87      1.21   -33.36%\n",
      "MSFT      26.53%    28.33%      0.76      1.06   -37.15%\n",
      "GOOGL     31.12%    31.30%      0.83      1.17   -44.32%\n",
      "SPY       17.94%    19.73%      0.66      0.80   -33.72%\n",
      "JPM       23.64%    29.89%      0.62      0.83   -43.63%\n"
     ]
    }
   ],
   "source": [
    "def calculate_performance_metrics(returns: pd.Series, rf_rate: float = 0.05) -> dict:\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    \n",
    "    # Annualize\n",
    "    ann_return = returns.mean() * TRADING_DAYS\n",
    "    ann_vol = returns.std() * np.sqrt(TRADING_DAYS)\n",
    "    \n",
    "    # Sharpe Ratio: (Return - Rf) / Volatility\n",
    "    sharpe = (ann_return - rf_rate) / ann_vol\n",
    "    \n",
    "    # Sortino Ratio: (Return - Rf) / Downside Volatility\n",
    "    downside_returns = returns[returns < 0]\n",
    "    downside_vol = downside_returns.std() * np.sqrt(TRADING_DAYS)\n",
    "    sortino = (ann_return - rf_rate) / downside_vol\n",
    "    \n",
    "    # Calmar Ratio: Return / Max Drawdown\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    rolling_max = cum_returns.cummax()\n",
    "    drawdown = (cum_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    calmar = ann_return / abs(max_drawdown)\n",
    "    \n",
    "    # Information Ratio (vs SPY)\n",
    "    benchmark_returns = simple_returns['SPY']\n",
    "    active_returns = returns - benchmark_returns\n",
    "    tracking_error = active_returns.std() * np.sqrt(TRADING_DAYS)\n",
    "    information_ratio = active_returns.mean() * TRADING_DAYS / tracking_error\n",
    "    \n",
    "    return {\n",
    "        'annual_return': ann_return,\n",
    "        'annual_volatility': ann_vol,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'sortino_ratio': sortino,\n",
    "        'calmar_ratio': calmar,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'information_ratio': information_ratio\n",
    "    }\n",
    "\n",
    "# Calculate for all stocks\n",
    "print(\"ðŸ“Š RISK-ADJUSTED PERFORMANCE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Ticker':<8} {'Ann Ret':<10} {'Ann Vol':<10} {'Sharpe':<10} {'Sortino':<10} {'Max DD':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for ticker in tickers:\n",
    "    metrics = calculate_performance_metrics(simple_returns[ticker])\n",
    "    print(f\"{ticker:<8} {metrics['annual_return']*100:>6.2f}%   {metrics['annual_volatility']*100:>6.2f}%   \"\n",
    "          f\"{metrics['sharpe_ratio']:>7.2f}   {metrics['sortino_ratio']:>7.2f}   {metrics['max_drawdown']*100:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae6e30",
   "metadata": {},
   "source": [
    "## 5. Drawdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0bc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TOP 5 DRAWDOWN PERIODS (AAPL)\n",
      "================================================================================\n",
      "        start     trough  max_drawdown  days_to_trough  days_to_recovery\n",
      "65 2023-07-18 2023-07-18      0.001340               0               1.0\n",
      "21 2019-11-12 2019-11-12      0.000916               0               1.0\n",
      "74 2024-12-06 2024-12-06      0.000823               0               3.0\n",
      "27 2019-12-27 2019-12-27      0.000380               0               3.0\n",
      "13 2019-09-06 2019-09-06      0.000094               0               3.0\n"
     ]
    }
   ],
   "source": [
    "def analyze_drawdowns(prices: pd.Series, top_n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze drawdown periods and recovery times.\n",
    "    \"\"\"\n",
    "    # Calculate drawdown series\n",
    "    rolling_max = prices.cummax()\n",
    "    drawdown = (prices - rolling_max) / rolling_max\n",
    "    \n",
    "    # Find drawdown periods\n",
    "    is_underwater = drawdown < 0\n",
    "    \n",
    "    # Find starts and ends of drawdown periods\n",
    "    starts = is_underwater & ~is_underwater.shift(1).fillna(False)\n",
    "    ends = ~is_underwater & is_underwater.shift(1).fillna(False)\n",
    "    \n",
    "    drawdown_periods = []\n",
    "    \n",
    "    start_dates = drawdown.index[starts]\n",
    "    end_dates = drawdown.index[ends]\n",
    "    \n",
    "    for i, start in enumerate(start_dates):\n",
    "        # Find the trough during this drawdown\n",
    "        if i < len(end_dates):\n",
    "            end = end_dates[i] if end_dates[i] > start else drawdown.index[-1]\n",
    "        else:\n",
    "            end = drawdown.index[-1]\n",
    "        \n",
    "        period_dd = drawdown.loc[start:end]\n",
    "        trough_date = period_dd.idxmin()\n",
    "        trough_value = period_dd.min()\n",
    "        \n",
    "        # Duration calculations\n",
    "        days_to_trough = (trough_date - start).days\n",
    "        days_to_recovery = (end - trough_date).days if end != drawdown.index[-1] else None\n",
    "        \n",
    "        drawdown_periods.append({\n",
    "            'start': start,\n",
    "            'trough': trough_date,\n",
    "            'end': end,\n",
    "            'max_drawdown': trough_value,\n",
    "            'days_to_trough': days_to_trough,\n",
    "            'days_to_recovery': days_to_recovery\n",
    "        })\n",
    "    \n",
    "    df_dd = pd.DataFrame(drawdown_periods)\n",
    "    df_dd = df_dd.nlargest(top_n, 'max_drawdown', keep='first')\n",
    "    df_dd['max_drawdown'] = df_dd['max_drawdown'].abs()  # Convert to positive for display\n",
    "    return df_dd.sort_values('max_drawdown', ascending=False)\n",
    "\n",
    "# Analyze AAPL drawdowns\n",
    "dd_analysis = analyze_drawdowns(df['AAPL'], top_n=5)\n",
    "\n",
    "print(\"ðŸ“Š TOP 5 DRAWDOWN PERIODS (AAPL)\")\n",
    "print(\"=\" * 80)\n",
    "print(dd_analysis[['start', 'trough', 'max_drawdown', 'days_to_trough', 'days_to_recovery']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854dcd28",
   "metadata": {},
   "source": [
    "## 6. Summary & Key Takeaways\n",
    "\n",
    "### âœ… What You Learned Today\n",
    "\n",
    "1. **Return types**: Simple (portfolio), Log (time series), Excess, Relative\n",
    "2. **Volatility methods**: Historical, Rolling, EWM, Parkinson\n",
    "3. **Risk metrics**: VaR, ES (CVaR) - historical vs parametric\n",
    "4. **Performance ratios**: Sharpe, Sortino, Calmar, Information Ratio\n",
    "5. **Drawdown analysis**: Max DD, recovery times, underwater periods\n",
    "\n",
    "### ðŸŽ¯ Interview Tips\n",
    "\n",
    "- **\"Why use Sortino over Sharpe?\"** - Sortino only penalizes downside volatility\n",
    "- **\"What's wrong with VaR?\"** - Doesn't tell you how bad losses can be (use ES)\n",
    "- **\"Why does Sharpe assume normal returns?\"** - It's based on mean/std which are moments of normal dist\n",
    "- **\"What's a good Sharpe ratio?\"** - >1 is good, >2 is great, >3 is suspicious\n",
    "\n",
    "### ðŸ“š Tomorrow: Correlation, Covariance & Factor Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
