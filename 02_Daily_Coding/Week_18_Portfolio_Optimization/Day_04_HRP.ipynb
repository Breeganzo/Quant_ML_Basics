{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4eb9660",
   "metadata": {},
   "source": [
    "# Day 4: Hierarchical Risk Parity (HRP)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the limitations of traditional mean-variance optimization\n",
    "- Learn the Hierarchical Risk Parity algorithm by Marcos López de Prado\n",
    "- Implement HRP from scratch: clustering, quasi-diagonalization, recursive bisection\n",
    "- Compare HRP with traditional portfolio optimization methods\n",
    "\n",
    "## Why HRP?\n",
    "\n",
    "Traditional Mean-Variance Optimization (MVO) suffers from:\n",
    "1. **Instability**: Small changes in inputs → large changes in weights\n",
    "2. **Concentration**: Often produces extreme/concentrated portfolios\n",
    "3. **Estimation error**: Covariance matrix inversion amplifies errors\n",
    "4. **Singularity issues**: When N > T, covariance matrix is singular\n",
    "\n",
    "HRP addresses these by:\n",
    "- Using hierarchical clustering to group similar assets\n",
    "- Avoiding matrix inversion entirely\n",
    "- Producing more stable, diversified portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4df9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c62447",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation\n",
    "\n",
    "Let's fetch a diversified set of ETFs representing different asset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc92b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversified ETF universe\n",
    "tickers = [\n",
    "    'SPY',   # S&P 500\n",
    "    'QQQ',   # Nasdaq 100\n",
    "    'IWM',   # Russell 2000\n",
    "    'EFA',   # International Developed\n",
    "    'EEM',   # Emerging Markets\n",
    "    'TLT',   # Long-Term Treasuries\n",
    "    'IEF',   # Intermediate Treasuries\n",
    "    'LQD',   # Investment Grade Corporate Bonds\n",
    "    'HYG',   # High Yield Bonds\n",
    "    'GLD',   # Gold\n",
    "    'VNQ',   # REITs\n",
    "    'DBC',   # Commodities\n",
    "]\n",
    "\n",
    "# Download data\n",
    "print(\"Downloading ETF data...\")\n",
    "data = yf.download(tickers, start='2018-01-01', end='2024-01-01', progress=False)['Adj Close']\n",
    "data = data.dropna()\n",
    "\n",
    "# Calculate returns\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "print(f\"\\nData shape: {returns.shape}\")\n",
    "print(f\"Date range: {returns.index[0].date()} to {returns.index[-1].date()}\")\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b82bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance and correlation matrices\n",
    "cov_matrix = returns.cov() * 252  # Annualized\n",
    "corr_matrix = returns.corr()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdYlGn', center=0, ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "ax.set_title('Correlation Matrix of ETF Returns', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e95a09",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. HRP Algorithm Overview\n",
    "\n",
    "The HRP algorithm consists of three main steps:\n",
    "\n",
    "### Step 1: Tree Clustering\n",
    "- Convert correlation matrix to distance matrix\n",
    "- Apply hierarchical clustering (agglomerative)\n",
    "- Result: A dendrogram showing asset relationships\n",
    "\n",
    "### Step 2: Quasi-Diagonalization\n",
    "- Reorder covariance matrix based on clustering\n",
    "- Similar assets are placed close together\n",
    "- Result: Block-diagonal-like structure\n",
    "\n",
    "### Step 3: Recursive Bisection\n",
    "- Split assets into two clusters\n",
    "- Allocate between clusters based on inverse variance\n",
    "- Recursively repeat for sub-clusters\n",
    "- Result: Final portfolio weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313cc01",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Step 1: Tree Clustering\n",
    "\n",
    "Convert correlation to distance and perform hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d291c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_to_distance(corr):\n",
    "    \"\"\"\n",
    "    Convert correlation matrix to distance matrix.\n",
    "    \n",
    "    Distance = sqrt(0.5 * (1 - correlation))\n",
    "    \n",
    "    Properties:\n",
    "    - correlation = 1 → distance = 0\n",
    "    - correlation = 0 → distance = 0.707\n",
    "    - correlation = -1 → distance = 1\n",
    "    \"\"\"\n",
    "    distance = np.sqrt(0.5 * (1 - corr))\n",
    "    return distance\n",
    "\n",
    "# Calculate distance matrix\n",
    "dist_matrix = correlation_to_distance(corr_matrix)\n",
    "\n",
    "print(\"Distance Matrix (sample):\")\n",
    "print(dist_matrix.iloc[:5, :5].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd4d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(dist_matrix, method='single'):\n",
    "    \"\"\"\n",
    "    Perform hierarchical clustering on distance matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dist_matrix : DataFrame\n",
    "        Distance matrix\n",
    "    method : str\n",
    "        Linkage method: 'single', 'complete', 'average', 'ward'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    linkage_matrix : ndarray\n",
    "        Hierarchical clustering encoded as linkage matrix\n",
    "    \"\"\"\n",
    "    # Convert to condensed distance matrix (upper triangular)\n",
    "    condensed_dist = squareform(dist_matrix.values)\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(condensed_dist, method=method)\n",
    "    \n",
    "    return linkage_matrix\n",
    "\n",
    "# Perform clustering\n",
    "linkage_mat = perform_clustering(dist_matrix, method='single')\n",
    "\n",
    "print(\"Linkage matrix shape:\", linkage_mat.shape)\n",
    "print(\"\\nLinkage matrix format: [cluster1, cluster2, distance, n_members]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dendrogram\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "dendrogram(\n",
    "    linkage_mat,\n",
    "    labels=returns.columns.tolist(),\n",
    "    leaf_rotation=45,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Hierarchical Clustering Dendrogram', fontsize=14)\n",
    "ax.set_xlabel('Assets')\n",
    "ax.set_ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Lower merge height = more similar assets\")\n",
    "print(\"- Clusters: Equities (SPY, QQQ, IWM), Bonds (TLT, IEF), etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339039c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Step 2: Quasi-Diagonalization\n",
    "\n",
    "Reorder assets so that similar assets are adjacent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c6c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_diagonalize(linkage_mat, n_assets):\n",
    "    \"\"\"\n",
    "    Reorder assets based on hierarchical clustering.\n",
    "    \n",
    "    Returns the order of assets that quasi-diagonalizes the covariance matrix.\n",
    "    \"\"\"\n",
    "    # Get the order of leaves from dendrogram\n",
    "    sorted_indices = leaves_list(linkage_mat)\n",
    "    return sorted_indices\n",
    "\n",
    "# Get sorted order\n",
    "sorted_idx = quasi_diagonalize(linkage_mat, len(returns.columns))\n",
    "sorted_tickers = [returns.columns[i] for i in sorted_idx]\n",
    "\n",
    "print(\"Original order:\", list(returns.columns))\n",
    "print(\"\\nClustered order:\", sorted_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f38e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs quasi-diagonalized covariance matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original order\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            center=0, ax=axes[0], vmin=-1, vmax=1,\n",
    "            annot_kws={'size': 8})\n",
    "axes[0].set_title('Original Correlation Matrix', fontsize=12)\n",
    "\n",
    "# Quasi-diagonalized order\n",
    "corr_sorted = corr_matrix.iloc[sorted_idx, sorted_idx]\n",
    "sns.heatmap(corr_sorted, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "            center=0, ax=axes[1], vmin=-1, vmax=1,\n",
    "            annot_kws={'size': 8})\n",
    "axes[1].set_title('Quasi-Diagonalized Correlation Matrix', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how similar assets (high correlation) are now adjacent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae82f7",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Step 3: Recursive Bisection\n",
    "\n",
    "The core allocation algorithm that splits clusters and allocates based on variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_variance(cov, assets):\n",
    "    \"\"\"\n",
    "    Calculate the variance of a cluster using inverse-variance weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cov : DataFrame\n",
    "        Covariance matrix\n",
    "    assets : list\n",
    "        List of asset names in the cluster\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_var : float\n",
    "        Variance of the cluster portfolio\n",
    "    \"\"\"\n",
    "    # Extract sub-covariance matrix\n",
    "    cov_slice = cov.loc[assets, assets]\n",
    "    \n",
    "    # Inverse variance weights within cluster\n",
    "    variances = np.diag(cov_slice)\n",
    "    inv_var = 1.0 / variances\n",
    "    weights = inv_var / inv_var.sum()\n",
    "    \n",
    "    # Portfolio variance: w' * Σ * w\n",
    "    cluster_var = np.dot(weights, np.dot(cov_slice, weights))\n",
    "    \n",
    "    return cluster_var\n",
    "\n",
    "# Test\n",
    "test_assets = ['SPY', 'QQQ', 'IWM']\n",
    "test_var = get_cluster_variance(cov_matrix, test_assets)\n",
    "print(f\"Cluster variance for {test_assets}: {test_var:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e66390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_bisection(cov, sorted_assets):\n",
    "    \"\"\"\n",
    "    Perform recursive bisection to calculate HRP weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cov : DataFrame\n",
    "        Covariance matrix\n",
    "    sorted_assets : list\n",
    "        Assets in quasi-diagonalized order\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : Series\n",
    "        HRP portfolio weights\n",
    "    \"\"\"\n",
    "    # Initialize weights\n",
    "    weights = pd.Series(1.0, index=sorted_assets)\n",
    "    \n",
    "    # List of clusters to process\n",
    "    clusters = [sorted_assets]\n",
    "    \n",
    "    while len(clusters) > 0:\n",
    "        # Split each cluster\n",
    "        new_clusters = []\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            if len(cluster) > 1:\n",
    "                # Split cluster in half\n",
    "                mid = len(cluster) // 2\n",
    "                cluster_1 = cluster[:mid]\n",
    "                cluster_2 = cluster[mid:]\n",
    "                \n",
    "                # Calculate cluster variances\n",
    "                var_1 = get_cluster_variance(cov, cluster_1)\n",
    "                var_2 = get_cluster_variance(cov, cluster_2)\n",
    "                \n",
    "                # Allocation factor (inverse variance weighting)\n",
    "                alpha = 1 - var_1 / (var_1 + var_2)\n",
    "                \n",
    "                # Update weights\n",
    "                weights[cluster_1] *= alpha\n",
    "                weights[cluster_2] *= (1 - alpha)\n",
    "                \n",
    "                # Add sub-clusters for further processing\n",
    "                if len(cluster_1) > 1:\n",
    "                    new_clusters.append(cluster_1)\n",
    "                if len(cluster_2) > 1:\n",
    "                    new_clusters.append(cluster_2)\n",
    "        \n",
    "        clusters = new_clusters\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Calculate HRP weights\n",
    "hrp_weights = recursive_bisection(cov_matrix, sorted_tickers)\n",
    "\n",
    "# Reorder to original ticker order\n",
    "hrp_weights = hrp_weights.reindex(returns.columns)\n",
    "\n",
    "print(\"HRP Weights:\")\n",
    "print(hrp_weights.sort_values(ascending=False).round(4))\n",
    "print(f\"\\nSum of weights: {hrp_weights.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30f5a9",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Complete HRP Implementation\n",
    "\n",
    "Let's wrap everything into a clean class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40083d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRiskParity:\n",
    "    \"\"\"\n",
    "    Hierarchical Risk Parity (HRP) Portfolio Optimization.\n",
    "    \n",
    "    Based on: López de Prado, M. (2016). Building Diversified Portfolios \n",
    "    that Outperform Out-of-Sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linkage_method='single'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        linkage_method : str\n",
    "            Hierarchical clustering linkage method\n",
    "            Options: 'single', 'complete', 'average', 'ward'\n",
    "        \"\"\"\n",
    "        self.linkage_method = linkage_method\n",
    "        self.weights = None\n",
    "        self.linkage_matrix = None\n",
    "        self.sorted_assets = None\n",
    "        \n",
    "    def _correlation_to_distance(self, corr):\n",
    "        \"\"\"Convert correlation to distance matrix.\"\"\"\n",
    "        return np.sqrt(0.5 * (1 - corr))\n",
    "    \n",
    "    def _get_cluster_variance(self, cov, assets):\n",
    "        \"\"\"Calculate cluster variance using inverse-variance weights.\"\"\"\n",
    "        cov_slice = cov.loc[assets, assets]\n",
    "        variances = np.diag(cov_slice)\n",
    "        inv_var = 1.0 / variances\n",
    "        w = inv_var / inv_var.sum()\n",
    "        return np.dot(w, np.dot(cov_slice, w))\n",
    "    \n",
    "    def _recursive_bisection(self, cov, sorted_assets):\n",
    "        \"\"\"Perform recursive bisection for weight allocation.\"\"\"\n",
    "        weights = pd.Series(1.0, index=sorted_assets)\n",
    "        clusters = [sorted_assets]\n",
    "        \n",
    "        while clusters:\n",
    "            new_clusters = []\n",
    "            for cluster in clusters:\n",
    "                if len(cluster) > 1:\n",
    "                    mid = len(cluster) // 2\n",
    "                    c1, c2 = cluster[:mid], cluster[mid:]\n",
    "                    \n",
    "                    var1 = self._get_cluster_variance(cov, c1)\n",
    "                    var2 = self._get_cluster_variance(cov, c2)\n",
    "                    \n",
    "                    alpha = 1 - var1 / (var1 + var2)\n",
    "                    \n",
    "                    weights[c1] *= alpha\n",
    "                    weights[c2] *= (1 - alpha)\n",
    "                    \n",
    "                    if len(c1) > 1: new_clusters.append(c1)\n",
    "                    if len(c2) > 1: new_clusters.append(c2)\n",
    "            \n",
    "            clusters = new_clusters\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def fit(self, returns):\n",
    "        \"\"\"\n",
    "        Fit HRP model to returns data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns : DataFrame\n",
    "            Asset returns (T x N)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # Calculate covariance and correlation\n",
    "        cov = returns.cov()\n",
    "        corr = returns.corr()\n",
    "        \n",
    "        # Step 1: Tree clustering\n",
    "        dist = self._correlation_to_distance(corr)\n",
    "        condensed_dist = squareform(dist.values)\n",
    "        self.linkage_matrix = linkage(condensed_dist, method=self.linkage_method)\n",
    "        \n",
    "        # Step 2: Quasi-diagonalization\n",
    "        sorted_idx = leaves_list(self.linkage_matrix)\n",
    "        self.sorted_assets = [returns.columns[i] for i in sorted_idx]\n",
    "        \n",
    "        # Step 3: Recursive bisection\n",
    "        weights = self._recursive_bisection(cov, self.sorted_assets)\n",
    "        \n",
    "        # Reorder to original order\n",
    "        self.weights = weights.reindex(returns.columns)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return portfolio weights.\"\"\"\n",
    "        return self.weights\n",
    "    \n",
    "    def plot_dendrogram(self, figsize=(12, 5)):\n",
    "        \"\"\"Plot the hierarchical clustering dendrogram.\"\"\"\n",
    "        if self.linkage_matrix is None:\n",
    "            raise ValueError(\"Must call fit() first\")\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        dendrogram(self.linkage_matrix, labels=self.sorted_assets,\n",
    "                  leaf_rotation=45, ax=ax)\n",
    "        ax.set_title('HRP Dendrogram')\n",
    "        ax.set_ylabel('Distance')\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_weights(self, figsize=(10, 5)):\n",
    "        \"\"\"Plot portfolio weights.\"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Must call fit() first\")\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        self.weights.sort_values().plot(kind='barh', ax=ax, color='steelblue')\n",
    "        ax.set_xlabel('Weight')\n",
    "        ax.set_title('HRP Portfolio Weights')\n",
    "        ax.axvline(x=1/len(self.weights), color='red', linestyle='--', \n",
    "                   label='Equal Weight')\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e756fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the HRP class\n",
    "hrp = HierarchicalRiskParity(linkage_method='single')\n",
    "hrp.fit(returns)\n",
    "\n",
    "print(\"HRP Portfolio Weights:\")\n",
    "print(hrp.get_weights().round(4))\n",
    "\n",
    "# Plot\n",
    "hrp.plot_weights()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81894b7a",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Comparison with Other Methods\n",
    "\n",
    "Let's compare HRP with:\n",
    "1. Equal Weight (1/N)\n",
    "2. Inverse Variance\n",
    "3. Mean-Variance Optimization (Minimum Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa4245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_weight(returns):\n",
    "    \"\"\"Equal weight portfolio.\"\"\"\n",
    "    n = len(returns.columns)\n",
    "    return pd.Series(1/n, index=returns.columns)\n",
    "\n",
    "def inverse_variance(returns):\n",
    "    \"\"\"Inverse variance (risk parity on individual assets).\"\"\"\n",
    "    cov = returns.cov()\n",
    "    var = np.diag(cov)\n",
    "    inv_var = 1 / var\n",
    "    weights = inv_var / inv_var.sum()\n",
    "    return pd.Series(weights, index=returns.columns)\n",
    "\n",
    "def min_variance(returns):\n",
    "    \"\"\"Minimum variance portfolio (analytical solution).\"\"\"\n",
    "    cov = returns.cov()\n",
    "    n = len(returns.columns)\n",
    "    \n",
    "    # Analytical solution: w = Σ^(-1) * 1 / (1' * Σ^(-1) * 1)\n",
    "    try:\n",
    "        cov_inv = np.linalg.inv(cov)\n",
    "        ones = np.ones(n)\n",
    "        weights = cov_inv @ ones / (ones @ cov_inv @ ones)\n",
    "        return pd.Series(weights, index=returns.columns)\n",
    "    except:\n",
    "        # If singular, use pseudo-inverse\n",
    "        cov_inv = np.linalg.pinv(cov)\n",
    "        ones = np.ones(n)\n",
    "        weights = cov_inv @ ones / (ones @ cov_inv @ ones)\n",
    "        return pd.Series(weights, index=returns.columns)\n",
    "\n",
    "# Calculate all weights\n",
    "weights_dict = {\n",
    "    'Equal Weight': equal_weight(returns),\n",
    "    'Inverse Variance': inverse_variance(returns),\n",
    "    'Min Variance': min_variance(returns),\n",
    "    'HRP': hrp.get_weights()\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "weights_df = pd.DataFrame(weights_dict)\n",
    "print(\"Portfolio Weights Comparison:\")\n",
    "print(weights_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(returns.columns))\n",
    "width = 0.2\n",
    "\n",
    "for i, (name, weights) in enumerate(weights_dict.items()):\n",
    "    ax.bar(x + i*width, weights.values, width, label=name, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Asset')\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_title('Portfolio Weights: Different Methods')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(returns.columns, rotation=45)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2921c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_metrics(returns, weights):\n",
    "    \"\"\"\n",
    "    Calculate portfolio performance metrics.\n",
    "    \"\"\"\n",
    "    # Portfolio returns\n",
    "    port_ret = (returns * weights).sum(axis=1)\n",
    "    \n",
    "    # Annualized metrics\n",
    "    ann_ret = port_ret.mean() * 252\n",
    "    ann_vol = port_ret.std() * np.sqrt(252)\n",
    "    sharpe = ann_ret / ann_vol\n",
    "    \n",
    "    # Max drawdown\n",
    "    cum_ret = (1 + port_ret).cumprod()\n",
    "    rolling_max = cum_ret.expanding().max()\n",
    "    drawdown = (cum_ret - rolling_max) / rolling_max\n",
    "    max_dd = drawdown.min()\n",
    "    \n",
    "    # Concentration (Herfindahl index)\n",
    "    hhi = (weights ** 2).sum()\n",
    "    effective_n = 1 / hhi  # Effective number of assets\n",
    "    \n",
    "    return {\n",
    "        'Annual Return': f\"{ann_ret:.2%}\",\n",
    "        'Annual Volatility': f\"{ann_vol:.2%}\",\n",
    "        'Sharpe Ratio': f\"{sharpe:.3f}\",\n",
    "        'Max Drawdown': f\"{max_dd:.2%}\",\n",
    "        'Effective N': f\"{effective_n:.1f}\",\n",
    "        'HHI': f\"{hhi:.4f}\"\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all portfolios\n",
    "metrics = {}\n",
    "for name, weights in weights_dict.items():\n",
    "    metrics[name] = portfolio_metrics(returns, weights)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(\"\\nPortfolio Performance Comparison (In-Sample):\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, weights in weights_dict.items():\n",
    "    port_ret = (returns * weights).sum(axis=1)\n",
    "    cum_ret = (1 + port_ret).cumprod()\n",
    "    ax.plot(cum_ret.index, cum_ret.values, label=name, linewidth=1.5)\n",
    "\n",
    "ax.set_title('Cumulative Returns: Different Portfolio Methods')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb87c7",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Out-of-Sample Testing with Rolling Windows\n",
    "\n",
    "The real test of HRP is out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_backtest(returns, window=252, rebalance_freq=21):\n",
    "    \"\"\"\n",
    "    Perform rolling window backtest for different portfolio methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : DataFrame\n",
    "        Asset returns\n",
    "    window : int\n",
    "        Lookback window for estimation (days)\n",
    "    rebalance_freq : int\n",
    "        Rebalancing frequency (days)\n",
    "    \"\"\"\n",
    "    results = {name: [] for name in ['Equal Weight', 'Inverse Variance', 'Min Variance', 'HRP']}\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(window, len(returns), rebalance_freq):\n",
    "        # Estimation window\n",
    "        train = returns.iloc[i-window:i]\n",
    "        \n",
    "        # Test period (until next rebalance)\n",
    "        test_end = min(i + rebalance_freq, len(returns))\n",
    "        test = returns.iloc[i:test_end]\n",
    "        \n",
    "        if len(test) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate weights\n",
    "        w_eq = equal_weight(train)\n",
    "        w_iv = inverse_variance(train)\n",
    "        w_mv = min_variance(train)\n",
    "        \n",
    "        hrp_model = HierarchicalRiskParity()\n",
    "        hrp_model.fit(train)\n",
    "        w_hrp = hrp_model.get_weights()\n",
    "        \n",
    "        # Calculate test period returns\n",
    "        for name, w in [('Equal Weight', w_eq), ('Inverse Variance', w_iv), \n",
    "                        ('Min Variance', w_mv), ('HRP', w_hrp)]:\n",
    "            port_ret = (test * w).sum(axis=1)\n",
    "            results[name].extend(port_ret.values)\n",
    "        \n",
    "        dates.extend(test.index)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results, index=dates)\n",
    "    return results_df\n",
    "\n",
    "print(\"Running rolling backtest (this may take a moment)...\")\n",
    "backtest_results = rolling_backtest(returns, window=252, rebalance_freq=21)\n",
    "print(f\"Backtest period: {backtest_results.index[0].date()} to {backtest_results.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd013ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate out-of-sample metrics\n",
    "oos_metrics = {}\n",
    "\n",
    "for col in backtest_results.columns:\n",
    "    ret = backtest_results[col]\n",
    "    \n",
    "    ann_ret = ret.mean() * 252\n",
    "    ann_vol = ret.std() * np.sqrt(252)\n",
    "    sharpe = ann_ret / ann_vol\n",
    "    \n",
    "    cum_ret = (1 + ret).cumprod()\n",
    "    rolling_max = cum_ret.expanding().max()\n",
    "    max_dd = ((cum_ret - rolling_max) / rolling_max).min()\n",
    "    \n",
    "    oos_metrics[col] = {\n",
    "        'Annual Return': f\"{ann_ret:.2%}\",\n",
    "        'Annual Volatility': f\"{ann_vol:.2%}\",\n",
    "        'Sharpe Ratio': f\"{sharpe:.3f}\",\n",
    "        'Max Drawdown': f\"{max_dd:.2%}\"\n",
    "    }\n",
    "\n",
    "print(\"\\nOut-of-Sample Performance:\")\n",
    "print(pd.DataFrame(oos_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot out-of-sample cumulative returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Cumulative returns\n",
    "cum_returns = (1 + backtest_results).cumprod()\n",
    "for col in cum_returns.columns:\n",
    "    axes[0].plot(cum_returns.index, cum_returns[col], label=col, linewidth=1.5)\n",
    "\n",
    "axes[0].set_title('Out-of-Sample Cumulative Returns')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdowns\n",
    "for col in cum_returns.columns:\n",
    "    rolling_max = cum_returns[col].expanding().max()\n",
    "    drawdown = (cum_returns[col] - rolling_max) / rolling_max\n",
    "    axes[1].fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, label=col)\n",
    "\n",
    "axes[1].set_title('Drawdowns')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Drawdown')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4560af8",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Stability Analysis\n",
    "\n",
    "One key advantage of HRP is weight stability over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09287c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight_turnover(returns, method_func, window=252, rebalance_freq=21):\n",
    "    \"\"\"\n",
    "    Calculate weight turnover over time.\n",
    "    \"\"\"\n",
    "    prev_weights = None\n",
    "    turnovers = []\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(window, len(returns), rebalance_freq):\n",
    "        train = returns.iloc[i-window:i]\n",
    "        \n",
    "        if method_func == 'hrp':\n",
    "            model = HierarchicalRiskParity()\n",
    "            model.fit(train)\n",
    "            weights = model.get_weights()\n",
    "        else:\n",
    "            weights = method_func(train)\n",
    "        \n",
    "        if prev_weights is not None:\n",
    "            turnover = np.abs(weights - prev_weights).sum() / 2\n",
    "            turnovers.append(turnover)\n",
    "            dates.append(returns.index[i])\n",
    "        \n",
    "        prev_weights = weights\n",
    "    \n",
    "    return pd.Series(turnovers, index=dates)\n",
    "\n",
    "# Calculate turnover for each method\n",
    "turnover_eq = calculate_weight_turnover(returns, equal_weight)\n",
    "turnover_iv = calculate_weight_turnover(returns, inverse_variance)\n",
    "turnover_mv = calculate_weight_turnover(returns, min_variance)\n",
    "turnover_hrp = calculate_weight_turnover(returns, 'hrp')\n",
    "\n",
    "# Summary\n",
    "turnover_summary = pd.DataFrame({\n",
    "    'Equal Weight': [turnover_eq.mean(), turnover_eq.std()],\n",
    "    'Inverse Variance': [turnover_iv.mean(), turnover_iv.std()],\n",
    "    'Min Variance': [turnover_mv.mean(), turnover_mv.std()],\n",
    "    'HRP': [turnover_hrp.mean(), turnover_hrp.std()]\n",
    "}, index=['Mean Turnover', 'Std Turnover'])\n",
    "\n",
    "print(\"Weight Turnover Analysis (lower = more stable):\")\n",
    "print(turnover_summary.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5dbfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot turnover over time\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(turnover_iv.index, turnover_iv.values, label='Inverse Variance', alpha=0.7)\n",
    "ax.plot(turnover_mv.index, turnover_mv.values, label='Min Variance', alpha=0.7)\n",
    "ax.plot(turnover_hrp.index, turnover_hrp.values, label='HRP', alpha=0.7)\n",
    "\n",
    "ax.set_title('Portfolio Turnover Over Time')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Turnover (fraction of portfolio)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: HRP typically shows lower and more stable turnover than MVO.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5ef53",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Interview Questions & Key Takeaways\n",
    "\n",
    "### Common Interview Questions:\n",
    "\n",
    "**Q1: What problem does HRP solve?**\n",
    "> HRP addresses the instability and concentration issues of mean-variance optimization by using hierarchical clustering and avoiding matrix inversion.\n",
    "\n",
    "**Q2: Explain the three steps of HRP.**\n",
    "> 1. **Tree Clustering**: Convert correlation to distance, perform hierarchical clustering\n",
    "> 2. **Quasi-Diagonalization**: Reorder assets so similar ones are adjacent\n",
    "> 3. **Recursive Bisection**: Split clusters and allocate using inverse variance weights\n",
    "\n",
    "**Q3: Why does HRP avoid matrix inversion?**\n",
    "> Matrix inversion amplifies estimation errors and can be numerically unstable, especially when N > T or assets are highly correlated.\n",
    "\n",
    "**Q4: How does HRP compare to Risk Parity?**\n",
    "> Traditional Risk Parity ignores correlations (or uses inverse covariance). HRP accounts for the hierarchical structure of correlations while maintaining stability.\n",
    "\n",
    "**Q5: What are limitations of HRP?**\n",
    "> - No explicit return forecasts (like min variance)\n",
    "> - Results depend on linkage method choice\n",
    "> - May underperform when correlations are stable and known\n",
    "\n",
    "### Key Takeaways:\n",
    "1. HRP is a machine learning approach to portfolio construction\n",
    "2. It produces more stable, diversified portfolios than MVO\n",
    "3. Works well when covariance estimates are noisy\n",
    "4. Lower turnover = lower transaction costs\n",
    "5. Particularly useful for large universes where N > T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ff967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"DAY 4 COMPLETE: Hierarchical Risk Parity\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Key Concepts Covered:\n",
    "✓ Why traditional MVO fails\n",
    "✓ Correlation-to-distance transformation\n",
    "✓ Hierarchical clustering for asset grouping\n",
    "✓ Quasi-diagonalization of covariance matrix\n",
    "✓ Recursive bisection for weight allocation\n",
    "✓ Complete HRP implementation\n",
    "✓ Comparison with Equal Weight, IVP, Min Variance\n",
    "✓ Out-of-sample rolling backtest\n",
    "✓ Turnover/stability analysis\n",
    "\n",
    "Tomorrow: Black-Litterman Model\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
