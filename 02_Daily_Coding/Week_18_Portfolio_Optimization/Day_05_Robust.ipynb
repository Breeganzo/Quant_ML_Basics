{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d931caf",
   "metadata": {},
   "source": [
    "# Day 5: Robust Portfolio Optimization\n",
    "\n",
    "## Week 18 - Portfolio Optimization\n",
    "\n",
    "**Date:** January 23, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Understand estimation error** in portfolio optimization\n",
    "2. **Implement shrinkage estimators** for covariance matrices\n",
    "3. **Build uncertainty sets** for robust optimization\n",
    "4. **Compare robust vs. classical portfolios** out-of-sample\n",
    "\n",
    "---\n",
    "\n",
    "## Why Robust Optimization?\n",
    "\n",
    "Classical Mean-Variance optimization suffers from:\n",
    "- **Estimation error amplification**: Small errors in inputs → Large errors in weights\n",
    "- **Extreme weights**: Optimizer exploits estimation errors\n",
    "- **Poor out-of-sample performance**: Optimized portfolios often underperform naive strategies\n",
    "\n",
    "Robust optimization addresses these by:\n",
    "- Acknowledging parameter uncertainty\n",
    "- Optimizing for worst-case scenarios within uncertainty sets\n",
    "- Shrinking extreme estimates toward structured targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "from typing import Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Robust Portfolio Optimization - Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12b004",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Data Preparation and Estimation Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b798baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download market data\n",
    "tickers = ['SPY', 'QQQ', 'IWM', 'EFA', 'EEM', 'TLT', 'GLD', 'VNQ', 'LQD', 'HYG']\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2025-12-31'\n",
    "\n",
    "print(f\"Downloading data for {len(tickers)} assets...\")\n",
    "data = yf.download(tickers, start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "print(f\"Data shape: {returns.shape}\")\n",
    "print(f\"Date range: {returns.index[0].date()} to {returns.index[-1].date()}\")\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa08c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate estimation error with bootstrap analysis\n",
    "def bootstrap_parameters(returns: pd.DataFrame, n_bootstrap: int = 500) -> Dict:\n",
    "    \"\"\"\n",
    "    Bootstrap estimation to quantify parameter uncertainty.\n",
    "    \n",
    "    Returns distribution of estimated means, volatilities, and correlations.\n",
    "    \"\"\"\n",
    "    n_obs = len(returns)\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    means_boot = np.zeros((n_bootstrap, n_assets))\n",
    "    vols_boot = np.zeros((n_bootstrap, n_assets))\n",
    "    sharpe_boot = np.zeros((n_bootstrap, n_assets))\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Sample with replacement\n",
    "        sample_idx = np.random.choice(n_obs, size=n_obs, replace=True)\n",
    "        sample_returns = returns.iloc[sample_idx]\n",
    "        \n",
    "        means_boot[i] = sample_returns.mean().values * 252\n",
    "        vols_boot[i] = sample_returns.std().values * np.sqrt(252)\n",
    "        sharpe_boot[i] = means_boot[i] / vols_boot[i]\n",
    "    \n",
    "    return {\n",
    "        'means': means_boot,\n",
    "        'vols': vols_boot,\n",
    "        'sharpe': sharpe_boot\n",
    "    }\n",
    "\n",
    "# Run bootstrap\n",
    "boot_results = bootstrap_parameters(returns)\n",
    "\n",
    "# Display estimation uncertainty\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Mean returns uncertainty\n",
    "ax = axes[0]\n",
    "mean_means = boot_results['means'].mean(axis=0)\n",
    "mean_stds = boot_results['means'].std(axis=0)\n",
    "x = np.arange(len(tickers))\n",
    "ax.bar(x, mean_means, yerr=2*mean_stds, capsize=3, alpha=0.7, color='steelblue')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Annualized Return')\n",
    "ax.set_title('Mean Returns with 95% CI')\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Sharpe ratio uncertainty\n",
    "ax = axes[1]\n",
    "sharpe_means = boot_results['sharpe'].mean(axis=0)\n",
    "sharpe_stds = boot_results['sharpe'].std(axis=0)\n",
    "ax.bar(x, sharpe_means, yerr=2*sharpe_stds, capsize=3, alpha=0.7, color='darkgreen')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Sharpe Ratio')\n",
    "ax.set_title('Sharpe Ratios with 95% CI')\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Coefficient of variation for means (relative uncertainty)\n",
    "ax = axes[2]\n",
    "cv = np.abs(mean_stds / (mean_means + 1e-8))\n",
    "ax.bar(x, cv, alpha=0.7, color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('CV (Std/Mean)')\n",
    "ax.set_title('Relative Estimation Uncertainty')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: Expected returns have VERY high estimation error\")\n",
    "print(\"Covariance estimates are more stable than mean estimates\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43f8f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Shrinkage Estimators for Covariance Matrices\n",
    "\n",
    "### The Shrinkage Framework\n",
    "\n",
    "Shrinkage estimator: $\\hat{\\Sigma}_{shrink} = \\delta \\cdot F + (1-\\delta) \\cdot S$\n",
    "\n",
    "Where:\n",
    "- $S$ = Sample covariance matrix\n",
    "- $F$ = Structured target (shrinkage target)\n",
    "- $\\delta$ = Shrinkage intensity (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovarianceShrinkage:\n",
    "    \"\"\"\n",
    "    Collection of covariance shrinkage estimators.\n",
    "    \n",
    "    Implements:\n",
    "    1. Ledoit-Wolf shrinkage to identity\n",
    "    2. Ledoit-Wolf shrinkage to constant correlation\n",
    "    3. Oracle Approximating Shrinkage (OAS)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame):\n",
    "        self.returns = returns\n",
    "        self.T, self.N = returns.shape\n",
    "        self.sample_cov = returns.cov().values\n",
    "        \n",
    "    def ledoit_wolf_identity(self) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Ledoit-Wolf shrinkage toward scaled identity matrix.\n",
    "        \n",
    "        Target: F = trace(S)/N * I\n",
    "        \"\"\"\n",
    "        X = self.returns.values\n",
    "        T, N = self.T, self.N\n",
    "        \n",
    "        # Sample covariance\n",
    "        S = self.sample_cov\n",
    "        \n",
    "        # Target: scaled identity\n",
    "        mu = np.trace(S) / N\n",
    "        F = mu * np.eye(N)\n",
    "        \n",
    "        # Compute optimal shrinkage intensity\n",
    "        # Using Ledoit-Wolf formula\n",
    "        X_centered = X - X.mean(axis=0)\n",
    "        \n",
    "        # Sum of squared off-diagonal elements\n",
    "        delta = S - F\n",
    "        \n",
    "        # pi: sum of squared differences from sample to target\n",
    "        pi = np.sum(delta ** 2)\n",
    "        \n",
    "        # rho: asymptotic sum of squared shrinkage residuals\n",
    "        y = X_centered ** 2\n",
    "        phi_diag = np.sum((y.T @ y / T - S ** 2)) / T\n",
    "        \n",
    "        rho = 0\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i == j:\n",
    "                    rho += np.var(y[:, i]) / T\n",
    "                else:\n",
    "                    rho += np.cov(y[:, i], y[:, j])[0, 1] / T\n",
    "        \n",
    "        # Optimal shrinkage\n",
    "        kappa = (pi - rho) / pi if pi > 0 else 0\n",
    "        delta_star = max(0, min(1, kappa / T))\n",
    "        \n",
    "        # Shrunk covariance\n",
    "        shrunk_cov = delta_star * F + (1 - delta_star) * S\n",
    "        \n",
    "        return shrunk_cov, delta_star\n",
    "    \n",
    "    def ledoit_wolf_constant_correlation(self) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Ledoit-Wolf shrinkage toward constant correlation matrix.\n",
    "        \n",
    "        Target: F_ij = sqrt(S_ii * S_jj) * rho_bar for i != j\n",
    "        \"\"\"\n",
    "        X = self.returns.values\n",
    "        T, N = self.T, self.N\n",
    "        \n",
    "        # Sample covariance and correlation\n",
    "        S = self.sample_cov\n",
    "        vols = np.sqrt(np.diag(S))\n",
    "        corr = S / np.outer(vols, vols)\n",
    "        np.fill_diagonal(corr, 1.0)\n",
    "        \n",
    "        # Average correlation (excluding diagonal)\n",
    "        rho_bar = (np.sum(corr) - N) / (N * (N - 1))\n",
    "        \n",
    "        # Target matrix: constant correlation\n",
    "        F_corr = np.full((N, N), rho_bar)\n",
    "        np.fill_diagonal(F_corr, 1.0)\n",
    "        F = np.outer(vols, vols) * F_corr\n",
    "        \n",
    "        # Compute shrinkage intensity (simplified)\n",
    "        X_centered = X - X.mean(axis=0)\n",
    "        X_std = X_centered / vols\n",
    "        \n",
    "        # Estimate pi (sum of squared errors)\n",
    "        pi = np.sum((S - F) ** 2)\n",
    "        \n",
    "        # Simple approximation for optimal shrinkage\n",
    "        gamma = np.sum((corr - rho_bar) ** 2) - np.sum((corr - rho_bar).diagonal() ** 2)\n",
    "        kappa = gamma / (T * pi) if pi > 0 else 0\n",
    "        delta_star = max(0, min(1, kappa))\n",
    "        \n",
    "        # Shrunk covariance\n",
    "        shrunk_cov = delta_star * F + (1 - delta_star) * S\n",
    "        \n",
    "        return shrunk_cov, delta_star\n",
    "    \n",
    "    def oracle_approximating_shrinkage(self) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"\n",
    "        Oracle Approximating Shrinkage (OAS) estimator.\n",
    "        \n",
    "        More efficient than Ledoit-Wolf for small samples.\n",
    "        \"\"\"\n",
    "        T, N = self.T, self.N\n",
    "        S = self.sample_cov\n",
    "        \n",
    "        # Target: scaled identity\n",
    "        mu = np.trace(S) / N\n",
    "        F = mu * np.eye(N)\n",
    "        \n",
    "        # OAS shrinkage intensity\n",
    "        trace_S2 = np.trace(S @ S)\n",
    "        trace_S_2 = np.trace(S) ** 2\n",
    "        \n",
    "        # OAS formula\n",
    "        rho_num = (1 - 2/N) * trace_S2 + trace_S_2\n",
    "        rho_den = (T + 1 - 2/N) * (trace_S2 - trace_S_2 / N)\n",
    "        \n",
    "        delta_star = max(0, min(1, rho_num / rho_den if rho_den > 0 else 0))\n",
    "        \n",
    "        # Shrunk covariance\n",
    "        shrunk_cov = delta_star * F + (1 - delta_star) * S\n",
    "        \n",
    "        return shrunk_cov, delta_star\n",
    "\n",
    "\n",
    "# Compute shrinkage estimators\n",
    "shrinkage = CovarianceShrinkage(returns)\n",
    "\n",
    "cov_lw_identity, delta_identity = shrinkage.ledoit_wolf_identity()\n",
    "cov_lw_constcorr, delta_constcorr = shrinkage.ledoit_wolf_constant_correlation()\n",
    "cov_oas, delta_oas = shrinkage.oracle_approximating_shrinkage()\n",
    "cov_sample = returns.cov().values\n",
    "\n",
    "print(\"Shrinkage Intensities:\")\n",
    "print(f\"  Ledoit-Wolf (Identity):      δ = {delta_identity:.4f}\")\n",
    "print(f\"  Ledoit-Wolf (Const Corr):    δ = {delta_constcorr:.4f}\")\n",
    "print(f\"  OAS:                         δ = {delta_oas:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b990f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize covariance matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "cov_matrices = {\n",
    "    'Sample Covariance': cov_sample,\n",
    "    'LW Shrink to Identity': cov_lw_identity,\n",
    "    'LW Shrink to Const Corr': cov_lw_constcorr,\n",
    "    'OAS': cov_oas\n",
    "}\n",
    "\n",
    "# Convert to correlation for visualization\n",
    "for ax, (name, cov) in zip(axes.flat, cov_matrices.items()):\n",
    "    vols = np.sqrt(np.diag(cov))\n",
    "    corr = cov / np.outer(vols, vols)\n",
    "    \n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "                xticklabels=tickers, yticklabels=tickers, ax=ax,\n",
    "                vmin=-1, vmax=1)\n",
    "    ax.set_title(f'{name}\\n(as correlation matrix)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f023e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare eigenvalue distributions (condition numbers)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Eigenvalues\n",
    "ax = axes[0]\n",
    "for name, cov in cov_matrices.items():\n",
    "    eigenvalues = np.linalg.eigvalsh(cov)\n",
    "    eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "    ax.semilogy(range(1, len(eigenvalues) + 1), eigenvalues, 'o-', label=name, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Eigenvalue Rank')\n",
    "ax.set_ylabel('Eigenvalue (log scale)')\n",
    "ax.set_title('Eigenvalue Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Condition numbers\n",
    "ax = axes[1]\n",
    "condition_numbers = []\n",
    "for name, cov in cov_matrices.items():\n",
    "    cond = np.linalg.cond(cov)\n",
    "    condition_numbers.append((name, cond))\n",
    "\n",
    "names, conds = zip(*condition_numbers)\n",
    "colors = ['coral', 'steelblue', 'darkgreen', 'purple']\n",
    "bars = ax.bar(range(len(names)), conds, color=colors, alpha=0.7)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(['Sample', 'LW-Identity', 'LW-ConstCorr', 'OAS'], rotation=15)\n",
    "ax.set_ylabel('Condition Number')\n",
    "ax.set_title('Matrix Condition Numbers (lower = more stable)')\n",
    "\n",
    "for bar, cond in zip(bars, conds):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{cond:.1f}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: Shrinkage reduces condition number\")\n",
    "print(\"Lower condition number = more numerically stable optimization\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b91e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Robust Optimization with Uncertainty Sets\n",
    "\n",
    "### Uncertainty Set Framework\n",
    "\n",
    "Instead of point estimates, we define uncertainty sets:\n",
    "\n",
    "**Mean uncertainty:** $\\mu \\in \\mathcal{U}_\\mu = \\{\\mu : \\|\\mu - \\hat{\\mu}\\| \\leq \\kappa_\\mu\\}$\n",
    "\n",
    "**Covariance uncertainty:** $\\Sigma \\in \\mathcal{U}_\\Sigma$\n",
    "\n",
    "The robust problem becomes:\n",
    "\n",
    "$$\\max_w \\min_{\\mu \\in \\mathcal{U}_\\mu} w^T \\mu - \\frac{\\lambda}{2} w^T \\Sigma w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPortfolioOptimizer:\n",
    "    \"\"\"\n",
    "    Robust portfolio optimization with uncertainty sets.\n",
    "    \n",
    "    Implements:\n",
    "    1. Classical Mean-Variance (benchmark)\n",
    "    2. Robust Mean optimization (box uncertainty)\n",
    "    3. Robust Mean optimization (ellipsoidal uncertainty)\n",
    "    4. Resampled optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame, risk_aversion: float = 2.0):\n",
    "        self.returns = returns\n",
    "        self.mu = returns.mean().values * 252  # Annualized\n",
    "        self.Sigma = returns.cov().values * 252  # Annualized\n",
    "        self.N = len(returns.columns)\n",
    "        self.T = len(returns)\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.asset_names = returns.columns.tolist()\n",
    "        \n",
    "        # Estimate uncertainty\n",
    "        self._estimate_uncertainty()\n",
    "    \n",
    "    def _estimate_uncertainty(self):\n",
    "        \"\"\"Estimate parameter uncertainty from data.\"\"\"\n",
    "        # Standard error of mean estimates\n",
    "        self.mu_se = np.sqrt(np.diag(self.Sigma) / self.T)\n",
    "        \n",
    "        # Box uncertainty: 2 standard errors\n",
    "        self.mu_box_radius = 2 * self.mu_se\n",
    "        \n",
    "        # Ellipsoidal uncertainty: use inverse covariance\n",
    "        # Scaled by chi-squared critical value\n",
    "        chi2_crit = stats.chi2.ppf(0.95, self.N)\n",
    "        self.ellipsoid_radius = np.sqrt(chi2_crit / self.T)\n",
    "    \n",
    "    def classical_mean_variance(self, target_return: Optional[float] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Classical Mean-Variance optimization.\n",
    "        \n",
    "        max w'μ - (λ/2) w'Σw\n",
    "        s.t. sum(w) = 1\n",
    "        \"\"\"\n",
    "        def objective(w):\n",
    "            port_return = w @ self.mu\n",
    "            port_var = w @ self.Sigma @ w\n",
    "            return -(port_return - 0.5 * self.risk_aversion * port_var)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        \n",
    "        if target_return is not None:\n",
    "            constraints.append({'type': 'eq', 'fun': lambda w: w @ self.mu - target_return})\n",
    "        \n",
    "        bounds = [(0, 1) for _ in range(self.N)]  # Long-only\n",
    "        w0 = np.ones(self.N) / self.N\n",
    "        \n",
    "        result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    def robust_box_uncertainty(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Robust optimization with box uncertainty set.\n",
    "        \n",
    "        Worst-case mean: μ_wc = μ - sign(w) * κ\n",
    "        \n",
    "        For long-only, worst-case is always: μ_wc = μ - κ\n",
    "        \"\"\"\n",
    "        def objective(w):\n",
    "            # Worst-case return (subtract uncertainty for long positions)\n",
    "            worst_case_mu = self.mu - self.mu_box_radius * np.sign(w + 1e-8)\n",
    "            port_return = w @ worst_case_mu\n",
    "            port_var = w @ self.Sigma @ w\n",
    "            return -(port_return - 0.5 * self.risk_aversion * port_var)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(self.N)]\n",
    "        w0 = np.ones(self.N) / self.N\n",
    "        \n",
    "        result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    def robust_ellipsoidal_uncertainty(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Robust optimization with ellipsoidal uncertainty set.\n",
    "        \n",
    "        Uncertainty set: ||Σ^(-1/2)(μ - μ̂)||₂ ≤ κ\n",
    "        \n",
    "        Worst-case return: w'μ̂ - κ * ||Σ^(1/2)w||₂\n",
    "        \"\"\"\n",
    "        # Compute Sigma^(1/2) for penalty term\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(self.Sigma)\n",
    "        Sigma_sqrt = eigenvectors @ np.diag(np.sqrt(np.maximum(eigenvalues, 0))) @ eigenvectors.T\n",
    "        \n",
    "        def objective(w):\n",
    "            port_return = w @ self.mu\n",
    "            port_var = w @ self.Sigma @ w\n",
    "            \n",
    "            # Ellipsoidal penalty: reduces expected return\n",
    "            uncertainty_penalty = self.ellipsoid_radius * np.sqrt(w @ self.Sigma @ w / self.T)\n",
    "            \n",
    "            robust_return = port_return - uncertainty_penalty\n",
    "            return -(robust_return - 0.5 * self.risk_aversion * port_var)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(self.N)]\n",
    "        w0 = np.ones(self.N) / self.N\n",
    "        \n",
    "        result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    def resampled_optimization(self, n_samples: int = 500) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Resampled Efficient Frontier (Michaud, 1998).\n",
    "        \n",
    "        Bootstrap parameters and average optimal weights.\n",
    "        \"\"\"\n",
    "        weights_samples = np.zeros((n_samples, self.N))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Bootstrap returns\n",
    "            sample_idx = np.random.choice(self.T, size=self.T, replace=True)\n",
    "            sample_returns = self.returns.iloc[sample_idx]\n",
    "            \n",
    "            # Estimate parameters from bootstrap sample\n",
    "            mu_boot = sample_returns.mean().values * 252\n",
    "            Sigma_boot = sample_returns.cov().values * 252\n",
    "            \n",
    "            # Optimize with bootstrap parameters\n",
    "            def objective(w):\n",
    "                port_return = w @ mu_boot\n",
    "                port_var = w @ Sigma_boot @ w\n",
    "                return -(port_return - 0.5 * self.risk_aversion * port_var)\n",
    "            \n",
    "            constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "            bounds = [(0, 1) for _ in range(self.N)]\n",
    "            w0 = np.ones(self.N) / self.N\n",
    "            \n",
    "            try:\n",
    "                result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "                weights_samples[i] = result.x\n",
    "            except:\n",
    "                weights_samples[i] = w0\n",
    "        \n",
    "        # Average weights\n",
    "        return weights_samples.mean(axis=0)\n",
    "    \n",
    "    def minimum_variance(self, use_shrinkage: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Global Minimum Variance portfolio.\n",
    "        \n",
    "        Ignores expected returns entirely.\n",
    "        \"\"\"\n",
    "        if use_shrinkage:\n",
    "            shrinkage = CovarianceShrinkage(self.returns)\n",
    "            Sigma, _ = shrinkage.ledoit_wolf_constant_correlation()\n",
    "            Sigma = Sigma * 252\n",
    "        else:\n",
    "            Sigma = self.Sigma\n",
    "        \n",
    "        def objective(w):\n",
    "            return w @ Sigma @ w\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(self.N)]\n",
    "        w0 = np.ones(self.N) / self.N\n",
    "        \n",
    "        result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = RobustPortfolioOptimizer(returns, risk_aversion=2.0)\n",
    "\n",
    "print(\"RobustPortfolioOptimizer initialized\")\n",
    "print(f\"Assets: {optimizer.asset_names}\")\n",
    "print(f\"Observations: {optimizer.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed242143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all portfolio weights\n",
    "print(\"Computing portfolios...\")\n",
    "\n",
    "portfolios = {\n",
    "    'Equal Weight': np.ones(optimizer.N) / optimizer.N,\n",
    "    'Classical MV': optimizer.classical_mean_variance(),\n",
    "    'Min Variance': optimizer.minimum_variance(use_shrinkage=False),\n",
    "    'Min Var (Shrunk)': optimizer.minimum_variance(use_shrinkage=True),\n",
    "    'Robust Box': optimizer.robust_box_uncertainty(),\n",
    "    'Robust Ellipsoid': optimizer.robust_ellipsoidal_uncertainty(),\n",
    "    'Resampled': optimizer.resampled_optimization(n_samples=200)\n",
    "}\n",
    "\n",
    "# Create weights DataFrame\n",
    "weights_df = pd.DataFrame(portfolios, index=optimizer.asset_names).T\n",
    "print(\"\\nPortfolio Weights:\")\n",
    "print(weights_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize portfolio weights\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "weights_df.plot(kind='bar', ax=ax, colormap='tab10', width=0.8)\n",
    "ax.set_ylabel('Weight')\n",
    "ax.set_xlabel('Portfolio Strategy')\n",
    "ax.set_title('Portfolio Weights Comparison: Classical vs Robust Methods')\n",
    "ax.legend(title='Assets', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weight concentration metrics\n",
    "print(\"\\nWeight Concentration Metrics:\")\n",
    "print(\"=\"*60)\n",
    "for name, weights in portfolios.items():\n",
    "    hhi = np.sum(weights ** 2)  # Herfindahl-Hirschman Index\n",
    "    max_weight = np.max(weights)\n",
    "    n_effective = 1 / hhi  # Effective number of assets\n",
    "    print(f\"{name:20s}: HHI={hhi:.4f}, MaxWt={max_weight:.4f}, N_eff={n_effective:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288313cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Out-of-Sample Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_backtest(returns: pd.DataFrame, \n",
    "                     estimation_window: int = 252,\n",
    "                     rebalance_freq: int = 21,\n",
    "                     risk_aversion: float = 2.0) -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Rolling window backtest comparing robust vs classical portfolios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : Daily returns DataFrame\n",
    "    estimation_window : Days for parameter estimation\n",
    "    rebalance_freq : Days between rebalancing\n",
    "    risk_aversion : Risk aversion parameter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of portfolio return series\n",
    "    \"\"\"\n",
    "    n_obs = len(returns)\n",
    "    N = returns.shape[1]\n",
    "    \n",
    "    # Initialize tracking\n",
    "    portfolio_returns = {name: [] for name in ['Equal Weight', 'Classical MV', \n",
    "                                                'Min Variance', 'Robust Box', 'Resampled']}\n",
    "    dates = []\n",
    "    \n",
    "    # Current weights\n",
    "    current_weights = {name: np.ones(N) / N for name in portfolio_returns.keys()}\n",
    "    \n",
    "    for t in range(estimation_window, n_obs):\n",
    "        # Rebalance at specified frequency\n",
    "        if (t - estimation_window) % rebalance_freq == 0:\n",
    "            # Estimation window\n",
    "            est_returns = returns.iloc[t-estimation_window:t]\n",
    "            \n",
    "            try:\n",
    "                optimizer = RobustPortfolioOptimizer(est_returns, risk_aversion)\n",
    "                \n",
    "                current_weights['Equal Weight'] = np.ones(N) / N\n",
    "                current_weights['Classical MV'] = optimizer.classical_mean_variance()\n",
    "                current_weights['Min Variance'] = optimizer.minimum_variance(use_shrinkage=True)\n",
    "                current_weights['Robust Box'] = optimizer.robust_box_uncertainty()\n",
    "                current_weights['Resampled'] = optimizer.resampled_optimization(n_samples=100)\n",
    "            except Exception as e:\n",
    "                pass  # Keep previous weights if optimization fails\n",
    "        \n",
    "        # Record returns for this period\n",
    "        daily_return = returns.iloc[t].values\n",
    "        dates.append(returns.index[t])\n",
    "        \n",
    "        for name, weights in current_weights.items():\n",
    "            port_ret = weights @ daily_return\n",
    "            portfolio_returns[name].append(port_ret)\n",
    "    \n",
    "    # Convert to Series\n",
    "    return {name: pd.Series(rets, index=dates) \n",
    "            for name, rets in portfolio_returns.items()}\n",
    "\n",
    "\n",
    "# Run backtest\n",
    "print(\"Running rolling backtest (this may take a minute)...\")\n",
    "backtest_results = rolling_backtest(returns, estimation_window=252, rebalance_freq=21)\n",
    "print(\"Backtest complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d43371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "def calculate_metrics(returns_series: pd.Series) -> Dict:\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    ann_return = returns_series.mean() * 252\n",
    "    ann_vol = returns_series.std() * np.sqrt(252)\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else 0\n",
    "    \n",
    "    # Drawdown\n",
    "    cum_returns = (1 + returns_series).cumprod()\n",
    "    rolling_max = cum_returns.cummax()\n",
    "    drawdowns = (cum_returns - rolling_max) / rolling_max\n",
    "    max_dd = drawdowns.min()\n",
    "    \n",
    "    # Sortino ratio\n",
    "    downside_returns = returns_series[returns_series < 0]\n",
    "    downside_vol = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else 0\n",
    "    sortino = ann_return / downside_vol if downside_vol > 0 else 0\n",
    "    \n",
    "    # Calmar ratio\n",
    "    calmar = -ann_return / max_dd if max_dd < 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Annual Return': ann_return,\n",
    "        'Annual Vol': ann_vol,\n",
    "        'Sharpe': sharpe,\n",
    "        'Sortino': sortino,\n",
    "        'Max Drawdown': max_dd,\n",
    "        'Calmar': calmar\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all portfolios\n",
    "metrics_df = pd.DataFrame({name: calculate_metrics(rets) \n",
    "                           for name, rets in backtest_results.items()}).T\n",
    "\n",
    "print(\"\\nOut-of-Sample Performance Metrics:\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize backtest results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cumulative returns\n",
    "ax = axes[0, 0]\n",
    "for name, rets in backtest_results.items():\n",
    "    cum_rets = (1 + rets).cumprod()\n",
    "    ax.plot(cum_rets.index, cum_rets.values, label=name, linewidth=1.5)\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Out-of-Sample Cumulative Returns')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling Sharpe ratio (252-day)\n",
    "ax = axes[0, 1]\n",
    "for name, rets in backtest_results.items():\n",
    "    rolling_sharpe = (rets.rolling(252).mean() * 252) / (rets.rolling(252).std() * np.sqrt(252))\n",
    "    ax.plot(rolling_sharpe.index, rolling_sharpe.values, label=name, linewidth=1.5)\n",
    "ax.set_ylabel('Rolling Sharpe (252d)')\n",
    "ax.set_title('Rolling Sharpe Ratio')\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdowns\n",
    "ax = axes[1, 0]\n",
    "for name, rets in backtest_results.items():\n",
    "    cum_rets = (1 + rets).cumprod()\n",
    "    rolling_max = cum_rets.cummax()\n",
    "    drawdowns = (cum_rets - rolling_max) / rolling_max\n",
    "    ax.fill_between(drawdowns.index, drawdowns.values, 0, alpha=0.3, label=name)\n",
    "ax.set_ylabel('Drawdown')\n",
    "ax.set_title('Underwater Curve (Drawdowns)')\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance metrics bar chart\n",
    "ax = axes[1, 1]\n",
    "metrics_plot = metrics_df[['Sharpe', 'Sortino', 'Calmar']]\n",
    "metrics_plot.plot(kind='bar', ax=ax, colormap='viridis', width=0.7)\n",
    "ax.set_ylabel('Ratio')\n",
    "ax.set_title('Risk-Adjusted Performance Comparison')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.legend(loc='upper right')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef3ed67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Black-Litterman with Uncertainty\n",
    "\n",
    "Black-Litterman naturally incorporates uncertainty through investor views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackLittermanRobust:\n",
    "    \"\"\"\n",
    "    Black-Litterman model with explicit uncertainty handling.\n",
    "    \n",
    "    The model naturally blends market equilibrium with uncertain views.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame, market_caps: Optional[np.ndarray] = None,\n",
    "                 risk_aversion: float = 2.5, tau: float = 0.05):\n",
    "        self.returns = returns\n",
    "        self.N = returns.shape[1]\n",
    "        self.asset_names = returns.columns.tolist()\n",
    "        \n",
    "        # Use shrunk covariance\n",
    "        shrinkage = CovarianceShrinkage(returns)\n",
    "        self.Sigma, _ = shrinkage.ledoit_wolf_constant_correlation()\n",
    "        self.Sigma = self.Sigma * 252  # Annualize\n",
    "        \n",
    "        # Market caps (default: equal)\n",
    "        self.market_caps = market_caps if market_caps is not None else np.ones(self.N)\n",
    "        self.w_mkt = self.market_caps / np.sum(self.market_caps)\n",
    "        \n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Implied equilibrium returns\n",
    "        self.pi = risk_aversion * self.Sigma @ self.w_mkt\n",
    "    \n",
    "    def add_views(self, P: np.ndarray, Q: np.ndarray, \n",
    "                  confidences: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Incorporate investor views into posterior expected returns.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        P : View matrix (K x N), K views on N assets\n",
    "        Q : View returns (K,), expected returns for each view\n",
    "        confidences : View confidences (K,), 0 to 1 scale\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Posterior expected returns (N,)\n",
    "        \"\"\"\n",
    "        K = len(Q)  # Number of views\n",
    "        \n",
    "        # Omega: view uncertainty covariance (diagonal)\n",
    "        if confidences is None:\n",
    "            confidences = np.ones(K) * 0.5\n",
    "        \n",
    "        # Scale omega based on confidence (lower confidence = higher uncertainty)\n",
    "        omega_diag = (1 / confidences - 1) * np.diag(P @ self.Sigma @ P.T) * self.tau\n",
    "        Omega = np.diag(omega_diag)\n",
    "        \n",
    "        # Prior covariance of expected returns\n",
    "        tau_Sigma = self.tau * self.Sigma\n",
    "        \n",
    "        # Posterior expected returns (Black-Litterman formula)\n",
    "        inv_tau_Sigma = np.linalg.inv(tau_Sigma)\n",
    "        inv_Omega = np.linalg.inv(Omega)\n",
    "        \n",
    "        M = np.linalg.inv(inv_tau_Sigma + P.T @ inv_Omega @ P)\n",
    "        posterior_mu = M @ (inv_tau_Sigma @ self.pi + P.T @ inv_Omega @ Q)\n",
    "        \n",
    "        return posterior_mu\n",
    "    \n",
    "    def optimize(self, mu: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Optimize portfolio given expected returns.\n",
    "        \"\"\"\n",
    "        def objective(w):\n",
    "            return -(w @ mu - 0.5 * self.risk_aversion * w @ self.Sigma @ w)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(self.N)]\n",
    "        w0 = self.w_mkt.copy()\n",
    "        \n",
    "        result = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "\n",
    "\n",
    "# Example: Black-Litterman with views\n",
    "bl_model = BlackLittermanRobust(returns, risk_aversion=2.5, tau=0.05)\n",
    "\n",
    "print(\"Implied Equilibrium Returns (from market portfolio):\")\n",
    "for name, ret in zip(bl_model.asset_names, bl_model.pi):\n",
    "    print(f\"  {name}: {ret*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffd1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add investor views\n",
    "# View 1: QQQ will outperform SPY by 3%\n",
    "# View 2: EEM will underperform EFA by 2%\n",
    "# View 3: TLT will return 4%\n",
    "\n",
    "K = 3  # Number of views\n",
    "N = len(tickers)\n",
    "\n",
    "# Build view matrix P\n",
    "P = np.zeros((K, N))\n",
    "\n",
    "# View 1: QQQ - SPY = 3%\n",
    "P[0, tickers.index('QQQ')] = 1\n",
    "P[0, tickers.index('SPY')] = -1\n",
    "\n",
    "# View 2: EFA - EEM = 2%\n",
    "P[1, tickers.index('EFA')] = 1\n",
    "P[1, tickers.index('EEM')] = -1\n",
    "\n",
    "# View 3: TLT absolute = 4%\n",
    "P[2, tickers.index('TLT')] = 1\n",
    "\n",
    "# View returns\n",
    "Q = np.array([0.03, 0.02, 0.04])\n",
    "\n",
    "# Confidences (higher = more confident)\n",
    "confidences = np.array([0.6, 0.4, 0.7])\n",
    "\n",
    "# Compute posterior returns\n",
    "posterior_mu = bl_model.add_views(P, Q, confidences)\n",
    "\n",
    "# Compare prior (equilibrium) vs posterior\n",
    "print(\"\\nView Incorporation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Asset':<10} {'Equilibrium':>12} {'Posterior':>12} {'Change':>10}\")\n",
    "print(\"-\"*60)\n",
    "for i, name in enumerate(tickers):\n",
    "    change = (posterior_mu[i] - bl_model.pi[i]) * 100\n",
    "    print(f\"{name:<10} {bl_model.pi[i]*100:>11.2f}% {posterior_mu[i]*100:>11.2f}% {change:>+9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize portfolios: prior vs posterior\n",
    "w_prior = bl_model.optimize(bl_model.pi)\n",
    "w_posterior = bl_model.optimize(posterior_mu)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Returns comparison\n",
    "ax = axes[0]\n",
    "x = np.arange(N)\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, bl_model.pi * 100, width, label='Equilibrium', color='steelblue', alpha=0.7)\n",
    "ax.bar(x + width/2, posterior_mu * 100, width, label='Posterior (with views)', color='coral', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Expected Return (%)')\n",
    "ax.set_title('Black-Litterman: Prior vs Posterior Returns')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Weights comparison\n",
    "ax = axes[1]\n",
    "ax.bar(x - width/2, w_prior * 100, width, label='Equilibrium Portfolio', color='steelblue', alpha=0.7)\n",
    "ax.bar(x + width/2, w_posterior * 100, width, label='Posterior Portfolio', color='coral', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Weight (%)')\n",
    "ax.set_title('Black-Litterman: Portfolio Weights')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a16d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Interview Questions and Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902c9ca",
   "metadata": {},
   "source": [
    "### Common Interview Questions\n",
    "\n",
    "**Q1: Why does classical Mean-Variance optimization often perform poorly out-of-sample?**\n",
    "\n",
    "**A1:** Classical MV optimization suffers from:\n",
    "1. **Estimation error amplification**: The optimizer treats estimated parameters as exact truth and exploits estimation errors\n",
    "2. **Extreme positions**: Takes large positions in assets with overestimated returns or underestimated correlations\n",
    "3. **Instability**: Small changes in inputs lead to large weight changes\n",
    "4. **High condition number**: Sample covariance may be nearly singular with few observations\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Explain the Ledoit-Wolf shrinkage estimator.**\n",
    "\n",
    "**A2:** Ledoit-Wolf shrinks the sample covariance toward a structured target:\n",
    "\n",
    "$\\hat{\\Sigma} = \\delta F + (1-\\delta)S$\n",
    "\n",
    "Where:\n",
    "- $S$ = Sample covariance (unbiased but high variance)\n",
    "- $F$ = Target (biased but low variance), e.g., scaled identity or constant correlation\n",
    "- $\\delta$ = Optimal shrinkage intensity (derived analytically)\n",
    "\n",
    "The optimal $\\delta$ minimizes the expected Frobenius norm between the true and estimated covariance matrices.\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What are uncertainty sets in robust optimization?**\n",
    "\n",
    "**A3:** Uncertainty sets define regions where true parameters might lie:\n",
    "\n",
    "- **Box uncertainty**: $\\mu_i \\in [\\hat{\\mu}_i - \\kappa_i, \\hat{\\mu}_i + \\kappa_i]$\n",
    "- **Ellipsoidal uncertainty**: $\\|\\Sigma^{-1/2}(\\mu - \\hat{\\mu})\\|_2 \\leq \\kappa$\n",
    "\n",
    "The robust problem optimizes for the worst case within the uncertainty set, leading to more conservative but stable portfolios.\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: How does Black-Litterman handle estimation uncertainty?**\n",
    "\n",
    "**A4:** Black-Litterman naturally incorporates uncertainty through:\n",
    "\n",
    "1. **Prior**: Equilibrium returns from market portfolio (stable anchor)\n",
    "2. **Views**: Investor opinions with explicit uncertainty (Ω matrix)\n",
    "3. **Posterior**: Bayesian combination weighted by relative confidences\n",
    "\n",
    "The τ parameter controls prior uncertainty, while Ω controls view uncertainty. Lower confidence views have less impact on the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate sensitivity analysis: how weights change with small parameter changes\n",
    "\n",
    "def sensitivity_analysis(returns: pd.DataFrame, n_perturbations: int = 50):\n",
    "    \"\"\"\n",
    "    Analyze weight sensitivity to small parameter perturbations.\n",
    "    \"\"\"\n",
    "    base_mu = returns.mean().values * 252\n",
    "    base_Sigma = returns.cov().values * 252\n",
    "    N = len(base_mu)\n",
    "    \n",
    "    # Store weight variations\n",
    "    classical_weights = []\n",
    "    robust_weights = []\n",
    "    \n",
    "    for _ in range(n_perturbations):\n",
    "        # Perturb means by small amount (10% of standard error)\n",
    "        mu_perturb = base_mu + np.random.randn(N) * 0.01\n",
    "        \n",
    "        # Classical optimization\n",
    "        def obj_classical(w):\n",
    "            return -(w @ mu_perturb - 0.5 * 2.0 * w @ base_Sigma @ w)\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(N)]\n",
    "        w0 = np.ones(N) / N\n",
    "        \n",
    "        res = minimize(obj_classical, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        classical_weights.append(res.x)\n",
    "        \n",
    "        # Robust optimization (with penalty for uncertainty)\n",
    "        def obj_robust(w):\n",
    "            penalty = 0.1 * np.sqrt(w @ base_Sigma @ w)\n",
    "            return -(w @ mu_perturb - penalty - 0.5 * 2.0 * w @ base_Sigma @ w)\n",
    "        \n",
    "        res = minimize(obj_robust, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "        robust_weights.append(res.x)\n",
    "    \n",
    "    return np.array(classical_weights), np.array(robust_weights)\n",
    "\n",
    "# Run sensitivity analysis\n",
    "classical_w, robust_w = sensitivity_analysis(returns)\n",
    "\n",
    "# Visualize weight stability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "classical_std = classical_w.std(axis=0)\n",
    "robust_std = robust_w.std(axis=0)\n",
    "\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, classical_std * 100, width, label='Classical MV', color='coral', alpha=0.7)\n",
    "ax.bar(x + width/2, robust_std * 100, width, label='Robust', color='steelblue', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Weight Std Dev (%)')\n",
    "ax.set_title('Weight Stability: Std Dev Under Parameter Perturbation')\n",
    "ax.legend()\n",
    "\n",
    "# Box plot of weights\n",
    "ax = axes[1]\n",
    "positions = np.arange(len(tickers)) * 2\n",
    "bp1 = ax.boxplot(classical_w * 100, positions=positions - 0.4, widths=0.6, \n",
    "                 patch_artist=True, boxprops=dict(facecolor='coral', alpha=0.7))\n",
    "bp2 = ax.boxplot(robust_w * 100, positions=positions + 0.4, widths=0.6,\n",
    "                 patch_artist=True, boxprops=dict(facecolor='steelblue', alpha=0.7))\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(tickers, rotation=45)\n",
    "ax.set_ylabel('Weight (%)')\n",
    "ax.set_title('Weight Distributions Under Parameter Uncertainty')\n",
    "ax.legend([bp1['boxes'][0], bp2['boxes'][0]], ['Classical MV', 'Robust'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHT: Robust methods produce more stable weights\")\n",
    "print(\"Classical MV weights vary significantly with small input changes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33954d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Robust Portfolio Optimization\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Estimation error is the enemy**: Mean returns have very high estimation uncertainty\n",
    "\n",
    "2. **Shrinkage estimators**:\n",
    "   - Reduce covariance matrix condition number\n",
    "   - Trade off bias for variance reduction\n",
    "   - Ledoit-Wolf provides optimal shrinkage analytically\n",
    "\n",
    "3. **Uncertainty sets**:\n",
    "   - Acknowledge parameter uncertainty explicitly\n",
    "   - Box and ellipsoidal sets are most common\n",
    "   - Leads to more conservative, stable portfolios\n",
    "\n",
    "4. **Black-Litterman**:\n",
    "   - Natural framework for incorporating uncertainty\n",
    "   - Blends equilibrium (stable) with views (uncertain)\n",
    "   - Produces intuitive, stable portfolios\n",
    "\n",
    "5. **Practical guidance**:\n",
    "   - Always use shrinkage for covariance estimation\n",
    "   - Consider ignoring return estimates (min variance)\n",
    "   - Resampling provides simple robustification\n",
    "   - Backtest with rolling windows to assess real-world performance\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. Ledoit, O., & Wolf, M. (2004). A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\n",
    "2. Michaud, R. O. (1998). Efficient Asset Management: A Practical Guide to Stock Portfolio Optimization\n",
    "3. Goldfarb, D., & Iyengar, G. (2003). Robust Portfolio Selection Problems\n",
    "4. Black, F., & Litterman, R. (1992). Global Portfolio Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe97953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary: Best practices checklist\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROBUST PORTFOLIO OPTIMIZATION: BEST PRACTICES CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checklist = [\n",
    "    \"✓ Use shrinkage estimators for covariance (Ledoit-Wolf, OAS)\",\n",
    "    \"✓ Quantify estimation uncertainty via bootstrap\",\n",
    "    \"✓ Consider ignoring return estimates (minimum variance)\",\n",
    "    \"✓ Apply constraints (long-only, max weight, sector limits)\",\n",
    "    \"✓ Use resampling to average over parameter uncertainty\",\n",
    "    \"✓ Implement Black-Litterman for view incorporation\",\n",
    "    \"✓ Backtest with rolling windows (out-of-sample)\",\n",
    "    \"✓ Compare against naive benchmarks (1/N, equal weight)\",\n",
    "    \"✓ Monitor weight stability and turnover\",\n",
    "    \"✓ Report risk-adjusted metrics (Sharpe, Sortino, Calmar)\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Day 5 Complete: Robust Portfolio Optimization\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
