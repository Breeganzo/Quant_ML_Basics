{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197e44bd",
   "metadata": {},
   "source": [
    "# Day 3: Risk Parity Portfolio Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the concept of risk contribution and marginal risk\n",
    "- Implement Equal Risk Contribution (ERC) portfolios\n",
    "- Build risk parity optimization from scratch\n",
    "- Compare risk parity to mean-variance optimization\n",
    "- Apply risk parity to multi-asset portfolios\n",
    "\n",
    "## Topics Covered\n",
    "1. Risk Contribution Fundamentals\n",
    "2. Equal Risk Contribution (ERC)\n",
    "3. Risk Parity Optimization Methods\n",
    "4. Practical Implementation\n",
    "5. Extensions and Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd3ce9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Risk Contribution Fundamentals\n",
    "\n",
    "### Understanding Portfolio Risk Decomposition\n",
    "\n",
    "Portfolio volatility can be decomposed into contributions from each asset:\n",
    "\n",
    "**Portfolio Variance:**\n",
    "$$\\sigma_p^2 = w^T \\Sigma w$$\n",
    "\n",
    "**Marginal Risk Contribution (MRC):**\n",
    "$$MRC_i = \\frac{\\partial \\sigma_p}{\\partial w_i} = \\frac{(\\Sigma w)_i}{\\sigma_p}$$\n",
    "\n",
    "**Total Risk Contribution (TRC):**\n",
    "$$TRC_i = w_i \\times MRC_i = \\frac{w_i (\\Sigma w)_i}{\\sigma_p}$$\n",
    "\n",
    "**Key Property - Euler Decomposition:**\n",
    "$$\\sum_{i=1}^{n} TRC_i = \\sigma_p$$\n",
    "\n",
    "**Percentage Risk Contribution:**\n",
    "$$RC_i = \\frac{TRC_i}{\\sigma_p} = \\frac{w_i (\\Sigma w)_i}{w^T \\Sigma w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskContributionAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze risk contributions of portfolio assets.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize with return series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns : pd.DataFrame\n",
    "            Asset returns (T x N)\n",
    "        \"\"\"\n",
    "        self.returns = returns\n",
    "        self.cov_matrix = returns.cov() * 252  # Annualized\n",
    "        self.asset_names = returns.columns.tolist()\n",
    "        self.n_assets = len(self.asset_names)\n",
    "    \n",
    "    def portfolio_volatility(self, weights: np.ndarray) -> float:\n",
    "        \"\"\"Calculate portfolio volatility.\"\"\"\n",
    "        return np.sqrt(weights @ self.cov_matrix.values @ weights)\n",
    "    \n",
    "    def marginal_risk_contribution(self, weights: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate Marginal Risk Contribution (MRC) for each asset.\n",
    "        MRC_i = d(sigma_p) / d(w_i)\n",
    "        \"\"\"\n",
    "        sigma_p = self.portfolio_volatility(weights)\n",
    "        mrc = (self.cov_matrix.values @ weights) / sigma_p\n",
    "        return mrc\n",
    "    \n",
    "    def total_risk_contribution(self, weights: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate Total Risk Contribution (TRC) for each asset.\n",
    "        TRC_i = w_i * MRC_i\n",
    "        \"\"\"\n",
    "        mrc = self.marginal_risk_contribution(weights)\n",
    "        trc = weights * mrc\n",
    "        return trc\n",
    "    \n",
    "    def percentage_risk_contribution(self, weights: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate Percentage Risk Contribution (RC%) for each asset.\n",
    "        RC_i = TRC_i / sigma_p\n",
    "        \"\"\"\n",
    "        trc = self.total_risk_contribution(weights)\n",
    "        sigma_p = self.portfolio_volatility(weights)\n",
    "        return trc / sigma_p\n",
    "    \n",
    "    def risk_decomposition_report(self, weights: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate comprehensive risk decomposition report.\n",
    "        \"\"\"\n",
    "        sigma_p = self.portfolio_volatility(weights)\n",
    "        mrc = self.marginal_risk_contribution(weights)\n",
    "        trc = self.total_risk_contribution(weights)\n",
    "        prc = self.percentage_risk_contribution(weights)\n",
    "        \n",
    "        report = pd.DataFrame({\n",
    "            'Asset': self.asset_names,\n",
    "            'Weight (%)': weights * 100,\n",
    "            'Volatility (%)': np.sqrt(np.diag(self.cov_matrix.values)) * 100,\n",
    "            'MRC': mrc,\n",
    "            'TRC': trc,\n",
    "            'Risk Contribution (%)': prc * 100\n",
    "        }).set_index('Asset')\n",
    "        \n",
    "        # Add totals\n",
    "        report.loc['TOTAL'] = [\n",
    "            report['Weight (%)'].sum(),\n",
    "            np.nan,\n",
    "            np.nan,\n",
    "            report['TRC'].sum(),\n",
    "            report['Risk Contribution (%)'].sum()\n",
    "        ]\n",
    "        \n",
    "        print(f\"Portfolio Volatility: {sigma_p*100:.2f}%\")\n",
    "        print(f\"Sum of TRC (should equal sigma_p): {trc.sum()*100:.2f}%\")\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data\n",
    "tickers = ['SPY', 'TLT', 'GLD', 'VNQ', 'EFA']  # Stocks, Bonds, Gold, REITs, International\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(\"Downloading price data...\")\n",
    "data = yf.download(tickers, start=start_date, end=end_date, progress=False)['Adj Close']\n",
    "returns = data.pct_change().dropna()\n",
    "\n",
    "print(f\"Data period: {returns.index[0].strftime('%Y-%m-%d')} to {returns.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"Number of observations: {len(returns)}\")\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84317435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = RiskContributionAnalyzer(returns)\n",
    "\n",
    "# Compare risk contributions for different weight schemes\n",
    "n_assets = len(tickers)\n",
    "\n",
    "# Equal Weight Portfolio\n",
    "equal_weights = np.ones(n_assets) / n_assets\n",
    "print(\"=\"*60)\n",
    "print(\"EQUAL WEIGHT PORTFOLIO\")\n",
    "print(\"=\"*60)\n",
    "ew_report = analyzer.risk_decomposition_report(equal_weights)\n",
    "print(ew_report.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc359e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60/40 Portfolio (assuming SPY is first, TLT is second)\n",
    "portfolio_60_40 = np.array([0.60, 0.40, 0.0, 0.0, 0.0])\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"60/40 PORTFOLIO (SPY/TLT)\")\n",
    "print(\"=\"*60)\n",
    "p60_40_report = analyzer.risk_decomposition_report(portfolio_60_40)\n",
    "print(p60_40_report.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk contributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Equal Weight\n",
    "ax1 = axes[0]\n",
    "ew_rc = analyzer.percentage_risk_contribution(equal_weights)\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, n_assets))\n",
    "ax1.bar(tickers, equal_weights * 100, alpha=0.7, label='Weight', color=colors)\n",
    "ax1.bar(tickers, ew_rc * 100, alpha=0.5, label='Risk Contribution', \n",
    "        edgecolor='black', linewidth=2, fill=False)\n",
    "ax1.set_ylabel('Percentage (%)')\n",
    "ax1.set_title('Equal Weight: Weights vs Risk Contributions')\n",
    "ax1.legend()\n",
    "ax1.axhline(y=20, color='red', linestyle='--', alpha=0.5, label='Equal (20%)')\n",
    "\n",
    "# 60/40\n",
    "ax2 = axes[1]\n",
    "p60_40_rc = analyzer.percentage_risk_contribution(portfolio_60_40)\n",
    "width = 0.35\n",
    "x = np.arange(n_assets)\n",
    "ax2.bar(x - width/2, portfolio_60_40 * 100, width, alpha=0.7, label='Weight', color=colors)\n",
    "ax2.bar(x + width/2, p60_40_rc * 100, width, alpha=0.7, label='Risk Contribution', color='red')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(tickers)\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.set_title('60/40 Portfolio: Weights vs Risk Contributions')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Equal weights â‰  Equal risk contributions!\")\n",
    "print(f\"SPY contributes {ew_rc[0]*100:.1f}% of risk with only {equal_weights[0]*100:.1f}% weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4048580",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Equal Risk Contribution (ERC) Portfolio\n",
    "\n",
    "### The Risk Parity Concept\n",
    "\n",
    "**Objective:** Find weights such that each asset contributes equally to portfolio risk.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "For an ERC portfolio with $n$ assets:\n",
    "$$RC_i = \\frac{1}{n} \\quad \\forall i$$\n",
    "\n",
    "This means:\n",
    "$$w_i (\\Sigma w)_i = w_j (\\Sigma w)_j \\quad \\forall i,j$$\n",
    "\n",
    "### Optimization Problem\n",
    "\n",
    "**Minimize the sum of squared differences in risk contributions:**\n",
    "$$\\min_w \\sum_{i=1}^{n} \\sum_{j=1}^{n} (w_i (\\Sigma w)_i - w_j (\\Sigma w)_j)^2$$\n",
    "\n",
    "Subject to:\n",
    "- $\\sum_{i=1}^{n} w_i = 1$ (fully invested)\n",
    "- $w_i \\geq 0$ (long only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskParityOptimizer:\n",
    "    \"\"\"\n",
    "    Risk Parity / Equal Risk Contribution Portfolio Optimizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cov_matrix: np.ndarray, asset_names: list = None):\n",
    "        \"\"\"\n",
    "        Initialize optimizer with covariance matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        cov_matrix : np.ndarray\n",
    "            Covariance matrix (N x N)\n",
    "        asset_names : list\n",
    "            Names of assets\n",
    "        \"\"\"\n",
    "        self.cov_matrix = np.array(cov_matrix)\n",
    "        self.n_assets = len(cov_matrix)\n",
    "        self.asset_names = asset_names or [f'Asset_{i}' for i in range(self.n_assets)]\n",
    "    \n",
    "    def portfolio_volatility(self, weights: np.ndarray) -> float:\n",
    "        \"\"\"Calculate portfolio volatility.\"\"\"\n",
    "        return np.sqrt(weights @ self.cov_matrix @ weights)\n",
    "    \n",
    "    def risk_contribution(self, weights: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate risk contribution for each asset.\n",
    "        \"\"\"\n",
    "        sigma_p = self.portfolio_volatility(weights)\n",
    "        # Marginal contribution to risk\n",
    "        mrc = (self.cov_matrix @ weights) / sigma_p\n",
    "        # Risk contribution\n",
    "        rc = weights * mrc\n",
    "        return rc / sigma_p  # Percentage contribution\n",
    "    \n",
    "    def _erc_objective(self, weights: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for ERC optimization.\n",
    "        Minimize sum of squared differences in risk contributions.\n",
    "        \"\"\"\n",
    "        rc = self.risk_contribution(weights)\n",
    "        target_rc = 1.0 / self.n_assets\n",
    "        return np.sum((rc - target_rc) ** 2)\n",
    "    \n",
    "    def _erc_objective_v2(self, weights: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Alternative objective: Minimize pairwise differences.\n",
    "        \"\"\"\n",
    "        sigma_sq = weights @ self.cov_matrix @ weights\n",
    "        risk_contrib = weights * (self.cov_matrix @ weights)\n",
    "        \n",
    "        obj = 0\n",
    "        for i in range(self.n_assets):\n",
    "            for j in range(i+1, self.n_assets):\n",
    "                obj += (risk_contrib[i] - risk_contrib[j]) ** 2\n",
    "        return obj\n",
    "    \n",
    "    def optimize_erc(self, method: str = 'SLSQP') -> dict:\n",
    "        \"\"\"\n",
    "        Optimize for Equal Risk Contribution portfolio.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Optimization method ('SLSQP', 'trust-constr')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Optimization results\n",
    "        \"\"\"\n",
    "        # Initial guess (inverse volatility weighted)\n",
    "        vols = np.sqrt(np.diag(self.cov_matrix))\n",
    "        x0 = (1/vols) / np.sum(1/vols)\n",
    "        \n",
    "        # Constraints\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Fully invested\n",
    "        ]\n",
    "        \n",
    "        # Bounds (long only)\n",
    "        bounds = [(0.001, 1.0) for _ in range(self.n_assets)]\n",
    "        \n",
    "        # Optimize\n",
    "        result = minimize(\n",
    "            self._erc_objective,\n",
    "            x0,\n",
    "            method=method,\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            options={'maxiter': 1000, 'ftol': 1e-12}\n",
    "        )\n",
    "        \n",
    "        optimal_weights = result.x / result.x.sum()  # Normalize\n",
    "        \n",
    "        return {\n",
    "            'weights': optimal_weights,\n",
    "            'volatility': self.portfolio_volatility(optimal_weights),\n",
    "            'risk_contributions': self.risk_contribution(optimal_weights),\n",
    "            'success': result.success,\n",
    "            'message': result.message\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f539501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for Risk Parity\n",
    "cov_matrix = returns.cov().values * 252  # Annualized\n",
    "rp_optimizer = RiskParityOptimizer(cov_matrix, tickers)\n",
    "\n",
    "# Find ERC weights\n",
    "erc_result = rp_optimizer.optimize_erc()\n",
    "\n",
    "print(\"EQUAL RISK CONTRIBUTION (ERC) PORTFOLIO\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOptimization Success: {erc_result['success']}\")\n",
    "print(f\"Portfolio Volatility: {erc_result['volatility']*100:.2f}%\\n\")\n",
    "\n",
    "erc_df = pd.DataFrame({\n",
    "    'Asset': tickers,\n",
    "    'Weight (%)': erc_result['weights'] * 100,\n",
    "    'Risk Contribution (%)': erc_result['risk_contributions'] * 100,\n",
    "    'Target RC (%)': np.ones(len(tickers)) * 100 / len(tickers)\n",
    "}).set_index('Asset')\n",
    "\n",
    "print(erc_df.round(2))\n",
    "print(f\"\\nSum of weights: {erc_result['weights'].sum()*100:.2f}%\")\n",
    "print(f\"Sum of risk contributions: {erc_result['risk_contributions'].sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ERC vs Equal Weight\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Weight comparison\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, equal_weights * 100, width, label='Equal Weight', alpha=0.8)\n",
    "ax1.bar(x + width/2, erc_result['weights'] * 100, width, label='Risk Parity', alpha=0.8)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(tickers)\n",
    "ax1.set_ylabel('Weight (%)')\n",
    "ax1.set_title('Portfolio Weights')\n",
    "ax1.legend()\n",
    "\n",
    "# Risk contribution comparison\n",
    "ax2 = axes[1]\n",
    "ew_rc = analyzer.percentage_risk_contribution(equal_weights)\n",
    "ax2.bar(x - width/2, ew_rc * 100, width, label='Equal Weight', alpha=0.8)\n",
    "ax2.bar(x + width/2, erc_result['risk_contributions'] * 100, width, label='Risk Parity', alpha=0.8)\n",
    "ax2.axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Target (20%)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(tickers)\n",
    "ax2.set_ylabel('Risk Contribution (%)')\n",
    "ax2.set_title('Risk Contributions')\n",
    "ax2.legend()\n",
    "\n",
    "# Pie charts\n",
    "ax3 = axes[2]\n",
    "ax3.pie(erc_result['risk_contributions'], labels=tickers, autopct='%1.1f%%',\n",
    "        colors=plt.cm.Set2(np.linspace(0, 1, len(tickers))))\n",
    "ax3.set_title('Risk Parity: Risk Contributions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260227b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Risk Parity Optimization Methods\n",
    "\n",
    "### Method 1: Newton-Raphson Approach (Spinu 2013)\n",
    "\n",
    "The risk budgeting problem can be reformulated as:\n",
    "$$\\min_y \\frac{1}{2} y^T \\Sigma y - \\sum_{i=1}^{n} b_i \\log(y_i)$$\n",
    "\n",
    "where $b_i$ is the target risk budget (for ERC, $b_i = 1/n$).\n",
    "\n",
    "### Method 2: Cyclical Coordinate Descent (CCD)\n",
    "\n",
    "Iteratively update each weight while keeping others fixed.\n",
    "\n",
    "### Method 3: Analytical Solution (Special Case)\n",
    "\n",
    "For diagonal covariance matrix (uncorrelated assets):\n",
    "$$w_i^{ERC} = \\frac{1/\\sigma_i}{\\sum_{j=1}^{n} 1/\\sigma_j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdac68d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRiskParity:\n",
    "    \"\"\"\n",
    "    Advanced Risk Parity methods with multiple optimization approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cov_matrix: np.ndarray):\n",
    "        self.cov_matrix = np.array(cov_matrix)\n",
    "        self.n_assets = len(cov_matrix)\n",
    "    \n",
    "    def inverse_volatility_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Simple inverse volatility weights.\n",
    "        Exact ERC solution for uncorrelated assets.\n",
    "        \"\"\"\n",
    "        vols = np.sqrt(np.diag(self.cov_matrix))\n",
    "        weights = (1 / vols) / np.sum(1 / vols)\n",
    "        return weights\n",
    "    \n",
    "    def newton_raphson_rb(self, budgets: np.ndarray = None, \n",
    "                          max_iter: int = 100, tol: float = 1e-10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Newton-Raphson method for risk budgeting (Spinu 2013).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        budgets : np.ndarray\n",
    "            Target risk budgets (default: equal)\n",
    "        max_iter : int\n",
    "            Maximum iterations\n",
    "        tol : float\n",
    "            Convergence tolerance\n",
    "        \"\"\"\n",
    "        if budgets is None:\n",
    "            budgets = np.ones(self.n_assets) / self.n_assets\n",
    "        \n",
    "        # Initialize with inverse vol weights\n",
    "        y = self.inverse_volatility_weights()\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # Gradient\n",
    "            Sigma_y = self.cov_matrix @ y\n",
    "            grad = Sigma_y - budgets / y\n",
    "            \n",
    "            # Hessian\n",
    "            hess = self.cov_matrix + np.diag(budgets / (y ** 2))\n",
    "            \n",
    "            # Newton step\n",
    "            delta = np.linalg.solve(hess, grad)\n",
    "            \n",
    "            # Line search with backtracking\n",
    "            alpha = 1.0\n",
    "            while np.any(y - alpha * delta <= 0):\n",
    "                alpha *= 0.5\n",
    "            \n",
    "            y_new = y - alpha * delta\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(y_new - y)) < tol:\n",
    "                break\n",
    "            \n",
    "            y = y_new\n",
    "        \n",
    "        # Normalize to sum to 1\n",
    "        weights = y / np.sum(y)\n",
    "        return weights\n",
    "    \n",
    "    def cyclical_coordinate_descent(self, budgets: np.ndarray = None,\n",
    "                                    max_iter: int = 500, tol: float = 1e-10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Cyclical Coordinate Descent for risk parity.\n",
    "        \n",
    "        Updates one weight at a time while keeping others fixed.\n",
    "        \"\"\"\n",
    "        if budgets is None:\n",
    "            budgets = np.ones(self.n_assets) / self.n_assets\n",
    "        \n",
    "        # Initialize\n",
    "        w = self.inverse_volatility_weights()\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            w_old = w.copy()\n",
    "            \n",
    "            for i in range(self.n_assets):\n",
    "                # Compute portfolio variance contribution terms\n",
    "                sigma_i_sq = self.cov_matrix[i, i]\n",
    "                cov_i_rest = np.delete(self.cov_matrix[i, :], i)\n",
    "                w_rest = np.delete(w, i)\n",
    "                \n",
    "                # Quadratic equation coefficients\n",
    "                a = sigma_i_sq\n",
    "                b = 2 * np.dot(cov_i_rest, w_rest)\n",
    "                c_term = -budgets[i] * np.sqrt(w @ self.cov_matrix @ w)\n",
    "                \n",
    "                # Solve quadratic\n",
    "                discriminant = b**2 - 4*a*c_term\n",
    "                if discriminant >= 0:\n",
    "                    w[i] = (-b + np.sqrt(discriminant)) / (2 * a)\n",
    "                    w[i] = max(w[i], 1e-10)  # Ensure positive\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(w - w_old)) < tol:\n",
    "                break\n",
    "        \n",
    "        # Normalize\n",
    "        return w / np.sum(w)\n",
    "    \n",
    "    def risk_budgeting(self, budgets: np.ndarray, method: str = 'SLSQP') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        General risk budgeting portfolio.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        budgets : np.ndarray\n",
    "            Target risk budgets (should sum to 1)\n",
    "        method : str\n",
    "            Optimization method\n",
    "        \"\"\"\n",
    "        def objective(w):\n",
    "            sigma_sq = w @ self.cov_matrix @ w\n",
    "            risk_contrib = w * (self.cov_matrix @ w) / np.sqrt(sigma_sq)\n",
    "            pct_contrib = risk_contrib / np.sqrt(sigma_sq)\n",
    "            return np.sum((pct_contrib - budgets) ** 2)\n",
    "        \n",
    "        # Initial guess\n",
    "        x0 = self.inverse_volatility_weights()\n",
    "        \n",
    "        # Constraints and bounds\n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0.001, 1.0) for _ in range(self.n_assets)]\n",
    "        \n",
    "        result = minimize(objective, x0, method=method,\n",
    "                         bounds=bounds, constraints=constraints)\n",
    "        \n",
    "        return result.x / result.x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different risk parity methods\n",
    "advanced_rp = AdvancedRiskParity(cov_matrix)\n",
    "\n",
    "# Method 1: Simple Inverse Volatility\n",
    "inv_vol_weights = advanced_rp.inverse_volatility_weights()\n",
    "\n",
    "# Method 2: Newton-Raphson\n",
    "nr_weights = advanced_rp.newton_raphson_rb()\n",
    "\n",
    "# Method 3: Cyclical Coordinate Descent\n",
    "ccd_weights = advanced_rp.cyclical_coordinate_descent()\n",
    "\n",
    "# Method 4: SLSQP (from before)\n",
    "slsqp_weights = erc_result['weights']\n",
    "\n",
    "# Compare results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Inverse Vol': inv_vol_weights * 100,\n",
    "    'Newton-Raphson': nr_weights * 100,\n",
    "    'CCD': ccd_weights * 100,\n",
    "    'SLSQP': slsqp_weights * 100\n",
    "}, index=tickers)\n",
    "\n",
    "print(\"WEIGHT COMPARISON ACROSS METHODS (%)\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.round(2))\n",
    "print(f\"\\nMax weight difference: {comparison_df.max().max() - comparison_df.min().min():.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify risk contributions for each method\n",
    "def get_risk_contributions(weights, cov):\n",
    "    sigma_p = np.sqrt(weights @ cov @ weights)\n",
    "    mrc = (cov @ weights) / sigma_p\n",
    "    return (weights * mrc) / sigma_p * 100\n",
    "\n",
    "rc_comparison = pd.DataFrame({\n",
    "    'Inverse Vol': get_risk_contributions(inv_vol_weights, cov_matrix),\n",
    "    'Newton-Raphson': get_risk_contributions(nr_weights, cov_matrix),\n",
    "    'CCD': get_risk_contributions(ccd_weights, cov_matrix),\n",
    "    'SLSQP': get_risk_contributions(slsqp_weights, cov_matrix)\n",
    "}, index=tickers)\n",
    "\n",
    "print(\"\\nRISK CONTRIBUTION COMPARISON (%)\")\n",
    "print(\"=\"*60)\n",
    "print(rc_comparison.round(4))\n",
    "print(f\"\\nTarget: {100/len(tickers):.2f}% each\")\n",
    "print(f\"Max deviation from target (SLSQP): {abs(rc_comparison['SLSQP'] - 20).max():.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f96a5",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Practical Implementation - Backtesting Risk Parity\n",
    "\n",
    "### Comparing Portfolio Strategies\n",
    "1. Equal Weight (1/N)\n",
    "2. Risk Parity (ERC)\n",
    "3. Inverse Volatility\n",
    "4. Minimum Variance\n",
    "5. 60/40 Stock/Bond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626efb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioBacktester:\n",
    "    \"\"\"\n",
    "    Backtest multiple portfolio strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame, lookback: int = 252):\n",
    "        \"\"\"\n",
    "        Initialize backtester.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns : pd.DataFrame\n",
    "            Daily returns\n",
    "        lookback : int\n",
    "            Lookback period for covariance estimation\n",
    "        \"\"\"\n",
    "        self.returns = returns\n",
    "        self.lookback = lookback\n",
    "        self.n_assets = returns.shape[1]\n",
    "    \n",
    "    def equal_weight(self, cov: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Equal weight portfolio.\"\"\"\n",
    "        return np.ones(self.n_assets) / self.n_assets\n",
    "    \n",
    "    def inverse_volatility(self, cov: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Inverse volatility weighted portfolio.\"\"\"\n",
    "        vols = np.sqrt(np.diag(cov))\n",
    "        return (1 / vols) / np.sum(1 / vols)\n",
    "    \n",
    "    def risk_parity(self, cov: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Risk parity (ERC) portfolio.\"\"\"\n",
    "        rp = AdvancedRiskParity(cov)\n",
    "        return rp.newton_raphson_rb()\n",
    "    \n",
    "    def minimum_variance(self, cov: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Minimum variance portfolio.\"\"\"\n",
    "        n = len(cov)\n",
    "        \n",
    "        def objective(w):\n",
    "            return w @ cov @ w\n",
    "        \n",
    "        constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n",
    "        bounds = [(0, 1) for _ in range(n)]\n",
    "        x0 = np.ones(n) / n\n",
    "        \n",
    "        result = minimize(objective, x0, method='SLSQP',\n",
    "                         bounds=bounds, constraints=constraints)\n",
    "        return result.x\n",
    "    \n",
    "    def static_60_40(self, cov: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"60/40 portfolio (first 2 assets).\"\"\"\n",
    "        weights = np.zeros(self.n_assets)\n",
    "        weights[0] = 0.60  # SPY\n",
    "        weights[1] = 0.40  # TLT\n",
    "        return weights\n",
    "    \n",
    "    def run_backtest(self, rebalance_freq: int = 21) -> dict:\n",
    "        \"\"\"\n",
    "        Run backtest for all strategies.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        rebalance_freq : int\n",
    "            Rebalancing frequency in days\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Portfolio values and weights over time\n",
    "        \"\"\"\n",
    "        strategies = {\n",
    "            'Equal Weight': self.equal_weight,\n",
    "            'Inverse Vol': self.inverse_volatility,\n",
    "            'Risk Parity': self.risk_parity,\n",
    "            'Min Variance': self.minimum_variance,\n",
    "            '60/40': self.static_60_40\n",
    "        }\n",
    "        \n",
    "        # Initialize\n",
    "        start_idx = self.lookback\n",
    "        dates = self.returns.index[start_idx:]\n",
    "        \n",
    "        portfolio_values = {name: [1.0] for name in strategies}\n",
    "        weights_history = {name: [] for name in strategies}\n",
    "        \n",
    "        current_weights = {name: None for name in strategies}\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            # Rebalance\n",
    "            if i % rebalance_freq == 0:\n",
    "                hist_returns = self.returns.iloc[start_idx+i-self.lookback:start_idx+i]\n",
    "                cov = hist_returns.cov().values * 252\n",
    "                \n",
    "                for name, strategy_func in strategies.items():\n",
    "                    try:\n",
    "                        current_weights[name] = strategy_func(cov)\n",
    "                    except:\n",
    "                        current_weights[name] = np.ones(self.n_assets) / self.n_assets\n",
    "            \n",
    "            # Calculate returns\n",
    "            daily_returns = self.returns.loc[date].values\n",
    "            \n",
    "            for name in strategies:\n",
    "                if current_weights[name] is not None:\n",
    "                    port_return = np.dot(current_weights[name], daily_returns)\n",
    "                    portfolio_values[name].append(\n",
    "                        portfolio_values[name][-1] * (1 + port_return)\n",
    "                    )\n",
    "                    weights_history[name].append(current_weights[name].copy())\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        portfolio_df = pd.DataFrame(portfolio_values, index=[dates[0] - pd.Timedelta(days=1)] + list(dates))\n",
    "        \n",
    "        return {\n",
    "            'portfolio_values': portfolio_df,\n",
    "            'weights_history': weights_history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest\n",
    "print(\"Running backtest...\")\n",
    "backtester = PortfolioBacktester(returns, lookback=252)\n",
    "backtest_results = backtester.run_backtest(rebalance_freq=21)  # Monthly rebalancing\n",
    "\n",
    "portfolio_values = backtest_results['portfolio_values']\n",
    "print(f\"Backtest period: {portfolio_values.index[0].strftime('%Y-%m-%d')} to {portfolio_values.index[-1].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "def calculate_metrics(values: pd.Series) -> dict:\n",
    "    \"\"\"Calculate portfolio performance metrics.\"\"\"\n",
    "    returns = values.pct_change().dropna()\n",
    "    \n",
    "    total_return = (values.iloc[-1] / values.iloc[0] - 1) * 100\n",
    "    years = (values.index[-1] - values.index[0]).days / 365.25\n",
    "    cagr = ((values.iloc[-1] / values.iloc[0]) ** (1/years) - 1) * 100\n",
    "    volatility = returns.std() * np.sqrt(252) * 100\n",
    "    sharpe = (returns.mean() * 252) / (returns.std() * np.sqrt(252))\n",
    "    \n",
    "    # Maximum Drawdown\n",
    "    cummax = values.cummax()\n",
    "    drawdown = (values - cummax) / cummax\n",
    "    max_dd = drawdown.min() * 100\n",
    "    \n",
    "    # Calmar Ratio\n",
    "    calmar = cagr / abs(max_dd)\n",
    "    \n",
    "    return {\n",
    "        'Total Return (%)': total_return,\n",
    "        'CAGR (%)': cagr,\n",
    "        'Volatility (%)': volatility,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown (%)': max_dd,\n",
    "        'Calmar Ratio': calmar\n",
    "    }\n",
    "\n",
    "# Calculate metrics for all strategies\n",
    "metrics_dict = {}\n",
    "for col in portfolio_values.columns:\n",
    "    metrics_dict[col] = calculate_metrics(portfolio_values[col])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_dict).T\n",
    "print(\"\\nPERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(metrics_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8478b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize backtest results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Portfolio values\n",
    "ax1 = axes[0, 0]\n",
    "for col in portfolio_values.columns:\n",
    "    ax1.plot(portfolio_values.index, portfolio_values[col], label=col, linewidth=2)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Portfolio Value')\n",
    "ax1.set_title('Portfolio Growth Comparison')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdowns\n",
    "ax2 = axes[0, 1]\n",
    "for col in portfolio_values.columns:\n",
    "    cummax = portfolio_values[col].cummax()\n",
    "    drawdown = (portfolio_values[col] - cummax) / cummax * 100\n",
    "    ax2.fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, label=col)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Drawdown (%)')\n",
    "ax2.set_title('Drawdown Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Risk-Return scatter\n",
    "ax3 = axes[1, 0]\n",
    "for col in portfolio_values.columns:\n",
    "    ax3.scatter(metrics_df.loc[col, 'Volatility (%)'], \n",
    "                metrics_df.loc[col, 'CAGR (%)'],\n",
    "                s=200, label=col)\n",
    "    ax3.annotate(col, (metrics_df.loc[col, 'Volatility (%)'], \n",
    "                       metrics_df.loc[col, 'CAGR (%)']),\n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "ax3.set_xlabel('Volatility (%)')\n",
    "ax3.set_ylabel('CAGR (%)')\n",
    "ax3.set_title('Risk-Return Profile')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "ax4 = axes[1, 1]\n",
    "metrics_to_plot = ['Sharpe Ratio', 'Calmar Ratio']\n",
    "x = np.arange(len(portfolio_values.columns))\n",
    "width = 0.35\n",
    "ax4.bar(x - width/2, metrics_df['Sharpe Ratio'], width, label='Sharpe Ratio', alpha=0.8)\n",
    "ax4.bar(x + width/2, metrics_df['Calmar Ratio'], width, label='Calmar Ratio', alpha=0.8)\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(portfolio_values.columns, rotation=45)\n",
    "ax4.set_ylabel('Ratio')\n",
    "ax4.set_title('Risk-Adjusted Performance')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e739327",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Extensions and Variations\n",
    "\n",
    "### 5.1 Risk Budgeting (Custom Risk Allocations)\n",
    "\n",
    "Instead of equal risk, allocate different risk budgets to each asset:\n",
    "$$RC_i = b_i \\quad \\text{where} \\quad \\sum_i b_i = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom risk budgeting example\n",
    "# Allocate 40% risk to equities, 30% to bonds, 10% to gold, 10% to REITs, 10% to international\n",
    "custom_budgets = np.array([0.40, 0.30, 0.10, 0.10, 0.10])\n",
    "\n",
    "print(\"CUSTOM RISK BUDGETING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Target Risk Budgets: {dict(zip(tickers, custom_budgets))}\")\n",
    "\n",
    "rb_weights = advanced_rp.risk_budgeting(custom_budgets)\n",
    "rb_rc = get_risk_contributions(rb_weights, cov_matrix)\n",
    "\n",
    "rb_df = pd.DataFrame({\n",
    "    'Target RC (%)': custom_budgets * 100,\n",
    "    'Actual RC (%)': rb_rc,\n",
    "    'Weight (%)': rb_weights * 100\n",
    "}, index=tickers)\n",
    "\n",
    "print(\"\\n\", rb_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df26630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize custom risk budgeting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Target vs Actual Risk Contributions\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(tickers))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, custom_budgets * 100, width, label='Target', alpha=0.8)\n",
    "ax1.bar(x + width/2, rb_rc, width, label='Actual', alpha=0.8)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(tickers)\n",
    "ax1.set_ylabel('Risk Contribution (%)')\n",
    "ax1.set_title('Target vs Actual Risk Contributions')\n",
    "ax1.legend()\n",
    "\n",
    "# Weights\n",
    "ax2 = axes[1]\n",
    "ax2.bar(tickers, rb_weights * 100, color=plt.cm.Set2(np.linspace(0, 1, len(tickers))))\n",
    "ax2.set_ylabel('Weight (%)')\n",
    "ax2.set_title('Risk Budgeting Portfolio Weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70985b",
   "metadata": {},
   "source": [
    "### 5.2 Leveraged Risk Parity\n",
    "\n",
    "Risk parity portfolios often have lower volatility than traditional portfolios.\n",
    "To achieve target volatility, leverage can be applied:\n",
    "\n",
    "$$w^{leveraged} = \\frac{\\sigma_{target}}{\\sigma_{RP}} \\times w^{RP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leverage_portfolio(weights: np.ndarray, cov: np.ndarray, \n",
    "                       target_vol: float) -> tuple:\n",
    "    \"\"\"\n",
    "    Apply leverage to achieve target volatility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weights : np.ndarray\n",
    "        Portfolio weights\n",
    "    cov : np.ndarray\n",
    "        Covariance matrix\n",
    "    target_vol : float\n",
    "        Target annualized volatility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (leveraged_weights, leverage_ratio)\n",
    "    \"\"\"\n",
    "    current_vol = np.sqrt(weights @ cov @ weights)\n",
    "    leverage = target_vol / current_vol\n",
    "    leveraged_weights = weights * leverage\n",
    "    \n",
    "    return leveraged_weights, leverage\n",
    "\n",
    "# Calculate leverage needed for different target volatilities\n",
    "rp_vol = np.sqrt(erc_result['weights'] @ cov_matrix @ erc_result['weights'])\n",
    "ew_vol = np.sqrt(equal_weights @ cov_matrix @ equal_weights)\n",
    "\n",
    "print(\"LEVERAGED RISK PARITY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Unleveraged Risk Parity Volatility: {rp_vol*100:.2f}%\")\n",
    "print(f\"Equal Weight Volatility: {ew_vol*100:.2f}%\")\n",
    "\n",
    "# Target equal weight volatility\n",
    "lev_weights, leverage = leverage_portfolio(erc_result['weights'], cov_matrix, ew_vol)\n",
    "\n",
    "print(f\"\\nLeverage needed to match EW volatility: {leverage:.2f}x\")\n",
    "print(f\"\\nLeveraged Risk Parity Weights:\")\n",
    "print(pd.DataFrame({\n",
    "    'Asset': tickers,\n",
    "    'Unleveraged (%)': erc_result['weights'] * 100,\n",
    "    'Leveraged (%)': lev_weights * 100\n",
    "}).set_index('Asset').round(2))\n",
    "\n",
    "print(f\"\\nTotal portfolio exposure: {lev_weights.sum()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fe16e",
   "metadata": {},
   "source": [
    "### 5.3 Hierarchical Risk Parity (HRP)\n",
    "\n",
    "An alternative approach that doesn't require matrix inversion and is more robust to estimation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, leaves_list\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "class HierarchicalRiskParity:\n",
    "    \"\"\"\n",
    "    Hierarchical Risk Parity (Lopez de Prado, 2016)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, returns: pd.DataFrame):\n",
    "        self.returns = returns\n",
    "        self.cov = returns.cov().values\n",
    "        self.corr = returns.corr().values\n",
    "        self.n_assets = len(returns.columns)\n",
    "        self.asset_names = returns.columns.tolist()\n",
    "    \n",
    "    def _get_quasi_diag(self, link: np.ndarray) -> list:\n",
    "        \"\"\"Sort clustered items by distance.\"\"\"\n",
    "        link = link.astype(int)\n",
    "        sorted_items = leaves_list(link)\n",
    "        return sorted_items.tolist()\n",
    "    \n",
    "    def _get_cluster_var(self, cov: np.ndarray, cluster_items: list) -> float:\n",
    "        \"\"\"Calculate variance of cluster.\"\"\"\n",
    "        cov_slice = cov[np.ix_(cluster_items, cluster_items)]\n",
    "        w = self._get_ivp(cov_slice)\n",
    "        return np.dot(w, np.dot(cov_slice, w))\n",
    "    \n",
    "    def _get_ivp(self, cov: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Get inverse variance portfolio weights.\"\"\"\n",
    "        ivp = 1 / np.diag(cov)\n",
    "        return ivp / ivp.sum()\n",
    "    \n",
    "    def _recursive_bisection(self, cov: np.ndarray, sorted_items: list) -> np.ndarray:\n",
    "        \"\"\"Compute HRP weights through recursive bisection.\"\"\"\n",
    "        w = np.ones(len(sorted_items))\n",
    "        cluster_items = [sorted_items]\n",
    "        \n",
    "        while len(cluster_items) > 0:\n",
    "            # Bisect\n",
    "            cluster_items = [\n",
    "                item[j:k] for item in cluster_items\n",
    "                for j, k in ((0, len(item)//2), (len(item)//2, len(item)))\n",
    "                if len(item) > 1\n",
    "            ]\n",
    "            \n",
    "            # Allocate weights\n",
    "            for i in range(0, len(cluster_items), 2):\n",
    "                if i + 1 >= len(cluster_items):\n",
    "                    break\n",
    "                    \n",
    "                cluster_0 = cluster_items[i]\n",
    "                cluster_1 = cluster_items[i + 1]\n",
    "                \n",
    "                var_0 = self._get_cluster_var(cov, cluster_0)\n",
    "                var_1 = self._get_cluster_var(cov, cluster_1)\n",
    "                \n",
    "                alpha = 1 - var_0 / (var_0 + var_1)\n",
    "                \n",
    "                w[cluster_0] *= alpha\n",
    "                w[cluster_1] *= 1 - alpha\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    def optimize(self) -> dict:\n",
    "        \"\"\"\n",
    "        Compute HRP weights.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : weights and clustering info\n",
    "        \"\"\"\n",
    "        # Step 1: Compute distance matrix\n",
    "        dist = np.sqrt((1 - self.corr) / 2)\n",
    "        \n",
    "        # Step 2: Hierarchical clustering\n",
    "        dist_condensed = squareform(dist)\n",
    "        link = linkage(dist_condensed, method='single')\n",
    "        \n",
    "        # Step 3: Quasi-diagonalization\n",
    "        sorted_items = self._get_quasi_diag(link)\n",
    "        \n",
    "        # Step 4: Recursive bisection\n",
    "        weights = self._recursive_bisection(self.cov, sorted_items)\n",
    "        \n",
    "        # Reorder weights to original order\n",
    "        final_weights = np.zeros(self.n_assets)\n",
    "        for i, item in enumerate(sorted_items):\n",
    "            final_weights[item] = weights[i]\n",
    "        \n",
    "        return {\n",
    "            'weights': final_weights,\n",
    "            'linkage': link,\n",
    "            'sorted_items': sorted_items\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate HRP weights\n",
    "hrp = HierarchicalRiskParity(returns)\n",
    "hrp_result = hrp.optimize()\n",
    "\n",
    "print(\"HIERARCHICAL RISK PARITY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare all methods\n",
    "all_weights = pd.DataFrame({\n",
    "    'Equal Weight': equal_weights * 100,\n",
    "    'Risk Parity': erc_result['weights'] * 100,\n",
    "    'HRP': hrp_result['weights'] * 100,\n",
    "    'Min Variance': backtester.minimum_variance(cov_matrix) * 100\n",
    "}, index=tickers)\n",
    "\n",
    "print(all_weights.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c163322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HRP clustering\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dendrogram\n",
    "ax1 = axes[0]\n",
    "dendrogram(hrp_result['linkage'], labels=tickers, ax=ax1)\n",
    "ax1.set_title('Hierarchical Clustering Dendrogram')\n",
    "ax1.set_ylabel('Distance')\n",
    "\n",
    "# Weight comparison\n",
    "ax2 = axes[1]\n",
    "all_weights.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_ylabel('Weight (%)')\n",
    "ax2.set_title('Portfolio Weight Comparison')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.set_xticklabels(tickers, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1b676",
   "metadata": {},
   "source": [
    "---\n",
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Sector Risk Parity\n",
    "Create a risk parity portfolio across different sectors (Technology, Healthcare, Financials, Energy, Consumer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "sector_tickers = ['XLK', 'XLV', 'XLF', 'XLE', 'XLY']  # Sector ETFs\n",
    "\n",
    "# Download data and implement risk parity\n",
    "# ...\n",
    "\n",
    "print(\"TODO: Implement sector risk parity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f0718",
   "metadata": {},
   "source": [
    "### Exercise 2: Dynamic Risk Parity with Regime Detection\n",
    "Implement a risk parity strategy that adjusts risk budgets based on market regime (bull/bear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "def detect_regime(returns: pd.Series, lookback: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    Detect market regime based on recent performance.\n",
    "    \n",
    "    Returns 'bull' or 'bear'\n",
    "    \"\"\"\n",
    "    # TODO: Implement regime detection\n",
    "    pass\n",
    "\n",
    "print(\"TODO: Implement regime-aware risk parity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdfb820",
   "metadata": {},
   "source": [
    "### Exercise 3: Transaction Cost Analysis\n",
    "Analyze the impact of transaction costs on risk parity rebalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ab026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "def calculate_turnover(weights_history: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average portfolio turnover.\n",
    "    \"\"\"\n",
    "    # TODO: Implement turnover calculation\n",
    "    pass\n",
    "\n",
    "print(\"TODO: Analyze transaction costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8c4f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. **What is the key insight behind risk parity?**\n",
    "   - Equal capital allocation â‰  equal risk allocation\n",
    "   - Low-risk assets need more weight to contribute equally to risk\n",
    "\n",
    "2. **Why does risk parity often overweight bonds?**\n",
    "   - Bonds typically have lower volatility\n",
    "   - Need larger allocation to achieve equal risk contribution\n",
    "\n",
    "3. **What are the criticisms of risk parity?**\n",
    "   - Ignores expected returns (focuses only on risk)\n",
    "   - May require leverage to achieve competitive returns\n",
    "   - Sensitive to correlation assumptions\n",
    "   - Can have concentrated positions in low-vol assets\n",
    "\n",
    "### Technical\n",
    "4. **Derive the risk contribution formula.**\n",
    "   - Start from portfolio variance, take partial derivative, apply Euler's theorem\n",
    "\n",
    "5. **How do you handle negative weights in risk parity?**\n",
    "   - Add long-only constraints\n",
    "   - Use bounded optimization\n",
    "\n",
    "6. **Compare Risk Parity vs. Minimum Variance.**\n",
    "   - MinVar minimizes total risk; RP equalizes risk contributions\n",
    "   - MinVar can concentrate in low-vol assets; RP diversifies risk\n",
    "\n",
    "### Practical\n",
    "7. **How would you implement risk parity for 500 assets?**\n",
    "   - Use efficient optimization methods (CCD, Newton)\n",
    "   - Consider HRP for large-scale portfolios\n",
    "   - Factor-based risk parity as alternative\n",
    "\n",
    "8. **How do you handle estimation error in the covariance matrix?**\n",
    "   - Shrinkage estimators\n",
    "   - Factor models\n",
    "   - Robust optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce8ebc",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "1. **Risk Contribution**: How much each asset contributes to portfolio risk\n",
    "2. **Equal Risk Contribution (ERC)**: Each asset contributes equally to risk\n",
    "3. **Risk Budgeting**: Custom risk allocations to each asset\n",
    "4. **Hierarchical Risk Parity**: Cluster-based approach, more robust\n",
    "\n",
    "### Implementation Methods\n",
    "- SLSQP optimization\n",
    "- Newton-Raphson\n",
    "- Cyclical Coordinate Descent\n",
    "- Inverse Volatility (approximation)\n",
    "\n",
    "### Key Formulas\n",
    "- Marginal Risk: $MRC_i = \\frac{(\\Sigma w)_i}{\\sigma_p}$\n",
    "- Risk Contribution: $RC_i = \\frac{w_i (\\Sigma w)_i}{w^T \\Sigma w}$\n",
    "- ERC Condition: $RC_i = 1/n \\; \\forall i$\n",
    "\n",
    "### Next Steps\n",
    "- Day 4: Black-Litterman Model\n",
    "- Explore factor-based risk parity\n",
    "- Study tail-risk parity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
