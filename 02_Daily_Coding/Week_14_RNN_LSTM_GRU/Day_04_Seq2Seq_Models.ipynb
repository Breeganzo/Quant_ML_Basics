{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524b1d17",
   "metadata": {},
   "source": [
    "# Week 14 - Day 4: Sequence-to-Sequence Models\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the **encoder-decoder architecture** for sequence modeling\n",
    "- Implement **multi-step forecasting** approaches\n",
    "- Master the **teacher forcing** technique for training\n",
    "- Build a **multi-horizon return prediction** system\n",
    "\n",
    "---\n",
    "\n",
    "## Why Seq2Seq for Finance?\n",
    "\n",
    "Traditional models predict one step ahead. In trading, we often need:\n",
    "- **Multi-day forecasts** for portfolio rebalancing\n",
    "- **Term structure predictions** (volatility curves)\n",
    "- **Scenario generation** for risk management\n",
    "\n",
    "Seq2Seq models excel at mapping **input sequences → output sequences** of arbitrary lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda89dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5bfc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Preparation\n",
    "\n",
    "We'll download stock data and prepare it for multi-horizon prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "ticker = 'SPY'\n",
    "data = yf.download(ticker, start='2015-01-01', end='2024-01-01', progress=False)\n",
    "\n",
    "# Extract Close prices and compute returns\n",
    "close = data['Close'].values.flatten()\n",
    "returns = np.diff(np.log(close))  # Log returns\n",
    "\n",
    "print(f\"Total trading days: {len(close)}\")\n",
    "print(f\"Returns shape: {returns.shape}\")\n",
    "print(f\"Returns statistics:\")\n",
    "print(f\"  Mean: {returns.mean():.6f}\")\n",
    "print(f\"  Std:  {returns.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97383248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize returns\n",
    "scaler = StandardScaler()\n",
    "returns_scaled = scaler.fit_transform(returns.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "axes[0].plot(close, color='blue', linewidth=0.8)\n",
    "axes[0].set_title(f'{ticker} Close Prices', fontsize=12)\n",
    "axes[0].set_xlabel('Days')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(returns_scaled, color='green', linewidth=0.5, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_title('Normalized Log Returns', fontsize=12)\n",
    "axes[1].set_xlabel('Days')\n",
    "axes[1].set_ylabel('Scaled Return')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819edc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Sequence Dataset Creation\n",
    "\n",
    "For Seq2Seq, we need:\n",
    "- **Encoder input**: Past `seq_len` observations\n",
    "- **Decoder target**: Future `forecast_horizon` observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq2seq_dataset(data, seq_len, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences for Seq2Seq model.\n",
    "    \n",
    "    Args:\n",
    "        data: 1D array of values\n",
    "        seq_len: Length of encoder input sequence\n",
    "        forecast_horizon: Length of decoder output sequence\n",
    "    \n",
    "    Returns:\n",
    "        X: Encoder inputs (batch, seq_len, 1)\n",
    "        Y: Decoder targets (batch, forecast_horizon, 1)\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for i in range(len(data) - seq_len - forecast_horizon + 1):\n",
    "        X.append(data[i:i + seq_len])\n",
    "        Y.append(data[i + seq_len:i + seq_len + forecast_horizon])\n",
    "    \n",
    "    X = np.array(X).reshape(-1, seq_len, 1)\n",
    "    Y = np.array(Y).reshape(-1, forecast_horizon, 1)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Parameters\n",
    "SEQ_LEN = 30           # Look back 30 days\n",
    "FORECAST_HORIZON = 5   # Predict 5 days ahead\n",
    "\n",
    "# Create dataset\n",
    "X, Y = create_seq2seq_dataset(returns_scaled, SEQ_LEN, FORECAST_HORIZON)\n",
    "\n",
    "print(f\"Encoder input shape (X): {X.shape}\")\n",
    "print(f\"Decoder target shape (Y): {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test split (70/15/15)\n",
    "n = len(X)\n",
    "train_size = int(0.7 * n)\n",
    "val_size = int(0.15 * n)\n",
    "\n",
    "X_train, Y_train = X[:train_size], Y[:train_size]\n",
    "X_val, Y_val = X[train_size:train_size + val_size], Y[train_size:train_size + val_size]\n",
    "X_test, Y_test = X[train_size + val_size:], Y[train_size + val_size:]\n",
    "\n",
    "print(f\"Training samples:   {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples:       {len(X_test)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "Y_train_t = torch.FloatTensor(Y_train).to(device)\n",
    "X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "Y_val_t = torch.FloatTensor(Y_val).to(device)\n",
    "X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "Y_test_t = torch.FloatTensor(Y_test).to(device)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, Y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_t, Y_val_t)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42591349",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Encoder-Decoder Architecture\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    ENCODER-DECODER ARCHITECTURE                  │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│   ENCODER                          DECODER                      │\n",
    "│   ┌───────────┐                    ┌───────────┐                │\n",
    "│   │   LSTM    │──── context ───▶   │   LSTM    │                │\n",
    "│   │  Layers   │    (h_n, c_n)      │  Layers   │                │\n",
    "│   └───────────┘                    └───────────┘                │\n",
    "│        ▲                                │                       │\n",
    "│        │                                ▼                       │\n",
    "│   [x₁, x₂, ..., xₜ]              [ŷ₁, ŷ₂, ..., ŷₕ]            │\n",
    "│   (Input Sequence)               (Predicted Sequence)          │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "1. **Encoder**: Processes input sequence → produces context vector (hidden state)\n",
    "2. **Context Vector**: Compressed representation of input sequence\n",
    "3. **Decoder**: Uses context to generate output sequence step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c92810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"LSTM Encoder for Seq2Seq model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, input_size)\n",
    "        Returns:\n",
    "            hidden: tuple of (h_n, c_n), each (num_layers, batch, hidden_size)\n",
    "        \"\"\"\n",
    "        # outputs: (batch, seq_len, hidden_size)\n",
    "        # h_n, c_n: (num_layers, batch, hidden_size)\n",
    "        outputs, (h_n, c_n) = self.lstm(x)\n",
    "        return (h_n, c_n)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"LSTM Decoder for Seq2Seq model.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, 1, input_size) - single timestep input\n",
    "            hidden: tuple of (h, c)\n",
    "        Returns:\n",
    "            output: (batch, 1, output_size)\n",
    "            hidden: updated hidden state\n",
    "        \"\"\"\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "print(\"Encoder-Decoder classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d702f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Teacher Forcing Technique\n",
    "\n",
    "### What is Teacher Forcing?\n",
    "\n",
    "During training, the decoder can use:\n",
    "- **Own predictions** (autoregressive) - errors compound\n",
    "- **Ground truth** (teacher forcing) - faster convergence, but exposure bias\n",
    "\n",
    "```\n",
    "WITHOUT Teacher Forcing:          WITH Teacher Forcing:\n",
    "                                  \n",
    "  Decoder input = ŷₜ₋₁            Decoder input = yₜ₋₁ (true)\n",
    "       │                               │\n",
    "       ▼                               ▼\n",
    "  [DECODER] ──▶ ŷₜ               [DECODER] ──▶ ŷₜ\n",
    "       │                               │\n",
    "       ▼                               ▼\n",
    "  (Error accumulates)            (Faster learning)\n",
    "```\n",
    "\n",
    "### Teacher Forcing Ratio\n",
    "- Start with high ratio (0.5-1.0) for fast initial learning\n",
    "- Gradually decrease to 0 (scheduled sampling)\n",
    "- Helps bridge train-test gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42703213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Sequence-to-Sequence model with teacher forcing support.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, forecast_horizon, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, trg=None, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Encoder input (batch, seq_len, input_size)\n",
    "            trg: Decoder target (batch, forecast_horizon, output_size) - optional\n",
    "            teacher_forcing_ratio: Probability of using ground truth as input\n",
    "        \n",
    "        Returns:\n",
    "            outputs: (batch, forecast_horizon, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        output_size = self.decoder.output_size\n",
    "        \n",
    "        # Store outputs\n",
    "        outputs = torch.zeros(batch_size, self.forecast_horizon, output_size).to(self.device)\n",
    "        \n",
    "        # Encode input sequence\n",
    "        hidden = self.encoder(src)\n",
    "        \n",
    "        # First decoder input: last value of encoder input\n",
    "        decoder_input = src[:, -1:, :]  # (batch, 1, input_size)\n",
    "        \n",
    "        # Decode step by step\n",
    "        for t in range(self.forecast_horizon):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t:t+1, :] = output\n",
    "            \n",
    "            # Teacher forcing decision\n",
    "            if trg is not None and np.random.random() < teacher_forcing_ratio:\n",
    "                # Use ground truth as next input\n",
    "                decoder_input = trg[:, t:t+1, :]\n",
    "            else:\n",
    "                # Use own prediction as next input\n",
    "                decoder_input = output\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "print(\"Seq2Seq model with teacher forcing defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# Initialize model components\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "decoder = Decoder(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, DROPOUT).to(device)\n",
    "model = Seq2Seq(encoder, decoder, FORECAST_HORIZON, device).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Hidden Size: {HIDDEN_SIZE}\")\n",
    "print(f\"  Num Layers: {NUM_LAYERS}\")\n",
    "print(f\"  Forecast Horizon: {FORECAST_HORIZON}\")\n",
    "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b460c22b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training with Scheduled Teacher Forcing\n",
    "\n",
    "We'll implement **scheduled sampling** where teacher forcing ratio decreases over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, teacher_forcing_ratio):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, Y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with teacher forcing\n",
    "        outputs = model(X_batch, Y_batch, teacher_forcing_ratio)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate model (no teacher forcing).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in loader:\n",
    "            # Forward pass without teacher forcing\n",
    "            outputs = model(X_batch, None, teacher_forcing_ratio=0.0)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "print(\"Training and evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "INITIAL_TF_RATIO = 0.5  # Initial teacher forcing ratio\n",
    "TF_DECAY = 0.95         # Decay rate per epoch\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "tf_ratios = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Starting training with scheduled teacher forcing...\\n\")\n",
    "\n",
    "tf_ratio = INITIAL_TF_RATIO\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, tf_ratio)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    tf_ratios.append(tf_ratio)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Decay teacher forcing ratio\n",
    "    tf_ratio = max(0.0, tf_ratio * TF_DECAY)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
    "              f\"Train Loss: {train_loss:.6f} | \"\n",
    "              f\"Val Loss: {val_loss:.6f} | \"\n",
    "              f\"TF Ratio: {tf_ratio:.3f} | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', color='blue', linewidth=1.5)\n",
    "axes[0].plot(val_losses, label='Val Loss', color='red', linewidth=1.5)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Teacher forcing ratio decay\n",
    "axes[1].plot(tf_ratios, color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Teacher Forcing Ratio')\n",
    "axes[1].set_title('Scheduled Sampling (TF Ratio Decay)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7370feb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Multi-Step Forecasting Approaches\n",
    "\n",
    "Let's compare different strategies for multi-step prediction:\n",
    "\n",
    "| Approach | Description | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Direct** | Separate model per horizon | No error accumulation | Many models to train |\n",
    "| **Recursive** | Feed predictions back | One model | Error compounds |\n",
    "| **Seq2Seq** | Single model, multi-output | End-to-end | More complex |\n",
    "| **Multi-Output** | Single forward pass | Fast inference | Fixed horizon |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Output Direct Prediction Model (for comparison)\n",
    "class DirectMultiStepLSTM(nn.Module):\n",
    "    \"\"\"Direct multi-step prediction: outputs all horizons at once.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_horizon):\n",
    "        super(DirectMultiStepLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Output all horizons in one shot\n",
    "        self.fc = nn.Linear(hidden_size, output_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use last hidden state\n",
    "        last_hidden = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        output = self.fc(last_hidden)     # (batch, output_horizon)\n",
    "        return output.unsqueeze(-1)       # (batch, output_horizon, 1)\n",
    "\n",
    "\n",
    "# Initialize direct model\n",
    "direct_model = DirectMultiStepLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, FORECAST_HORIZON).to(device)\n",
    "\n",
    "print(f\"Direct Multi-Step Model Parameters: {sum(p.numel() for p in direct_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523977bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train direct model for comparison\n",
    "direct_optimizer = optim.Adam(direct_model.parameters(), lr=LEARNING_RATE)\n",
    "direct_train_losses = []\n",
    "direct_val_losses = []\n",
    "\n",
    "print(\"Training Direct Multi-Step Model...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    direct_model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        direct_optimizer.zero_grad()\n",
    "        outputs = direct_model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(direct_model.parameters(), max_norm=1.0)\n",
    "        direct_optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate\n",
    "    direct_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            outputs = direct_model(X_batch)\n",
    "            val_loss += criterion(outputs, Y_batch).item()\n",
    "    \n",
    "    direct_train_losses.append(train_loss / len(train_loader))\n",
    "    direct_val_losses.append(val_loss / len(val_loader))\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
    "              f\"Train: {direct_train_losses[-1]:.6f} | \"\n",
    "              f\"Val: {direct_val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea42c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Multi-Horizon Return Prediction - Evaluation\n",
    "\n",
    "Now let's evaluate both models on the test set and analyze per-horizon performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(model, X, Y, model_name, is_seq2seq=True):\n",
    "    \"\"\"\n",
    "    Generate predictions and compute metrics.\n",
    "    \n",
    "    Returns:\n",
    "        predictions: numpy array\n",
    "        metrics: dict with MSE, MAE per horizon\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if is_seq2seq:\n",
    "            predictions = model(X, None, teacher_forcing_ratio=0.0)\n",
    "        else:\n",
    "            predictions = model(X)\n",
    "    \n",
    "    predictions = predictions.cpu().numpy()\n",
    "    Y_np = Y.cpu().numpy()\n",
    "    \n",
    "    # Per-horizon metrics\n",
    "    metrics = {'horizon': [], 'mse': [], 'mae': [], 'rmse': []}\n",
    "    \n",
    "    for h in range(FORECAST_HORIZON):\n",
    "        pred_h = predictions[:, h, 0]\n",
    "        true_h = Y_np[:, h, 0]\n",
    "        \n",
    "        mse = mean_squared_error(true_h, pred_h)\n",
    "        mae = mean_absolute_error(true_h, pred_h)\n",
    "        \n",
    "        metrics['horizon'].append(h + 1)\n",
    "        metrics['mse'].append(mse)\n",
    "        metrics['mae'].append(mae)\n",
    "        metrics['rmse'].append(np.sqrt(mse))\n",
    "    \n",
    "    return predictions, pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "# Evaluate Seq2Seq model\n",
    "seq2seq_preds, seq2seq_metrics = predict_and_evaluate(\n",
    "    model, X_test_t, Y_test_t, \"Seq2Seq\", is_seq2seq=True\n",
    ")\n",
    "\n",
    "# Evaluate Direct model\n",
    "direct_preds, direct_metrics = predict_and_evaluate(\n",
    "    direct_model, X_test_t, Y_test_t, \"Direct\", is_seq2seq=False\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SEQ2SEQ MODEL - Per-Horizon Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(seq2seq_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIRECT MODEL - Per-Horizon Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(direct_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Per-horizon comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "horizons = seq2seq_metrics['horizon'].values\n",
    "x = np.arange(len(horizons))\n",
    "width = 0.35\n",
    "\n",
    "# MSE comparison\n",
    "axes[0].bar(x - width/2, seq2seq_metrics['mse'], width, label='Seq2Seq', color='steelblue')\n",
    "axes[0].bar(x + width/2, direct_metrics['mse'], width, label='Direct', color='coral')\n",
    "axes[0].set_xlabel('Forecast Horizon (days)')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('MSE by Forecast Horizon')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(horizons)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(x - width/2, seq2seq_metrics['mae'], width, label='Seq2Seq', color='steelblue')\n",
    "axes[1].bar(x + width/2, direct_metrics['mae'], width, label='Direct', color='coral')\n",
    "axes[1].set_xlabel('Forecast Horizon (days)')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('MAE by Forecast Horizon')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(horizons)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions visualization\n",
    "Y_test_np = Y_test_t.cpu().numpy()\n",
    "\n",
    "# Select sample indices to visualize\n",
    "sample_indices = [0, 50, 100, 150]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, ax in zip(sample_indices, axes):\n",
    "    horizons = range(1, FORECAST_HORIZON + 1)\n",
    "    \n",
    "    ax.plot(horizons, Y_test_np[idx, :, 0], 'ko-', label='Actual', markersize=8, linewidth=2)\n",
    "    ax.plot(horizons, seq2seq_preds[idx, :, 0], 'bs--', label='Seq2Seq', markersize=6, linewidth=1.5)\n",
    "    ax.plot(horizons, direct_preds[idx, :, 0], 'r^--', label='Direct', markersize=6, linewidth=1.5)\n",
    "    \n",
    "    ax.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax.set_xlabel('Forecast Horizon (days)')\n",
    "    ax.set_ylabel('Scaled Return')\n",
    "    ax.set_title(f'Sample {idx}: Multi-Horizon Prediction')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(horizons)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20c9ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Directional Accuracy Analysis\n",
    "\n",
    "For trading, **direction** matters more than exact value. Let's analyze directional accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_directional_accuracy(predictions, actuals):\n",
    "    \"\"\"\n",
    "    Compute directional accuracy per horizon.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with horizon and accuracy\n",
    "    \"\"\"\n",
    "    results = {'horizon': [], 'accuracy': [], 'up_precision': [], 'down_precision': []}\n",
    "    \n",
    "    for h in range(predictions.shape[1]):\n",
    "        pred_dir = np.sign(predictions[:, h, 0])\n",
    "        true_dir = np.sign(actuals[:, h, 0])\n",
    "        \n",
    "        # Overall accuracy\n",
    "        accuracy = np.mean(pred_dir == true_dir)\n",
    "        \n",
    "        # Precision for up/down predictions\n",
    "        up_mask = pred_dir > 0\n",
    "        down_mask = pred_dir < 0\n",
    "        \n",
    "        up_precision = np.mean(true_dir[up_mask] > 0) if up_mask.sum() > 0 else 0\n",
    "        down_precision = np.mean(true_dir[down_mask] < 0) if down_mask.sum() > 0 else 0\n",
    "        \n",
    "        results['horizon'].append(h + 1)\n",
    "        results['accuracy'].append(accuracy)\n",
    "        results['up_precision'].append(up_precision)\n",
    "        results['down_precision'].append(down_precision)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compute directional accuracy\n",
    "seq2seq_dir = compute_directional_accuracy(seq2seq_preds, Y_test_np)\n",
    "direct_dir = compute_directional_accuracy(direct_preds, Y_test_np)\n",
    "\n",
    "print(\"SEQ2SEQ - Directional Accuracy\")\n",
    "print(seq2seq_dir.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\nDIRECT - Directional Accuracy\")\n",
    "print(direct_dir.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8715599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Directional accuracy\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(FORECAST_HORIZON)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, seq2seq_dir['accuracy'] * 100, width, label='Seq2Seq', color='steelblue')\n",
    "ax.bar(x + width/2, direct_dir['accuracy'] * 100, width, label='Direct', color='coral')\n",
    "\n",
    "ax.axhline(y=50, color='red', linestyle='--', linewidth=1.5, label='Random (50%)')\n",
    "\n",
    "ax.set_xlabel('Forecast Horizon (days)', fontsize=12)\n",
    "ax.set_ylabel('Directional Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Directional Accuracy by Horizon', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Day {i+1}' for i in range(FORECAST_HORIZON)])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(40, 60)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7424e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practical Trading Application\n",
    "\n",
    "Let's simulate a simple multi-horizon strategy using Seq2Seq predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1328e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_multi_horizon(predictions, actuals, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Backtest using multi-horizon predictions.\n",
    "    \n",
    "    Strategy:\n",
    "    - If average predicted return over horizon > threshold: Long\n",
    "    - If average predicted return over horizon < -threshold: Short\n",
    "    - Hold position for 'horizon' days\n",
    "    \"\"\"\n",
    "    n_samples = predictions.shape[0]\n",
    "    \n",
    "    # Average prediction across horizons\n",
    "    avg_pred = predictions.mean(axis=1).flatten()\n",
    "    \n",
    "    # Generate signals\n",
    "    signals = np.zeros(n_samples)\n",
    "    signals[avg_pred > threshold] = 1   # Long\n",
    "    signals[avg_pred < -threshold] = -1  # Short\n",
    "    \n",
    "    # Actual returns (sum over horizon)\n",
    "    actual_returns = actuals.sum(axis=1).flatten()\n",
    "    \n",
    "    # Strategy returns\n",
    "    strategy_returns = signals * actual_returns\n",
    "    \n",
    "    # Metrics\n",
    "    total_return = np.sum(strategy_returns)\n",
    "    hit_rate = np.mean((signals != 0) & (np.sign(strategy_returns) > 0))\n",
    "    trades = np.sum(signals != 0)\n",
    "    \n",
    "    return {\n",
    "        'signals': signals,\n",
    "        'strategy_returns': strategy_returns,\n",
    "        'total_return': total_return,\n",
    "        'hit_rate': hit_rate,\n",
    "        'num_trades': trades,\n",
    "        'sharpe': np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-8) * np.sqrt(252 / FORECAST_HORIZON)\n",
    "    }\n",
    "\n",
    "\n",
    "# Backtest both models\n",
    "seq2seq_bt = backtest_multi_horizon(seq2seq_preds, Y_test_np, threshold=0.1)\n",
    "direct_bt = backtest_multi_horizon(direct_preds, Y_test_np, threshold=0.1)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"MULTI-HORIZON BACKTEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\n{'Metric':<20} {'Seq2Seq':>12} {'Direct':>12}\")\n",
    "print(\"-\"*44)\n",
    "print(f\"{'Total Return':.<20} {seq2seq_bt['total_return']:>12.4f} {direct_bt['total_return']:>12.4f}\")\n",
    "print(f\"{'Hit Rate':.<20} {seq2seq_bt['hit_rate']:>12.2%} {direct_bt['hit_rate']:>12.2%}\")\n",
    "print(f\"{'Num Trades':.<20} {seq2seq_bt['num_trades']:>12} {direct_bt['num_trades']:>12}\")\n",
    "print(f\"{'Sharpe Ratio':.<20} {seq2seq_bt['sharpe']:>12.2f} {direct_bt['sharpe']:>12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative returns plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "seq2seq_cumret = np.cumsum(seq2seq_bt['strategy_returns'])\n",
    "direct_cumret = np.cumsum(direct_bt['strategy_returns'])\n",
    "buy_hold = np.cumsum(Y_test_np.sum(axis=1).flatten())  # Buy and hold\n",
    "\n",
    "ax.plot(seq2seq_cumret, label='Seq2Seq Strategy', color='steelblue', linewidth=2)\n",
    "ax.plot(direct_cumret, label='Direct Strategy', color='coral', linewidth=2)\n",
    "ax.plot(buy_hold, label='Buy & Hold', color='gray', linestyle='--', linewidth=1.5)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Cumulative Return (scaled)')\n",
    "ax.set_title('Multi-Horizon Strategy Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50525f5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "### Encoder-Decoder Architecture\n",
    "- **Encoder** compresses input sequence into context vector\n",
    "- **Decoder** generates output sequence from context\n",
    "- Suitable for variable-length input/output sequences\n",
    "\n",
    "### Teacher Forcing\n",
    "- Speeds up training by using ground truth as decoder input\n",
    "- **Scheduled sampling** gradually reduces TF ratio\n",
    "- Helps bridge train-test distribution gap\n",
    "\n",
    "### Multi-Step Forecasting\n",
    "- **Direct**: Train separate model per horizon\n",
    "- **Recursive**: Feed predictions back (error accumulation)\n",
    "- **Seq2Seq**: End-to-end multi-output (recommended)\n",
    "\n",
    "### Financial Applications\n",
    "- Multi-day return forecasting\n",
    "- Volatility term structure prediction\n",
    "- Risk scenario generation\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- Add **attention mechanism** to improve long-horizon predictions\n",
    "- Experiment with **bidirectional encoder**\n",
    "- Incorporate **multivariate inputs** (OHLCV, technical indicators)\n",
    "- Implement **beam search** for probabilistic forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"SESSION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nData: {ticker}\")\n",
    "print(f\"Input Sequence Length: {SEQ_LEN} days\")\n",
    "print(f\"Forecast Horizon: {FORECAST_HORIZON} days\")\n",
    "print(f\"\\nSeq2Seq Model:\")\n",
    "print(f\"  - Encoder: {NUM_LAYERS}-layer LSTM ({HIDDEN_SIZE} hidden units)\")\n",
    "print(f\"  - Decoder: {NUM_LAYERS}-layer LSTM ({HIDDEN_SIZE} hidden units)\")\n",
    "print(f\"  - Teacher Forcing: Scheduled ({INITIAL_TF_RATIO:.0%} → 0%)\")\n",
    "print(f\"  - Best Val Loss: {best_val_loss:.6f}\")\n",
    "print(f\"\\nDirect Model:\")\n",
    "print(f\"  - {NUM_LAYERS}-layer LSTM with multi-output head\")\n",
    "print(f\"  - Final Val Loss: {direct_val_losses[-1]:.6f}\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  - Seq2Seq Avg MSE: {seq2seq_metrics['mse'].mean():.6f}\")\n",
    "print(f\"  - Direct Avg MSE: {direct_metrics['mse'].mean():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
