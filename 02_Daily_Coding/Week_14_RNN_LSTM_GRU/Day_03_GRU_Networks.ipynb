{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67359b5a",
   "metadata": {},
   "source": [
    "# Week 14, Day 3: GRU Networks\n",
    "\n",
    "## Gated Recurrent Units for Financial Time Series\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand GRU architecture (reset and update gates)\n",
    "2. Compare GRU vs LSTM networks\n",
    "3. Learn when to use each architecture\n",
    "4. Implement GRU in PyTorch\n",
    "5. Build a volatility prediction model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c8e9c",
   "metadata": {},
   "source": [
    "## 1. GRU Architecture\n",
    "\n",
    "### What is a GRU?\n",
    "\n",
    "**Gated Recurrent Unit (GRU)** was introduced by Cho et al. (2014) as a simpler alternative to LSTM. It combines the forget and input gates into a single **update gate** and merges the cell state and hidden state.\n",
    "\n",
    "### GRU Gates\n",
    "\n",
    "GRU has **two gates** (vs LSTM's three):\n",
    "\n",
    "#### 1. Reset Gate ($r_t$)\n",
    "Controls how much of the previous hidden state to forget:\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "\n",
    "#### 2. Update Gate ($z_t$)\n",
    "Controls how much of the new candidate state to use:\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "\n",
    "### GRU Equations\n",
    "\n",
    "**Candidate hidden state:**\n",
    "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "\n",
    "**Final hidden state:**\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "```\n",
    "         ┌─────────────────────────────────────┐\n",
    "         │              GRU Cell               │\n",
    "         │                                     │\n",
    "    x_t ─┼──┬──────────┬──────────┬───────────┤\n",
    "         │  │          │          │           │\n",
    "         │  ▼          ▼          ▼           │\n",
    "         │ ┌──┐      ┌──┐      ┌─────┐       │\n",
    "         │ │z_t│     │r_t│     │ h̃_t │       │\n",
    "         │ │  │      │  │      │     │       │\n",
    "         │ └──┘      └──┘      └─────┘       │\n",
    "         │  │          │          │           │\n",
    "         │  └──────────┴──────────┘           │\n",
    "         │              │                     │\n",
    "         │              ▼                     │\n",
    " h_{t-1}─┼─────────►  (×)  ──────────────────►├── h_t\n",
    "         │                                     │\n",
    "         └─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9dbdb",
   "metadata": {},
   "source": [
    "## 2. GRU Cell from Scratch\n",
    "\n",
    "Let's implement a GRU cell manually to understand the mechanics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6414a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellFromScratch(nn.Module):\n",
    "    \"\"\"\n",
    "    Manual GRU Cell Implementation for educational purposes.\n",
    "    \n",
    "    GRU has 2 gates vs LSTM's 3:\n",
    "    - Reset gate (r): controls how much past info to forget\n",
    "    - Update gate (z): controls how much new info to add\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCellFromScratch, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Reset gate weights\n",
    "        self.W_xr = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hr = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "        # Update gate weights\n",
    "        self.W_xz = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hz = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "        # Candidate hidden state weights\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        Forward pass for single time step.\n",
    "        \n",
    "        Args:\n",
    "            x: input at time t, shape (batch, input_size)\n",
    "            h_prev: hidden state at t-1, shape (batch, hidden_size)\n",
    "            \n",
    "        Returns:\n",
    "            h_t: new hidden state\n",
    "        \"\"\"\n",
    "        # Reset gate: how much of past to forget\n",
    "        r_t = torch.sigmoid(self.W_xr(x) + self.W_hr(h_prev))\n",
    "        \n",
    "        # Update gate: how much of new info to use\n",
    "        z_t = torch.sigmoid(self.W_xz(x) + self.W_hz(h_prev))\n",
    "        \n",
    "        # Candidate hidden state (reset gate applied here)\n",
    "        h_candidate = torch.tanh(self.W_xh(x) + self.W_hh(r_t * h_prev))\n",
    "        \n",
    "        # Final hidden state: interpolation between old and new\n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_candidate\n",
    "        \n",
    "        return h_t, {'reset_gate': r_t, 'update_gate': z_t}\n",
    "\n",
    "# Test our implementation\n",
    "batch_size, input_size, hidden_size = 2, 4, 8\n",
    "gru_cell = GRUCellFromScratch(input_size, hidden_size)\n",
    "\n",
    "x = torch.randn(batch_size, input_size)\n",
    "h = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "h_new, gates = gru_cell(x, h)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden state shape: {h_new.shape}\")\n",
    "print(f\"Reset gate shape: {gates['reset_gate'].shape}\")\n",
    "print(f\"Update gate shape: {gates['update_gate'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ab9bb6",
   "metadata": {},
   "source": [
    "## 3. GRU vs LSTM Comparison\n",
    "\n",
    "### Architectural Differences\n",
    "\n",
    "| Aspect | LSTM | GRU |\n",
    "|--------|------|-----|\n",
    "| **Gates** | 3 (forget, input, output) | 2 (reset, update) |\n",
    "| **States** | 2 (cell + hidden) | 1 (hidden only) |\n",
    "| **Parameters** | ~4x more | ~3x more (vs vanilla RNN) |\n",
    "| **Computation** | Slower | Faster |\n",
    "| **Memory** | More | Less |\n",
    "\n",
    "### Parameter Count Comparison\n",
    "\n",
    "For input size $n$ and hidden size $h$:\n",
    "- **LSTM**: $4 \\times (n \\times h + h^2 + h) = 4(nh + h^2 + h)$\n",
    "- **GRU**: $3 \\times (n \\times h + h^2 + h) = 3(nh + h^2 + h)$\n",
    "\n",
    "GRU has **~25% fewer parameters** than LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b41f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Compare parameter counts\n",
    "input_size = 10\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "num_layers = 2\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    lstm_params = count_parameters(lstm)\n",
    "    gru_params = count_parameters(gru)\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Hidden Size': hidden_size,\n",
    "        'LSTM Params': lstm_params,\n",
    "        'GRU Params': gru_params,\n",
    "        'Reduction (%)': (1 - gru_params/lstm_params) * 100\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Parameter Comparison (2 layers):\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed comparison\n",
    "import time\n",
    "\n",
    "def benchmark_model(model, x, num_runs=100):\n",
    "    \"\"\"Benchmark forward pass speed.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warm up\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(x)\n",
    "    end = time.time()\n",
    "    \n",
    "    return (end - start) / num_runs * 1000  # ms per forward pass\n",
    "\n",
    "# Create test input\n",
    "batch_size, seq_len, input_size, hidden_size = 32, 60, 10, 64\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "gru = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)\n",
    "\n",
    "lstm_time = benchmark_model(lstm, x)\n",
    "gru_time = benchmark_model(gru, x)\n",
    "\n",
    "print(f\"\\nSpeed Comparison (seq_len={seq_len}, batch={batch_size}):\")\n",
    "print(f\"LSTM: {lstm_time:.3f} ms per forward pass\")\n",
    "print(f\"GRU:  {gru_time:.3f} ms per forward pass\")\n",
    "print(f\"GRU is {(lstm_time/gru_time - 1)*100:.1f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3698e0",
   "metadata": {},
   "source": [
    "## 4. When to Use Each Architecture\n",
    "\n",
    "### Use GRU When:\n",
    "- ✅ **Limited computational resources** (fewer parameters)\n",
    "- ✅ **Smaller datasets** (less overfitting risk)\n",
    "- ✅ **Shorter sequences** (less need for long-term memory)\n",
    "- ✅ **Real-time applications** (faster inference)\n",
    "- ✅ **Quick prototyping** (simpler to tune)\n",
    "\n",
    "### Use LSTM When:\n",
    "- ✅ **Very long sequences** (better gradient flow via cell state)\n",
    "- ✅ **Complex temporal dependencies** (more expressive)\n",
    "- ✅ **Large datasets** (can utilize extra capacity)\n",
    "- ✅ **Critical applications** (more battle-tested)\n",
    "\n",
    "### Financial Applications Guide:\n",
    "\n",
    "| Task | Recommendation | Reason |\n",
    "|------|----------------|--------|\n",
    "| Intraday volatility | GRU | Short sequences, speed matters |\n",
    "| Long-term trend | LSTM | Needs long memory |\n",
    "| HFT signals | GRU | Low latency critical |\n",
    "| Portfolio optimization | Either | Test both |\n",
    "| Sentiment analysis | GRU | Usually short text |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7a2c7",
   "metadata": {},
   "source": [
    "## 5. GRU Implementation in PyTorch\n",
    "\n",
    "Let's build a complete GRU model for financial prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU model for financial time series prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Multi-layer GRU with dropout\n",
    "    - Batch normalization\n",
    "    - Flexible output (single value or sequence)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size,\n",
    "                 dropout=0.2, bidirectional=False):\n",
    "        super(FinancialGRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # Input batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # GRU layers\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        fc_input_size = hidden_size * self.num_directions\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output prediction\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Batch normalization (transpose for BatchNorm1d)\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.permute(0, 2, 1)  # (batch, seq_len, features)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(\n",
    "            self.num_layers * self.num_directions,\n",
    "            batch_size,\n",
    "            self.hidden_size\n",
    "        ).to(x.device)\n",
    "        \n",
    "        # GRU forward pass\n",
    "        gru_out, h_n = self.gru(x, h0)\n",
    "        \n",
    "        # Use last hidden state for prediction\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward final states\n",
    "            h_final = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h_final = h_n[-1]\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(h_final)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test the model\n",
    "model = FinancialGRU(\n",
    "    input_size=5,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "x_test = torch.randn(16, 30, 5)  # (batch, seq_len, features)\n",
    "y_test = model(x_test)\n",
    "print(f\"Model output shape: {y_test.shape}\")\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814c427",
   "metadata": {},
   "source": [
    "## 6. Volatility Prediction - Practical Application\n",
    "\n",
    "We'll build a GRU model to predict realized volatility using stock price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ffe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download financial data\n",
    "ticker = 'SPY'\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "print(f\"Downloading {ticker} data...\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "# Use Close price\n",
    "df = pd.DataFrame()\n",
    "df['Close'] = data['Close']\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1913db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for volatility prediction\n",
    "def create_volatility_features(df, vol_window=20):\n",
    "    \"\"\"\n",
    "    Create features for volatility prediction.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with Close prices\n",
    "        vol_window: Window for realized volatility calculation\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with features and target\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Returns\n",
    "    features['returns'] = df['Close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "    \n",
    "    # Absolute returns (proxy for volatility)\n",
    "    features['abs_returns'] = features['returns'].abs()\n",
    "    \n",
    "    # Historical volatility (rolling std of returns)\n",
    "    features['vol_5d'] = features['returns'].rolling(5).std() * np.sqrt(252)\n",
    "    features['vol_10d'] = features['returns'].rolling(10).std() * np.sqrt(252)\n",
    "    features['vol_20d'] = features['returns'].rolling(20).std() * np.sqrt(252)\n",
    "    \n",
    "    # Volatility ratio (short-term / long-term)\n",
    "    features['vol_ratio'] = features['vol_5d'] / features['vol_20d']\n",
    "    \n",
    "    # Price momentum\n",
    "    features['momentum_5d'] = df['Close'].pct_change(5)\n",
    "    features['momentum_10d'] = df['Close'].pct_change(10)\n",
    "    \n",
    "    # Range-based volatility (Parkinson)\n",
    "    if 'High' in df.columns and 'Low' in df.columns:\n",
    "        features['range'] = (np.log(df['High']) - np.log(df['Low'])) ** 2\n",
    "        features['parkinson_vol'] = np.sqrt(features['range'].rolling(20).mean() / (4 * np.log(2))) * np.sqrt(252)\n",
    "    \n",
    "    # Target: Forward realized volatility (what we want to predict)\n",
    "    features['target_vol'] = features['returns'].rolling(vol_window).std().shift(-vol_window) * np.sqrt(252)\n",
    "    \n",
    "    return features.dropna()\n",
    "\n",
    "# Create features\n",
    "features_df = create_volatility_features(df)\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(features_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e680a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize volatility\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(df.index, df['Close'], 'b-', linewidth=0.8)\n",
    "axes[0].set_title(f'{ticker} Close Price', fontsize=12)\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "axes[1].plot(features_df.index, features_df['returns'], 'gray', linewidth=0.5, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].set_title('Daily Returns', fontsize=12)\n",
    "axes[1].set_ylabel('Return')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility\n",
    "axes[2].plot(features_df.index, features_df['vol_20d'], 'b-', label='20-day Vol', linewidth=1)\n",
    "axes[2].plot(features_df.index, features_df['target_vol'], 'r--', label='Target (Forward Vol)', linewidth=1, alpha=0.7)\n",
    "axes[2].set_title('Annualized Volatility', fontsize=12)\n",
    "axes[2].set_ylabel('Volatility')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for GRU\n",
    "def prepare_sequences(data, feature_cols, target_col, seq_length):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with features and target\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name\n",
    "        seq_length: Length of input sequences\n",
    "        \n",
    "    Returns:\n",
    "        X, y arrays\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    features = data[feature_cols].values\n",
    "    targets = data[target_col].values\n",
    "    \n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(targets[i+seq_length-1])  # Target at end of sequence\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define features to use\n",
    "feature_cols = ['returns', 'abs_returns', 'vol_5d', 'vol_10d', 'vol_20d', \n",
    "                'vol_ratio', 'momentum_5d', 'momentum_10d']\n",
    "target_col = 'target_vol'\n",
    "seq_length = 30  # 30 days of history\n",
    "\n",
    "# Create sequences\n",
    "X, y = prepare_sequences(features_df, feature_cols, target_col, seq_length)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5bd48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation/test split (temporal)\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Val:   {X_val.shape[0]} samples\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler_X.fit(X_train_flat)\n",
    "scaler_y.fit(y_train.reshape(-1, 1))\n",
    "\n",
    "# Transform all sets\n",
    "def scale_sequences(X, scaler):\n",
    "    \"\"\"Scale 3D sequence data.\"\"\"\n",
    "    original_shape = X.shape\n",
    "    X_flat = X.reshape(-1, original_shape[-1])\n",
    "    X_scaled = scaler.transform(X_flat)\n",
    "    return X_scaled.reshape(original_shape)\n",
    "\n",
    "X_train_scaled = scale_sequences(X_train, scaler_X)\n",
    "X_val_scaled = scale_sequences(X_val, scaler_X)\n",
    "X_test_scaled = scale_sequences(X_test, scaler_X)\n",
    "\n",
    "y_train_scaled = scaler_y.transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"Data scaled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled),\n",
    "    torch.FloatTensor(y_train_scaled)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val_scaled),\n",
    "    torch.FloatTensor(y_val_scaled)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_scaled),\n",
    "    torch.FloatTensor(y_test_scaled)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = FinancialGRU(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2,\n",
    "    bidirectional=False\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), np.array(predictions), np.array(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46273f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training GRU model...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_gru_model.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eea8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(train_losses, label='Train Loss', linewidth=1.5)\n",
    "ax.plot(val_losses, label='Validation Loss', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss (MSE)')\n",
    "ax.set_title('GRU Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667dd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_gru_model.pth'))\n",
    "\n",
    "# Get predictions\n",
    "test_loss, y_pred_scaled, y_true_scaled = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_true = scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Direction accuracy (volatility increasing or decreasing)\n",
    "y_true_diff = np.diff(y_true)\n",
    "y_pred_diff = np.diff(y_pred)\n",
    "direction_acc = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"MSE:              {mse:.6f}\")\n",
    "print(f\"RMSE:             {rmse:.6f}\")\n",
    "print(f\"MAE:              {mae:.6f}\")\n",
    "print(f\"R² Score:         {r2:.4f}\")\n",
    "print(f\"Direction Acc:    {direction_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Time series comparison\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(y_true, label='Actual Volatility', linewidth=1, alpha=0.8)\n",
    "ax1.plot(y_pred, label='Predicted Volatility', linewidth=1, alpha=0.8)\n",
    "ax1.set_title('Volatility Prediction - Test Set', fontsize=12)\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Annualized Volatility')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_true, y_pred, alpha=0.5, s=20)\n",
    "ax2.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Volatility')\n",
    "ax2.set_ylabel('Predicted Volatility')\n",
    "ax2.set_title(f'Actual vs Predicted (R² = {r2:.3f})', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction error over time\n",
    "ax3 = axes[1, 0]\n",
    "errors = y_pred - y_true\n",
    "ax3.plot(errors, linewidth=0.8, alpha=0.7)\n",
    "ax3.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "ax3.fill_between(range(len(errors)), errors, 0, alpha=0.3)\n",
    "ax3.set_title('Prediction Error Over Time', fontsize=12)\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Error (Pred - Actual)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(errors, bins=50, density=True, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "ax4.axvline(x=errors.mean(), color='g', linestyle='-', linewidth=2, label=f'Mean = {errors.mean():.4f}')\n",
    "ax4.set_title('Error Distribution', fontsize=12)\n",
    "ax4.set_xlabel('Prediction Error')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ada28",
   "metadata": {},
   "source": [
    "## 7. GRU vs LSTM Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7756563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model for comparison\n",
    "class FinancialLSTM(nn.Module):\n",
    "    \"\"\"LSTM model with same architecture as GRU for fair comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(FinancialLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(h_n[-1])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a21403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and compare both models\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, patience=15):\n",
    "    \"\"\"Train a model and return history.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_history.append(train_loss)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_weights)\n",
    "    return train_history, val_history, best_val_loss\n",
    "\n",
    "# Initialize fresh models\n",
    "gru_model = FinancialGRU(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "lstm_model = FinancialLSTM(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=1,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(\"Training GRU model...\")\n",
    "gru_train, gru_val, gru_best = train_model(gru_model, train_loader, val_loader)\n",
    "\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "lstm_train, lstm_val, lstm_best = train_model(lstm_model, train_loader, val_loader)\n",
    "\n",
    "print(f\"\\nGRU Best Val Loss:  {gru_best:.6f}\")\n",
    "print(f\"LSTM Best Val Loss: {lstm_best:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc230218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both on test set\n",
    "def get_test_metrics(model, test_loader, scaler_y):\n",
    "    \"\"\"Get test metrics for a model.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    _, y_pred_scaled, y_true_scaled = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    y_true = scaler_y.inverse_transform(y_true_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return {\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'R²': r2_score(y_true, y_pred),\n",
    "        'predictions': y_pred,\n",
    "        'actuals': y_true\n",
    "    }\n",
    "\n",
    "gru_metrics = get_test_metrics(gru_model, test_loader, scaler_y)\n",
    "lstm_metrics = get_test_metrics(lstm_model, test_loader, scaler_y)\n",
    "\n",
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['RMSE', 'MAE', 'R²', 'Parameters', 'Epochs Trained'],\n",
    "    'GRU': [\n",
    "        f\"{gru_metrics['RMSE']:.6f}\",\n",
    "        f\"{gru_metrics['MAE']:.6f}\",\n",
    "        f\"{gru_metrics['R²']:.4f}\",\n",
    "        f\"{count_parameters(gru_model):,}\",\n",
    "        len(gru_train)\n",
    "    ],\n",
    "    'LSTM': [\n",
    "        f\"{lstm_metrics['RMSE']:.6f}\",\n",
    "        f\"{lstm_metrics['MAE']:.6f}\",\n",
    "        f\"{lstm_metrics['R²']:.4f}\",\n",
    "        f\"{count_parameters(lstm_model):,}\",\n",
    "        len(lstm_train)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nGRU vs LSTM Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Training curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(gru_val, label='GRU', linewidth=2)\n",
    "ax1.plot(lstm_val, label='LSTM', linewidth=2)\n",
    "ax1.set_title('Validation Loss During Training', fontsize=12)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test predictions comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(gru_metrics['actuals'][:100], 'k-', label='Actual', linewidth=1.5, alpha=0.8)\n",
    "ax2.plot(gru_metrics['predictions'][:100], 'b--', label='GRU', linewidth=1.5, alpha=0.8)\n",
    "ax2.plot(lstm_metrics['predictions'][:100], 'r:', label='LSTM', linewidth=1.5, alpha=0.8)\n",
    "ax2.set_title('Predictions Comparison (First 100 Points)', fontsize=12)\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Volatility')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Error comparison\n",
    "ax3 = axes[1, 0]\n",
    "gru_errors = gru_metrics['predictions'] - gru_metrics['actuals']\n",
    "lstm_errors = lstm_metrics['predictions'] - lstm_metrics['actuals']\n",
    "ax3.boxplot([gru_errors, lstm_errors], labels=['GRU', 'LSTM'])\n",
    "ax3.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
    "ax3.set_title('Prediction Error Distribution', fontsize=12)\n",
    "ax3.set_ylabel('Error')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter efficiency\n",
    "ax4 = axes[1, 1]\n",
    "models = ['GRU', 'LSTM']\n",
    "params = [count_parameters(gru_model), count_parameters(lstm_model)]\n",
    "r2_scores = [gru_metrics['R²'], lstm_metrics['R²']]\n",
    "\n",
    "x_pos = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax4_twin = ax4.twinx()\n",
    "bars1 = ax4.bar(x_pos - width/2, params, width, label='Parameters', color='steelblue', alpha=0.7)\n",
    "bars2 = ax4_twin.bar(x_pos + width/2, r2_scores, width, label='R² Score', color='coral', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Model')\n",
    "ax4.set_ylabel('Parameters', color='steelblue')\n",
    "ax4_twin.set_ylabel('R² Score', color='coral')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(models)\n",
    "ax4.set_title('Parameter Efficiency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad642b",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### GRU Architecture Summary\n",
    "- **Simpler than LSTM**: 2 gates (reset, update) vs 3 gates\n",
    "- **No separate cell state**: Hidden state serves both purposes\n",
    "- **Faster training**: ~25% fewer parameters\n",
    "\n",
    "### When to Choose GRU\n",
    "1. **Computational constraints**: Faster training and inference\n",
    "2. **Smaller datasets**: Less prone to overfitting\n",
    "3. **Shorter sequences**: Adequate memory capacity\n",
    "4. **Quick prototyping**: Simpler hyperparameter tuning\n",
    "\n",
    "### Financial Applications\n",
    "- ✅ Volatility forecasting\n",
    "- ✅ Short-term price prediction\n",
    "- ✅ Real-time trading signals\n",
    "- ✅ Sentiment analysis\n",
    "\n",
    "### Best Practices\n",
    "1. **Always compare**: Test both GRU and LSTM on your specific task\n",
    "2. **Proper scaling**: StandardScaler or MinMaxScaler\n",
    "3. **Gradient clipping**: Prevent exploding gradients\n",
    "4. **Early stopping**: Prevent overfitting\n",
    "5. **Temporal split**: Never shuffle time series data\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Exercises\n",
    "\n",
    "1. **Bidirectional GRU**: Implement and compare bidirectional GRU for volatility prediction\n",
    "2. **Multi-step forecasting**: Modify the model to predict 5-day ahead volatility\n",
    "3. **Feature importance**: Use attention mechanism to identify important features\n",
    "4. **Ensemble**: Combine GRU and LSTM predictions\n",
    "5. **Different assets**: Test the model on crypto or forex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('best_gru_model.pth'):\n",
    "    os.remove('best_gru_model.pth')\n",
    "    \n",
    "print(\"Notebook completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
