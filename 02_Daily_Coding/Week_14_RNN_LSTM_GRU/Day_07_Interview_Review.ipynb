{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8194ff",
   "metadata": {},
   "source": [
    "# Week 14 - Day 7: Interview Review - RNN/LSTM/GRU\n",
    "\n",
    "## Learning Objectives\n",
    "- Master 10 essential RNN/LSTM interview questions\n",
    "- Understand common mistakes in sequence modeling\n",
    "- Build a multi-asset LSTM predictor (mini-project)\n",
    "- Review and consolidate Week 14 concepts\n",
    "\n",
    "**Date:** Week 14, Day 7  \n",
    "**Focus:** Interview Preparation & Practical Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ddac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad19176",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: 10 RNN/LSTM Interview Questions with Answers\n",
    "\n",
    "Master these questions to ace your quantitative finance interviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea91f6",
   "metadata": {},
   "source": [
    "## Question 1: What is the vanishing gradient problem and how do LSTMs solve it?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Vanishing Gradient Problem:**\n",
    "- In standard RNNs, gradients are multiplied through many time steps during backpropagation\n",
    "- If gradients < 1, they shrink exponentially → vanishing gradients\n",
    "- If gradients > 1, they explode exponentially → exploding gradients\n",
    "- Result: Network cannot learn long-term dependencies\n",
    "\n",
    "**LSTM Solution:**\n",
    "1. **Cell State (Highway):** Direct path for gradient flow with minimal transformations\n",
    "2. **Gating Mechanisms:** Control information flow additively, not multiplicatively\n",
    "3. **Forget Gate:** Decides what to discard from cell state\n",
    "4. **Input Gate:** Controls new information added to cell state\n",
    "5. **Output Gate:** Regulates what information is output\n",
    "\n",
    "The cell state acts as a \"conveyor belt\" allowing gradients to flow unchanged over many timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1923f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Gradient flow comparison\n",
    "def simulate_gradient_flow(n_timesteps, gradient_factor, is_lstm=False):\n",
    "    \"\"\"Simulate gradient magnitude through time steps\"\"\"\n",
    "    gradients = [1.0]  # Initial gradient\n",
    "    \n",
    "    for t in range(1, n_timesteps):\n",
    "        if is_lstm:\n",
    "            # LSTM: Additive updates preserve gradients better\n",
    "            # Simplified: forget_gate close to 1 preserves gradient\n",
    "            forget_gate = 0.9 + np.random.uniform(-0.05, 0.05)\n",
    "            gradients.append(gradients[-1] * forget_gate)\n",
    "        else:\n",
    "            # RNN: Multiplicative updates\n",
    "            gradients.append(gradients[-1] * gradient_factor)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Compare gradient flow\n",
    "timesteps = 50\n",
    "rnn_vanishing = simulate_gradient_flow(timesteps, 0.8, is_lstm=False)\n",
    "rnn_exploding = simulate_gradient_flow(timesteps, 1.2, is_lstm=False)\n",
    "lstm_gradients = simulate_gradient_flow(timesteps, 0.8, is_lstm=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(rnn_vanishing, 'r-', linewidth=2)\n",
    "axes[0].set_title('RNN: Vanishing Gradients (factor=0.8)', fontsize=12)\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Gradient Magnitude')\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(rnn_exploding, 'orange', linewidth=2)\n",
    "axes[1].set_title('RNN: Exploding Gradients (factor=1.2)', fontsize=12)\n",
    "axes[1].set_xlabel('Time Steps')\n",
    "axes[1].set_ylabel('Gradient Magnitude')\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "axes[2].plot(lstm_gradients, 'g-', linewidth=2)\n",
    "axes[2].set_title('LSTM: Stable Gradients', fontsize=12)\n",
    "axes[2].set_xlabel('Time Steps')\n",
    "axes[2].set_ylabel('Gradient Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"After {timesteps} steps:\")\n",
    "print(f\"  RNN Vanishing: {rnn_vanishing[-1]:.2e}\")\n",
    "print(f\"  RNN Exploding: {rnn_exploding[-1]:.2e}\")\n",
    "print(f\"  LSTM: {lstm_gradients[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d304ca9",
   "metadata": {},
   "source": [
    "## Question 2: Explain the difference between LSTM and GRU. When would you use each?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "| Aspect | LSTM | GRU |\n",
    "|--------|------|-----|\n",
    "| **Gates** | 3 (Forget, Input, Output) | 2 (Reset, Update) |\n",
    "| **States** | Cell state + Hidden state | Hidden state only |\n",
    "| **Parameters** | More (~4x hidden size²) | Fewer (~3x hidden size²) |\n",
    "| **Training Speed** | Slower | Faster |\n",
    "| **Memory** | Better for very long sequences | Good for moderate sequences |\n",
    "\n",
    "**When to use LSTM:**\n",
    "- Very long sequences (>100 timesteps)\n",
    "- Complex temporal patterns\n",
    "- When model capacity is important\n",
    "- Financial applications with subtle long-term dependencies\n",
    "\n",
    "**When to use GRU:**\n",
    "- Shorter sequences\n",
    "- Limited computational resources\n",
    "- Smaller datasets (less prone to overfitting)\n",
    "- Real-time inference requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838db9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count comparison\n",
    "input_size = 10\n",
    "hidden_size = 64\n",
    "\n",
    "lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Parameter Comparison (input=10, hidden=64):\")\n",
    "print(f\"  RNN:  {count_parameters(rnn):,} parameters\")\n",
    "print(f\"  GRU:  {count_parameters(gru):,} parameters\")\n",
    "print(f\"  LSTM: {count_parameters(lstm):,} parameters\")\n",
    "print(f\"\\nRatio: LSTM is {count_parameters(lstm)/count_parameters(gru):.2f}x larger than GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b2fab",
   "metadata": {},
   "source": [
    "## Question 3: What is teacher forcing? What are its advantages and disadvantages?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Teacher Forcing:**\n",
    "- Training technique where ground truth is fed as input at each time step\n",
    "- Instead of using model's own predictions, use actual values\n",
    "\n",
    "**Advantages:**\n",
    "1. Faster convergence\n",
    "2. More stable training\n",
    "3. Avoids error accumulation during training\n",
    "\n",
    "**Disadvantages:**\n",
    "1. **Exposure Bias:** Model never sees its own mistakes during training\n",
    "2. At inference, model uses its predictions → distribution mismatch\n",
    "3. Can lead to poor generalization\n",
    "\n",
    "**Solutions:**\n",
    "- **Scheduled Sampling:** Gradually decrease teacher forcing probability\n",
    "- **Professor Forcing:** Use adversarial training to match distributions\n",
    "- **Curriculum Learning:** Start with teacher forcing, gradually remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduled Sampling Implementation\n",
    "def scheduled_sampling_prob(epoch, total_epochs, strategy='linear'):\n",
    "    \"\"\"Calculate probability of using teacher forcing\"\"\"\n",
    "    if strategy == 'linear':\n",
    "        return max(0, 1 - epoch / total_epochs)\n",
    "    elif strategy == 'exponential':\n",
    "        return 0.99 ** epoch\n",
    "    elif strategy == 'inverse_sigmoid':\n",
    "        k = 5  # Steepness\n",
    "        return k / (k + np.exp(epoch / k))\n",
    "    return 1.0\n",
    "\n",
    "epochs = np.arange(100)\n",
    "linear = [scheduled_sampling_prob(e, 100, 'linear') for e in epochs]\n",
    "exponential = [scheduled_sampling_prob(e, 100, 'exponential') for e in epochs]\n",
    "inv_sigmoid = [scheduled_sampling_prob(e, 100, 'inverse_sigmoid') for e in epochs]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, linear, label='Linear Decay', linewidth=2)\n",
    "plt.plot(epochs, exponential, label='Exponential Decay', linewidth=2)\n",
    "plt.plot(epochs, inv_sigmoid, label='Inverse Sigmoid', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Teacher Forcing Probability')\n",
    "plt.title('Scheduled Sampling Strategies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830122a8",
   "metadata": {},
   "source": [
    "## Question 4: How do you handle variable-length sequences in PyTorch?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "1. **Padding + pack_padded_sequence:**\n",
    "   - Pad sequences to same length\n",
    "   - Use `pack_padded_sequence` before LSTM\n",
    "   - Use `pad_packed_sequence` after LSTM\n",
    "   - Most efficient and commonly used\n",
    "\n",
    "2. **Masking:**\n",
    "   - Create attention mask\n",
    "   - Apply mask in loss calculation\n",
    "\n",
    "3. **Bucketing:**\n",
    "   - Group similar-length sequences together\n",
    "   - Reduces padding waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45171e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable-length sequence handling demonstration\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "# Create sequences of different lengths\n",
    "seq1 = torch.randn(5, 3)   # Length 5\n",
    "seq2 = torch.randn(3, 3)   # Length 3\n",
    "seq3 = torch.randn(7, 3)   # Length 7\n",
    "sequences = [seq1, seq2, seq3]\n",
    "lengths = torch.tensor([5, 3, 7])\n",
    "\n",
    "# Pad sequences (batch_first=True)\n",
    "padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "print(f\"Padded shape: {padded.shape}\")  # [3, 7, 3] - batch, max_len, features\n",
    "\n",
    "# Sort by length (required for pack_padded_sequence)\n",
    "lengths_sorted, sort_idx = lengths.sort(descending=True)\n",
    "padded_sorted = padded[sort_idx]\n",
    "\n",
    "# Pack for efficient LSTM processing\n",
    "packed = pack_padded_sequence(padded_sorted, lengths_sorted.cpu(), batch_first=True)\n",
    "\n",
    "# Process with LSTM\n",
    "lstm = nn.LSTM(input_size=3, hidden_size=16, batch_first=True)\n",
    "packed_output, (h_n, c_n) = lstm(packed)\n",
    "\n",
    "# Unpack\n",
    "output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Restore original order\n",
    "_, unsort_idx = sort_idx.sort()\n",
    "output_original_order = output[unsort_idx]\n",
    "print(f\"Final output shape: {output_original_order.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59ae9c",
   "metadata": {},
   "source": [
    "## Question 5: What is the difference between stateful and stateless LSTMs?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "| Aspect | Stateless LSTM | Stateful LSTM |\n",
    "|--------|----------------|---------------|\n",
    "| **Hidden State** | Reset to zero after each batch | Preserved across batches |\n",
    "| **Use Case** | Independent sequences | Continuous time series |\n",
    "| **Training** | Simpler | Requires careful batch ordering |\n",
    "| **Memory** | Lower | Higher (stores states) |\n",
    "\n",
    "**Stateful LSTM Use Cases in Finance:**\n",
    "- Streaming real-time predictions\n",
    "- Very long sequences that don't fit in memory\n",
    "- Online learning scenarios\n",
    "\n",
    "**Implementation Note:**\n",
    "In PyTorch, pass `(h_0, c_0)` to maintain state; reset manually when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59461e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateful LSTM Example\n",
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.hidden = None\n",
    "        \n",
    "    def reset_hidden(self, batch_size):\n",
    "        \"\"\"Reset hidden state to zeros\"\"\"\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.hidden is None:\n",
    "            self.reset_hidden(x.size(0))\n",
    "        \n",
    "        out, self.hidden = self.lstm(x, self.hidden)\n",
    "        # Detach to prevent backprop through entire history\n",
    "        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        return out\n",
    "\n",
    "# Usage\n",
    "stateful_lstm = StatefulLSTM(input_size=5, hidden_size=32)\n",
    "\n",
    "# Process chunks of a long sequence\n",
    "print(\"Processing streaming data:\")\n",
    "stateful_lstm.reset_hidden(batch_size=1)\n",
    "\n",
    "for chunk_idx in range(3):\n",
    "    chunk = torch.randn(1, 10, 5)  # batch=1, seq_len=10, features=5\n",
    "    output = stateful_lstm(chunk)\n",
    "    print(f\"  Chunk {chunk_idx + 1}: output shape = {output.shape}, \"\n",
    "          f\"hidden mean = {stateful_lstm.hidden[0].mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872d366",
   "metadata": {},
   "source": [
    "## Question 6: How do you prevent overfitting in LSTM models?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Regularization Techniques:**\n",
    "\n",
    "1. **Dropout:**\n",
    "   - Regular dropout between layers\n",
    "   - **NOT** within recurrent connections (disrupts temporal learning)\n",
    "   - Use `dropout` parameter in `nn.LSTM` (applies between layers)\n",
    "\n",
    "2. **Recurrent Dropout (Variational):**\n",
    "   - Same dropout mask across time steps\n",
    "   - Preserves temporal coherence\n",
    "\n",
    "3. **Weight Decay (L2 Regularization):**\n",
    "   - Add to optimizer: `weight_decay=1e-5`\n",
    "\n",
    "4. **Early Stopping:**\n",
    "   - Monitor validation loss\n",
    "   - Stop when no improvement for N epochs\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - Add noise to inputs\n",
    "   - Time warping, scaling\n",
    "\n",
    "6. **Gradient Clipping:**\n",
    "   - Prevents gradient explosion\n",
    "   - `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd37ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with proper regularization\n",
    "class RegularizedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0  # Dropout between layers\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout before output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.dropout(lstm_out[:, -1, :])  # Last timestep + dropout\n",
    "        return self.fc(out)\n",
    "\n",
    "# Training with regularization techniques\n",
    "model = RegularizedLSTM(input_size=5, hidden_size=64, num_layers=2, output_size=1, dropout=0.3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 reg\n",
    "\n",
    "print(\"Regularization techniques applied:\")\n",
    "print(\"  ✓ Dropout between LSTM layers: 0.3\")\n",
    "print(\"  ✓ Dropout before output layer: 0.3\")\n",
    "print(\"  ✓ Weight decay (L2): 1e-5\")\n",
    "print(f\"\\nModel parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479ada8",
   "metadata": {},
   "source": [
    "## Question 7: How do you choose the sequence length (lookback window) for financial time series?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "1. **Domain Knowledge:**\n",
    "   - Trading days in a month (~21 days)\n",
    "   - Quarterly patterns (~63 days)\n",
    "   - Annual seasonality (~252 days)\n",
    "\n",
    "2. **Statistical Methods:**\n",
    "   - Autocorrelation analysis\n",
    "   - Partial autocorrelation (PACF)\n",
    "   - Cross-validation with different windows\n",
    "\n",
    "3. **Practical Factors:**\n",
    "   - Computational constraints\n",
    "   - Memory requirements\n",
    "   - Longer sequences = more parameters, overfitting risk\n",
    "\n",
    "4. **Empirical Validation:**\n",
    "   - Grid search over sequence lengths\n",
    "   - Monitor both training and validation metrics\n",
    "\n",
    "**Common Ranges:**\n",
    "- Intraday: 30-120 observations\n",
    "- Daily: 20-60 days\n",
    "- For regime detection: 60-252 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation analysis for sequence length selection\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "# Get sample data\n",
    "ticker = yf.Ticker('SPY')\n",
    "hist = ticker.history(period='2y')\n",
    "returns = hist['Close'].pct_change().dropna()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# ACF\n",
    "plot_acf(returns, ax=axes[0], lags=60, alpha=0.05)\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "axes[0].set_xlabel('Lag (Days)')\n",
    "\n",
    "# PACF\n",
    "plot_pacf(returns, ax=axes[1], lags=60, alpha=0.05)\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "axes[1].set_xlabel('Lag (Days)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find significant lags\n",
    "acf_values = acf(returns, nlags=60)\n",
    "confidence_interval = 1.96 / np.sqrt(len(returns))\n",
    "significant_lags = np.where(np.abs(acf_values) > confidence_interval)[0]\n",
    "print(f\"Significant ACF lags: {significant_lags[:10]}\")\n",
    "print(f\"\\nSuggested sequence lengths to try: [20, 30, 40, 60]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c27f8",
   "metadata": {},
   "source": [
    "## Question 8: What are attention mechanisms and how do they improve sequence models?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Attention Mechanism:**\n",
    "- Allows model to focus on relevant parts of input sequence\n",
    "- Learns weighted importance of each time step\n",
    "- Creates direct connections bypassing sequential bottleneck\n",
    "\n",
    "**Benefits:**\n",
    "1. **Long-range dependencies:** Direct connections to any timestep\n",
    "2. **Interpretability:** Attention weights show which inputs matter\n",
    "3. **Parallel computation:** (for self-attention)\n",
    "4. **Better gradient flow:** Shorter paths for backprop\n",
    "\n",
    "**Types:**\n",
    "- **Bahdanau Attention:** Additive attention\n",
    "- **Luong Attention:** Multiplicative attention\n",
    "- **Self-Attention:** Query, Key, Value from same sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc290ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Attention Layer Implementation\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: [batch, seq_len, hidden_size]\n",
    "        attention_weights = torch.softmax(\n",
    "            self.attention(lstm_output).squeeze(-1), dim=1\n",
    "        )\n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.bmm(\n",
    "            attention_weights.unsqueeze(1), lstm_output\n",
    "        ).squeeze(1)\n",
    "        return context, attention_weights\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.attention = AttentionLayer(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        context, attn_weights = self.attention(lstm_out)\n",
    "        output = self.fc(context)\n",
    "        return output, attn_weights\n",
    "\n",
    "# Test\n",
    "model = LSTMWithAttention(input_size=5, hidden_size=32, output_size=1)\n",
    "x = torch.randn(2, 20, 5)  # batch=2, seq_len=20, features=5\n",
    "output, attn = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn.shape}\")\n",
    "print(f\"\\nAttention weights (sum to 1): {attn[0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df087f",
   "metadata": {},
   "source": [
    "## Question 9: How do you handle non-stationarity in financial time series for LSTM models?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**The Problem:**\n",
    "- Financial prices are non-stationary (mean, variance change over time)\n",
    "- LSTMs assume patterns learned on training data apply to test data\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Transform to Returns:**\n",
    "   - `returns = (price_t - price_{t-1}) / price_{t-1}`\n",
    "   - More stationary than raw prices\n",
    "\n",
    "2. **Log Returns:**\n",
    "   - `log_returns = log(price_t / price_{t-1})`\n",
    "   - Better for longer horizons\n",
    "\n",
    "3. **Differencing:**\n",
    "   - First or second order differences\n",
    "\n",
    "4. **Rolling Normalization:**\n",
    "   - Z-score with rolling mean and std\n",
    "   - Adapts to changing market conditions\n",
    "\n",
    "5. **Volatility Scaling:**\n",
    "   - Divide by rolling volatility\n",
    "   - Removes heteroskedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3556f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-stationarity handling techniques\n",
    "prices = hist['Close'].values\n",
    "\n",
    "# Different transformations\n",
    "simple_returns = np.diff(prices) / prices[:-1]\n",
    "log_returns = np.diff(np.log(prices))\n",
    "\n",
    "# Rolling z-score normalization\n",
    "def rolling_zscore(data, window=20):\n",
    "    rolling_mean = pd.Series(data).rolling(window).mean()\n",
    "    rolling_std = pd.Series(data).rolling(window).std()\n",
    "    return ((data - rolling_mean) / rolling_std).values\n",
    "\n",
    "zscore_returns = rolling_zscore(simple_returns, window=20)\n",
    "\n",
    "# Volatility scaling\n",
    "rolling_vol = pd.Series(simple_returns).rolling(20).std().values\n",
    "vol_scaled = simple_returns / rolling_vol\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "axes[0, 0].plot(prices[-252:])\n",
    "axes[0, 0].set_title('Raw Prices (Non-Stationary)')\n",
    "\n",
    "axes[0, 1].plot(simple_returns[-252:])\n",
    "axes[0, 1].set_title('Simple Returns (More Stationary)')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1, 0].plot(zscore_returns[-252:])\n",
    "axes[1, 0].set_title('Rolling Z-Score Normalized')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1, 1].plot(vol_scaled[-252:])\n",
    "axes[1, 1].set_title('Volatility Scaled Returns')\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d724f96",
   "metadata": {},
   "source": [
    "## Question 10: How do you evaluate and compare LSTM models for financial prediction?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "**Regression Metrics (Price/Return Prediction):**\n",
    "- MSE, RMSE, MAE\n",
    "- R² Score\n",
    "- MAPE (Mean Absolute Percentage Error)\n",
    "\n",
    "**Directional Metrics:**\n",
    "- Directional Accuracy (DA)\n",
    "- Hit Rate\n",
    "\n",
    "**Financial Metrics:**\n",
    "- Sharpe Ratio of predictions\n",
    "- Maximum Drawdown\n",
    "- Information Coefficient (IC)\n",
    "- Profit Factor\n",
    "\n",
    "**Important Considerations:**\n",
    "1. Use walk-forward validation (not random split)\n",
    "2. Account for transaction costs\n",
    "3. Test on out-of-sample period\n",
    "4. Compare against benchmarks (buy-and-hold, naive forecast)\n",
    "5. Statistical significance testing (DM test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644868ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive LSTM evaluation metrics\n",
    "def evaluate_predictions(y_true, y_pred, returns_true=None):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Regression metrics\n",
    "    results['MSE'] = mean_squared_error(y_true, y_pred)\n",
    "    results['RMSE'] = np.sqrt(results['MSE'])\n",
    "    results['MAE'] = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    if len(y_true) > 1:\n",
    "        true_direction = np.sign(np.diff(y_true))\n",
    "        pred_direction = np.sign(np.diff(y_pred))\n",
    "        results['Directional_Accuracy'] = np.mean(true_direction == pred_direction)\n",
    "    \n",
    "    # Information Coefficient (Spearman correlation)\n",
    "    from scipy.stats import spearmanr\n",
    "    ic, _ = spearmanr(y_true, y_pred)\n",
    "    results['IC'] = ic\n",
    "    \n",
    "    # Strategy returns if trading based on predictions\n",
    "    if returns_true is not None:\n",
    "        signal = np.sign(y_pred[:-1])  # Position based on prediction\n",
    "        strategy_returns = signal * returns_true[1:]\n",
    "        results['Strategy_Return'] = np.sum(strategy_returns)\n",
    "        results['Sharpe_Ratio'] = np.mean(strategy_returns) / np.std(strategy_returns) * np.sqrt(252)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example with synthetic predictions\n",
    "np.random.seed(42)\n",
    "y_true = np.cumsum(np.random.randn(100) * 0.01) + 100\n",
    "y_pred = y_true + np.random.randn(100) * 0.5  # Add noise\n",
    "returns_true = np.diff(y_true) / y_true[:-1]\n",
    "\n",
    "metrics = evaluate_predictions(y_true, y_pred, returns_true)\n",
    "\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(\"-\" * 40)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3faef6",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Common Mistakes in Sequence Modeling\n",
    "\n",
    "Avoid these pitfalls to build robust LSTM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e7a8d",
   "metadata": {},
   "source": [
    "## Mistake 1: Data Leakage in Time Series\n",
    "\n",
    "**Problem:** Using future information in training\n",
    "\n",
    "**Common Sources:**\n",
    "- Normalizing with entire dataset statistics\n",
    "- Random train/test split instead of temporal split\n",
    "- Feature engineering using future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a01e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ WRONG: Normalize with all data\n",
    "def wrong_normalize(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data.reshape(-1, 1))  # Uses future data!\n",
    "\n",
    "# ✅ CORRECT: Normalize with only past data\n",
    "def correct_normalize(train_data, test_data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_data.reshape(-1, 1))  # Fit only on training\n",
    "    train_scaled = scaler.transform(train_data.reshape(-1, 1))\n",
    "    test_scaled = scaler.transform(test_data.reshape(-1, 1))  # Transform test\n",
    "    return train_scaled, test_scaled, scaler\n",
    "\n",
    "# ❌ WRONG: Random split\n",
    "def wrong_split(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)  # Shuffles!\n",
    "\n",
    "# ✅ CORRECT: Temporal split\n",
    "def correct_split(X, y, train_ratio=0.8):\n",
    "    split_idx = int(len(X) * train_ratio)\n",
    "    return X[:split_idx], X[split_idx:], y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(\"Data Leakage Prevention:\")\n",
    "print(\"  ✓ Use temporal train/test split\")\n",
    "print(\"  ✓ Fit scalers on training data only\")\n",
    "print(\"  ✓ Calculate features using only past data\")\n",
    "print(\"  ✓ Use walk-forward validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbc367",
   "metadata": {},
   "source": [
    "## Mistake 2: Improper Sequence Creation\n",
    "\n",
    "**Problem:** Creating sequences that leak information or misalign targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ WRONG: Target overlaps with input\n",
    "def wrong_create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length - 1])  # WRONG: Last input = target!\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ✅ CORRECT: Target is next value after sequence\n",
    "def correct_create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])  # CORRECT: Predict NEXT value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Demonstration\n",
    "sample_data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "seq_len = 3\n",
    "\n",
    "X_wrong, y_wrong = wrong_create_sequences(sample_data, seq_len)\n",
    "X_correct, y_correct = correct_create_sequences(sample_data, seq_len)\n",
    "\n",
    "print(\"Sample Data:\", sample_data)\n",
    "print(f\"\\n❌ WRONG sequence creation (target overlaps):\")\n",
    "print(f\"   X[0]: {X_wrong[0]} → y[0]: {y_wrong[0]} (3 is in both!)\")\n",
    "print(f\"\\n✅ CORRECT sequence creation:\")\n",
    "print(f\"   X[0]: {X_correct[0]} → y[0]: {y_correct[0]} (predicting next value)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965a527",
   "metadata": {},
   "source": [
    "## Mistake 3: Not Using Gradient Clipping\n",
    "\n",
    "**Problem:** Gradient explosion leads to NaN values and unstable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ WRONG: No gradient clipping\n",
    "def wrong_training_step(model, optimizer, criterion, X, y):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()  # Gradients can explode!\n",
    "    return loss\n",
    "\n",
    "# ✅ CORRECT: With gradient clipping\n",
    "def correct_training_step(model, optimizer, criterion, X, y, max_norm=1.0):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# Monitor gradient norms\n",
    "def get_gradient_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_norm += p.grad.data.norm(2).item() ** 2\n",
    "    return total_norm ** 0.5\n",
    "\n",
    "print(\"Gradient Clipping Best Practices:\")\n",
    "print(\"  ✓ Use clip_grad_norm_ with max_norm=1.0 to 5.0\")\n",
    "print(\"  ✓ Monitor gradient norms during training\")\n",
    "print(\"  ✓ If gradients consistently hit max, reduce learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b995d",
   "metadata": {},
   "source": [
    "## Mistake 4: Wrong Hidden State Initialization\n",
    "\n",
    "**Problem:** Hidden states not properly reset between batches or initialized with wrong shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb515ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ WRONG: Incorrect hidden state shape\n",
    "def wrong_init_hidden(batch_size, hidden_size, num_layers):\n",
    "    # Missing num_layers dimension!\n",
    "    h0 = torch.zeros(batch_size, hidden_size)\n",
    "    c0 = torch.zeros(batch_size, hidden_size)\n",
    "    return (h0, c0)\n",
    "\n",
    "# ✅ CORRECT: Proper initialization\n",
    "def correct_init_hidden(batch_size, hidden_size, num_layers, device='cpu'):\n",
    "    # Shape: (num_layers * num_directions, batch, hidden_size)\n",
    "    h0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, batch_size, hidden_size).to(device)\n",
    "    return (h0, c0)\n",
    "\n",
    "# Test\n",
    "batch_size, hidden_size, num_layers = 32, 64, 2\n",
    "h0, c0 = correct_init_hidden(batch_size, hidden_size, num_layers)\n",
    "print(f\"Correct hidden state shape: h0={h0.shape}, c0={c0.shape}\")\n",
    "print(f\"\\nExpected: (num_layers={num_layers}, batch={batch_size}, hidden={hidden_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd4b5c4",
   "metadata": {},
   "source": [
    "## Mistake 5: Ignoring Bidirectional Output Shape\n",
    "\n",
    "**Problem:** Not accounting for doubled hidden size in bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM output shapes\n",
    "input_size = 5\n",
    "hidden_size = 32\n",
    "batch_size = 16\n",
    "seq_len = 20\n",
    "\n",
    "# Unidirectional\n",
    "lstm_uni = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "\n",
    "# Bidirectional\n",
    "lstm_bi = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "out_uni, (h_uni, c_uni) = lstm_uni(x)\n",
    "out_bi, (h_bi, c_bi) = lstm_bi(x)\n",
    "\n",
    "print(\"Output Shapes:\")\n",
    "print(f\"  Unidirectional output: {out_uni.shape}\")\n",
    "print(f\"  Bidirectional output:  {out_bi.shape}  ← 2x hidden size!\")\n",
    "print(f\"\\nHidden State Shapes:\")\n",
    "print(f\"  Unidirectional h: {h_uni.shape}\")\n",
    "print(f\"  Bidirectional h:  {h_bi.shape}  ← 2x num_layers!\")\n",
    "\n",
    "# ✅ CORRECT: Account for bidirectional in FC layer\n",
    "fc_correct = nn.Linear(hidden_size * 2, 1)  # 2x for bidirectional\n",
    "print(f\"\\n✅ Correct FC input size for bidirectional: {hidden_size * 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689df49a",
   "metadata": {},
   "source": [
    "## Mistake 6: Not Handling NaN Values Properly\n",
    "\n",
    "**Problem:** NaN values in financial data cause training to fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f89a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper NaN handling for financial data\n",
    "def prepare_financial_data(df):\n",
    "    \"\"\"Robust data preparation pipeline\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Check for NaN\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    print(f\"NaN values found: {nan_count}\")\n",
    "    \n",
    "    # 2. Forward fill (most appropriate for time series)\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # 3. Backward fill remaining (at start)\n",
    "    df = df.bfill()\n",
    "    \n",
    "    # 4. Check for infinite values\n",
    "    inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    print(f\"Infinite values found: {inf_count}\")\n",
    "    \n",
    "    # 5. Replace infinite with NaN then fill\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()\n",
    "    \n",
    "    # 6. Final check\n",
    "    assert df.isna().sum().sum() == 0, \"Data still contains NaN!\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test\n",
    "test_df = pd.DataFrame({\n",
    "    'price': [100, np.nan, 102, 103, np.inf, 105],\n",
    "    'volume': [1000, 1100, np.nan, 1300, 1400, 1500]\n",
    "})\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(test_df)\n",
    "print(\"\\nAfter cleaning:\")\n",
    "cleaned_df = prepare_financial_data(test_df)\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df32e01b",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Mini-Project - Multi-Asset LSTM Predictor\n",
    "\n",
    "Build an LSTM model that predicts returns for multiple assets simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download multi-asset data\n",
    "tickers = ['SPY', 'QQQ', 'IWM', 'TLT', 'GLD']  # Diversified assets\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "print(\"Downloading multi-asset data...\")\n",
    "data = yf.download(tickers, start=start_date, end=end_date, progress=False)['Close']\n",
    "data = data.dropna()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "print(f\"Assets: {list(data.columns)}\")\n",
    "\n",
    "# Display price evolution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "normalized = data / data.iloc[0] * 100\n",
    "for col in normalized.columns:\n",
    "    ax.plot(normalized.index, normalized[col], label=col, linewidth=1.5)\n",
    "ax.set_title('Normalized Asset Prices (Base = 100)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Normalized Price')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd493399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for multi-asset prediction\n",
    "class MultiAssetDataset:\n",
    "    def __init__(self, prices_df, seq_length=30, train_ratio=0.8):\n",
    "        self.seq_length = seq_length\n",
    "        self.n_assets = len(prices_df.columns)\n",
    "        self.asset_names = list(prices_df.columns)\n",
    "        \n",
    "        # Calculate returns\n",
    "        self.returns = prices_df.pct_change().dropna()\n",
    "        \n",
    "        # Temporal split\n",
    "        split_idx = int(len(self.returns) * train_ratio)\n",
    "        self.train_returns = self.returns.iloc[:split_idx]\n",
    "        self.test_returns = self.returns.iloc[split_idx:]\n",
    "        \n",
    "        # Normalize using training data only\n",
    "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scaler.fit(self.train_returns.values)\n",
    "        \n",
    "        train_scaled = self.scaler.transform(self.train_returns.values)\n",
    "        test_scaled = self.scaler.transform(self.test_returns.values)\n",
    "        \n",
    "        # Create sequences\n",
    "        self.X_train, self.y_train = self._create_sequences(train_scaled)\n",
    "        self.X_test, self.y_test = self._create_sequences(test_scaled)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.X_train = torch.FloatTensor(self.X_train)\n",
    "        self.y_train = torch.FloatTensor(self.y_train)\n",
    "        self.X_test = torch.FloatTensor(self.X_test)\n",
    "        self.y_test = torch.FloatTensor(self.y_test)\n",
    "        \n",
    "    def _create_sequences(self, data):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.seq_length):\n",
    "            X.append(data[i:i + self.seq_length])\n",
    "            y.append(data[i + self.seq_length])  # Predict all assets\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def get_dataloaders(self, batch_size=32):\n",
    "        train_dataset = TensorDataset(self.X_train, self.y_train)\n",
    "        test_dataset = TensorDataset(self.X_test, self.y_test)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "\n",
    "# Create dataset\n",
    "dataset = MultiAssetDataset(data, seq_length=30, train_ratio=0.8)\n",
    "\n",
    "print(f\"Training samples: {len(dataset.X_train)}\")\n",
    "print(f\"Test samples: {len(dataset.X_test)}\")\n",
    "print(f\"Input shape: {dataset.X_train.shape}\")\n",
    "print(f\"Output shape: {dataset.y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b928ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Asset LSTM Model\n",
    "class MultiAssetLSTM(nn.Module):\n",
    "    def __init__(self, n_assets, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Shared LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_assets,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Output layer - predict all assets\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, n_assets)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, n_assets]\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden]\n",
    "        \n",
    "        # Attention\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.fc(context)\n",
    "        return output, attn_weights\n",
    "\n",
    "# Initialize model\n",
    "model = MultiAssetLSTM(\n",
    "    n_assets=dataset.n_assets,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with proper techniques\n",
    "def train_model(model, train_loader, test_loader, epochs=50, lr=0.001, patience=10):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                output, _ = model(X_batch)\n",
    "                test_loss += criterion(output, y_batch).item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, \"\n",
    "                  f\"Test Loss: {test_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# Train the model\n",
    "train_loader, test_loader = dataset.get_dataloaders(batch_size=32)\n",
    "\n",
    "print(\"Training Multi-Asset LSTM...\\n\")\n",
    "train_losses, test_losses = train_model(\n",
    "    model, train_loader, test_loader, \n",
    "    epochs=100, lr=0.001, patience=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(train_losses, label='Training Loss', linewidth=2)\n",
    "ax.plot(test_losses, label='Validation Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MSE Loss')\n",
    "ax.set_title('Multi-Asset LSTM Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8150af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_attention = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        output, attn = model(X_batch)\n",
    "        all_predictions.append(output.cpu().numpy())\n",
    "        all_targets.append(y_batch.numpy())\n",
    "        all_attention.append(attn.cpu().numpy())\n",
    "\n",
    "predictions = np.vstack(all_predictions)\n",
    "targets = np.vstack(all_targets)\n",
    "attention_weights = np.vstack(all_attention)\n",
    "\n",
    "# Inverse transform\n",
    "predictions_original = dataset.scaler.inverse_transform(predictions)\n",
    "targets_original = dataset.scaler.inverse_transform(targets)\n",
    "\n",
    "# Calculate metrics per asset\n",
    "print(\"Per-Asset Performance Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "for i, asset in enumerate(dataset.asset_names):\n",
    "    pred = predictions_original[:, i]\n",
    "    true = targets_original[:, i]\n",
    "    \n",
    "    mse = mean_squared_error(true, pred)\n",
    "    mae = mean_absolute_error(true, pred)\n",
    "    \n",
    "    # Directional accuracy\n",
    "    true_dir = np.sign(true)\n",
    "    pred_dir = np.sign(pred)\n",
    "    da = np.mean(true_dir == pred_dir)\n",
    "    \n",
    "    # Information coefficient\n",
    "    from scipy.stats import spearmanr\n",
    "    ic, _ = spearmanr(true, pred)\n",
    "    \n",
    "    results[asset] = {'MSE': mse, 'MAE': mae, 'DA': da, 'IC': ic}\n",
    "    print(f\"{asset:6} | MSE: {mse:.6f} | MAE: {mae:.6f} | DA: {da:.2%} | IC: {ic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cdac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "fig, axes = plt.subplots(len(dataset.asset_names), 1, figsize=(14, 3*len(dataset.asset_names)))\n",
    "\n",
    "for i, (ax, asset) in enumerate(zip(axes, dataset.asset_names)):\n",
    "    ax.plot(targets_original[:100, i] * 100, label='Actual', alpha=0.8, linewidth=1.5)\n",
    "    ax.plot(predictions_original[:100, i] * 100, label='Predicted', alpha=0.8, linewidth=1.5)\n",
    "    ax.set_title(f'{asset} - Returns Prediction (First 100 samples)')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "avg_attention = attention_weights.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(avg_attention)), avg_attention, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Time Step (Days Back)')\n",
    "plt.ylabel('Average Attention Weight')\n",
    "plt.title('LSTM Attention Pattern - Which Days Matter Most?')\n",
    "plt.xticks(range(0, len(avg_attention), 5))\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Highlight most important days\n",
    "top_days = np.argsort(avg_attention)[-5:]\n",
    "for day in top_days:\n",
    "    plt.bar(day, avg_attention[day], color='coral', alpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost important lookback days: {sorted(top_days)}\")\n",
    "print(f\"Most recent day attention: {avg_attention[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple trading strategy backtest\n",
    "def backtest_strategy(predictions, actual_returns, transaction_cost=0.001):\n",
    "    \"\"\"Backtest long-only strategy based on predictions\"\"\"\n",
    "    n_assets = predictions.shape[1]\n",
    "    \n",
    "    # Strategy: Go long assets with positive predicted return\n",
    "    signals = (predictions > 0).astype(float)\n",
    "    \n",
    "    # Equal weight among selected assets\n",
    "    weights = signals / (signals.sum(axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # Portfolio returns\n",
    "    portfolio_returns = (weights * actual_returns).sum(axis=1)\n",
    "    \n",
    "    # Transaction costs (when positions change)\n",
    "    turnover = np.abs(np.diff(weights, axis=0)).sum(axis=1)\n",
    "    costs = np.concatenate([[0], turnover * transaction_cost])\n",
    "    \n",
    "    net_returns = portfolio_returns - costs\n",
    "    cumulative_returns = (1 + net_returns).cumprod()\n",
    "    \n",
    "    # Metrics\n",
    "    total_return = cumulative_returns[-1] - 1\n",
    "    sharpe = np.mean(net_returns) / np.std(net_returns) * np.sqrt(252)\n",
    "    max_dd = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n",
    "    \n",
    "    return {\n",
    "        'cumulative_returns': cumulative_returns,\n",
    "        'total_return': total_return,\n",
    "        'sharpe_ratio': sharpe,\n",
    "        'max_drawdown': max_dd,\n",
    "        'daily_returns': net_returns\n",
    "    }\n",
    "\n",
    "# Run backtest\n",
    "strategy_results = backtest_strategy(predictions_original, targets_original)\n",
    "\n",
    "# Benchmark: Equal weight buy and hold\n",
    "equal_weight_returns = targets_original.mean(axis=1)\n",
    "benchmark_cum = (1 + equal_weight_returns).cumprod()\n",
    "\n",
    "print(\"Strategy Backtest Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total Return:    {strategy_results['total_return']:.2%}\")\n",
    "print(f\"Sharpe Ratio:    {strategy_results['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown:    {strategy_results['max_drawdown']:.2%}\")\n",
    "print(f\"\\nBenchmark (Equal Weight B&H):\")\n",
    "print(f\"Total Return:    {benchmark_cum[-1] - 1:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d791e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot strategy performance\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Cumulative returns\n",
    "axes[0].plot(strategy_results['cumulative_returns'], label='LSTM Strategy', linewidth=2)\n",
    "axes[0].plot(benchmark_cum, label='Equal Weight Benchmark', linewidth=2, alpha=0.7)\n",
    "axes[0].set_title('Strategy vs Benchmark Performance')\n",
    "axes[0].set_ylabel('Cumulative Return')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "drawdown = strategy_results['cumulative_returns'] / strategy_results['cumulative_returns'].cummax() - 1\n",
    "axes[1].fill_between(range(len(drawdown)), drawdown * 100, 0, alpha=0.5, color='red')\n",
    "axes[1].set_title('Strategy Drawdown')\n",
    "axes[1].set_xlabel('Trading Days')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c96f05",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Week 14 Summary\n",
    "\n",
    "## Key Concepts Covered This Week\n",
    "\n",
    "### Day 1: RNN Fundamentals\n",
    "- Recurrent architecture and hidden states\n",
    "- Backpropagation through time (BPTT)\n",
    "- Vanishing/exploding gradient problem\n",
    "\n",
    "### Day 2: LSTM Networks\n",
    "- Cell state and gating mechanisms\n",
    "- Forget, Input, and Output gates\n",
    "- Long-term dependency learning\n",
    "\n",
    "### Day 3: GRU Architecture\n",
    "- Simplified gating (Reset, Update)\n",
    "- Comparison with LSTM\n",
    "- When to use GRU vs LSTM\n",
    "\n",
    "### Day 4: Sequence-to-Sequence Models\n",
    "- Encoder-Decoder architecture\n",
    "- Multi-step forecasting\n",
    "- Attention mechanisms\n",
    "\n",
    "### Day 5: Advanced Techniques\n",
    "- Bidirectional LSTMs\n",
    "- Stacked LSTM layers\n",
    "- Regularization strategies\n",
    "\n",
    "### Day 6: Practical Applications\n",
    "- Price prediction\n",
    "- Volatility forecasting\n",
    "- Feature engineering for sequences\n",
    "\n",
    "### Day 7: Interview Review (Today)\n",
    "- 10 essential interview questions\n",
    "- Common mistakes to avoid\n",
    "- Multi-asset LSTM project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c96bf",
   "metadata": {},
   "source": [
    "## Quick Reference: LSTM Interview Cheat Sheet\n",
    "\n",
    "```\n",
    "LSTM GATES:\n",
    "├── Forget Gate: f_t = σ(W_f · [h_{t-1}, x_t] + b_f)\n",
    "├── Input Gate: i_t = σ(W_i · [h_{t-1}, x_t] + b_i)\n",
    "├── Cell Candidate: c̃_t = tanh(W_c · [h_{t-1}, x_t] + b_c)\n",
    "├── Cell Update: c_t = f_t * c_{t-1} + i_t * c̃_t\n",
    "├── Output Gate: o_t = σ(W_o · [h_{t-1}, x_t] + b_o)\n",
    "└── Hidden State: h_t = o_t * tanh(c_t)\n",
    "\n",
    "GRU GATES:\n",
    "├── Reset Gate: r_t = σ(W_r · [h_{t-1}, x_t] + b_r)\n",
    "├── Update Gate: z_t = σ(W_z · [h_{t-1}, x_t] + b_z)\n",
    "├── Candidate: h̃_t = tanh(W_h · [r_t * h_{t-1}, x_t] + b_h)\n",
    "└── Output: h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t\n",
    "\n",
    "PARAMETER COUNTS:\n",
    "├── LSTM: 4 * (input_size + hidden_size + 1) * hidden_size\n",
    "├── GRU:  3 * (input_size + hidden_size + 1) * hidden_size\n",
    "└── RNN:  (input_size + hidden_size + 1) * hidden_size\n",
    "\n",
    "KEY HYPERPARAMETERS:\n",
    "├── Sequence Length: 20-60 for daily data\n",
    "├── Hidden Size: 32-256\n",
    "├── Num Layers: 1-3\n",
    "├── Dropout: 0.1-0.5\n",
    "├── Learning Rate: 1e-4 to 1e-2\n",
    "└── Gradient Clip: 1.0-5.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9280b",
   "metadata": {},
   "source": [
    "## Week 14 Checklist\n",
    "\n",
    "### Core Concepts ✅\n",
    "- [ ] Understand RNN architecture and BPTT\n",
    "- [ ] Explain vanishing gradient problem\n",
    "- [ ] Describe LSTM gates in detail\n",
    "- [ ] Compare LSTM vs GRU\n",
    "- [ ] Implement attention mechanism\n",
    "\n",
    "### Practical Skills ✅\n",
    "- [ ] Build LSTM model in PyTorch\n",
    "- [ ] Handle variable-length sequences\n",
    "- [ ] Apply regularization techniques\n",
    "- [ ] Use gradient clipping\n",
    "- [ ] Implement early stopping\n",
    "\n",
    "### Financial Applications ✅\n",
    "- [ ] Preprocess time series correctly\n",
    "- [ ] Avoid data leakage\n",
    "- [ ] Choose appropriate sequence length\n",
    "- [ ] Evaluate with financial metrics\n",
    "- [ ] Backtest trading strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"WEEK 14 - DAY 7 COMPLETE: Interview Review\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n📚 Covered:\")\n",
    "print(\"   • 10 Essential RNN/LSTM Interview Questions\")\n",
    "print(\"   • 6 Common Mistakes in Sequence Modeling\")\n",
    "print(\"   • Multi-Asset LSTM Predictor Project\")\n",
    "print(\"\\n🎯 Mini-Project Results:\")\n",
    "print(f\"   • Assets: {', '.join(dataset.asset_names)}\")\n",
    "print(f\"   • Strategy Sharpe: {strategy_results['sharpe_ratio']:.2f}\")\n",
    "print(f\"   • Max Drawdown: {strategy_results['max_drawdown']:.1%}\")\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"   • Week 15: Attention & Transformers for Finance\")\n",
    "print(\"   • Practice implementing from scratch\")\n",
    "print(\"   • Review interview questions daily\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
