{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb423e2b",
   "metadata": {},
   "source": [
    "# Day 2: Word Embeddings for Financial Text Analysis\n",
    "## Word2Vec, GloVe, and Financial NLP Applications\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the theory behind word embeddings and distributed representations\n",
    "- Implement Word2Vec (Skip-gram and CBOW) for financial text\n",
    "- Work with pre-trained GloVe embeddings\n",
    "- Build custom financial word embeddings\n",
    "- Apply embeddings to sentiment analysis and document similarity\n",
    "\n",
    "## Why Word Embeddings in Finance?\n",
    "- Capture semantic relationships in financial text (e.g., \"bullish\" â‰ˆ \"optimistic\")\n",
    "- Enable numerical representation of news, earnings calls, SEC filings\n",
    "- Foundation for advanced NLP models in trading systems\n",
    "- Measure semantic similarity between financial documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3053d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d66c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Word embedding libraries\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5101987f",
   "metadata": {},
   "source": [
    "## 2. Theory: From One-Hot to Distributed Representations\n",
    "\n",
    "### The Problem with One-Hot Encoding\n",
    "- **Sparse**: Vector size = vocabulary size (can be 100K+ words)\n",
    "- **No semantics**: cos(\"bullish\", \"bearish\") = cos(\"bullish\", \"apple\") = 0\n",
    "- **No generalization**: Model can't leverage word relationships\n",
    "\n",
    "### Word Embeddings Solution\n",
    "- **Dense vectors**: Typically 50-300 dimensions\n",
    "- **Semantic meaning**: Similar words have similar vectors\n",
    "- **Learned from context**: \"You shall know a word by the company it keeps\" (Firth, 1957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba72f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate One-Hot vs Embedding representation\n",
    "\n",
    "# Simple vocabulary\n",
    "vocab = ['stock', 'bond', 'equity', 'bullish', 'bearish', 'market']\n",
    "\n",
    "# One-hot encoding\n",
    "print(\"ONE-HOT ENCODING\")\n",
    "print(\"=\"*50)\n",
    "for i, word in enumerate(vocab):\n",
    "    one_hot = np.zeros(len(vocab))\n",
    "    one_hot[i] = 1\n",
    "    print(f\"{word:10} -> {one_hot}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Problems:\")\n",
    "print(f\"- Vector dimension: {len(vocab)} (grows with vocabulary)\")\n",
    "print(f\"- Cosine similarity between any two words: 0\")\n",
    "print(f\"- No semantic information captured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothetical embedding (what we want to learn)\n",
    "print(\"\\nWORD EMBEDDINGS (Hypothetical)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simulated 4-dimensional embeddings\n",
    "embeddings = {\n",
    "    'stock':   np.array([0.8, 0.2, 0.6, 0.1]),\n",
    "    'equity':  np.array([0.7, 0.3, 0.6, 0.2]),  # Similar to stock\n",
    "    'bond':    np.array([0.3, 0.8, 0.4, 0.1]),\n",
    "    'bullish': np.array([0.5, 0.4, 0.9, 0.8]),\n",
    "    'bearish': np.array([0.5, 0.4, 0.1, 0.2]),  # Opposite sentiment\n",
    "    'market':  np.array([0.6, 0.5, 0.5, 0.5]),\n",
    "}\n",
    "\n",
    "for word, vec in embeddings.items():\n",
    "    print(f\"{word:10} -> {vec}\")\n",
    "\n",
    "# Calculate similarities\n",
    "print(\"\\nCosine Similarities:\")\n",
    "print(f\"stock-equity: {cosine_similarity([embeddings['stock']], [embeddings['equity']])[0,0]:.3f}\")\n",
    "print(f\"stock-bond:   {cosine_similarity([embeddings['stock']], [embeddings['bond']])[0,0]:.3f}\")\n",
    "print(f\"bullish-bearish: {cosine_similarity([embeddings['bullish']], [embeddings['bearish']])[0,0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512492c",
   "metadata": {},
   "source": [
    "## 3. Word2Vec: Architecture and Training\n",
    "\n",
    "### Two Architectures:\n",
    "\n",
    "**1. Skip-gram**: Predict context words from center word\n",
    "- Input: \"earnings\"\n",
    "- Output: [\"quarterly\", \"report\", \"beat\", \"expectations\"]\n",
    "- Better for rare words, smaller datasets\n",
    "\n",
    "**2. CBOW (Continuous Bag of Words)**: Predict center word from context\n",
    "- Input: [\"quarterly\", \"report\", \"beat\", \"expectations\"]\n",
    "- Output: \"earnings\"\n",
    "- Faster training, better for frequent words\n",
    "\n",
    "### Key Hyperparameters:\n",
    "- `vector_size`: Embedding dimension (100-300 typical)\n",
    "- `window`: Context window size (5-10 for finance)\n",
    "- `min_count`: Minimum word frequency threshold\n",
    "- `sg`: 0=CBOW, 1=Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5542252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample financial corpus\n",
    "financial_corpus = [\n",
    "    \"The stock market rallied on strong earnings reports from major tech companies.\",\n",
    "    \"Investors turned bullish after the Federal Reserve signaled interest rate cuts.\",\n",
    "    \"Bond yields fell as traders anticipated looser monetary policy.\",\n",
    "    \"The equity market experienced high volatility amid trade war concerns.\",\n",
    "    \"Hedge funds increased their bullish positions in technology stocks.\",\n",
    "    \"The bearish sentiment drove stock prices lower across all sectors.\",\n",
    "    \"Market analysts expect the bull market to continue through next quarter.\",\n",
    "    \"Corporate earnings exceeded analyst expectations driving stock prices higher.\",\n",
    "    \"The Federal Reserve maintained its hawkish stance on inflation.\",\n",
    "    \"Treasury yields spiked following stronger than expected jobs data.\",\n",
    "    \"Investors rotated from growth stocks to value stocks.\",\n",
    "    \"The market correction wiped out gains from the previous quarter.\",\n",
    "    \"Options traders bet on increased volatility ahead of earnings season.\",\n",
    "    \"The company announced a stock buyback program worth billions.\",\n",
    "    \"Dividend stocks outperformed during the market downturn.\",\n",
    "    \"Credit spreads widened signaling increased default risk.\",\n",
    "    \"The IPO market remained strong with several high profile listings.\",\n",
    "    \"Portfolio managers reduced equity exposure and increased cash holdings.\",\n",
    "    \"Market makers reported record trading volumes during the selloff.\",\n",
    "    \"The central bank intervention stabilized currency markets.\",\n",
    "    \"Quantitative trading firms capitalized on market inefficiencies.\",\n",
    "    \"Risk parity strategies underperformed during the volatility spike.\",\n",
    "    \"Momentum stocks led the market rally while value lagged.\",\n",
    "    \"The yield curve inverted raising recession concerns.\",\n",
    "    \"Algorithmic trading accounted for majority of daily volume.\",\n",
    "    \"Institutional investors accumulated positions in defensive sectors.\",\n",
    "    \"The company beat earnings estimates and raised forward guidance.\",\n",
    "    \"Short sellers covered positions as the stock price surged.\",\n",
    "    \"Market sentiment turned negative on geopolitical tensions.\",\n",
    "    \"The tech sector led the market higher on strong revenue growth.\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(financial_corpus)} sentences\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for sent in financial_corpus[:3]:\n",
    "    print(f\"  - {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing for financial text\n",
    "\n",
    "def preprocess_financial_text(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Preprocess financial text for word embedding training.\n",
    "    \n",
    "    For word embeddings, we often keep stopwords because:\n",
    "    - Context matters for learning word relationships\n",
    "    - Skip-gram/CBOW use surrounding words\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters but keep important financial symbols\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess corpus\n",
    "processed_corpus = [preprocess_financial_text(sent) for sent in financial_corpus]\n",
    "\n",
    "print(\"Original:\", financial_corpus[0])\n",
    "print(\"Processed:\", processed_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect bigrams (two-word phrases) - important for finance\n",
    "# Examples: \"interest_rate\", \"federal_reserve\", \"stock_market\"\n",
    "\n",
    "# Train phrase detector\n",
    "phrases = Phrases(processed_corpus, min_count=2, threshold=5)\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "# Apply bigrams to corpus\n",
    "corpus_with_bigrams = [bigram[sent] for sent in processed_corpus]\n",
    "\n",
    "print(\"Without bigrams:\", processed_corpus[0])\n",
    "print(\"With bigrams:\", corpus_with_bigrams[0])\n",
    "\n",
    "# Check detected phrases\n",
    "print(\"\\nDetected phrases:\")\n",
    "for phrase, score in phrases.find_phrases(processed_corpus).items():\n",
    "    print(f\"  {phrase}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde74f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model - Skip-gram\n",
    "\n",
    "w2v_skipgram = Word2Vec(\n",
    "    sentences=corpus_with_bigrams,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window\n",
    "    min_count=1,          # Minimum word frequency (low for small corpus)\n",
    "    workers=4,            # Parallel training threads\n",
    "    sg=1,                 # Skip-gram (1) vs CBOW (0)\n",
    "    epochs=100,           # Training epochs (more for small corpus)\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Skip-gram Model Summary:\")\n",
    "print(f\"  Vocabulary size: {len(w2v_skipgram.wv)}\")\n",
    "print(f\"  Embedding dimension: {w2v_skipgram.wv.vector_size}\")\n",
    "print(f\"  Training words: {w2v_skipgram.corpus_total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d513675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model - CBOW\n",
    "\n",
    "w2v_cbow = Word2Vec(\n",
    "    sentences=corpus_with_bigrams,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=0,                 # CBOW\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"CBOW Model Summary:\")\n",
    "print(f\"  Vocabulary size: {len(w2v_cbow.wv)}\")\n",
    "print(f\"  Embedding dimension: {w2v_cbow.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore learned embeddings\n",
    "\n",
    "def explore_embeddings(model, word):\n",
    "    \"\"\"Explore word embedding properties.\"\"\"\n",
    "    if word not in model.wv:\n",
    "        print(f\"'{word}' not in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"Vector shape: {model.wv[word].shape}\")\n",
    "    print(f\"Vector (first 10 dims): {model.wv[word][:10].round(3)}\")\n",
    "    print(f\"\\nMost similar words:\")\n",
    "    for similar_word, similarity in model.wv.most_similar(word, topn=5):\n",
    "        print(f\"  {similar_word}: {similarity:.3f}\")\n",
    "\n",
    "# Explore financial terms\n",
    "print(\"=\"*60)\n",
    "explore_embeddings(w2v_skipgram, 'market')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "explore_embeddings(w2v_skipgram, 'bullish')\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "explore_embeddings(w2v_skipgram, 'stock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0315f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word analogies: A is to B as C is to ?\n",
    "# Classic example: king - man + woman = queen\n",
    "# Financial: bullish - positive + negative = bearish?\n",
    "\n",
    "def word_analogy(model, word_a, word_b, word_c):\n",
    "    \"\"\"\n",
    "    Find word D such that: A is to B as C is to D\n",
    "    Computed as: D = B - A + C\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[word_b, word_c],\n",
    "            negative=[word_a],\n",
    "            topn=3\n",
    "        )\n",
    "        print(f\"{word_a} : {word_b} :: {word_c} : ?\")\n",
    "        for word, score in result:\n",
    "            print(f\"  -> {word} ({score:.3f})\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Word not in vocabulary: {e}\")\n",
    "\n",
    "# Note: With small corpus, analogies may not work well\n",
    "# This demonstrates the concept\n",
    "print(\"Word Analogies (limited by small corpus):\")\n",
    "print(\"=\"*50)\n",
    "word_analogy(w2v_skipgram, 'stock', 'stocks', 'market')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31b647",
   "metadata": {},
   "source": [
    "## 4. Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, words=None, method='tsne', perplexity=5):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings in 2D using t-SNE or PCA.\n",
    "    \"\"\"\n",
    "    if words is None:\n",
    "        words = list(model.wv.key_to_index.keys())[:50]\n",
    "    \n",
    "    # Filter words that exist in vocabulary\n",
    "    words = [w for w in words if w in model.wv]\n",
    "    \n",
    "    # Get vectors\n",
    "    vectors = np.array([model.wv[w] for w in words])\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(perplexity, len(words)-1))\n",
    "    else:\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    coords = reducer.fit_transform(vectors)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], alpha=0.7, s=100)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        ax.annotate(word, (coords[i, 0], coords[i, 1]), \n",
    "                   fontsize=10, alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f'Word Embeddings Visualization ({method.upper()})', fontsize=14)\n",
    "    ax.set_xlabel('Dimension 1')\n",
    "    ax.set_ylabel('Dimension 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize all words\n",
    "visualize_embeddings(w2v_skipgram, method='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed95d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific financial word groups\n",
    "financial_words = [\n",
    "    'stock', 'stocks', 'equity', 'market', 'bond', 'bonds',\n",
    "    'bullish', 'bearish', 'rally', 'selloff',\n",
    "    'earnings', 'revenue', 'growth', 'profit',\n",
    "    'volatility', 'risk', 'trading', 'investors'\n",
    "]\n",
    "\n",
    "# Filter to words in vocabulary\n",
    "available_words = [w for w in financial_words if w in w2v_skipgram.wv]\n",
    "print(f\"Available words: {available_words}\")\n",
    "\n",
    "if len(available_words) >= 5:\n",
    "    visualize_embeddings(w2v_skipgram, words=available_words, method='pca')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe0b1d",
   "metadata": {},
   "source": [
    "## 5. GloVe: Global Vectors for Word Representation\n",
    "\n",
    "### Key Differences from Word2Vec:\n",
    "- **Global statistics**: Uses word co-occurrence matrix from entire corpus\n",
    "- **Objective**: Weighted least squares on log co-occurrence counts\n",
    "- **Formula**: $w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j = \\log(X_{ij})$\n",
    "\n",
    "### Advantages:\n",
    "- Better captures global corpus statistics\n",
    "- Often performs better on word analogy tasks\n",
    "- Pre-trained on massive corpora (Wikipedia, Common Crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f491923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "# Available models: glove-wiki-gigaword-50, glove-wiki-gigaword-100, \n",
    "#                   glove-wiki-gigaword-200, glove-wiki-gigaword-300\n",
    "\n",
    "print(\"Loading pre-trained GloVe embeddings (this may take a moment)...\")\n",
    "try:\n",
    "    glove = api.load('glove-wiki-gigaword-100')  # 100-dimensional GloVe\n",
    "    print(f\"\\nGloVe Model loaded successfully!\")\n",
    "    print(f\"  Vocabulary size: {len(glove)}\")\n",
    "    print(f\"  Embedding dimension: {glove.vector_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GloVe: {e}\")\n",
    "    print(\"Continuing with Word2Vec models only.\")\n",
    "    glove = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore GloVe embeddings for financial terms\n",
    "if glove is not None:\n",
    "    financial_terms = ['stock', 'bond', 'equity', 'market', 'bullish', \n",
    "                       'bearish', 'rally', 'crash', 'inflation', 'recession']\n",
    "    \n",
    "    print(\"Financial Term Similarities (GloVe):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for term in financial_terms:\n",
    "        if term in glove:\n",
    "            print(f\"\\n'{term}' most similar to:\")\n",
    "            for word, sim in glove.most_similar(term, topn=5):\n",
    "                print(f\"    {word}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8313fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word analogies with GloVe - much better with large pre-trained model\n",
    "if glove is not None:\n",
    "    print(\"Word Analogies with GloVe:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    analogies = [\n",
    "        ('man', 'woman', 'king'),           # Classic: king - man + woman = queen\n",
    "        ('stock', 'stocks', 'bond'),        # Singular to plural\n",
    "        ('buy', 'sell', 'long'),            # Trading opposites: long/short\n",
    "        ('profit', 'loss', 'gain'),         # Financial opposites\n",
    "        ('company', 'ceo', 'country'),      # Leadership analogy\n",
    "    ]\n",
    "    \n",
    "    for a, b, c in analogies:\n",
    "        try:\n",
    "            result = glove.most_similar(positive=[b, c], negative=[a], topn=3)\n",
    "            print(f\"\\n{a} : {b} :: {c} : ?\")\n",
    "            for word, score in result:\n",
    "                print(f\"    -> {word} ({score:.3f})\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Word not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf2870",
   "metadata": {},
   "source": [
    "## 6. Financial Sentiment Lexicon with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ac8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build financial sentiment lexicon using embeddings\n",
    "\n",
    "# Seed words for positive and negative sentiment\n",
    "positive_seeds = ['bullish', 'growth', 'profit', 'gain', 'surge', 'rally', \n",
    "                  'outperform', 'upgrade', 'strong', 'beat']\n",
    "negative_seeds = ['bearish', 'decline', 'loss', 'drop', 'crash', 'selloff',\n",
    "                  'underperform', 'downgrade', 'weak', 'miss']\n",
    "\n",
    "def expand_sentiment_lexicon(model, seed_words, topn=10):\n",
    "    \"\"\"\n",
    "    Expand sentiment lexicon using word embeddings.\n",
    "    Find words similar to seed words.\n",
    "    \"\"\"\n",
    "    # Filter seeds that exist in vocabulary\n",
    "    valid_seeds = [w for w in seed_words if w in model]\n",
    "    \n",
    "    if not valid_seeds:\n",
    "        return []\n",
    "    \n",
    "    # Find similar words\n",
    "    expanded = set(valid_seeds)\n",
    "    for seed in valid_seeds:\n",
    "        similar = model.most_similar(seed, topn=topn)\n",
    "        for word, score in similar:\n",
    "            if score > 0.5:  # Similarity threshold\n",
    "                expanded.add(word)\n",
    "    \n",
    "    return list(expanded)\n",
    "\n",
    "if glove is not None:\n",
    "    # Expand lexicons\n",
    "    expanded_positive = expand_sentiment_lexicon(glove, positive_seeds)\n",
    "    expanded_negative = expand_sentiment_lexicon(glove, negative_seeds)\n",
    "    \n",
    "    print(\"Expanded Positive Sentiment Lexicon:\")\n",
    "    print(f\"  Original: {len(positive_seeds)} words\")\n",
    "    print(f\"  Expanded: {len(expanded_positive)} words\")\n",
    "    print(f\"  Sample: {list(expanded_positive)[:15]}\")\n",
    "    \n",
    "    print(\"\\nExpanded Negative Sentiment Lexicon:\")\n",
    "    print(f\"  Original: {len(negative_seeds)} words\")\n",
    "    print(f\"  Expanded: {len(expanded_negative)} words\")\n",
    "    print(f\"  Sample: {list(expanded_negative)[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment scoring function using embeddings\n",
    "\n",
    "def embedding_sentiment_score(text, model, pos_seeds, neg_seeds):\n",
    "    \"\"\"\n",
    "    Calculate sentiment score using word embedding similarity.\n",
    "    \n",
    "    Approach: Compare text words to positive/negative seed centroids.\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    tokens = preprocess_financial_text(text)\n",
    "    tokens = [t for t in tokens if t in model]\n",
    "    \n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate centroids\n",
    "    valid_pos = [w for w in pos_seeds if w in model]\n",
    "    valid_neg = [w for w in neg_seeds if w in model]\n",
    "    \n",
    "    if not valid_pos or not valid_neg:\n",
    "        return 0.0\n",
    "    \n",
    "    pos_centroid = np.mean([model[w] for w in valid_pos], axis=0)\n",
    "    neg_centroid = np.mean([model[w] for w in valid_neg], axis=0)\n",
    "    \n",
    "    # Calculate text embedding (average of word vectors)\n",
    "    text_embedding = np.mean([model[t] for t in tokens], axis=0)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    pos_sim = cosine_similarity([text_embedding], [pos_centroid])[0, 0]\n",
    "    neg_sim = cosine_similarity([text_embedding], [neg_centroid])[0, 0]\n",
    "    \n",
    "    # Sentiment score: difference between positive and negative similarity\n",
    "    return pos_sim - neg_sim\n",
    "\n",
    "# Test on sample headlines\n",
    "if glove is not None:\n",
    "    test_headlines = [\n",
    "        \"Stock market surges on strong earnings beat\",\n",
    "        \"Markets crash amid recession fears\",\n",
    "        \"Company reports steady growth in quarterly revenue\",\n",
    "        \"Massive selloff wipes out gains\",\n",
    "        \"Investors optimistic about economic outlook\",\n",
    "        \"Concerns grow over rising inflation\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Embedding-based Sentiment Scores:\")\n",
    "    print(\"=\"*60)\n",
    "    for headline in test_headlines:\n",
    "        score = embedding_sentiment_score(headline, glove, positive_seeds, negative_seeds)\n",
    "        sentiment = \"POSITIVE\" if score > 0.02 else \"NEGATIVE\" if score < -0.02 else \"NEUTRAL\"\n",
    "        print(f\"{score:+.4f} [{sentiment:8}] {headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43956583",
   "metadata": {},
   "source": [
    "## 7. Document Embeddings for Financial Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document embedding methods\n",
    "\n",
    "def document_embedding_average(text, model):\n",
    "    \"\"\"\n",
    "    Simple average of word vectors.\n",
    "    Fast but loses word order and importance.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_financial_text(text)\n",
    "    vectors = [model[t] for t in tokens if t in model]\n",
    "    \n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "def document_embedding_tfidf_weighted(text, model, idf_weights=None):\n",
    "    \"\"\"\n",
    "    TF-IDF weighted average of word vectors.\n",
    "    Emphasizes important/rare words.\n",
    "    \"\"\"\n",
    "    tokens = preprocess_financial_text(text)\n",
    "    \n",
    "    if idf_weights is None:\n",
    "        # Simple frequency-based weighting if no IDF available\n",
    "        weights = {t: 1.0 for t in tokens}\n",
    "    else:\n",
    "        weights = {t: idf_weights.get(t, 1.0) for t in tokens}\n",
    "    \n",
    "    weighted_vectors = []\n",
    "    total_weight = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in model:\n",
    "            weight = weights[token]\n",
    "            weighted_vectors.append(model[token] * weight)\n",
    "            total_weight += weight\n",
    "    \n",
    "    if not weighted_vectors or total_weight == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.sum(weighted_vectors, axis=0) / total_weight\n",
    "\n",
    "\n",
    "# Test document embeddings\n",
    "if glove is not None:\n",
    "    doc1 = \"The stock market rallied strongly on positive earnings surprises.\"\n",
    "    doc2 = \"Equity markets surged following better than expected corporate profits.\"\n",
    "    doc3 = \"Bond yields fell as traders sought safe haven assets.\"\n",
    "    \n",
    "    emb1 = document_embedding_average(doc1, glove)\n",
    "    emb2 = document_embedding_average(doc2, glove)\n",
    "    emb3 = document_embedding_average(doc3, glove)\n",
    "    \n",
    "    print(\"Document Similarity (Cosine):\")\n",
    "    print(f\"  Doc1 vs Doc2: {cosine_similarity([emb1], [emb2])[0,0]:.4f} (similar topic)\")\n",
    "    print(f\"  Doc1 vs Doc3: {cosine_similarity([emb1], [emb3])[0,0]:.4f} (different topic)\")\n",
    "    print(f\"  Doc2 vs Doc3: {cosine_similarity([emb2], [emb3])[0,0]:.4f} (different topic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e840b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial document clustering using embeddings\n",
    "\n",
    "if glove is not None:\n",
    "    sample_docs = [\n",
    "        # Earnings related\n",
    "        \"Company beat earnings expectations and raised guidance.\",\n",
    "        \"Quarterly profits exceeded analyst estimates significantly.\",\n",
    "        \"Revenue growth accelerated in the latest quarter.\",\n",
    "        \n",
    "        # Market movement related\n",
    "        \"Stock market indices hit record highs today.\",\n",
    "        \"Markets rallied on optimism about trade deal.\",\n",
    "        \"Equity indices surged in afternoon trading.\",\n",
    "        \n",
    "        # Economic policy related\n",
    "        \"Federal Reserve signaled potential rate cuts ahead.\",\n",
    "        \"Central bank maintains accommodative monetary policy.\",\n",
    "        \"Interest rate decision expected next week.\",\n",
    "        \n",
    "        # Risk related\n",
    "        \"Market volatility spiked on geopolitical tensions.\",\n",
    "        \"Investors flee to safe haven assets amid uncertainty.\",\n",
    "        \"Risk appetite declined sharply this week.\"\n",
    "    ]\n",
    "    \n",
    "    # Create document embeddings\n",
    "    doc_embeddings = np.array([document_embedding_average(doc, glove) for doc in sample_docs])\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(doc_embeddings)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=[f\"Doc{i+1}\" for i in range(len(sample_docs))],\n",
    "                yticklabels=[f\"Doc{i+1}\" for i in range(len(sample_docs))],\n",
    "                annot=True, fmt='.2f', cmap='RdYlGn', center=0.5)\n",
    "    plt.title('Document Similarity Matrix\\n(Docs 1-3: Earnings, 4-6: Market, 7-9: Policy, 10-12: Risk)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df77e6",
   "metadata": {},
   "source": [
    "## 8. Training Domain-Specific Financial Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate larger financial corpus for better embeddings\n",
    "# In practice, you would use SEC filings, news articles, earnings calls\n",
    "\n",
    "extended_financial_corpus = financial_corpus + [\n",
    "    # More market movement text\n",
    "    \"The S&P 500 index rose two percent on strong economic data.\",\n",
    "    \"NASDAQ composite fell sharply amid tech selloff.\",\n",
    "    \"Dow Jones industrial average reached new highs.\",\n",
    "    \"Small cap stocks outperformed large caps this quarter.\",\n",
    "    \"Emerging markets rallied on dollar weakness.\",\n",
    "    \n",
    "    # Earnings and company news\n",
    "    \"The company reported record quarterly earnings per share.\",\n",
    "    \"Management raised full year revenue guidance.\",\n",
    "    \"Cost cutting measures improved profit margins.\",\n",
    "    \"New product launches drove top line growth.\",\n",
    "    \"Restructuring charges impacted bottom line results.\",\n",
    "    \n",
    "    # Macroeconomic\n",
    "    \"Employment data showed stronger than expected job growth.\",\n",
    "    \"Inflation remained elevated above central bank target.\",\n",
    "    \"Consumer spending increased despite rising prices.\",\n",
    "    \"Manufacturing sector contracted for third straight month.\",\n",
    "    \"Housing market showed signs of cooling.\",\n",
    "    \n",
    "    # Trading and positioning\n",
    "    \"Institutional investors increased their equity allocations.\",\n",
    "    \"Hedge funds reduced net long exposure to technology.\",\n",
    "    \"Options activity suggested elevated uncertainty.\",\n",
    "    \"Short interest declined from recent peaks.\",\n",
    "    \"Trading volumes surged on expiration day.\",\n",
    "    \n",
    "    # Fixed income\n",
    "    \"Investment grade credit spreads tightened.\",\n",
    "    \"High yield bond issuance reached record levels.\",\n",
    "    \"Duration risk increased in bond portfolios.\",\n",
    "    \"Floating rate loans gained investor interest.\",\n",
    "    \"Sovereign debt concerns resurfaced in Europe.\"\n",
    "]\n",
    "\n",
    "print(f\"Extended corpus size: {len(extended_financial_corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1649497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess extended corpus\n",
    "processed_extended = [preprocess_financial_text(sent) for sent in extended_financial_corpus]\n",
    "\n",
    "# Detect bigrams\n",
    "phrases_extended = Phrases(processed_extended, min_count=2, threshold=3)\n",
    "bigram_extended = Phraser(phrases_extended)\n",
    "corpus_extended_bigrams = [bigram_extended[sent] for sent in processed_extended]\n",
    "\n",
    "# Train improved model\n",
    "w2v_financial = Word2Vec(\n",
    "    sentences=corpus_extended_bigrams,\n",
    "    vector_size=100,\n",
    "    window=7,             # Larger window for financial context\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,                 # Skip-gram\n",
    "    epochs=200,           # More epochs for small corpus\n",
    "    negative=10,          # More negative samples\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Financial Word2Vec Model:\")\n",
    "print(f\"  Vocabulary size: {len(w2v_financial.wv)}\")\n",
    "print(f\"  Vector dimension: {w2v_financial.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare custom financial model with GloVe\n",
    "\n",
    "def compare_embeddings(word, custom_model, pretrained_model):\n",
    "    \"\"\"Compare word similarities between custom and pretrained models.\"\"\"\n",
    "    print(f\"\\nSimilar words to '{word}':\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"Custom Financial Model:\")\n",
    "    if word in custom_model.wv:\n",
    "        for w, s in custom_model.wv.most_similar(word, topn=5):\n",
    "            print(f\"    {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"    Word not in vocabulary\")\n",
    "    \n",
    "    print(\"\\nPre-trained GloVe:\")\n",
    "    if pretrained_model and word in pretrained_model:\n",
    "        for w, s in pretrained_model.most_similar(word, topn=5):\n",
    "            print(f\"    {w}: {s:.3f}\")\n",
    "    else:\n",
    "        print(\"    Word not in vocabulary or model not loaded\")\n",
    "\n",
    "# Compare for financial terms\n",
    "print(\"=\"*60)\n",
    "print(\"EMBEDDING COMPARISON: Custom vs Pre-trained\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for term in ['market', 'earnings', 'volatility']:\n",
    "    compare_embeddings(term, w2v_financial, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b304667",
   "metadata": {},
   "source": [
    "## 9. Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Financial News Similarity Search\n",
    "\n",
    "class FinancialNewsSimilarity:\n",
    "    \"\"\"\n",
    "    Find similar financial news using word embeddings.\n",
    "    Useful for: news clustering, duplicate detection, related article recommendation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        self.model = embedding_model\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"Add documents to the search index.\"\"\"\n",
    "        self.documents = documents\n",
    "        self.embeddings = [\n",
    "            document_embedding_average(doc, self.model) \n",
    "            for doc in documents\n",
    "        ]\n",
    "        self.embeddings = np.array(self.embeddings)\n",
    "    \n",
    "    def find_similar(self, query, topn=5):\n",
    "        \"\"\"Find documents most similar to query.\"\"\"\n",
    "        query_embedding = document_embedding_average(query, self.model)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        \n",
    "        # Get top matches\n",
    "        top_indices = np.argsort(similarities)[-topn:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Demo\n",
    "if glove is not None:\n",
    "    news_search = FinancialNewsSimilarity(glove)\n",
    "    news_search.add_documents(extended_financial_corpus)\n",
    "    \n",
    "    query = \"Stock prices increased after positive earnings announcement\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"\\nMost Similar Documents:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for result in news_search.find_similar(query, topn=5):\n",
    "        print(f\"  [{result['similarity']:.3f}] {result['document']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 2: Financial Concept Clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_financial_terms(model, terms, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Cluster financial terms using their embeddings.\n",
    "    \"\"\"\n",
    "    # Filter terms in vocabulary\n",
    "    valid_terms = [t for t in terms if t in model]\n",
    "    \n",
    "    if len(valid_terms) < n_clusters:\n",
    "        print(\"Not enough valid terms for clustering\")\n",
    "        return None\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = np.array([model[t] for t in valid_terms])\n",
    "    \n",
    "    # Cluster\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Group terms by cluster\n",
    "    cluster_groups = {i: [] for i in range(n_clusters)}\n",
    "    for term, cluster in zip(valid_terms, clusters):\n",
    "        cluster_groups[cluster].append(term)\n",
    "    \n",
    "    return cluster_groups\n",
    "\n",
    "# Financial terms to cluster\n",
    "if glove is not None:\n",
    "    financial_terms = [\n",
    "        # Asset classes\n",
    "        'stock', 'bond', 'equity', 'commodity', 'currency', 'option',\n",
    "        # Sentiment\n",
    "        'bullish', 'bearish', 'optimistic', 'pessimistic',\n",
    "        # Market actions\n",
    "        'buy', 'sell', 'hold', 'trade', 'invest',\n",
    "        # Performance\n",
    "        'profit', 'loss', 'gain', 'return', 'yield',\n",
    "        # Market state\n",
    "        'rally', 'crash', 'correction', 'volatility', 'stability',\n",
    "        # Economic\n",
    "        'inflation', 'recession', 'growth', 'employment', 'gdp'\n",
    "    ]\n",
    "    \n",
    "    clusters = cluster_financial_terms(glove, financial_terms, n_clusters=5)\n",
    "    \n",
    "    if clusters:\n",
    "        print(\"Financial Term Clusters:\")\n",
    "        print(\"=\"*60)\n",
    "        for cluster_id, terms in clusters.items():\n",
    "            print(f\"\\nCluster {cluster_id + 1}:\")\n",
    "            print(f\"  {', '.join(terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 3: Out-of-Vocabulary (OOV) Handling\n",
    "\n",
    "def handle_oov(word, model, method='similar'):\n",
    "    \"\"\"\n",
    "    Handle out-of-vocabulary words.\n",
    "    \n",
    "    Methods:\n",
    "    - 'zero': Return zero vector\n",
    "    - 'similar': Find most similar in-vocabulary word\n",
    "    - 'subword': Average of character n-gram embeddings (simplified)\n",
    "    \"\"\"\n",
    "    if word in model:\n",
    "        return model[word], word\n",
    "    \n",
    "    if method == 'zero':\n",
    "        return np.zeros(model.vector_size), '<UNK>'\n",
    "    \n",
    "    elif method == 'similar':\n",
    "        # Find most similar word in vocabulary (simple edit distance approach)\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for vocab_word in list(model.key_to_index.keys())[:10000]:  # Check first 10k words\n",
    "            # Simple character overlap score\n",
    "            overlap = len(set(word) & set(vocab_word)) / max(len(word), len(vocab_word))\n",
    "            if overlap > best_score:\n",
    "                best_score = overlap\n",
    "                best_match = vocab_word\n",
    "        \n",
    "        if best_match:\n",
    "            return model[best_match], f\"{best_match} (substitute for {word})\"\n",
    "        return np.zeros(model.vector_size), '<UNK>'\n",
    "    \n",
    "    return np.zeros(model.vector_size), '<UNK>'\n",
    "\n",
    "# Test OOV handling\n",
    "if glove is not None:\n",
    "    oov_words = ['cryptocurrency', 'fintech', 'defi', 'nft']\n",
    "    \n",
    "    print(\"Out-of-Vocabulary Handling:\")\n",
    "    print(\"=\"*60)\n",
    "    for word in oov_words:\n",
    "        in_vocab = word in glove\n",
    "        if not in_vocab:\n",
    "            _, substitute = handle_oov(word, glove, method='similar')\n",
    "            print(f\"  '{word}': OOV -> {substitute}\")\n",
    "        else:\n",
    "            print(f\"  '{word}': In vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101633e0",
   "metadata": {},
   "source": [
    "## 10. Model Persistence and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96612b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load Word2Vec models\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "models_dir = 'models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save full model (can continue training)\n",
    "model_path = os.path.join(models_dir, 'financial_w2v.model')\n",
    "w2v_financial.save(model_path)\n",
    "print(f\"Full model saved to: {model_path}\")\n",
    "\n",
    "# Save only word vectors (smaller, faster loading)\n",
    "vectors_path = os.path.join(models_dir, 'financial_w2v.vectors')\n",
    "w2v_financial.wv.save(vectors_path)\n",
    "print(f\"Vectors saved to: {vectors_path}\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = Word2Vec.load(model_path)\n",
    "print(f\"\\nLoaded model vocabulary size: {len(loaded_model.wv)}\")\n",
    "\n",
    "# Load only vectors (lightweight)\n",
    "loaded_vectors = KeyedVectors.load(vectors_path)\n",
    "print(f\"Loaded vectors vocabulary size: {len(loaded_vectors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a63dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "\n",
    "best_practices = \"\"\"\n",
    "WORD EMBEDDINGS BEST PRACTICES FOR FINANCE\n",
    "============================================\n",
    "\n",
    "1. DATA PREPARATION\n",
    "   - Use domain-specific text (SEC filings, earnings calls, news)\n",
    "   - Preserve financial bigrams (interest_rate, earnings_per_share)\n",
    "   - Consider keeping stopwords for embedding training\n",
    "   - Clean but don't over-process financial terminology\n",
    "\n",
    "2. MODEL SELECTION\n",
    "   - Word2Vec Skip-gram: Better for rare financial terms\n",
    "   - Word2Vec CBOW: Faster, better for common terms\n",
    "   - GloVe pre-trained: Good baseline, transfer learning\n",
    "   - FastText: Better OOV handling with subword information\n",
    "\n",
    "3. HYPERPARAMETERS\n",
    "   - vector_size: 100-300 (100 often sufficient for finance)\n",
    "   - window: 5-10 (larger for capturing financial context)\n",
    "   - min_count: 5-10 for large corpus, 1-2 for small\n",
    "   - epochs: More for smaller corpora\n",
    "\n",
    "4. EVALUATION\n",
    "   - Word similarity tasks (financial synonyms)\n",
    "   - Analogy tasks (stock:stocks::bond:?)\n",
    "   - Downstream task performance (sentiment, classification)\n",
    "   - Qualitative inspection of nearest neighbors\n",
    "\n",
    "5. DEPLOYMENT\n",
    "   - Save KeyedVectors for inference (smaller, faster)\n",
    "   - Consider quantization for production\n",
    "   - Handle OOV words gracefully\n",
    "   - Version control your models\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f476e",
   "metadata": {},
   "source": [
    "## 11. Interview Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "interview_questions = \"\"\"\n",
    "WORD EMBEDDINGS INTERVIEW QUESTIONS\n",
    "====================================\n",
    "\n",
    "CONCEPTUAL:\n",
    "\n",
    "Q1: Explain the difference between Word2Vec Skip-gram and CBOW.\n",
    "A: Skip-gram predicts context words given center word (better for rare words).\n",
    "   CBOW predicts center word given context (faster, better for frequent words).\n",
    "\n",
    "Q2: Why do word embeddings capture semantic meaning?\n",
    "A: Words in similar contexts have similar embeddings. The distributional \n",
    "   hypothesis: \"You shall know a word by the company it keeps.\" Training\n",
    "   optimizes vectors so semantically related words are geometrically close.\n",
    "\n",
    "Q3: What's the advantage of GloVe over Word2Vec?\n",
    "A: GloVe uses global co-occurrence statistics (entire corpus at once),\n",
    "   while Word2Vec uses local context windows. GloVe often performs better\n",
    "   on analogy tasks and can be more efficient to train.\n",
    "\n",
    "Q4: How would you handle out-of-vocabulary words in production?\n",
    "A: Options include: (1) Return zero/random vector, (2) Use subword \n",
    "   embeddings (FastText), (3) Find similar in-vocabulary word, \n",
    "   (4) Use character-level models, (5) Hash trick for unknown words.\n",
    "\n",
    "PRACTICAL:\n",
    "\n",
    "Q5: How would you build a financial sentiment analyzer using embeddings?\n",
    "A: Create positive/negative seed word centroids, compute document embedding\n",
    "   as weighted average of word vectors, measure cosine similarity to \n",
    "   each centroid. The closer centroid determines sentiment.\n",
    "\n",
    "Q6: What's a good embedding dimension for financial NLP?\n",
    "A: 100-300 dimensions typically work well. 100d often sufficient for \n",
    "   finance-specific tasks. Larger dimensions may overfit on small corpora.\n",
    "   Validate using downstream task performance.\n",
    "\n",
    "Q7: How do you evaluate embedding quality?\n",
    "A: (1) Intrinsic: word similarity correlation, analogy accuracy\n",
    "   (2) Extrinsic: downstream task performance (sentiment, classification)\n",
    "   (3) Qualitative: inspect nearest neighbors for known relationships\n",
    "\n",
    "Q8: When would you train custom embeddings vs use pre-trained?\n",
    "A: Custom: Large domain corpus, domain-specific vocabulary, \n",
    "   specialized semantics (financial jargon means different things).\n",
    "   Pre-trained: Limited data, general concepts, quick prototyping.\n",
    "   Hybrid: Fine-tune pre-trained on domain data.\n",
    "\"\"\"\n",
    "\n",
    "print(interview_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225dee99",
   "metadata": {},
   "source": [
    "## 12. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercises = \"\"\"\n",
    "PRACTICE EXERCISES\n",
    "==================\n",
    "\n",
    "Exercise 1: Custom Financial Embeddings\n",
    "---------------------------------------\n",
    "Collect 1000+ sentences from financial news sources.\n",
    "Train Word2Vec with different hyperparameters.\n",
    "Compare: vector_size (50, 100, 200), window (3, 5, 10), sg (0, 1).\n",
    "Evaluate using financial word similarity and analogy tasks.\n",
    "\n",
    "Exercise 2: Sector Classification\n",
    "---------------------------------\n",
    "Create document embeddings for company descriptions.\n",
    "Use k-means or hierarchical clustering to group by sector.\n",
    "Evaluate cluster quality against known sector labels.\n",
    "\n",
    "Exercise 3: News Similarity Engine\n",
    "-----------------------------------\n",
    "Build a news article similarity search system.\n",
    "Index 100+ financial news articles with embeddings.\n",
    "Implement efficient similarity search (approximate nearest neighbors).\n",
    "Test with various query types.\n",
    "\n",
    "Exercise 4: Embedding Visualization Dashboard\n",
    "----------------------------------------------\n",
    "Create interactive visualization of financial term embeddings.\n",
    "Allow filtering by category (assets, sentiment, actions).\n",
    "Show nearest neighbors on hover.\n",
    "Use Plotly or Bokeh for interactivity.\n",
    "\n",
    "Exercise 5: Transfer Learning Comparison\n",
    "-----------------------------------------\n",
    "Compare sentiment classification accuracy using:\n",
    "(a) Random initialization\n",
    "(b) GloVe embeddings\n",
    "(c) Financial-specific embeddings\n",
    "(d) Fine-tuned embeddings\n",
    "Document the performance differences.\n",
    "\"\"\"\n",
    "\n",
    "print(exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41cb40",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Word Embeddings Theory**: From one-hot to distributed representations\n",
    "2. **Word2Vec**: Skip-gram and CBOW architectures, training process\n",
    "3. **GloVe**: Global vectors using co-occurrence statistics\n",
    "4. **Financial Applications**: Sentiment analysis, document similarity, clustering\n",
    "5. **Best Practices**: Preprocessing, hyperparameters, evaluation, deployment\n",
    "\n",
    "### Next Steps:\n",
    "- Day 3: Sentiment Analysis with Transformers (BERT, FinBERT)\n",
    "- Day 4: Named Entity Recognition for Financial Text\n",
    "- Day 5: News-based Trading Signals\n",
    "\n",
    "### Resources:\n",
    "- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)\n",
    "- [GloVe Paper](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- [Gensim Documentation](https://radimrehurek.com/gensim/)\n",
    "- [Financial Word Embeddings Research](https://arxiv.org/abs/2006.08997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# Remove models directory if desired\n",
    "# shutil.rmtree('models', ignore_errors=True)\n",
    "\n",
    "print(\"Day 2 Complete: Word Embeddings for Financial Text Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key takeaways:\")\n",
    "print(\"  1. Word embeddings capture semantic relationships\")\n",
    "print(\"  2. Word2Vec learns from local context windows\")\n",
    "print(\"  3. GloVe uses global co-occurrence statistics\")\n",
    "print(\"  4. Domain-specific training improves financial NLP\")\n",
    "print(\"  5. Document embeddings enable similarity search\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
