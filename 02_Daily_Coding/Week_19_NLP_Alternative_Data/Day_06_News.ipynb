{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502ae9b4",
   "metadata": {},
   "source": [
    "# Day 06: News-Based Trading Signals & Event Detection\n",
    "\n",
    "## Week 19: NLP & Alternative Data\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "1. **News Data Processing**: Acquire, clean, and structure financial news data\n",
    "2. **Sentiment Analysis**: Apply NLP models to score news sentiment\n",
    "3. **Event Detection**: Identify market-moving events using NER and classification\n",
    "4. **Signal Generation**: Convert news sentiment into actionable trading signals\n",
    "5. **Backtesting**: Evaluate news-based strategies with realistic assumptions\n",
    "\n",
    "### Topics Covered\n",
    "- Financial news data sources and APIs\n",
    "- Text preprocessing for financial text\n",
    "- Sentiment analysis with VADER, TextBlob, and FinBERT\n",
    "- Named Entity Recognition (NER) for financial entities\n",
    "- Event classification (earnings, M&A, regulatory)\n",
    "- News momentum and decay effects\n",
    "- Event study methodology\n",
    "- Interview preparation questions\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight**: News-based trading exploits the information asymmetry between when news is published and when markets fully price in the information. Success depends on speed of processing, accuracy of sentiment analysis, and proper handling of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da33bd",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK for text processing\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# TextBlob for sentiment\n",
    "try:\n",
    "    from textblob import TextBlob\n",
    "except ImportError:\n",
    "    print(\"Install textblob: pip install textblob\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "\n",
    "print(\"âœ… Core libraries loaded successfully!\")\n",
    "print(f\"ðŸ“… Current Date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Transformers for advanced NLP (FinBERT)\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"âœ… Transformers library available for FinBERT sentiment analysis\")\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"âš ï¸ Transformers not installed. Install with: pip install transformers torch\")\n",
    "\n",
    "# Optional: spaCy for NER\n",
    "try:\n",
    "    import spacy\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        SPACY_AVAILABLE = True\n",
    "        print(\"âœ… spaCy loaded with en_core_web_sm model\")\n",
    "    except OSError:\n",
    "        SPACY_AVAILABLE = False\n",
    "        print(\"âš ï¸ spaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    print(\"âš ï¸ spaCy not installed. Install with: pip install spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce13f4",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess News Data\n",
    "\n",
    "### 2.1 Understanding News Data Sources\n",
    "\n",
    "**Common Financial News Data Sources:**\n",
    "- **Premium APIs**: Bloomberg, Reuters, Dow Jones, RavenPack\n",
    "- **Free/Affordable**: Alpha Vantage News, Finnhub, NewsAPI, GDELT\n",
    "- **Social/Alternative**: Twitter/X, Reddit, StockTwits\n",
    "- **SEC Filings**: 8-K filings for material events\n",
    "\n",
    "**Key Data Fields:**\n",
    "- `timestamp`: Publication time (critical for trading)\n",
    "- `headline`: Short title (most important for quick signals)\n",
    "- `summary`: Brief description\n",
    "- `body`: Full article text\n",
    "- `source`: News provider\n",
    "- `tickers`: Associated stock symbols\n",
    "- `categories`: News type (earnings, M&A, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic financial news dataset for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample financial news headlines with various sentiment patterns\n",
    "news_data = {\n",
    "    'timestamp': pd.date_range(start='2025-01-01', periods=100, freq='4H'),\n",
    "    'ticker': np.random.choice(['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA'], 100),\n",
    "    'headline': [\n",
    "        # Positive news\n",
    "        \"Apple Reports Record-Breaking Q4 Earnings, Beats Estimates\",\n",
    "        \"Microsoft Cloud Revenue Surges 30% in Quarterly Results\",\n",
    "        \"Google Announces Revolutionary AI Partnership Worth $10B\",\n",
    "        \"Amazon Prime Membership Hits All-Time High\",\n",
    "        \"Tesla Deliveries Exceed Analyst Expectations by 15%\",\n",
    "        \"Meta's VR Division Shows Strong User Growth\",\n",
    "        \"NVIDIA Secures Major Government AI Contract\",\n",
    "        \"Apple Stock Upgraded to Strong Buy by Goldman Sachs\",\n",
    "        \"Microsoft Azure Wins Massive Pentagon Cloud Deal\",\n",
    "        \"Google Search Market Share Reaches New Peak\",\n",
    "        # Negative news\n",
    "        \"Apple Faces Major Supply Chain Disruptions in China\",\n",
    "        \"Microsoft Layoffs Expand to 10,000 Employees\",\n",
    "        \"Google Hit with $5 Billion Antitrust Fine in EU\",\n",
    "        \"Amazon Workers Strike Affects Holiday Deliveries\",\n",
    "        \"Tesla Recalls 500,000 Vehicles Over Safety Concerns\",\n",
    "        \"Meta Faces Regulatory Scrutiny Over Privacy Violations\",\n",
    "        \"NVIDIA GPU Shortage Worsens Amid Demand Surge\",\n",
    "        \"Apple iPhone Sales Decline for Third Consecutive Quarter\",\n",
    "        \"Microsoft Earnings Miss Analyst Expectations\",\n",
    "        \"Google Advertising Revenue Growth Slows Significantly\",\n",
    "        # Neutral/Mixed news\n",
    "        \"Apple Plans New Product Launch Event Next Month\",\n",
    "        \"Microsoft CEO Discusses Future of Remote Work\",\n",
    "        \"Google Reorganizes Cloud Division Leadership\",\n",
    "        \"Amazon Opens New Distribution Center in Texas\",\n",
    "        \"Tesla Updates Autopilot Software Features\",\n",
    "        \"Meta Announces Changes to Content Moderation Policy\",\n",
    "        \"NVIDIA Releases New Graphics Card Lineup\",\n",
    "        \"Apple Expands Services to New Markets\",\n",
    "        \"Microsoft Invests in Quantum Computing Research\",\n",
    "        \"Google Maps Adds New Navigation Features\",\n",
    "    ] * 3 + [\n",
    "        \"Apple and Microsoft Announce Strategic Partnership\",\n",
    "        \"Fed Decision Could Impact Tech Stocks\",\n",
    "        \"Semiconductor Industry Faces New Challenges\",\n",
    "        \"Tech Giants Report Mixed Quarterly Results\",\n",
    "        \"Market Volatility Affects Tech Sector\",\n",
    "        \"New Regulations May Impact Big Tech Companies\",\n",
    "        \"AI Investment Boom Continues Across Tech Sector\",\n",
    "        \"Cloud Computing Market Reaches $500B Milestone\",\n",
    "        \"Cybersecurity Concerns Rise for Tech Companies\",\n",
    "        \"Tech IPO Market Shows Signs of Recovery\"\n",
    "    ],\n",
    "    'source': np.random.choice(['Reuters', 'Bloomberg', 'CNBC', 'WSJ', 'MarketWatch'], 100),\n",
    "    'category': np.random.choice(['Earnings', 'M&A', 'Product', 'Legal', 'Market', 'Personnel'], 100)\n",
    "}\n",
    "\n",
    "news_df = pd.DataFrame(news_data)\n",
    "news_df = news_df.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "news_df['news_id'] = range(len(news_df))\n",
    "\n",
    "print(f\"ðŸ“° Loaded {len(news_df)} news articles\")\n",
    "print(f\"ðŸ“… Date range: {news_df['timestamp'].min()} to {news_df['timestamp'].max()}\")\n",
    "print(f\"ðŸ“Š Tickers covered: {news_df['ticker'].nunique()}\")\n",
    "news_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b546989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic price data aligned with news\n",
    "def generate_price_data(tickers, start_date, periods):\n",
    "    \"\"\"Generate synthetic price data with news-driven movements.\"\"\"\n",
    "    price_data = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Random walk with drift\n",
    "        returns = np.random.normal(0.0005, 0.02, periods)\n",
    "        prices = 100 * np.cumprod(1 + returns)\n",
    "        \n",
    "        dates = pd.date_range(start=start_date, periods=periods, freq='H')\n",
    "        \n",
    "        for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "            price_data.append({\n",
    "                'timestamp': date,\n",
    "                'ticker': ticker,\n",
    "                'price': price,\n",
    "                'volume': np.random.randint(1000000, 10000000)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(price_data)\n",
    "\n",
    "# Generate hourly price data\n",
    "tickers = news_df['ticker'].unique()\n",
    "price_df = generate_price_data(tickers, '2025-01-01', 400)\n",
    "\n",
    "print(f\"ðŸ’¹ Generated {len(price_df)} price observations\")\n",
    "print(f\"ðŸ“ˆ Price data sample:\")\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12646cf6",
   "metadata": {},
   "source": [
    "### 2.2 Handling News Timestamps and Market Hours\n",
    "\n",
    "**Critical Considerations:**\n",
    "- News published after market close affects next day's open\n",
    "- Pre-market news (4am-9:30am ET) affects opening prices\n",
    "- After-hours news (4pm-8pm ET) affects next day\n",
    "- Weekend news accumulates until Monday open\n",
    "\n",
    "**Timestamp Alignment Strategy:**\n",
    "```\n",
    "News Time          â†’  Effective Trading Time\n",
    "Before 9:30am ET   â†’  Same day open (9:30am)\n",
    "9:30am - 4:00pm ET â†’  Current trading session\n",
    "4:00pm - Midnight  â†’  Next trading day open\n",
    "Weekends/Holidays  â†’  Next trading day open\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_news_to_trading_time(news_timestamp):\n",
    "    \"\"\"\n",
    "    Align news publication time to effective trading time.\n",
    "    Assumes US Eastern Time and NYSE trading hours.\n",
    "    \"\"\"\n",
    "    # Market hours (simplified - doesn't account for holidays)\n",
    "    market_open = 9.5  # 9:30 AM\n",
    "    market_close = 16  # 4:00 PM\n",
    "    \n",
    "    hour = news_timestamp.hour + news_timestamp.minute / 60\n",
    "    day_of_week = news_timestamp.weekday()\n",
    "    \n",
    "    # Weekend news â†’ Monday open\n",
    "    if day_of_week >= 5:  # Saturday or Sunday\n",
    "        days_until_monday = 7 - day_of_week\n",
    "        next_monday = news_timestamp + timedelta(days=days_until_monday)\n",
    "        return next_monday.replace(hour=9, minute=30, second=0, microsecond=0)\n",
    "    \n",
    "    # Pre-market news â†’ Same day open\n",
    "    if hour < market_open:\n",
    "        return news_timestamp.replace(hour=9, minute=30, second=0, microsecond=0)\n",
    "    \n",
    "    # During market hours â†’ Current time\n",
    "    if market_open <= hour < market_close:\n",
    "        return news_timestamp\n",
    "    \n",
    "    # After hours â†’ Next day open\n",
    "    next_day = news_timestamp + timedelta(days=1)\n",
    "    if next_day.weekday() >= 5:  # If next day is weekend\n",
    "        days_until_monday = 7 - next_day.weekday()\n",
    "        next_day = next_day + timedelta(days=days_until_monday)\n",
    "    return next_day.replace(hour=9, minute=30, second=0, microsecond=0)\n",
    "\n",
    "# Apply alignment\n",
    "news_df['effective_time'] = news_df['timestamp'].apply(align_news_to_trading_time)\n",
    "news_df['news_delay_hours'] = (news_df['effective_time'] - news_df['timestamp']).dt.total_seconds() / 3600\n",
    "\n",
    "print(\"ðŸ“Š News timing analysis:\")\n",
    "print(f\"Average delay to effective time: {news_df['news_delay_hours'].mean():.2f} hours\")\n",
    "print(f\"Max delay (weekend news): {news_df['news_delay_hours'].max():.2f} hours\")\n",
    "news_df[['timestamp', 'effective_time', 'news_delay_hours', 'headline']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e4f1b",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning and Tokenization\n",
    "\n",
    "### 3.1 Text Preprocessing Pipeline\n",
    "\n",
    "Financial text preprocessing requires special handling:\n",
    "- Preserve financial terminology (P/E, M&A, Q1, 10-K)\n",
    "- Keep numbers and percentages (they carry meaning!)\n",
    "- Handle ticker symbols ($AAPL, AAPL)\n",
    "- Process company name variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Specialized text preprocessor for financial news.\n",
    "    Preserves financially relevant information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # Remove financially relevant words from stopwords\n",
    "        financial_keep = {'up', 'down', 'above', 'below', 'over', 'under', 'high', 'low', 'more', 'less'}\n",
    "        self.stop_words = self.stop_words - financial_keep\n",
    "        \n",
    "        # Financial terms to preserve\n",
    "        self.financial_terms = {\n",
    "            'etf', 'ipo', 'pe', 'eps', 'ebitda', 'gaap', 'sec', 'fda', 'ftc',\n",
    "            'ceo', 'cfo', 'coo', 'fed', 'fomc', 'gdp', 'cpi', 'pmi', 'm&a',\n",
    "            'q1', 'q2', 'q3', 'q4', 'fy', 'yoy', 'qoq', 'ytd', 'roi', 'roa'\n",
    "        }\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning while preserving financial information.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Normalize ticker symbols ($AAPL â†’ AAPL)\n",
    "        text = re.sub(r'\\$([A-Za-z]+)', r'\\1', text)\n",
    "        \n",
    "        # Preserve percentages and numbers with context\n",
    "        text = re.sub(r'(\\d+)%', r'\\1 percent', text)\n",
    "        text = re.sub(r'\\$(\\d+(?:\\.\\d+)?)\\s*(million|billion|trillion)?', \n",
    "                      r'\\1 \\2 dollars', text)\n",
    "        \n",
    "        # Remove special characters but keep apostrophes and hyphens\n",
    "        text = re.sub(r\"[^\\w\\s\\'\\-]\", ' ', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text with financial awareness.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        \n",
    "        # Filter tokens\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # Keep financial terms\n",
    "            if token.lower() in self.financial_terms:\n",
    "                filtered.append(token.lower())\n",
    "            # Keep if not a stopword and has meaningful length\n",
    "            elif token.lower() not in self.stop_words and len(token) > 2:\n",
    "                filtered.append(token.lower())\n",
    "            # Keep numbers\n",
    "            elif token.replace('.', '').replace(',', '').isdigit():\n",
    "                filtered.append(token)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def get_ngrams(self, text, n=2):\n",
    "        \"\"\"Extract n-grams from text.\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FinancialTextPreprocessor()\n",
    "\n",
    "# Test preprocessing\n",
    "sample_headline = \"Apple ($AAPL) Reports 15% Revenue Growth in Q4, Beats Wall Street Estimates by $2 Billion\"\n",
    "print(f\"Original: {sample_headline}\")\n",
    "print(f\"Cleaned: {preprocessor.clean_text(sample_headline)}\")\n",
    "print(f\"Tokens: {preprocessor.tokenize(sample_headline)}\")\n",
    "print(f\"Bigrams: {preprocessor.get_ngrams(sample_headline, 2)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to news dataset\n",
    "news_df['cleaned_headline'] = news_df['headline'].apply(preprocessor.clean_text)\n",
    "news_df['tokens'] = news_df['headline'].apply(preprocessor.tokenize)\n",
    "news_df['token_count'] = news_df['tokens'].apply(len)\n",
    "\n",
    "# Analyze token distribution\n",
    "print(\"ðŸ“Š Token Analysis:\")\n",
    "print(f\"Average tokens per headline: {news_df['token_count'].mean():.1f}\")\n",
    "print(f\"Max tokens: {news_df['token_count'].max()}\")\n",
    "print(f\"Min tokens: {news_df['token_count'].min()}\")\n",
    "\n",
    "# Most common words across all headlines\n",
    "all_tokens = [token for tokens in news_df['tokens'] for token in tokens]\n",
    "word_freq = Counter(all_tokens)\n",
    "print(f\"\\nðŸ”¤ Top 15 most common words:\")\n",
    "for word, count in word_freq.most_common(15):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf103a79",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis on News Headlines\n",
    "\n",
    "### 4.1 Sentiment Analysis Approaches\n",
    "\n",
    "**Three Common Methods:**\n",
    "\n",
    "| Method | Pros | Cons | Speed |\n",
    "|--------|------|------|-------|\n",
    "| **VADER** | Pre-built, good for social media | Not trained on financial text | âš¡ Very Fast |\n",
    "| **TextBlob** | Simple API, polarity + subjectivity | Generic, not financial-specific | âš¡ Fast |\n",
    "| **FinBERT** | Trained on financial text, most accurate | Slow, requires GPU for scale | ðŸ¢ Slow |\n",
    "\n",
    "**Key Insight**: For high-frequency news trading, speed matters. VADER/TextBlob for quick signals, FinBERT for overnight batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc3d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Multi-method sentiment analyzer optimized for financial text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # VADER sentiment analyzer\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Enhance VADER lexicon with financial terms\n",
    "        financial_lexicon = {\n",
    "            'beat': 2.0, 'beats': 2.0, 'exceeded': 2.0, 'surged': 2.5,\n",
    "            'soared': 2.5, 'jumped': 2.0, 'rallied': 2.0, 'upgraded': 2.0,\n",
    "            'outperformed': 2.0, 'record': 1.5, 'growth': 1.5, 'profit': 1.5,\n",
    "            'gain': 1.5, 'gains': 1.5, 'bullish': 2.5, 'optimistic': 2.0,\n",
    "            \n",
    "            'missed': -2.0, 'miss': -2.0, 'fell': -1.5, 'dropped': -1.5,\n",
    "            'plunged': -2.5, 'crashed': -3.0, 'downgraded': -2.0,\n",
    "            'underperformed': -2.0, 'loss': -2.0, 'losses': -2.0,\n",
    "            'decline': -1.5, 'declined': -1.5, 'bearish': -2.5,\n",
    "            'recall': -2.0, 'lawsuit': -1.5, 'fine': -1.5, 'fined': -1.5,\n",
    "            'layoffs': -2.0, 'layoff': -2.0, 'bankruptcy': -3.0,\n",
    "            'investigation': -1.5, 'fraud': -3.0, 'scandal': -2.5,\n",
    "            \n",
    "            'volatility': -0.5, 'uncertain': -0.5, 'mixed': 0.0\n",
    "        }\n",
    "        self.vader.lexicon.update(financial_lexicon)\n",
    "        \n",
    "    def vader_sentiment(self, text):\n",
    "        \"\"\"Get VADER sentiment scores.\"\"\"\n",
    "        scores = self.vader.polarity_scores(text)\n",
    "        return {\n",
    "            'compound': scores['compound'],\n",
    "            'positive': scores['pos'],\n",
    "            'negative': scores['neg'],\n",
    "            'neutral': scores['neu']\n",
    "        }\n",
    "    \n",
    "    def textblob_sentiment(self, text):\n",
    "        \"\"\"Get TextBlob sentiment scores.\"\"\"\n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            'polarity': blob.sentiment.polarity,  # -1 to 1\n",
    "            'subjectivity': blob.sentiment.subjectivity  # 0 to 1\n",
    "        }\n",
    "    \n",
    "    def classify_sentiment(self, compound_score, thresholds=(-0.15, 0.15)):\n",
    "        \"\"\"Classify sentiment based on compound score.\"\"\"\n",
    "        neg_thresh, pos_thresh = thresholds\n",
    "        if compound_score >= pos_thresh:\n",
    "            return 'positive'\n",
    "        elif compound_score <= neg_thresh:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"Complete sentiment analysis.\"\"\"\n",
    "        vader = self.vader_sentiment(text)\n",
    "        textblob = self.textblob_sentiment(text)\n",
    "        \n",
    "        return {\n",
    "            'vader_compound': vader['compound'],\n",
    "            'vader_pos': vader['positive'],\n",
    "            'vader_neg': vader['negative'],\n",
    "            'textblob_polarity': textblob['polarity'],\n",
    "            'textblob_subjectivity': textblob['subjectivity'],\n",
    "            'sentiment_class': self.classify_sentiment(vader['compound'])\n",
    "        }\n",
    "\n",
    "# Initialize analyzer\n",
    "sentiment_analyzer = FinancialSentimentAnalyzer()\n",
    "\n",
    "# Test on sample headlines\n",
    "test_headlines = [\n",
    "    \"Apple Reports Record-Breaking Q4 Earnings, Beats Estimates\",\n",
    "    \"Tesla Recalls 500,000 Vehicles Over Safety Concerns\",\n",
    "    \"Microsoft Announces New Product Launch Event\"\n",
    "]\n",
    "\n",
    "print(\"ðŸŽ¯ Sentiment Analysis Examples:\\n\")\n",
    "for headline in test_headlines:\n",
    "    result = sentiment_analyzer.analyze(headline)\n",
    "    print(f\"Headline: {headline}\")\n",
    "    print(f\"  VADER Compound: {result['vader_compound']:.3f}\")\n",
    "    print(f\"  TextBlob Polarity: {result['textblob_polarity']:.3f}\")\n",
    "    print(f\"  Classification: {result['sentiment_class'].upper()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to entire dataset\n",
    "sentiment_results = news_df['headline'].apply(sentiment_analyzer.analyze)\n",
    "sentiment_df = pd.DataFrame(sentiment_results.tolist())\n",
    "\n",
    "# Merge with original data\n",
    "news_df = pd.concat([news_df, sentiment_df], axis=1)\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"ðŸ“Š Sentiment Distribution:\")\n",
    "print(news_df['sentiment_class'].value_counts())\n",
    "print(f\"\\nðŸ“ˆ Average VADER Compound: {news_df['vader_compound'].mean():.3f}\")\n",
    "print(f\"ðŸ“ˆ Average TextBlob Polarity: {news_df['textblob_polarity'].mean():.3f}\")\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Histogram of VADER compound scores\n",
    "axes[0].hist(news_df['vader_compound'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', label='Neutral')\n",
    "axes[0].axvline(x=0.15, color='green', linestyle='--', alpha=0.5, label='Positive threshold')\n",
    "axes[0].axvline(x=-0.15, color='orange', linestyle='--', alpha=0.5, label='Negative threshold')\n",
    "axes[0].set_xlabel('VADER Compound Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Sentiment Scores')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sentiment class pie chart\n",
    "sentiment_counts = news_df['sentiment_class'].value_counts()\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray'}\n",
    "axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "            colors=[colors[c] for c in sentiment_counts.index])\n",
    "axes[1].set_title('Sentiment Classification')\n",
    "\n",
    "# VADER vs TextBlob correlation\n",
    "axes[2].scatter(news_df['vader_compound'], news_df['textblob_polarity'], alpha=0.5, color='purple')\n",
    "axes[2].set_xlabel('VADER Compound')\n",
    "axes[2].set_ylabel('TextBlob Polarity')\n",
    "axes[2].set_title('VADER vs TextBlob Correlation')\n",
    "correlation = news_df['vader_compound'].corr(news_df['textblob_polarity'])\n",
    "axes[2].text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=axes[2].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e193a32",
   "metadata": {},
   "source": [
    "### 4.2 FinBERT for Financial Sentiment (Advanced)\n",
    "\n",
    "FinBERT is a BERT model pre-trained on financial text, providing state-of-the-art accuracy for financial sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86412077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_sentiment(headlines, batch_size=16):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using FinBERT model.\n",
    "    More accurate but slower than VADER/TextBlob.\n",
    "    \"\"\"\n",
    "    if not TRANSFORMERS_AVAILABLE:\n",
    "        print(\"âš ï¸ Transformers not available. Using VADER fallback.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load FinBERT model\n",
    "        finbert = pipeline(\"sentiment-analysis\", \n",
    "                          model=\"ProsusAI/finbert\",\n",
    "                          tokenizer=\"ProsusAI/finbert\",\n",
    "                          truncation=True,\n",
    "                          max_length=512)\n",
    "        \n",
    "        results = []\n",
    "        for i in range(0, len(headlines), batch_size):\n",
    "            batch = headlines[i:i+batch_size].tolist()\n",
    "            batch_results = finbert(batch)\n",
    "            results.extend(batch_results)\n",
    "        \n",
    "        # Convert to scores\n",
    "        sentiment_scores = []\n",
    "        for result in results:\n",
    "            label = result['label']\n",
    "            score = result['score']\n",
    "            \n",
    "            if label == 'positive':\n",
    "                sentiment_scores.append(score)\n",
    "            elif label == 'negative':\n",
    "                sentiment_scores.append(-score)\n",
    "            else:\n",
    "                sentiment_scores.append(0)\n",
    "        \n",
    "        return sentiment_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ FinBERT error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example FinBERT usage (only run if transformers available)\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    print(\"ðŸ¤– Running FinBERT on sample headlines...\")\n",
    "    sample_headlines = news_df['headline'].head(10)\n",
    "    finbert_scores = finbert_sentiment(sample_headlines)\n",
    "    \n",
    "    if finbert_scores:\n",
    "        for headline, score in zip(sample_headlines, finbert_scores):\n",
    "            print(f\"[{score:+.3f}] {headline[:60]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸ FinBERT not available. Skipping advanced sentiment analysis.\")\n",
    "    print(\"   Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714a317",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition for Event Detection\n",
    "\n",
    "NER helps identify key entities in news:\n",
    "- **Organizations**: Company names, regulators (SEC, FDA)\n",
    "- **People**: CEOs, executives, analysts\n",
    "- **Monetary Values**: Deal sizes, earnings figures\n",
    "- **Dates**: Event dates, guidance periods\n",
    "- **Locations**: Geographic markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab684e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialEntityExtractor:\n",
    "    \"\"\"\n",
    "    Extract financial entities from news text.\n",
    "    Uses spaCy if available, otherwise regex-based extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use_spacy = SPACY_AVAILABLE\n",
    "        if self.use_spacy:\n",
    "            self.nlp = nlp\n",
    "        \n",
    "        # Regex patterns for fallback\n",
    "        self.patterns = {\n",
    "            'money': re.compile(r'\\$\\d+(?:\\.\\d+)?(?:\\s*(?:million|billion|trillion|M|B|T))?', re.I),\n",
    "            'percentage': re.compile(r'\\d+(?:\\.\\d+)?%'),\n",
    "            'ticker': re.compile(r'\\b[A-Z]{2,5}\\b'),\n",
    "            'quarter': re.compile(r'\\b(?:Q[1-4]|FY\\d{2,4})\\b', re.I),\n",
    "            'date': re.compile(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:,?\\s+\\d{4})?\\b', re.I)\n",
    "        }\n",
    "        \n",
    "        # Known company mappings\n",
    "        self.company_tickers = {\n",
    "            'apple': 'AAPL', 'microsoft': 'MSFT', 'google': 'GOOGL',\n",
    "            'alphabet': 'GOOGL', 'amazon': 'AMZN', 'tesla': 'TSLA',\n",
    "            'meta': 'META', 'facebook': 'META', 'nvidia': 'NVDA'\n",
    "        }\n",
    "        \n",
    "    def extract_entities_spacy(self, text):\n",
    "        \"\"\"Extract entities using spaCy NER.\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = {\n",
    "            'organizations': [],\n",
    "            'people': [],\n",
    "            'money': [],\n",
    "            'dates': [],\n",
    "            'locations': []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'ORG':\n",
    "                entities['organizations'].append(ent.text)\n",
    "            elif ent.label_ == 'PERSON':\n",
    "                entities['people'].append(ent.text)\n",
    "            elif ent.label_ == 'MONEY':\n",
    "                entities['money'].append(ent.text)\n",
    "            elif ent.label_ in ('DATE', 'TIME'):\n",
    "                entities['dates'].append(ent.text)\n",
    "            elif ent.label_ in ('GPE', 'LOC'):\n",
    "                entities['locations'].append(ent.text)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def extract_entities_regex(self, text):\n",
    "        \"\"\"Extract entities using regex patterns.\"\"\"\n",
    "        entities = {\n",
    "            'organizations': [],\n",
    "            'money': self.patterns['money'].findall(text),\n",
    "            'percentages': self.patterns['percentage'].findall(text),\n",
    "            'tickers': self.patterns['ticker'].findall(text),\n",
    "            'quarters': self.patterns['quarter'].findall(text)\n",
    "        }\n",
    "        \n",
    "        # Find company names\n",
    "        text_lower = text.lower()\n",
    "        for company, ticker in self.company_tickers.items():\n",
    "            if company in text_lower:\n",
    "                entities['organizations'].append(company.title())\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def extract(self, text):\n",
    "        \"\"\"Extract all entities from text.\"\"\"\n",
    "        if self.use_spacy:\n",
    "            spacy_entities = self.extract_entities_spacy(text)\n",
    "            regex_entities = self.extract_entities_regex(text)\n",
    "            # Merge results\n",
    "            spacy_entities['percentages'] = regex_entities['percentages']\n",
    "            spacy_entities['quarters'] = regex_entities['quarters']\n",
    "            return spacy_entities\n",
    "        else:\n",
    "            return self.extract_entities_regex(text)\n",
    "    \n",
    "    def identify_tickers(self, text):\n",
    "        \"\"\"Identify stock tickers mentioned in text.\"\"\"\n",
    "        tickers = set()\n",
    "        \n",
    "        # Direct ticker mentions\n",
    "        text_upper = text.upper()\n",
    "        known_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA']\n",
    "        for ticker in known_tickers:\n",
    "            if ticker in text_upper:\n",
    "                tickers.add(ticker)\n",
    "        \n",
    "        # Company name to ticker mapping\n",
    "        text_lower = text.lower()\n",
    "        for company, ticker in self.company_tickers.items():\n",
    "            if company in text_lower:\n",
    "                tickers.add(ticker)\n",
    "        \n",
    "        return list(tickers)\n",
    "\n",
    "# Initialize entity extractor\n",
    "entity_extractor = FinancialEntityExtractor()\n",
    "\n",
    "# Test NER\n",
    "test_text = \"Apple CEO Tim Cook announced $10 billion investment in Q4 2025. Microsoft responded with their own AI initiative worth $5.5 billion.\"\n",
    "entities = entity_extractor.extract(test_text)\n",
    "\n",
    "print(\"ðŸ” Named Entity Recognition Results:\\n\")\n",
    "print(f\"Text: {test_text}\\n\")\n",
    "for entity_type, values in entities.items():\n",
    "    if values:\n",
    "        print(f\"  {entity_type.upper()}: {values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85123056",
   "metadata": {},
   "source": [
    "## 6. Building a News Sentiment Scoring System\n",
    "\n",
    "### 6.1 Aggregating Sentiment by Ticker and Time Window\n",
    "\n",
    "Key considerations:\n",
    "- **Time decay**: Recent news is more relevant than old news\n",
    "- **Source weighting**: Reuters/Bloomberg > random blog\n",
    "- **Volume weighting**: More coverage = more significant event\n",
    "- **Sentiment momentum**: Change in sentiment direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be15e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSentimentAggregator:\n",
    "    \"\"\"\n",
    "    Aggregate news sentiment into trading signals by ticker and time window.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, decay_halflife_hours=24, source_weights=None):\n",
    "        self.decay_halflife = decay_halflife_hours\n",
    "        self.source_weights = source_weights or {\n",
    "            'Reuters': 1.0,\n",
    "            'Bloomberg': 1.0,\n",
    "            'WSJ': 0.9,\n",
    "            'CNBC': 0.8,\n",
    "            'MarketWatch': 0.7\n",
    "        }\n",
    "        \n",
    "    def calculate_decay_weight(self, hours_old):\n",
    "        \"\"\"Exponential decay weight based on news age.\"\"\"\n",
    "        return np.exp(-np.log(2) * hours_old / self.decay_halflife)\n",
    "    \n",
    "    def get_source_weight(self, source):\n",
    "        \"\"\"Get credibility weight for news source.\"\"\"\n",
    "        return self.source_weights.get(source, 0.5)\n",
    "    \n",
    "    def aggregate_sentiment(self, news_df, ticker, reference_time, lookback_hours=48):\n",
    "        \"\"\"\n",
    "        Aggregate sentiment for a ticker at a specific time.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Aggregated sentiment metrics\n",
    "        \"\"\"\n",
    "        # Filter news for ticker within lookback window\n",
    "        start_time = reference_time - timedelta(hours=lookback_hours)\n",
    "        mask = (\n",
    "            (news_df['ticker'] == ticker) &\n",
    "            (news_df['timestamp'] >= start_time) &\n",
    "            (news_df['timestamp'] <= reference_time)\n",
    "        )\n",
    "        relevant_news = news_df[mask].copy()\n",
    "        \n",
    "        if len(relevant_news) == 0:\n",
    "            return {\n",
    "                'aggregate_sentiment': 0,\n",
    "                'sentiment_momentum': 0,\n",
    "                'news_volume': 0,\n",
    "                'sentiment_dispersion': 0\n",
    "            }\n",
    "        \n",
    "        # Calculate weights\n",
    "        relevant_news['hours_old'] = (reference_time - relevant_news['timestamp']).dt.total_seconds() / 3600\n",
    "        relevant_news['decay_weight'] = relevant_news['hours_old'].apply(self.calculate_decay_weight)\n",
    "        relevant_news['source_weight'] = relevant_news['source'].apply(self.get_source_weight)\n",
    "        relevant_news['total_weight'] = relevant_news['decay_weight'] * relevant_news['source_weight']\n",
    "        \n",
    "        # Weighted sentiment\n",
    "        weighted_sentiment = (\n",
    "            (relevant_news['vader_compound'] * relevant_news['total_weight']).sum() /\n",
    "            relevant_news['total_weight'].sum()\n",
    "        )\n",
    "        \n",
    "        # Sentiment momentum (recent vs older news)\n",
    "        recent_cutoff = lookback_hours / 2\n",
    "        recent_mask = relevant_news['hours_old'] <= recent_cutoff\n",
    "        if recent_mask.sum() > 0 and (~recent_mask).sum() > 0:\n",
    "            recent_sentiment = relevant_news.loc[recent_mask, 'vader_compound'].mean()\n",
    "            older_sentiment = relevant_news.loc[~recent_mask, 'vader_compound'].mean()\n",
    "            momentum = recent_sentiment - older_sentiment\n",
    "        else:\n",
    "            momentum = 0\n",
    "        \n",
    "        return {\n",
    "            'aggregate_sentiment': weighted_sentiment,\n",
    "            'sentiment_momentum': momentum,\n",
    "            'news_volume': len(relevant_news),\n",
    "            'sentiment_dispersion': relevant_news['vader_compound'].std() if len(relevant_news) > 1 else 0,\n",
    "            'avg_source_quality': relevant_news['source_weight'].mean()\n",
    "        }\n",
    "\n",
    "# Initialize aggregator\n",
    "sentiment_aggregator = NewsSentimentAggregator(decay_halflife_hours=24)\n",
    "\n",
    "# Example aggregation\n",
    "reference_time = news_df['timestamp'].max()\n",
    "test_ticker = 'AAPL'\n",
    "\n",
    "agg_result = sentiment_aggregator.aggregate_sentiment(news_df, test_ticker, reference_time)\n",
    "\n",
    "print(f\"ðŸ“Š Aggregated Sentiment for {test_ticker} at {reference_time}:\")\n",
    "for key, value in agg_result.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentiment time series for all tickers\n",
    "def build_sentiment_timeseries(news_df, tickers, freq='D'):\n",
    "    \"\"\"\n",
    "    Build a time series of aggregated sentiment for each ticker.\n",
    "    \"\"\"\n",
    "    # Get date range\n",
    "    start_date = news_df['timestamp'].min().normalize()\n",
    "    end_date = news_df['timestamp'].max().normalize()\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "    \n",
    "    sentiment_series = []\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        for date in dates:\n",
    "            agg = sentiment_aggregator.aggregate_sentiment(news_df, ticker, date, lookback_hours=48)\n",
    "            sentiment_series.append({\n",
    "                'date': date,\n",
    "                'ticker': ticker,\n",
    "                **agg\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(sentiment_series)\n",
    "\n",
    "# Build time series\n",
    "tickers = news_df['ticker'].unique()\n",
    "sentiment_ts = build_sentiment_timeseries(news_df, tickers, freq='D')\n",
    "\n",
    "print(f\"ðŸ“ˆ Built sentiment time series with {len(sentiment_ts)} observations\")\n",
    "print(f\"ðŸ“… Date range: {sentiment_ts['date'].min()} to {sentiment_ts['date'].max()}\")\n",
    "\n",
    "# Visualize sentiment over time for one ticker\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "ticker_data = sentiment_ts[sentiment_ts['ticker'] == 'AAPL']\n",
    "\n",
    "# Sentiment over time\n",
    "axes[0].plot(ticker_data['date'], ticker_data['aggregate_sentiment'], \n",
    "             color='steelblue', linewidth=2, label='Aggregate Sentiment')\n",
    "axes[0].fill_between(ticker_data['date'], ticker_data['aggregate_sentiment'], \n",
    "                     alpha=0.3, color='steelblue')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].set_ylabel('Sentiment Score')\n",
    "axes[0].set_title('AAPL News Sentiment Over Time')\n",
    "axes[0].legend()\n",
    "\n",
    "# News volume\n",
    "axes[1].bar(ticker_data['date'], ticker_data['news_volume'], \n",
    "            color='coral', alpha=0.7, label='News Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Number of Articles')\n",
    "axes[1].set_title('AAPL News Volume Over Time')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80f364",
   "metadata": {},
   "source": [
    "## 7. Detecting Market-Moving Events\n",
    "\n",
    "### 7.1 Event Classification\n",
    "\n",
    "Not all news is equal. Market-moving events include:\n",
    "- **Earnings announcements**: Beats/misses, guidance changes\n",
    "- **M&A activity**: Acquisitions, mergers, divestitures\n",
    "- **Regulatory events**: FDA approvals, SEC investigations, antitrust\n",
    "- **Management changes**: CEO departures, board changes\n",
    "- **Product launches**: New products, discontinuations\n",
    "- **Macro events**: Fed decisions, economic data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
