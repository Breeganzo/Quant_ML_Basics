{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279dad48",
   "metadata": {},
   "source": [
    "# Week 19: NLP & Alternative Data Theory\n",
    "## Quantitative Finance Applications\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this module, you will understand:\n",
    "\n",
    "1. **NLP Fundamentals** - Text preprocessing, tokenization, and representation\n",
    "2. **Sentiment Analysis** - Dictionary-based and ML approaches for financial text\n",
    "3. **Alternative Data** - Sources, quality assessment, and alpha generation\n",
    "4. **Practical Applications** - News-based signals and sentiment factor construction\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Key Topics\n",
    "\n",
    "| Topic | Description | Finance Application |\n",
    "|-------|-------------|---------------------|\n",
    "| Text Preprocessing | Cleaning, tokenization, normalization | Earnings call analysis |\n",
    "| BOW/TF-IDF | Document representation | News classification |\n",
    "| Word Embeddings | Semantic representations | Similar company detection |\n",
    "| Sentiment Analysis | Polarity scoring | Trading signals |\n",
    "| Alternative Data | Non-traditional data sources | Alpha generation |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Why NLP in Quantitative Finance?\n",
    "\n",
    "> *\"The market is driven by two emotions: fear and greed. NLP helps us quantify these emotions.\"*\n",
    "\n",
    "**Information Sources:**\n",
    "- 10-K/10-Q filings (SEC EDGAR)\n",
    "- Earnings call transcripts\n",
    "- News articles and press releases\n",
    "- Social media (Twitter/X, Reddit, StockTwits)\n",
    "- Analyst reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dd167",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: NLP Fundamentals\n",
    "\n",
    "## 1.1 Text Preprocessing Pipeline\n",
    "\n",
    "Text preprocessing is the foundation of any NLP pipeline. Raw text must be transformed into a clean, standardized format before analysis.\n",
    "\n",
    "### Preprocessing Steps:\n",
    "\n",
    "```\n",
    "Raw Text â†’ Lowercase â†’ Remove Punctuation â†’ Tokenize â†’ Remove Stopwords â†’ Stem/Lemmatize â†’ Clean Text\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Step | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Tokenization** | Split text into words/sentences | \"Apple rises\" â†’ [\"Apple\", \"rises\"] |\n",
    "| **Lowercasing** | Normalize case | \"AAPL\" â†’ \"aapl\" |\n",
    "| **Stopword Removal** | Remove common words | Remove \"the\", \"is\", \"and\" |\n",
    "| **Stemming** | Reduce to root form (crude) | \"running\" â†’ \"run\" |\n",
    "| **Lemmatization** | Reduce to dictionary form | \"better\" â†’ \"good\" |\n",
    "\n",
    "### Financial Text Challenges:\n",
    "- **Ticker symbols**: $AAPL, TSLA\n",
    "- **Numbers and percentages**: 10%, $1.5B\n",
    "- **Financial jargon**: \"bullish\", \"dovish\", \"hawkish\"\n",
    "- **Negation handling**: \"not good\" vs \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1227ab33",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# NLP Libraries\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"NLTK version: {nltk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Pipeline Implementation\n",
    "\n",
    "class FinancialTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing pipeline for financial text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Financial-specific stopwords to keep\n",
    "        self.financial_keep = {'up', 'down', 'high', 'low', 'above', 'below', 'not'}\n",
    "        self.stop_words -= self.financial_keep\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning.\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        # Keep ticker symbols (convert $AAPL to AAPL)\n",
    "        text = re.sub(r'\\$([A-Za-z]+)', r'\\1', text)\n",
    "        # Remove special characters but keep numbers\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s%.]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords while keeping financial terms.\"\"\"\n",
    "        return [t for t in tokens if t not in self.stop_words]\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        \"\"\"Apply Porter stemming.\"\"\"\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        \"\"\"Apply lemmatization.\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    def preprocess(self, text, use_stemming=False):\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        if use_stemming:\n",
    "            tokens = self.stem(tokens)\n",
    "        else:\n",
    "            tokens = self.lemmatize(tokens)\n",
    "            \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "preprocessor = FinancialTextPreprocessor()\n",
    "\n",
    "# Sample financial headlines\n",
    "sample_texts = [\n",
    "    \"Apple $AAPL surges 5% after beating earnings estimates!\",\n",
    "    \"Fed signals interest rate cuts are coming in 2024\",\n",
    "    \"Tesla's stock is NOT performing well amid EV competition\",\n",
    "    \"Goldman Sachs downgrades tech sector to 'Underweight'\",\n",
    "    \"Bitcoin crashes below $30,000 on regulatory concerns\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT PREPROCESSING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in sample_texts:\n",
    "    cleaned = preprocessor.preprocess(text)\n",
    "    print(f\"\\nðŸ“ Original: {text}\")\n",
    "    print(f\"âœ… Cleaned:  {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291ca44",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization Deep Dive\n",
    "\n",
    "### Types of Tokenization:\n",
    "\n",
    "| Type | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| **Word Tokenization** | Split by words | General NLP tasks |\n",
    "| **Sentence Tokenization** | Split by sentences | Document summarization |\n",
    "| **Subword Tokenization** | Split into subwords | Modern transformers (BPE, WordPiece) |\n",
    "| **Character Tokenization** | Split by characters | Spelling correction |\n",
    "\n",
    "### Stemming vs Lemmatization:\n",
    "\n",
    "**Stemming** (Rule-based, faster):\n",
    "- \"running\" â†’ \"run\"\n",
    "- \"studies\" â†’ \"studi\" âš ï¸ (not a real word)\n",
    "- \"better\" â†’ \"better\" (doesn't change)\n",
    "\n",
    "**Lemmatization** (Dictionary-based, accurate):\n",
    "- \"running\" â†’ \"run\"\n",
    "- \"studies\" â†’ \"study\" âœ…\n",
    "- \"better\" â†’ \"good\" âœ…\n",
    "\n",
    "> **ðŸ’¡ Interview Tip**: For financial text, lemmatization is preferred because accuracy matters more than speed. Stemming can produce non-words that lose meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbda43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming vs Lemmatization Comparison\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "comparison_words = [\n",
    "    \"running\", \"studies\", \"better\", \"earnings\", \"increasing\",\n",
    "    \"diversified\", \"volatility\", \"analysts\", \"underperforming\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEMMING VS LEMMATIZATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<20} {'Stemmed':<20} {'Lemmatized':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for word in comparison_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"{word:<20} {stemmed:<20} {lemmatized:<20}\")\n",
    "\n",
    "# N-grams demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"N-GRAM TOKENIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_sentence = \"The Federal Reserve raised interest rates by 25 basis points\"\n",
    "\n",
    "# Unigrams, Bigrams, Trigrams\n",
    "from nltk import ngrams\n",
    "\n",
    "tokens = word_tokenize(sample_sentence.lower())\n",
    "\n",
    "print(f\"\\nðŸ“ Sentence: {sample_sentence}\")\n",
    "print(f\"\\n1ï¸âƒ£ Unigrams: {tokens}\")\n",
    "print(f\"\\n2ï¸âƒ£ Bigrams: {list(ngrams(tokens, 2))}\")\n",
    "print(f\"\\n3ï¸âƒ£ Trigrams: {list(ngrams(tokens, 3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd4bc6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Text Representation\n",
    "\n",
    "## 2.1 Bag of Words (BoW) Model\n",
    "\n",
    "The Bag of Words model represents text as a vector of word counts, ignoring word order.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "For a vocabulary $V = \\{w_1, w_2, ..., w_n\\}$ and a document $d$:\n",
    "\n",
    "$$\\text{BoW}(d) = [c(w_1, d), c(w_2, d), ..., c(w_n, d)]$$\n",
    "\n",
    "where $c(w_i, d)$ is the count of word $w_i$ in document $d$.\n",
    "\n",
    "### Properties:\n",
    "- âœ… Simple and interpretable\n",
    "- âœ… Works well for document classification\n",
    "- âŒ Ignores word order and context\n",
    "- âŒ High dimensionality (vocabulary size)\n",
    "- âŒ Sparse representations\n",
    "\n",
    "### Example:\n",
    "```\n",
    "Doc 1: \"Stock price rises\"     â†’ [1, 1, 1, 0, 0]\n",
    "Doc 2: \"Stock price falls\"     â†’ [1, 1, 0, 1, 0]\n",
    "Doc 3: \"Price rises sharply\"   â†’ [0, 1, 1, 0, 1]\n",
    "\n",
    "Vocabulary: [stock, price, rises, falls, sharply]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d97f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words Implementation\n",
    "\n",
    "# Sample financial news headlines\n",
    "financial_headlines = [\n",
    "    \"Apple stock surges on strong iPhone sales\",\n",
    "    \"Tesla stock falls amid production concerns\",\n",
    "    \"Fed raises interest rates by 25 basis points\",\n",
    "    \"Goldman Sachs reports record quarterly earnings\",\n",
    "    \"Oil prices surge on supply disruption fears\",\n",
    "    \"Tech stocks fall on inflation concerns\",\n",
    "    \"Apple reports strong quarterly earnings growth\",\n",
    "    \"Bitcoin crashes below support level\"\n",
    "]\n",
    "\n",
    "# Create Bag of Words using CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=20,  # Limit vocabulary size\n",
    "    ngram_range=(1, 1)  # Unigrams only\n",
    ")\n",
    "\n",
    "bow_matrix = bow_vectorizer.fit_transform(financial_headlines)\n",
    "\n",
    "# Display results\n",
    "feature_names = bow_vectorizer.get_feature_names_out()\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"Doc_{i+1}\" for i in range(len(financial_headlines))]\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAG OF WORDS REPRESENTATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“Š Vocabulary Size: {len(feature_names)}\")\n",
    "print(f\"ðŸ“Š Document Count: {len(financial_headlines)}\")\n",
    "print(f\"ðŸ“Š Matrix Shape: {bow_matrix.shape}\")\n",
    "print(f\"ðŸ“Š Sparsity: {1 - bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1]):.2%}\")\n",
    "\n",
    "print(\"\\nðŸ”¤ Vocabulary:\", list(feature_names))\n",
    "print(\"\\nðŸ“‹ Document-Term Matrix:\")\n",
    "display(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bb705",
   "metadata": {},
   "source": [
    "## 2.2 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "TF-IDF addresses a key limitation of BoW: common words get too much weight.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "**Term Frequency (TF):**\n",
    "$$\\text{TF}(t, d) = \\frac{\\text{count of term } t \\text{ in document } d}{\\text{total terms in document } d}$$\n",
    "\n",
    "**Inverse Document Frequency (IDF):**\n",
    "$$\\text{IDF}(t, D) = \\log\\left(\\frac{N}{|\\{d \\in D : t \\in d\\}|}\\right)$$\n",
    "\n",
    "where $N$ is the total number of documents.\n",
    "\n",
    "**TF-IDF Score:**\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$$\n",
    "\n",
    "### Intuition:\n",
    "- **High TF-IDF**: Term is frequent in this document but rare overall â†’ **Important**\n",
    "- **Low TF-IDF**: Term is either rare in this document or common everywhere â†’ **Less important**\n",
    "\n",
    "### Finance Application:\n",
    "- Words like \"stock\", \"market\" appear everywhere â†’ Low IDF\n",
    "- Company-specific terms or events â†’ High TF-IDF\n",
    "- Helps identify what makes a document unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6253920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Implementation\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=20,\n",
    "    ngram_range=(1, 2)  # Include bigrams\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(financial_headlines)\n",
    "\n",
    "# Display results\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_features,\n",
    "    index=[f\"Doc_{i+1}\" for i in range(len(financial_headlines))]\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TF-IDF REPRESENTATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“Š Vocabulary Size: {len(tfidf_features)}\")\n",
    "print(f\"ðŸ“Š Matrix Shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "print(\"\\nðŸ”¤ Top Features (includes bigrams):\")\n",
    "print(list(tfidf_features))\n",
    "\n",
    "# Show TF-IDF scores rounded for readability\n",
    "print(\"\\nðŸ“‹ TF-IDF Matrix (rounded to 2 decimals):\")\n",
    "display(tfidf_df.round(2))\n",
    "\n",
    "# Find most important terms per document\n",
    "print(\"\\nðŸ† Most Important Terms per Document:\")\n",
    "for i, headline in enumerate(financial_headlines):\n",
    "    row = tfidf_df.iloc[i]\n",
    "    top_terms = row.nlargest(3)\n",
    "    print(f\"\\nDoc {i+1}: {headline}\")\n",
    "    print(f\"   Top terms: {', '.join([f'{term}({score:.2f})' for term, score in top_terms.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177563d",
   "metadata": {},
   "source": [
    "## 2.3 Word Embeddings\n",
    "\n",
    "Word embeddings represent words as dense vectors in a continuous space where semantically similar words are close together.\n",
    "\n",
    "### Evolution of Word Representations:\n",
    "\n",
    "| Method | Year | Dimensionality | Context |\n",
    "|--------|------|----------------|---------|\n",
    "| One-Hot | Classic | V (vocabulary size) | None |\n",
    "| TF-IDF | Classic | V | Document-level |\n",
    "| **Word2Vec** | 2013 | 50-300 | Local window |\n",
    "| **GloVe** | 2014 | 50-300 | Global co-occurrence |\n",
    "| **FastText** | 2016 | 50-300 | Subword |\n",
    "| **BERT** | 2018 | 768+ | Full context (bidirectional) |\n",
    "\n",
    "### Word2Vec Architectures:\n",
    "\n",
    "**1. CBOW (Continuous Bag of Words)**\n",
    "- Predicts target word from context words\n",
    "- Faster to train\n",
    "- Better for frequent words\n",
    "\n",
    "**2. Skip-gram**\n",
    "- Predicts context words from target word\n",
    "- Better for rare words\n",
    "- More commonly used\n",
    "\n",
    "### Training Objective (Skip-gram with Negative Sampling):\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{(w,c) \\in D} \\left[ \\log \\sigma(v_c \\cdot v_w) + \\sum_{i=1}^{k} \\log \\sigma(-v_{n_i} \\cdot v_w) \\right]$$\n",
    "\n",
    "where:\n",
    "- $(w, c)$ are word-context pairs\n",
    "- $v_w$, $v_c$ are word and context vectors\n",
    "- $n_i$ are negative samples\n",
    "- $\\sigma$ is the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f314dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Demonstration (Conceptual)\n",
    "# Note: Real Word2Vec requires large corpus. This demonstrates the concept.\n",
    "\n",
    "# Simulated word embeddings for financial terms (3D for visualization)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create realistic financial word embedding clusters\n",
    "financial_embeddings = {\n",
    "    # Bullish terms (cluster 1)\n",
    "    'bullish': np.array([0.8, 0.9, 0.7]),\n",
    "    'surge': np.array([0.85, 0.95, 0.75]),\n",
    "    'rally': np.array([0.75, 0.85, 0.8]),\n",
    "    'growth': np.array([0.7, 0.8, 0.65]),\n",
    "    'profit': np.array([0.9, 0.85, 0.7]),\n",
    "    \n",
    "    # Bearish terms (cluster 2)\n",
    "    'bearish': np.array([-0.8, -0.7, 0.6]),\n",
    "    'crash': np.array([-0.85, -0.75, 0.65]),\n",
    "    'decline': np.array([-0.7, -0.65, 0.55]),\n",
    "    'loss': np.array([-0.75, -0.8, 0.6]),\n",
    "    'sell': np.array([-0.6, -0.55, 0.5]),\n",
    "    \n",
    "    # Neutral/Technical terms (cluster 3)\n",
    "    'volatility': np.array([0.1, -0.1, -0.8]),\n",
    "    'volume': np.array([0.15, 0.05, -0.75]),\n",
    "    'spread': np.array([0.05, -0.05, -0.7]),\n",
    "    'liquidity': np.array([0.0, 0.1, -0.85]),\n",
    "}\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORD EMBEDDING SIMILARITIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find most similar words\n",
    "def most_similar(word, embeddings, top_n=3):\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "    \n",
    "    word_vec = embeddings[word]\n",
    "    similarities = []\n",
    "    \n",
    "    for other_word, other_vec in embeddings.items():\n",
    "        if other_word != word:\n",
    "            sim = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((other_word, sim))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Test similarity\n",
    "test_words = ['bullish', 'crash', 'volatility']\n",
    "for word in test_words:\n",
    "    similar = most_similar(word, financial_embeddings)\n",
    "    print(f\"\\nðŸ“Š Most similar to '{word}':\")\n",
    "    for sim_word, score in similar:\n",
    "        print(f\"   {sim_word}: {score:.3f}\")\n",
    "\n",
    "# Word analogies demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WORD ANALOGIES (king - man + woman = queen)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# bullish - profit + loss â‰ˆ bearish?\n",
    "result = financial_embeddings['bullish'] - financial_embeddings['profit'] + financial_embeddings['loss']\n",
    "print(\"\\nðŸ§® 'bullish' - 'profit' + 'loss' = ?\")\n",
    "\n",
    "# Find closest word to result\n",
    "closest = [(w, cosine_similarity(result, v)) for w, v in financial_embeddings.items()]\n",
    "closest = sorted(closest, key=lambda x: x[1], reverse=True)[:3]\n",
    "print(f\"   Result closest to: {closest[0][0]} (similarity: {closest[0][1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ea8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word Embeddings in 2D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare data for visualization\n",
    "words = list(financial_embeddings.keys())\n",
    "vectors = np.array([financial_embeddings[w] for w in words])\n",
    "\n",
    "# Reduce to 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by category\n",
    "colors = {'bullish': 'green', 'bearish': 'red', 'neutral': 'blue'}\n",
    "word_colors = []\n",
    "for word in words:\n",
    "    if word in ['bullish', 'surge', 'rally', 'growth', 'profit']:\n",
    "        word_colors.append('green')\n",
    "    elif word in ['bearish', 'crash', 'decline', 'loss', 'sell']:\n",
    "        word_colors.append('red')\n",
    "    else:\n",
    "        word_colors.append('blue')\n",
    "\n",
    "scatter = ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1], c=word_colors, s=200, alpha=0.6)\n",
    "\n",
    "# Add labels\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                fontsize=12, ha='center', va='bottom',\n",
    "                fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax.set_title('Financial Word Embeddings Visualization\\n(Green=Bullish, Red=Bearish, Blue=Neutral)', \n",
    "             fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight: Semantically similar financial terms cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32d2c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Sentiment Analysis for Finance\n",
    "\n",
    "## 3.1 Overview of Sentiment Analysis Approaches\n",
    "\n",
    "Sentiment analysis extracts subjective information (opinions, emotions) from text.\n",
    "\n",
    "### Approaches:\n",
    "\n",
    "| Approach | Method | Pros | Cons |\n",
    "|----------|--------|------|------|\n",
    "| **Dictionary-Based** | Predefined word lists | Simple, interpretable | Domain-specific issues |\n",
    "| **ML-Based** | Train classifier on labeled data | Learns from data | Needs training data |\n",
    "| **Deep Learning** | Neural networks (BERT, etc.) | State-of-the-art accuracy | Computationally expensive |\n",
    "\n",
    "### Financial Sentiment Dictionaries:\n",
    "\n",
    "1. **Loughran-McDonald (LM)** - Specifically designed for financial text\n",
    "   - 354 negative words (e.g., \"loss\", \"decline\", \"adverse\")\n",
    "   - 75 positive words (e.g., \"gain\", \"profit\", \"growth\")\n",
    "   \n",
    "2. **Harvard General Inquirer (GI)** - General purpose\n",
    "   - Not optimal for finance (e.g., \"liability\" is negative in general, neutral in finance)\n",
    "\n",
    "3. **VADER** - Social media optimized\n",
    "   - Handles emoticons, slang, intensifiers\n",
    "   - Good for tweets, Reddit posts\n",
    "\n",
    "> **ðŸ’¡ Key Insight**: Words have different meanings in finance!\n",
    "> - \"Liability\" â†’ Negative (general) vs Neutral (finance)\n",
    "> - \"Risk\" â†’ Negative (general) vs Neutral/Technical (finance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary-Based Sentiment Analysis\n",
    "\n",
    "# Simplified Loughran-McDonald Dictionary\n",
    "lm_positive = {\n",
    "    'achieve', 'advantage', 'beneficial', 'benefit', 'best', 'better', \n",
    "    'breakthrough', 'efficient', 'enhance', 'excellent', 'exceptional',\n",
    "    'favorable', 'gain', 'good', 'great', 'growth', 'highest', 'improve',\n",
    "    'improvement', 'innovation', 'opportunity', 'optimal', 'outperform',\n",
    "    'positive', 'profit', 'profitable', 'progress', 'prosper', 'strength',\n",
    "    'strong', 'succeed', 'success', 'successful', 'superior', 'surpass'\n",
    "}\n",
    "\n",
    "lm_negative = {\n",
    "    'abandon', 'adverse', 'against', 'bankruptcy', 'catastrophe', 'concern',\n",
    "    'concerns', 'crisis', 'critical', 'decline', 'decrease', 'default',\n",
    "    'deficit', 'delay', 'deteriorate', 'difficult', 'difficulty', 'disappoint',\n",
    "    'disappointing', 'downgrade', 'downturn', 'failure', 'fall', 'falling',\n",
    "    'fear', 'impair', 'impairment', 'investigation', 'lawsuit', 'litigation',\n",
    "    'loss', 'losses', 'negative', 'problem', 'problems', 'recession', 'risk',\n",
    "    'risky', 'slowdown', 'terminate', 'threat', 'unable', 'uncertain',\n",
    "    'uncertainty', 'unfavorable', 'weak', 'weakness', 'worse', 'worst'\n",
    "}\n",
    "\n",
    "def lm_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment score using Loughran-McDonald dictionary.\n",
    "    Score = (Positive - Negative) / Total Words\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    positive_count = sum(1 for w in words if w in lm_positive)\n",
    "    negative_count = sum(1 for w in words if w in lm_negative)\n",
    "    total_words = len(words)\n",
    "    \n",
    "    if total_words == 0:\n",
    "        return 0, 0, 0, 0\n",
    "    \n",
    "    # Normalized sentiment score\n",
    "    sentiment_score = (positive_count - negative_count) / total_words\n",
    "    \n",
    "    return sentiment_score, positive_count, negative_count, total_words\n",
    "\n",
    "# Test on financial headlines\n",
    "test_headlines = [\n",
    "    \"Company reports strong growth and excellent quarterly profit\",\n",
    "    \"Stock crashes amid bankruptcy concerns and declining sales\",\n",
    "    \"Fed maintains neutral stance on interest rates\",\n",
    "    \"Tesla faces production problems but shows improvement in deliveries\",\n",
    "    \"Apple achieves breakthrough success with new product innovation\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOUGHRAN-MCDONALD SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "for headline in test_headlines:\n",
    "    score, pos, neg, total = lm_sentiment_score(headline)\n",
    "    results.append({\n",
    "        'Headline': headline[:50] + '...' if len(headline) > 50 else headline,\n",
    "        'Positive': pos,\n",
    "        'Negative': neg,\n",
    "        'Score': score,\n",
    "        'Sentiment': 'Positive' if score > 0.05 else ('Negative' if score < -0.05 else 'Neutral')\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f59514",
   "metadata": {},
   "source": [
    "## 3.2 VADER Sentiment Analysis\n",
    "\n",
    "**VADER** (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analyzer specifically designed for social media.\n",
    "\n",
    "### Key Features:\n",
    "- Handles **emoticons**: ðŸ˜Š, ðŸ˜¢\n",
    "- Handles **intensifiers**: \"extremely good\" vs \"good\"\n",
    "- Handles **negation**: \"not good\"\n",
    "- Handles **punctuation**: \"good!\" vs \"good\"\n",
    "- Handles **capitalization**: \"GOOD\" vs \"good\"\n",
    "\n",
    "### VADER Output:\n",
    "- `neg`: Negative proportion (0-1)\n",
    "- `neu`: Neutral proportion (0-1)  \n",
    "- `pos`: Positive proportion (0-1)\n",
    "- `compound`: Normalized score (-1 to +1)\n",
    "\n",
    "### Compound Score Interpretation:\n",
    "- **compound >= 0.05**: Positive\n",
    "- **compound <= -0.05**: Negative\n",
    "- **-0.05 < compound < 0.05**: Neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER Sentiment Analysis\n",
    "# Note: In production, use: from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Simplified VADER-like implementation for demonstration\n",
    "class SimpleVADER:\n",
    "    \"\"\"\n",
    "    Simplified VADER-like sentiment analyzer for demonstration.\n",
    "    Real VADER has 7,500+ lexicon entries with valence scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simplified lexicon with valence scores (-4 to +4)\n",
    "        self.lexicon = {\n",
    "            # Positive words\n",
    "            'good': 1.9, 'great': 3.1, 'excellent': 3.2, 'amazing': 3.1,\n",
    "            'profit': 2.0, 'growth': 1.8, 'surge': 2.5, 'rally': 2.3,\n",
    "            'beat': 2.0, 'outperform': 2.5, 'bullish': 2.8, 'success': 2.5,\n",
    "            'improve': 1.8, 'gain': 2.0, 'breakthrough': 2.8, 'innovation': 2.0,\n",
    "            \n",
    "            # Negative words\n",
    "            'bad': -1.9, 'terrible': -3.0, 'awful': -3.0, 'poor': -1.8,\n",
    "            'loss': -2.0, 'decline': -1.8, 'crash': -3.0, 'plunge': -2.8,\n",
    "            'miss': -1.5, 'underperform': -2.0, 'bearish': -2.5, 'fail': -2.5,\n",
    "            'concern': -1.5, 'risk': -1.0, 'crisis': -3.0, 'bankruptcy': -3.5,\n",
    "            'lawsuit': -2.0, 'investigation': -2.0, 'warning': -1.8,\n",
    "        }\n",
    "        \n",
    "        # Intensifiers\n",
    "        self.intensifiers = {\n",
    "            'very': 1.5, 'extremely': 2.0, 'incredibly': 1.8, 'really': 1.3,\n",
    "            'slightly': 0.5, 'somewhat': 0.7\n",
    "        }\n",
    "        \n",
    "        # Negation words\n",
    "        self.negations = {'not', 'no', \"n't\", 'never', 'neither', 'nobody', 'nothing'}\n",
    "    \n",
    "    def polarity_scores(self, text):\n",
    "        \"\"\"Calculate sentiment scores for text.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        total_valence = 0\n",
    "        word_count = 0\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            # Clean word\n",
    "            word = ''.join(c for c in word if c.isalpha())\n",
    "            \n",
    "            if word in self.lexicon:\n",
    "                valence = self.lexicon[word]\n",
    "                \n",
    "                # Check for negation (within 3 words before)\n",
    "                for j in range(max(0, i-3), i):\n",
    "                    prev_word = ''.join(c for c in words[j] if c.isalpha())\n",
    "                    if prev_word in self.negations:\n",
    "                        valence *= -0.5\n",
    "                        break\n",
    "                \n",
    "                # Check for intensifiers\n",
    "                if i > 0:\n",
    "                    prev_word = ''.join(c for c in words[i-1] if c.isalpha())\n",
    "                    if prev_word in self.intensifiers:\n",
    "                        valence *= self.intensifiers[prev_word]\n",
    "                \n",
    "                total_valence += valence\n",
    "                word_count += 1\n",
    "        \n",
    "        # Normalize compound score to [-1, 1]\n",
    "        compound = total_valence / (np.sqrt(total_valence**2 + 15)) if total_valence != 0 else 0\n",
    "        \n",
    "        # Calculate proportions\n",
    "        if word_count == 0:\n",
    "            return {'neg': 0, 'neu': 1, 'pos': 0, 'compound': 0}\n",
    "        \n",
    "        pos_prop = max(0, compound) \n",
    "        neg_prop = max(0, -compound)\n",
    "        neu_prop = 1 - (pos_prop + neg_prop)\n",
    "        \n",
    "        return {\n",
    "            'neg': round(neg_prop, 3),\n",
    "            'neu': round(neu_prop, 3),\n",
    "            'pos': round(pos_prop, 3),\n",
    "            'compound': round(compound, 4)\n",
    "        }\n",
    "\n",
    "# Test VADER\n",
    "vader = SimpleVADER()\n",
    "\n",
    "social_media_texts = [\n",
    "    \"$AAPL is looking extremely bullish! ðŸš€\",\n",
    "    \"Not good news for Tesla investors today\",\n",
    "    \"Very concerned about the market crash\",\n",
    "    \"BREAKING: Company reports amazing profit growth!!!\",\n",
    "    \"This stock is not terrible but not great either\",\n",
    "    \"Extremely bearish on crypto right now\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VADER SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "vader_results = []\n",
    "for text in social_media_texts:\n",
    "    scores = vader.polarity_scores(text)\n",
    "    sentiment = 'Positive' if scores['compound'] >= 0.05 else ('Negative' if scores['compound'] <= -0.05 else 'Neutral')\n",
    "    vader_results.append({\n",
    "        'Text': text[:45] + '...' if len(text) > 45 else text,\n",
    "        'Compound': scores['compound'],\n",
    "        'Pos': scores['pos'],\n",
    "        'Neg': scores['neg'],\n",
    "        'Neu': scores['neu'],\n",
    "        'Sentiment': sentiment\n",
    "    })\n",
    "\n",
    "vader_df = pd.DataFrame(vader_results)\n",
    "display(vader_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb25757",
   "metadata": {},
   "source": [
    "## 3.3 Machine Learning Sentiment Classification\n",
    "\n",
    "For better accuracy, we can train ML classifiers on labeled financial text data.\n",
    "\n",
    "### Common Approaches:\n",
    "\n",
    "| Model | Features | Pros | Cons |\n",
    "|-------|----------|------|------|\n",
    "| **Naive Bayes** | TF-IDF | Fast, good baseline | Independence assumption |\n",
    "| **Logistic Regression** | TF-IDF | Interpretable, regularizable | Linear boundaries |\n",
    "| **SVM** | TF-IDF | Good with high-dimensional data | Slow on large datasets |\n",
    "| **Random Forest** | TF-IDF | Handles non-linearity | Memory intensive |\n",
    "| **BERT/FinBERT** | Contextual embeddings | State-of-the-art | Computationally expensive |\n",
    "\n",
    "### Training Process:\n",
    "1. Collect labeled financial text (positive/negative/neutral)\n",
    "2. Preprocess text (tokenize, clean, normalize)\n",
    "3. Create features (TF-IDF, embeddings)\n",
    "4. Train classifier\n",
    "5. Evaluate on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Sentiment Classifier\n",
    "\n",
    "# Create synthetic labeled dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Labeled financial headlines dataset\n",
    "labeled_data = [\n",
    "    # Positive (1)\n",
    "    (\"Apple beats earnings expectations with record iPhone sales\", 1),\n",
    "    (\"Tesla stock surges 10% on strong delivery numbers\", 1),\n",
    "    (\"Company reports exceptional profit growth this quarter\", 1),\n",
    "    (\"Investors celebrate breakthrough innovation announcement\", 1),\n",
    "    (\"Market rallies on positive economic data\", 1),\n",
    "    (\"Strong quarterly results exceed analyst forecasts\", 1),\n",
    "    (\"Revenue growth accelerates beyond expectations\", 1),\n",
    "    (\"Share price hits all-time high on bullish outlook\", 1),\n",
    "    (\"Company announces successful product launch\", 1),\n",
    "    (\"Profit margins improve significantly year over year\", 1),\n",
    "    \n",
    "    # Negative (0)\n",
    "    (\"Stock crashes amid bankruptcy concerns\", 0),\n",
    "    (\"Company faces lawsuit over accounting fraud\", 0),\n",
    "    (\"Revenue decline raises investor concerns\", 0),\n",
    "    (\"Market plunges on recession fears\", 0),\n",
    "    (\"Investigation launched into financial irregularities\", 0),\n",
    "    (\"Quarterly loss exceeds analyst expectations\", 0),\n",
    "    (\"Downgrade sparks massive sell-off\", 0),\n",
    "    (\"Production problems lead to delivery delays\", 0),\n",
    "    (\"Company warns of declining demand\", 0),\n",
    "    (\"Share price collapses on poor earnings report\", 0),\n",
    "    \n",
    "    # Neutral (2)\n",
    "    (\"Fed maintains current interest rate policy\", 2),\n",
    "    (\"Company announces quarterly earnings call date\", 2),\n",
    "    (\"Stock trades sideways amid low volume\", 2),\n",
    "    (\"Analyst maintains hold rating on shares\", 2),\n",
    "    (\"Company completes previously announced acquisition\", 2),\n",
    "    (\"Board appoints new chief financial officer\", 2),\n",
    "    (\"Dividend payment date set for next month\", 2),\n",
    "    (\"Company files annual report with SEC\", 2),\n",
    "]\n",
    "\n",
    "# Prepare data\n",
    "texts = [t[0] for t in labeled_data]\n",
    "labels = [t[1] for t in labeled_data]\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=100, ngram_range=(1, 2))\n",
    "X = tfidf.fit_transform(texts)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ML SENTIMENT CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“Š Dataset size: {len(texts)}\")\n",
    "print(f\"ðŸ“Š Training size: {X_train.shape[0]}\")\n",
    "print(f\"ðŸ“Š Test size: {X_test.shape[0]}\")\n",
    "print(f\"ðŸ“Š Feature dimensions: {X.shape[1]}\")\n",
    "print(f\"ðŸ“Š Class distribution: {dict(Counter(labels))}\")\n",
    "\n",
    "# Train classifiers\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    results[name] = {'train_acc': train_acc, 'test_acc': test_acc}\n",
    "    print(f\"\\nðŸ¤– {name}:\")\n",
    "    print(f\"   Training Accuracy: {train_acc:.2%}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d387375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis (Logistic Regression)\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "lr_model = models['Logistic Regression']\n",
    "\n",
    "# For multiclass, we look at coefficients per class\n",
    "# Class 0 = Negative, Class 1 = Positive, Class 2 = Neutral\n",
    "class_names = ['Negative', 'Positive', 'Neutral']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (class_name, ax) in enumerate(zip(class_names, axes)):\n",
    "    coef = lr_model.coef_[idx]\n",
    "    \n",
    "    # Get top features\n",
    "    top_positive_idx = np.argsort(coef)[-10:]\n",
    "    top_negative_idx = np.argsort(coef)[:10]\n",
    "    \n",
    "    top_idx = np.concatenate([top_negative_idx, top_positive_idx])\n",
    "    top_features = feature_names[top_idx]\n",
    "    top_coefs = coef[top_idx]\n",
    "    \n",
    "    colors = ['red' if c < 0 else 'green' for c in top_coefs]\n",
    "    \n",
    "    ax.barh(range(len(top_idx)), top_coefs, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(top_idx)))\n",
    "    ax.set_yticklabels(top_features)\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_title(f'{class_name} Class Features')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Most Important Features by Sentiment Class', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test on new headlines\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION ON NEW HEADLINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_headlines = [\n",
    "    \"Bitcoin surges to new all-time high amid institutional buying\",\n",
    "    \"Company faces regulatory investigation into business practices\",\n",
    "    \"Quarterly earnings report scheduled for next Tuesday\"\n",
    "]\n",
    "\n",
    "new_features = tfidf.transform(new_headlines)\n",
    "predictions = lr_model.predict(new_features)\n",
    "probabilities = lr_model.predict_proba(new_features)\n",
    "\n",
    "for headline, pred, probs in zip(new_headlines, predictions, probabilities):\n",
    "    sentiment = class_names[pred]\n",
    "    print(f\"\\nðŸ“° \\\"{headline}\\\"\")\n",
    "    print(f\"   Prediction: {sentiment}\")\n",
    "    print(f\"   Probabilities: Neg={probs[0]:.2%}, Pos={probs[1]:.2%}, Neu={probs[2]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac3b22a",
   "metadata": {},
   "source": [
    "## 3.4 Modern Approaches: FinBERT\n",
    "\n",
    "**FinBERT** is a BERT model fine-tuned on financial text for sentiment analysis.\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "```\n",
    "Input Text â†’ Tokenization â†’ BERT Encoder â†’ [CLS] Token â†’ Classification Head â†’ Sentiment\n",
    "```\n",
    "\n",
    "### Why FinBERT?\n",
    "1. **Pre-trained on financial corpus**: Understands financial jargon\n",
    "2. **Contextual embeddings**: Same word can have different meanings\n",
    "3. **Transfer learning**: Leverages knowledge from massive text corpora\n",
    "\n",
    "### FinBERT vs Traditional Methods:\n",
    "\n",
    "| Aspect | Dictionary | ML + TF-IDF | FinBERT |\n",
    "|--------|------------|-------------|---------|\n",
    "| Context | âŒ None | âŒ Limited | âœ… Full |\n",
    "| Domain-specific | âš ï¸ Manual | âš ï¸ Training data | âœ… Pre-trained |\n",
    "| Negation | âš ï¸ Rules | âš ï¸ Learned | âœ… Contextual |\n",
    "| Accuracy | Low-Medium | Medium | High |\n",
    "| Speed | Fast | Fast | Slow |\n",
    "| Resources | Low | Low | High (GPU) |\n",
    "\n",
    "### Example Usage (Conceptual):\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load FinBERT\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Predict\n",
    "inputs = tokenizer(\"Apple stock surges on earnings beat\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "# Output: [negative_prob, neutral_prob, positive_prob]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3088f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Alternative Data\n",
    "\n",
    "## 4.1 What is Alternative Data?\n",
    "\n",
    "**Alternative data** refers to non-traditional data sources used to generate alpha or improve risk management in financial markets.\n",
    "\n",
    "### Traditional vs Alternative Data:\n",
    "\n",
    "| Traditional Data | Alternative Data |\n",
    "|------------------|------------------|\n",
    "| Price, volume | Satellite imagery |\n",
    "| Financial statements | Credit card transactions |\n",
    "| Economic indicators | Web traffic |\n",
    "| Analyst reports | Social media sentiment |\n",
    "| Earnings calls | Geolocation data |\n",
    "| SEC filings | App usage |\n",
    "\n",
    "### The Alternative Data Landscape:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ALTERNATIVE DATA UNIVERSE                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚    EXHAUST      â”‚    SENSORS      â”‚      WEB/DIGITAL           â”‚\n",
    "â”‚    DATA         â”‚    DATA         â”‚      DATA                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ â€¢ Credit cards  â”‚ â€¢ Satellite     â”‚ â€¢ News sentiment           â”‚\n",
    "â”‚ â€¢ POS data      â”‚ â€¢ Weather       â”‚ â€¢ Social media             â”‚\n",
    "â”‚ â€¢ Email receiptsâ”‚ â€¢ IoT devices   â”‚ â€¢ Web scraping             â”‚\n",
    "â”‚ â€¢ App usage     â”‚ â€¢ Foot traffic  â”‚ â€¢ Search trends            â”‚\n",
    "â”‚ â€¢ Job postings  â”‚ â€¢ Ship tracking â”‚ â€¢ Product reviews          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Data Sources Overview\n",
    "\n",
    "alt_data_sources = {\n",
    "    'Category': [\n",
    "        'Satellite Imagery', 'Satellite Imagery', \n",
    "        'Web/Social', 'Web/Social', 'Web/Social',\n",
    "        'Transaction', 'Transaction',\n",
    "        'Geolocation', 'Geolocation',\n",
    "        'Other', 'Other'\n",
    "    ],\n",
    "    'Data Type': [\n",
    "        'Parking lot cars', 'Oil storage tanks',\n",
    "        'News sentiment', 'Twitter mentions', 'Google Trends',\n",
    "        'Credit card spend', 'Email receipts',\n",
    "        'Foot traffic', 'Ship tracking (AIS)',\n",
    "        'Job postings', 'Patent filings'\n",
    "    ],\n",
    "    'Use Case': [\n",
    "        'Retail sales prediction', 'Oil inventory levels',\n",
    "        'Market sentiment', 'Brand sentiment', 'Interest trends',\n",
    "        'Consumer spending', 'E-commerce tracking',\n",
    "        'Store visits', 'Commodity supply',\n",
    "        'Company growth', 'Innovation tracking'\n",
    "    ],\n",
    "    'Alpha Decay': [\n",
    "        'Days', 'Days',\n",
    "        'Hours-Days', 'Minutes-Hours', 'Days',\n",
    "        'Weeks', 'Weeks',\n",
    "        'Days-Weeks', 'Days',\n",
    "        'Weeks-Months', 'Months'\n",
    "    ],\n",
    "    'Example Provider': [\n",
    "        'Orbital Insight', 'Kayrros',\n",
    "        'RavenPack', 'StockTwits', 'Google',\n",
    "        'Second Measure', 'Earnest Research',\n",
    "        'SafeGraph', 'MarineTraffic',\n",
    "        'LinkUp', 'PatSnap'\n",
    "    ]\n",
    "}\n",
    "\n",
    "alt_df = pd.DataFrame(alt_data_sources)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALTERNATIVE DATA LANDSCAPE\")\n",
    "print(\"=\" * 80)\n",
    "display(alt_df)\n",
    "\n",
    "# Visualize by category\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "category_counts = alt_df['Category'].value_counts()\n",
    "colors = plt.cm.Set3(range(len(category_counts)))\n",
    "wedges, texts, autotexts = ax.pie(category_counts, labels=category_counts.index, \n",
    "                                   autopct='%1.0f%%', colors=colors)\n",
    "ax.set_title('Alternative Data by Category', fontsize=14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
