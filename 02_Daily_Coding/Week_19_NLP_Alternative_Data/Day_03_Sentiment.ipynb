{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13639fc",
   "metadata": {},
   "source": [
    "# Day 03: Sentiment Analysis for Trading Signals\n",
    "\n",
    "## Week 19: NLP & Alternative Data\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. **Understand sentiment analysis fundamentals** for financial applications\n",
    "2. **Implement lexicon-based methods** (VADER, Loughran-McDonald)\n",
    "3. **Apply ML-based sentiment models** (FinBERT, distilBERT)\n",
    "4. **Convert sentiment scores to trading signals**\n",
    "5. **Backtest sentiment-based strategies**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Financial Sentiment Analysis](#1-introduction)\n",
    "2. [Text Preprocessing for Financial Data](#2-preprocessing)\n",
    "3. [Lexicon-Based Sentiment Analysis](#3-lexicon)\n",
    "4. [ML-Based Sentiment (FinBERT)](#4-finbert)\n",
    "5. [Aggregating Sentiment Signals](#5-aggregation)\n",
    "6. [Building Trading Signals from Sentiment](#6-signals)\n",
    "7. [Backtesting Sentiment Strategies](#7-backtesting)\n",
    "8. [Interview Questions](#8-interview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629844f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction to Financial Sentiment Analysis <a id='1-introduction'></a>\n",
    "\n",
    "### Why Sentiment Matters in Trading\n",
    "\n",
    "- **Market psychology**: Prices reflect collective sentiment\n",
    "- **Information asymmetry**: Text data contains alpha not yet in prices\n",
    "- **Lead-lag relationships**: Sentiment often precedes price moves\n",
    "\n",
    "### Sources of Sentiment Data\n",
    "\n",
    "| Source | Latency | Signal Quality | Cost |\n",
    "|--------|---------|----------------|------|\n",
    "| News Articles | Minutes | High | High |\n",
    "| Social Media | Seconds | Variable | Low |\n",
    "| Earnings Calls | Hours | Very High | Medium |\n",
    "| SEC Filings | Days | High | Low |\n",
    "| Analyst Reports | Hours | Very High | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP imports\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Core libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install vaderSentiment nltk transformers torch yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9928c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP-specific imports\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    print(\"✓ VADER loaded\")\n",
    "except ImportError:\n",
    "    print(\"✗ Install: pip install vaderSentiment\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    print(\"✓ NLTK loaded\")\n",
    "except ImportError:\n",
    "    print(\"✗ Install: pip install nltk\")\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    print(\"✓ yfinance loaded\")\n",
    "except ImportError:\n",
    "    print(\"✗ Install: pip install yfinance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2057d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Text Preprocessing for Financial Data <a id='2-preprocessing'></a>\n",
    "\n",
    "### Key Preprocessing Steps\n",
    "\n",
    "1. **Lowercasing** - Normalize case\n",
    "2. **Remove noise** - HTML, special characters\n",
    "3. **Tokenization** - Split into words\n",
    "4. **Remove stopwords** - Keep meaningful words\n",
    "5. **Lemmatization** - Reduce to root forms\n",
    "6. **Handle financial terms** - Preserve important phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessor optimized for financial text.\n",
    "    Preserves important financial terms and phrases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Financial terms to preserve (don't remove as stopwords)\n",
    "        self.financial_terms = {\n",
    "            'up', 'down', 'high', 'low', 'above', 'below',\n",
    "            'buy', 'sell', 'long', 'short', 'bull', 'bear',\n",
    "            'gain', 'loss', 'profit', 'revenue', 'earnings'\n",
    "        }\n",
    "        \n",
    "        # Remove financial terms from stopwords\n",
    "        self.stop_words -= self.financial_terms\n",
    "        \n",
    "        # Important financial bigrams/phrases\n",
    "        self.financial_phrases = [\n",
    "            'beat expectations', 'miss expectations',\n",
    "            'guidance raised', 'guidance lowered',\n",
    "            'stock buyback', 'dividend increase',\n",
    "            'revenue growth', 'profit margin',\n",
    "            'market share', 'price target'\n",
    "        ]\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Basic text cleaning.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep financial symbols\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\$\\%\\.\\-]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text: str) -> list:\n",
    "        \"\"\"Tokenize and lemmatize text.\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        processed = [\n",
    "            self.lemmatizer.lemmatize(token)\n",
    "            for token in tokens\n",
    "            if token not in self.stop_words and len(token) > 2\n",
    "        ]\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def process(self, text: str) -> str:\n",
    "        \"\"\"Full preprocessing pipeline returning string.\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FinancialTextPreprocessor()\n",
    "\n",
    "# Example\n",
    "sample_text = \"\"\"\n",
    "Apple Inc. (AAPL) reported Q4 earnings that BEAT analyst expectations!\n",
    "Revenue grew 15% YoY to $89.5B. The company raised guidance for next quarter.\n",
    "Stock price jumped 5% in after-hours trading. Analysts remain bullish.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nProcessed text:\")\n",
    "print(preprocessor.process(sample_text))\n",
    "print(\"\\nTokens:\")\n",
    "print(preprocessor.tokenize(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1301e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Lexicon-Based Sentiment Analysis <a id='3-lexicon'></a>\n",
    "\n",
    "### 3.1 VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "\n",
    "VADER is specifically designed for social media but works well for financial news:\n",
    "- **Compound score**: -1 (most negative) to +1 (most positive)\n",
    "- Handles emojis, slang, and intensifiers\n",
    "- Fast and doesn't require training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADERSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    VADER-based sentiment analyzer for financial text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Add financial lexicon updates\n",
    "        financial_lexicon = {\n",
    "            'bullish': 3.0,\n",
    "            'bearish': -3.0,\n",
    "            'outperform': 2.5,\n",
    "            'underperform': -2.5,\n",
    "            'upgrade': 2.5,\n",
    "            'downgrade': -2.5,\n",
    "            'beat': 2.0,\n",
    "            'miss': -2.0,\n",
    "            'exceeds': 2.0,\n",
    "            'disappoints': -2.0,\n",
    "            'rally': 2.0,\n",
    "            'plunge': -2.5,\n",
    "            'surge': 2.5,\n",
    "            'crash': -3.0,\n",
    "            'soar': 2.5,\n",
    "            'tumble': -2.0,\n",
    "            'bankruptcy': -3.5,\n",
    "            'default': -3.0,\n",
    "            'dividend': 1.5,\n",
    "            'buyback': 1.5,\n",
    "            'guidance': 0.5,\n",
    "            'momentum': 1.0,\n",
    "            'volatility': -0.5,\n",
    "            'recession': -2.5,\n",
    "            'growth': 1.5,\n",
    "            'profit': 1.5,\n",
    "            'loss': -1.5,\n",
    "        }\n",
    "        \n",
    "        self.analyzer.lexicon.update(financial_lexicon)\n",
    "    \n",
    "    def analyze(self, text: str) -> dict:\n",
    "        \"\"\"Get sentiment scores for text.\"\"\"\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        return scores\n",
    "    \n",
    "    def get_compound(self, text: str) -> float:\n",
    "        \"\"\"Get compound sentiment score.\"\"\"\n",
    "        return self.analyze(text)['compound']\n",
    "    \n",
    "    def classify(self, text: str, threshold: float = 0.05) -> str:\n",
    "        \"\"\"Classify sentiment as positive, negative, or neutral.\"\"\"\n",
    "        compound = self.get_compound(text)\n",
    "        \n",
    "        if compound >= threshold:\n",
    "            return 'positive'\n",
    "        elif compound <= -threshold:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "\n",
    "# Initialize analyzer\n",
    "vader = VADERSentimentAnalyzer()\n",
    "\n",
    "# Test on financial headlines\n",
    "headlines = [\n",
    "    \"Apple beats earnings expectations, stock surges 5%\",\n",
    "    \"Tesla misses delivery targets, shares plunge in premarket\",\n",
    "    \"Fed holds rates steady, market remains uncertain\",\n",
    "    \"Amazon announces $10B stock buyback program\",\n",
    "    \"Bank warns of potential recession risks ahead\",\n",
    "    \"Tech rally continues as investors remain bullish\",\n",
    "    \"Company files for bankruptcy after years of losses\"\n",
    "]\n",
    "\n",
    "print(\"VADER Sentiment Analysis Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for headline in headlines:\n",
    "    scores = vader.analyze(headline)\n",
    "    sentiment = vader.classify(headline)\n",
    "    results.append({\n",
    "        'headline': headline[:50] + '...' if len(headline) > 50 else headline,\n",
    "        'compound': scores['compound'],\n",
    "        'positive': scores['pos'],\n",
    "        'negative': scores['neg'],\n",
    "        'neutral': scores['neu'],\n",
    "        'sentiment': sentiment\n",
    "    })\n",
    "\n",
    "vader_df = pd.DataFrame(results)\n",
    "print(vader_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be2efe",
   "metadata": {},
   "source": [
    "### 3.2 Loughran-McDonald Financial Sentiment Dictionary\n",
    "\n",
    "The **Loughran-McDonald** dictionary is specifically designed for financial text:\n",
    "- Developed from 10-K filings\n",
    "- Different word lists: Positive, Negative, Uncertainty, Litigious, etc.\n",
    "- More accurate for formal financial documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoughranMcDonaldSentiment:\n",
    "    \"\"\"\n",
    "    Loughran-McDonald dictionary-based sentiment for financial text.\n",
    "    Uses a simplified version of the dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simplified Loughran-McDonald word lists\n",
    "        self.positive_words = {\n",
    "            'accomplish', 'accomplishment', 'achieve', 'achievement', 'advantage',\n",
    "            'beneficial', 'benefit', 'best', 'better', 'boost', 'breakthrough',\n",
    "            'creative', 'delight', 'deliver', 'desirable', 'dream', 'easy',\n",
    "            'effective', 'efficiency', 'efficient', 'enhance', 'enjoy', 'enthusiasm',\n",
    "            'excellent', 'exceptional', 'exciting', 'exclusive', 'favorable', 'gain',\n",
    "            'good', 'great', 'grow', 'growth', 'happy', 'highest', 'ideal',\n",
    "            'improve', 'improvement', 'incredible', 'innovative', 'leader', 'leadership',\n",
    "            'opportunity', 'optimal', 'optimistic', 'outperform', 'outstanding', 'perfect',\n",
    "            'positive', 'profitable', 'profitability', 'progress', 'prosper', 'record',\n",
    "            'reward', 'rewarding', 'solid', 'strength', 'strengthen', 'strong',\n",
    "            'succeed', 'success', 'successful', 'superior', 'surpass', 'top', 'upturn',\n",
    "            'win', 'winner', 'winning'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'abandon', 'adverse', 'against', 'allegation', 'argue', 'bad', 'bankruptcy',\n",
    "            'blame', 'breach', 'burden', 'catastrophe', 'challenge', 'claim', 'closure',\n",
    "            'collapse', 'concern', 'conflict', 'crisis', 'critical', 'damage', 'danger',\n",
    "            'decline', 'default', 'deficit', 'delay', 'deteriorate', 'difficult',\n",
    "            'difficulty', 'disappoint', 'disappointing', 'disaster', 'disclose', 'disclosure',\n",
    "            'doubt', 'downturn', 'drop', 'failure', 'fall', 'fear', 'fraud', 'harm',\n",
    "            'hurt', 'impair', 'impairment', 'impossible', 'inability', 'inadequate',\n",
    "            'investigation', 'lawsuit', 'layoff', 'liquidation', 'litigation', 'lose',\n",
    "            'loss', 'losses', 'negative', 'negligence', 'obstacle', 'penalty', 'plunge',\n",
    "            'poor', 'problem', 'recall', 'recession', 'restructuring', 'risk', 'risky',\n",
    "            'scandal', 'setback', 'shortage', 'slowdown', 'slump', 'struggle', 'subprime',\n",
    "            'terminate', 'threat', 'trouble', 'tumble', 'uncertain', 'uncertainty',\n",
    "            'underperform', 'unfavorable', 'violation', 'volatile', 'volatility',\n",
    "            'weak', 'weakness', 'worse', 'worsen', 'worst', 'writedown', 'writeoff'\n",
    "        }\n",
    "        \n",
    "        self.uncertainty_words = {\n",
    "            'almost', 'anticipate', 'apparent', 'appear', 'approximate', 'assume',\n",
    "            'believe', 'conditional', 'confuse', 'contingency', 'contingent', 'could',\n",
    "            'depend', 'doubt', 'estimate', 'expect', 'forecast', 'hope', 'if',\n",
    "            'indefinite', 'indicate', 'likelihood', 'likely', 'may', 'maybe', 'might',\n",
    "            'pending', 'perhaps', 'possibility', 'possible', 'possibly', 'potential',\n",
    "            'predict', 'probable', 'probably', 'project', 'risk', 'roughly', 'seem',\n",
    "            'sometimes', 'suggest', 'suppose', 'uncertain', 'uncertainty', 'unclear',\n",
    "            'unknown', 'unlikely', 'unpredictable', 'unsure', 'variable', 'volatility'\n",
    "        }\n",
    "    \n",
    "    def analyze(self, text: str) -> dict:\n",
    "        \"\"\"Analyze text using Loughran-McDonald dictionary.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        total_words = len(words)\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return {'positive': 0, 'negative': 0, 'uncertainty': 0, 'sentiment': 0}\n",
    "        \n",
    "        pos_count = sum(1 for w in words if w in self.positive_words)\n",
    "        neg_count = sum(1 for w in words if w in self.negative_words)\n",
    "        unc_count = sum(1 for w in words if w in self.uncertainty_words)\n",
    "        \n",
    "        # Normalize by total words\n",
    "        pos_score = pos_count / total_words\n",
    "        neg_score = neg_count / total_words\n",
    "        unc_score = unc_count / total_words\n",
    "        \n",
    "        # Net sentiment\n",
    "        sentiment = (pos_count - neg_count) / total_words\n",
    "        \n",
    "        return {\n",
    "            'positive': pos_score,\n",
    "            'negative': neg_score,\n",
    "            'uncertainty': unc_score,\n",
    "            'sentiment': sentiment,\n",
    "            'pos_words': pos_count,\n",
    "            'neg_words': neg_count\n",
    "        }\n",
    "\n",
    "\n",
    "# Test Loughran-McDonald\n",
    "lm = LoughranMcDonaldSentiment()\n",
    "\n",
    "# Example: 10-K excerpt\n",
    "text_10k = \"\"\"\n",
    "Our business faces significant risks and uncertainties that could materially affect \n",
    "our future financial results. Competition continues to increase, and we may face \n",
    "challenges in maintaining our market position. However, we believe our strong \n",
    "leadership and innovative products position us well for future growth and success.\n",
    "\"\"\"\n",
    "\n",
    "lm_scores = lm.analyze(text_10k)\n",
    "print(\"Loughran-McDonald Analysis:\")\n",
    "print(f\"  Positive score:    {lm_scores['positive']:.4f} ({lm_scores['pos_words']} words)\")\n",
    "print(f\"  Negative score:    {lm_scores['negative']:.4f} ({lm_scores['neg_words']} words)\")\n",
    "print(f\"  Uncertainty score: {lm_scores['uncertainty']:.4f}\")\n",
    "print(f\"  Net sentiment:     {lm_scores['sentiment']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d3793",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ML-Based Sentiment (FinBERT) <a id='4-finbert'></a>\n",
    "\n",
    "### FinBERT: BERT Pre-trained on Financial Text\n",
    "\n",
    "FinBERT is a BERT model fine-tuned on financial text:\n",
    "- Better context understanding than lexicon methods\n",
    "- Handles negation and complex sentences\n",
    "- Three classes: positive, negative, neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae73291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if transformers is available\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(\"✓ Transformers library available\")\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"✗ Transformers not available. Install with: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinBERTSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    FinBERT-based sentiment analyzer.\n",
    "    Uses the ProsusAI/finbert model from Hugging Face.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"ProsusAI/finbert\"):\n",
    "        if not TRANSFORMERS_AVAILABLE:\n",
    "            raise ImportError(\"transformers library required\")\n",
    "        \n",
    "        print(f\"Loading {model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # FinBERT labels\n",
    "        self.labels = ['positive', 'negative', 'neutral']\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    def analyze(self, text: str) -> dict:\n",
    "        \"\"\"Analyze sentiment of a single text.\"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "        \n",
    "        probs = probabilities[0].tolist()\n",
    "        \n",
    "        return {\n",
    "            'label': self.labels[np.argmax(probs)],\n",
    "            'positive': probs[0],\n",
    "            'negative': probs[1],\n",
    "            'neutral': probs[2],\n",
    "            'compound': probs[0] - probs[1]  # Net sentiment\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, texts: list, batch_size: int = 8) -> list:\n",
    "        \"\"\"Analyze sentiment of multiple texts.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            for j, probs in enumerate(probabilities.tolist()):\n",
    "                results.append({\n",
    "                    'text': batch[j][:50] + '...' if len(batch[j]) > 50 else batch[j],\n",
    "                    'label': self.labels[np.argmax(probs)],\n",
    "                    'positive': probs[0],\n",
    "                    'negative': probs[1],\n",
    "                    'neutral': probs[2],\n",
    "                    'compound': probs[0] - probs[1]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Note: Only run if transformers is available and you have enough memory\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nFinBERT is available. Uncomment below to load and test.\")\n",
    "    print(\"Note: First run will download ~440MB model.\")\n",
    "\n",
    "# Uncomment to test:\n",
    "# finbert = FinBERTSentimentAnalyzer()\n",
    "# result = finbert.analyze(\"Apple stock surges after beating earnings expectations\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated FinBERT results for demonstration\n",
    "# (Use actual FinBERT in production)\n",
    "\n",
    "def simulate_finbert_results(headlines: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate FinBERT-like results for demonstration.\n",
    "    In practice, use actual FinBERT model.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Use VADER as proxy and add noise\n",
    "    vader = VADERSentimentAnalyzer()\n",
    "    \n",
    "    results = []\n",
    "    for headline in headlines:\n",
    "        compound = vader.get_compound(headline)\n",
    "        \n",
    "        # Convert to probabilities (simulated)\n",
    "        if compound > 0.2:\n",
    "            pos = 0.6 + np.random.uniform(0, 0.3)\n",
    "            neg = np.random.uniform(0, 0.15)\n",
    "            neu = 1 - pos - neg\n",
    "            label = 'positive'\n",
    "        elif compound < -0.2:\n",
    "            neg = 0.6 + np.random.uniform(0, 0.3)\n",
    "            pos = np.random.uniform(0, 0.15)\n",
    "            neu = 1 - pos - neg\n",
    "            label = 'negative'\n",
    "        else:\n",
    "            neu = 0.5 + np.random.uniform(0, 0.3)\n",
    "            pos = np.random.uniform(0.1, 0.25)\n",
    "            neg = 1 - neu - pos\n",
    "            label = 'neutral'\n",
    "        \n",
    "        results.append({\n",
    "            'headline': headline[:50] + '...' if len(headline) > 50 else headline,\n",
    "            'label': label,\n",
    "            'positive': round(pos, 3),\n",
    "            'negative': round(neg, 3),\n",
    "            'neutral': round(neu, 3),\n",
    "            'compound': round(pos - neg, 3)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Compare VADER vs \"FinBERT\" (simulated)\n",
    "print(\"Simulated FinBERT Results (for demonstration):\")\n",
    "print(\"=\" * 80)\n",
    "finbert_df = simulate_finbert_results(headlines)\n",
    "print(finbert_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9698098",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Aggregating Sentiment Signals <a id='5-aggregation'></a>\n",
    "\n",
    "### Methods for Aggregating Multiple Sentiment Scores\n",
    "\n",
    "When dealing with multiple news articles or sources:\n",
    "\n",
    "1. **Simple Average**: Mean of all sentiment scores\n",
    "2. **Time-Weighted**: Recent articles weighted more heavily\n",
    "3. **Volume-Weighted**: Weight by article importance/reach\n",
    "4. **Exponential Moving Average**: Smooth signal over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAggregator:\n",
    "    \"\"\"\n",
    "    Aggregates multiple sentiment scores into a single signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def simple_average(sentiments: list) -> float:\n",
    "        \"\"\"Simple mean of sentiment scores.\"\"\"\n",
    "        if not sentiments:\n",
    "            return 0.0\n",
    "        return np.mean(sentiments)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_weighted_average(\n",
    "        sentiments: list,\n",
    "        timestamps: list,\n",
    "        half_life_hours: float = 24\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Time-weighted average with exponential decay.\n",
    "        More recent articles have higher weight.\n",
    "        \"\"\"\n",
    "        if not sentiments:\n",
    "            return 0.0\n",
    "        \n",
    "        now = max(timestamps)\n",
    "        decay_rate = np.log(2) / half_life_hours\n",
    "        \n",
    "        weights = []\n",
    "        for ts in timestamps:\n",
    "            hours_ago = (now - ts).total_seconds() / 3600\n",
    "            weight = np.exp(-decay_rate * hours_ago)\n",
    "            weights.append(weight)\n",
    "        \n",
    "        weights = np.array(weights)\n",
    "        weights /= weights.sum()  # Normalize\n",
    "        \n",
    "        return np.dot(sentiments, weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def volume_weighted_average(\n",
    "        sentiments: list,\n",
    "        volumes: list\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Volume-weighted average (e.g., by article views or source importance).\n",
    "        \"\"\"\n",
    "        if not sentiments:\n",
    "            return 0.0\n",
    "        \n",
    "        volumes = np.array(volumes)\n",
    "        weights = volumes / volumes.sum()\n",
    "        \n",
    "        return np.dot(sentiments, weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ema(sentiments: pd.Series, span: int = 5) -> pd.Series:\n",
    "        \"\"\"Exponential moving average of sentiment.\"\"\"\n",
    "        return sentiments.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_articles = 20\n",
    "\n",
    "# Simulate news articles over past 48 hours\n",
    "base_time = datetime(2025, 1, 23, 9, 30)\n",
    "news_data = pd.DataFrame({\n",
    "    'timestamp': [base_time - timedelta(hours=np.random.uniform(0, 48)) for _ in range(n_articles)],\n",
    "    'headline': [f\"News article {i+1} about AAPL\" for i in range(n_articles)],\n",
    "    'sentiment': np.random.uniform(-0.5, 0.5, n_articles),\n",
    "    'views': np.random.randint(1000, 100000, n_articles)  # Article views\n",
    "})\n",
    "\n",
    "news_data = news_data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate aggregated sentiment\n",
    "aggregator = SentimentAggregator()\n",
    "\n",
    "simple_avg = aggregator.simple_average(news_data['sentiment'].tolist())\n",
    "time_weighted = aggregator.time_weighted_average(\n",
    "    news_data['sentiment'].tolist(),\n",
    "    news_data['timestamp'].tolist(),\n",
    "    half_life_hours=12\n",
    ")\n",
    "volume_weighted = aggregator.volume_weighted_average(\n",
    "    news_data['sentiment'].tolist(),\n",
    "    news_data['views'].tolist()\n",
    ")\n",
    "\n",
    "print(\"Sentiment Aggregation Methods:\")\n",
    "print(f\"  Simple Average:      {simple_avg:.4f}\")\n",
    "print(f\"  Time-Weighted (12h): {time_weighted:.4f}\")\n",
    "print(f\"  Volume-Weighted:     {volume_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ddbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment over time with EMA\n",
    "news_data = news_data.set_index('timestamp').sort_index()\n",
    "news_data['sentiment_ema'] = aggregator.ema(news_data['sentiment'], span=5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.scatter(news_data.index, news_data['sentiment'], \n",
    "           alpha=0.6, s=news_data['views']/2000, label='Individual Articles')\n",
    "ax.plot(news_data.index, news_data['sentiment_ema'], \n",
    "        color='red', linewidth=2, label='EMA (span=5)')\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axhline(y=simple_avg, color='green', linestyle=':', label=f'Simple Avg: {simple_avg:.3f}')\n",
    "\n",
    "ax.set_xlabel('Timestamp')\n",
    "ax.set_ylabel('Sentiment Score')\n",
    "ax.set_title('News Sentiment Over Time (Size = Article Views)')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab3ae8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Building Trading Signals from Sentiment <a id='6-signals'></a>\n",
    "\n",
    "### Signal Generation Approaches\n",
    "\n",
    "1. **Threshold-Based**: Go long if sentiment > threshold\n",
    "2. **Z-Score**: Trade when sentiment deviates from historical mean\n",
    "3. **Sentiment Momentum**: Trade sentiment changes\n",
    "4. **Cross-Sectional**: Rank stocks by sentiment, long/short extremes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSignalGenerator:\n",
    "    \"\"\"\n",
    "    Generates trading signals from sentiment data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lookback: int = 20):\n",
    "        self.lookback = lookback\n",
    "    \n",
    "    def threshold_signal(\n",
    "        self,\n",
    "        sentiment: pd.Series,\n",
    "        long_threshold: float = 0.1,\n",
    "        short_threshold: float = -0.1\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Simple threshold-based signal.\n",
    "        Returns: 1 (long), -1 (short), 0 (neutral)\n",
    "        \"\"\"\n",
    "        signal = pd.Series(0, index=sentiment.index)\n",
    "        signal[sentiment > long_threshold] = 1\n",
    "        signal[sentiment < short_threshold] = -1\n",
    "        return signal\n",
    "    \n",
    "    def zscore_signal(\n",
    "        self,\n",
    "        sentiment: pd.Series,\n",
    "        zscore_threshold: float = 1.0\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Z-score based signal using rolling statistics.\n",
    "        Trade when sentiment deviates significantly from recent mean.\n",
    "        \"\"\"\n",
    "        rolling_mean = sentiment.rolling(self.lookback).mean()\n",
    "        rolling_std = sentiment.rolling(self.lookback).std()\n",
    "        \n",
    "        zscore = (sentiment - rolling_mean) / rolling_std\n",
    "        \n",
    "        signal = pd.Series(0, index=sentiment.index)\n",
    "        signal[zscore > zscore_threshold] = 1\n",
    "        signal[zscore < -zscore_threshold] = -1\n",
    "        \n",
    "        return signal\n",
    "    \n",
    "    def momentum_signal(\n",
    "        self,\n",
    "        sentiment: pd.Series,\n",
    "        change_threshold: float = 0.05\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Sentiment momentum signal.\n",
    "        Trade based on change in sentiment.\n",
    "        \"\"\"\n",
    "        sentiment_change = sentiment.diff()\n",
    "        \n",
    "        signal = pd.Series(0, index=sentiment.index)\n",
    "        signal[sentiment_change > change_threshold] = 1\n",
    "        signal[sentiment_change < -change_threshold] = -1\n",
    "        \n",
    "        return signal\n",
    "    \n",
    "    def composite_signal(\n",
    "        self,\n",
    "        sentiment: pd.Series,\n",
    "        price: pd.Series = None\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Composite signal combining multiple methods.\n",
    "        Returns continuous signal strength (-1 to 1).\n",
    "        \"\"\"\n",
    "        # Individual signals\n",
    "        threshold_sig = self.threshold_signal(sentiment)\n",
    "        zscore_sig = self.zscore_signal(sentiment)\n",
    "        momentum_sig = self.momentum_signal(sentiment)\n",
    "        \n",
    "        # Combine with equal weights\n",
    "        composite = (threshold_sig + zscore_sig + momentum_sig) / 3\n",
    "        \n",
    "        return composite\n",
    "\n",
    "\n",
    "# Generate synthetic daily sentiment data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=250, freq='B')\n",
    "\n",
    "# Simulate sentiment with mean-reversion and trends\n",
    "sentiment_noise = np.random.randn(250) * 0.1\n",
    "sentiment_trend = np.sin(np.linspace(0, 4*np.pi, 250)) * 0.2\n",
    "sentiment_series = pd.Series(\n",
    "    sentiment_trend + sentiment_noise,\n",
    "    index=dates,\n",
    "    name='sentiment'\n",
    ")\n",
    "\n",
    "# Generate signals\n",
    "signal_gen = SentimentSignalGenerator(lookback=20)\n",
    "\n",
    "signals_df = pd.DataFrame({\n",
    "    'sentiment': sentiment_series,\n",
    "    'threshold_signal': signal_gen.threshold_signal(sentiment_series),\n",
    "    'zscore_signal': signal_gen.zscore_signal(sentiment_series),\n",
    "    'momentum_signal': signal_gen.momentum_signal(sentiment_series),\n",
    "    'composite_signal': signal_gen.composite_signal(sentiment_series)\n",
    "})\n",
    "\n",
    "print(\"Signal Statistics:\")\n",
    "print(signals_df.describe().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signals\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Sentiment\n",
    "axes[0].plot(signals_df['sentiment'], color='blue', linewidth=1)\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=0.1, color='green', linestyle=':', alpha=0.7)\n",
    "axes[0].axhline(y=-0.1, color='red', linestyle=':', alpha=0.7)\n",
    "axes[0].set_ylabel('Sentiment')\n",
    "axes[0].set_title('Raw Sentiment Score')\n",
    "axes[0].fill_between(signals_df.index, 0, signals_df['sentiment'], \n",
    "                     where=signals_df['sentiment'] > 0, color='green', alpha=0.3)\n",
    "axes[0].fill_between(signals_df.index, 0, signals_df['sentiment'], \n",
    "                     where=signals_df['sentiment'] < 0, color='red', alpha=0.3)\n",
    "\n",
    "# Threshold signal\n",
    "axes[1].plot(signals_df['threshold_signal'], color='purple', linewidth=1, drawstyle='steps-post')\n",
    "axes[1].set_ylabel('Signal')\n",
    "axes[1].set_title('Threshold-Based Signal')\n",
    "axes[1].set_ylim(-1.5, 1.5)\n",
    "\n",
    "# Z-score signal\n",
    "axes[2].plot(signals_df['zscore_signal'], color='orange', linewidth=1, drawstyle='steps-post')\n",
    "axes[2].set_ylabel('Signal')\n",
    "axes[2].set_title('Z-Score Signal')\n",
    "axes[2].set_ylim(-1.5, 1.5)\n",
    "\n",
    "# Composite signal\n",
    "axes[3].plot(signals_df['composite_signal'], color='black', linewidth=1.5)\n",
    "axes[3].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[3].set_ylabel('Signal')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].set_title('Composite Signal (Continuous)')\n",
    "axes[3].fill_between(signals_df.index, 0, signals_df['composite_signal'], \n",
    "                     where=signals_df['composite_signal'] > 0, color='green', alpha=0.3)\n",
    "axes[3].fill_between(signals_df.index, 0, signals_df['composite_signal'], \n",
    "                     where=signals_df['composite_signal'] < 0, color='red', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284b21b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Backtesting Sentiment Strategies <a id='7-backtesting'></a>\n",
    "\n",
    "### Simple Backtest Framework\n",
    "\n",
    "Test sentiment signals against actual price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3314ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentBacktester:\n",
    "    \"\"\"\n",
    "    Simple backtester for sentiment-based strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices: pd.Series, sentiment: pd.Series):\n",
    "        # Align data\n",
    "        self.data = pd.DataFrame({\n",
    "            'price': prices,\n",
    "            'sentiment': sentiment\n",
    "        }).dropna()\n",
    "        \n",
    "        self.data['returns'] = self.data['price'].pct_change()\n",
    "    \n",
    "    def run_backtest(\n",
    "        self,\n",
    "        signal: pd.Series,\n",
    "        transaction_cost: float = 0.001\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run backtest with given signal.\n",
    "        Signal should be -1, 0, or 1.\n",
    "        \"\"\"\n",
    "        # Align signal with data\n",
    "        bt = self.data.copy()\n",
    "        bt['signal'] = signal.reindex(bt.index).fillna(0)\n",
    "        \n",
    "        # Shift signal by 1 to avoid look-ahead bias\n",
    "        bt['position'] = bt['signal'].shift(1).fillna(0)\n",
    "        \n",
    "        # Calculate strategy returns\n",
    "        bt['strategy_returns'] = bt['position'] * bt['returns']\n",
    "        \n",
    "        # Transaction costs\n",
    "        bt['trades'] = bt['position'].diff().abs()\n",
    "        bt['tc'] = bt['trades'] * transaction_cost\n",
    "        bt['strategy_returns_net'] = bt['strategy_returns'] - bt['tc']\n",
    "        \n",
    "        # Cumulative returns\n",
    "        bt['cum_returns'] = (1 + bt['returns']).cumprod() - 1\n",
    "        bt['cum_strategy'] = (1 + bt['strategy_returns_net']).cumprod() - 1\n",
    "        \n",
    "        return bt\n",
    "    \n",
    "    def calculate_metrics(self, bt: pd.DataFrame) -> dict:\n",
    "        \"\"\"Calculate performance metrics.\"\"\"\n",
    "        returns = bt['strategy_returns_net'].dropna()\n",
    "        \n",
    "        total_return = (1 + returns).prod() - 1\n",
    "        annual_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "        volatility = returns.std() * np.sqrt(252)\n",
    "        sharpe = annual_return / volatility if volatility > 0 else 0\n",
    "        \n",
    "        # Max drawdown\n",
    "        cum_returns = (1 + returns).cumprod()\n",
    "        rolling_max = cum_returns.expanding().max()\n",
    "        drawdown = (cum_returns - rolling_max) / rolling_max\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Win rate\n",
    "        winning_trades = (returns > 0).sum()\n",
    "        total_trades = (returns != 0).sum()\n",
    "        win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'Total Return': f\"{total_return:.2%}\",\n",
    "            'Annual Return': f\"{annual_return:.2%}\",\n",
    "            'Volatility': f\"{volatility:.2%}\",\n",
    "            'Sharpe Ratio': f\"{sharpe:.2f}\",\n",
    "            'Max Drawdown': f\"{max_drawdown:.2%}\",\n",
    "            'Win Rate': f\"{win_rate:.2%}\",\n",
    "            'Total Trades': int(bt['trades'].sum() / 2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download real price data\n",
    "try:\n",
    "    ticker = \"SPY\"\n",
    "    prices = yf.download(ticker, start='2024-01-01', end='2025-01-01', progress=False)['Close']\n",
    "    prices = prices.squeeze()\n",
    "    print(f\"Downloaded {len(prices)} days of {ticker} data\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "    # Create synthetic prices\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2024-01-01', periods=250, freq='B')\n",
    "    returns = np.random.randn(250) * 0.01 + 0.0003\n",
    "    prices = pd.Series(100 * np.exp(np.cumsum(returns)), index=dates, name='Close')\n",
    "    print(\"Using synthetic price data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaaa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sentiment that has some predictive power\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make sentiment partially predictive of returns\n",
    "price_returns = prices.pct_change().shift(-1)  # Next day returns\n",
    "noise = pd.Series(np.random.randn(len(prices)) * 0.15, index=prices.index)\n",
    "\n",
    "# Sentiment = weak signal + noise\n",
    "synthetic_sentiment = 0.3 * np.sign(price_returns.fillna(0)) + noise\n",
    "synthetic_sentiment = synthetic_sentiment.clip(-1, 1)\n",
    "synthetic_sentiment.name = 'sentiment'\n",
    "\n",
    "# Run backtest\n",
    "backtester = SentimentBacktester(prices, synthetic_sentiment)\n",
    "\n",
    "# Generate signal\n",
    "signal_generator = SentimentSignalGenerator(lookback=20)\n",
    "trading_signal = signal_generator.threshold_signal(\n",
    "    synthetic_sentiment,\n",
    "    long_threshold=0.1,\n",
    "    short_threshold=-0.1\n",
    ")\n",
    "\n",
    "# Run backtest\n",
    "bt_results = backtester.run_backtest(trading_signal, transaction_cost=0.001)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = backtester.calculate_metrics(bt_results)\n",
    "\n",
    "print(\"\\nBacktest Results (Threshold Signal):\")\n",
    "print(\"=\" * 40)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different signal methods\n",
    "signal_methods = {\n",
    "    'Threshold': signal_generator.threshold_signal(synthetic_sentiment),\n",
    "    'Z-Score': signal_generator.zscore_signal(synthetic_sentiment),\n",
    "    'Momentum': signal_generator.momentum_signal(synthetic_sentiment),\n",
    "    'Composite': np.sign(signal_generator.composite_signal(synthetic_sentiment))\n",
    "}\n",
    "\n",
    "all_metrics = {}\n",
    "all_bt = {}\n",
    "\n",
    "for name, signal in signal_methods.items():\n",
    "    bt = backtester.run_backtest(signal)\n",
    "    all_bt[name] = bt\n",
    "    all_metrics[name] = backtester.calculate_metrics(bt)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics).T\n",
    "print(\"\\nSignal Method Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Cumulative returns comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(bt_results['cum_returns'] * 100, label='Buy & Hold', color='gray', linewidth=2)\n",
    "\n",
    "colors = ['blue', 'green', 'orange', 'purple']\n",
    "for (name, bt), color in zip(all_bt.items(), colors):\n",
    "    ax1.plot(bt['cum_strategy'] * 100, label=name, color=color, linewidth=1.5)\n",
    "\n",
    "ax1.set_ylabel('Cumulative Return (%)')\n",
    "ax1.set_title('Sentiment Strategy Performance Comparison')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment and signal\n",
    "ax2 = axes[1]\n",
    "ax2.plot(synthetic_sentiment, color='blue', alpha=0.6, label='Sentiment')\n",
    "ax2.fill_between(bt_results.index, 0, bt_results['position'], \n",
    "                  where=bt_results['position'] > 0, color='green', alpha=0.3, label='Long')\n",
    "ax2.fill_between(bt_results.index, 0, bt_results['position'], \n",
    "                  where=bt_results['position'] < 0, color='red', alpha=0.3, label='Short')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('Sentiment / Position')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_title('Sentiment Score and Trading Position')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aba826",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Interview Questions <a id='8-interview'></a>\n",
    "\n",
    "### Conceptual Questions\n",
    "\n",
    "**Q1: What are the key differences between lexicon-based and ML-based sentiment analysis?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "| Aspect | Lexicon-Based | ML-Based |\n",
    "|--------|---------------|----------|\n",
    "| Speed | Very fast | Slower (inference) |\n",
    "| Interpretability | High (word counts) | Lower (black box) |\n",
    "| Context | Limited (word-level) | Rich (sentence-level) |\n",
    "| Negation handling | Poor | Good |\n",
    "| Domain adaptation | Requires custom lexicons | Requires fine-tuning |\n",
    "| Training data | None needed | Requires labeled data |\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q2: How would you handle look-ahead bias in sentiment-based trading signals?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "1. **Timestamp alignment**: Use only sentiment available before the trading decision\n",
    "2. **Signal lag**: Shift signal by 1 period before calculating returns\n",
    "3. **Point-in-time data**: Use data as it was known at that moment (no revisions)\n",
    "4. **News timing**: Account for news publication time vs. market hours\n",
    "5. **Embargo periods**: Consider when earnings calls become public\n",
    "\n",
    "</details>\n",
    "\n",
    "**Q3: How does sentiment decay affect trading signals?**\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "- **Information half-life**: News impact decays over time\n",
    "- **Signal dilution**: Old news becomes less relevant\n",
    "- **Regime changes**: Market conditions affect sentiment-price relationship\n",
    "- **Solution**: Use time-weighted aggregation or exponential decay\n",
    "- **Typical decay**: 12-48 hours for news, longer for earnings\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42571a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interview Coding Challenge: Sentiment-Price Correlation Analysis\n",
    "\n",
    "def analyze_sentiment_price_relationship(\n",
    "    sentiment: pd.Series,\n",
    "    prices: pd.Series,\n",
    "    lags: list = [0, 1, 2, 3, 5]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze lead-lag relationship between sentiment and returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentiment : pd.Series\n",
    "        Daily sentiment scores\n",
    "    prices : pd.Series\n",
    "        Daily prices\n",
    "    lags : list\n",
    "        Number of days to lag returns (positive = future returns)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with correlation analysis\n",
    "    \"\"\"\n",
    "    returns = prices.pct_change()\n",
    "    \n",
    "    results = []\n",
    "    for lag in lags:\n",
    "        # Shift returns (positive lag = future returns)\n",
    "        if lag >= 0:\n",
    "            lagged_returns = returns.shift(-lag)\n",
    "            description = f\"Returns t+{lag}\"\n",
    "        else:\n",
    "            lagged_returns = returns.shift(-lag)\n",
    "            description = f\"Returns t{lag}\"\n",
    "        \n",
    "        # Align and calculate correlation\n",
    "        aligned = pd.concat([sentiment, lagged_returns], axis=1).dropna()\n",
    "        aligned.columns = ['sentiment', 'returns']\n",
    "        \n",
    "        corr = aligned['sentiment'].corr(aligned['returns'])\n",
    "        \n",
    "        # Information coefficient (rank correlation)\n",
    "        ic = aligned['sentiment'].corr(aligned['returns'], method='spearman')\n",
    "        \n",
    "        results.append({\n",
    "            'Lag': lag,\n",
    "            'Description': description,\n",
    "            'Pearson Corr': round(corr, 4),\n",
    "            'Spearman IC': round(ic, 4),\n",
    "            'N': len(aligned)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "correlation_analysis = analyze_sentiment_price_relationship(\n",
    "    synthetic_sentiment,\n",
    "    prices,\n",
    "    lags=[0, 1, 2, 3, 5, 10]\n",
    ")\n",
    "\n",
    "print(\"Sentiment-Price Lead-Lag Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(correlation_analysis.to_string(index=False))\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Positive correlation at t+1 suggests sentiment predicts next-day returns\")\n",
    "print(\"- Higher Spearman IC indicates better rank-ordering ability\")\n",
    "print(\"- Decay in correlation over lags shows information half-life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a8081",
   "metadata": {},
   "source": [
    "### Practical Interview Question\n",
    "\n",
    "**Q4: Design a sentiment-based trading system for a fund processing 10,000 news articles daily.**\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "News Feed → Pre-processing → Entity Extraction → Sentiment Scoring → Aggregation → Signal Generation → Execution\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Data Ingestion**:\n",
    "   - Multiple news sources (Reuters, Bloomberg, social media)\n",
    "   - Streaming pipeline (Kafka/Kinesis)\n",
    "   - Deduplication and filtering\n",
    "\n",
    "2. **NLP Pipeline**:\n",
    "   - Named Entity Recognition (link articles to tickers)\n",
    "   - FinBERT for sentiment scoring\n",
    "   - Batch processing for efficiency (GPU clusters)\n",
    "\n",
    "3. **Signal Generation**:\n",
    "   - Time-weighted aggregation per ticker\n",
    "   - Cross-sectional ranking\n",
    "   - Confidence scoring based on article volume\n",
    "\n",
    "4. **Risk Management**:\n",
    "   - Position limits based on sentiment confidence\n",
    "   - Correlation with existing positions\n",
    "   - Drawdown controls\n",
    "\n",
    "5. **Monitoring**:\n",
    "   - Real-time sentiment dashboard\n",
    "   - Model drift detection\n",
    "   - PnL attribution to sentiment signals\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b9faf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Preprocessing matters**: Financial text requires domain-specific handling\n",
    "\n",
    "2. **Multiple approaches**: Lexicon (fast, interpretable) vs ML (accurate, contextual)\n",
    "\n",
    "3. **Aggregation is crucial**: How you combine multiple signals affects performance\n",
    "\n",
    "4. **Avoid look-ahead bias**: Always lag signals before calculating returns\n",
    "\n",
    "5. **Transaction costs**: Include realistic costs in backtests\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Day 04: News Event Detection\n",
    "- Day 05: Entity Recognition for Finance\n",
    "- Day 06: Topic Modeling on Earnings Calls\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created for Week 19: NLP & Alternative Data*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
