{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad453cfc",
   "metadata": {},
   "source": [
    "# Day 4: FinBERT and Transformer Models for Financial Text\n",
    "\n",
    "## Week 19: NLP & Alternative Data\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand transformer architecture and self-attention mechanism\n",
    "- Master FinBERT for financial sentiment analysis\n",
    "- Learn tokenization strategies for financial text\n",
    "- Build sentiment classification pipelines for news and earnings calls\n",
    "- Fine-tune transformers on custom financial datasets\n",
    "- Extract embeddings for trading signals and document similarity\n",
    "\n",
    "### Key Interview Topics\n",
    "- BERT vs traditional NLP methods (TF-IDF, Word2Vec)\n",
    "- Self-attention mechanism and positional encoding\n",
    "- Transfer learning in NLP for finance\n",
    "- Fine-tuning strategies and domain adaptation\n",
    "- Handling financial jargon and numerical data in text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aee075",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, ConstantLR\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification,\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    pipeline, Trainer, TrainingArguments,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    accuracy_score, f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2af737",
   "metadata": {},
   "source": [
    "## 2. Understanding Transformer Architecture\n",
    "\n",
    "### The Revolution in NLP\n",
    "\n",
    "The Transformer architecture, introduced in \"Attention Is All You Need\" (Vaswani et al., 2017), revolutionized NLP by replacing recurrence with **self-attention**, enabling:\n",
    "\n",
    "1. **Parallelization**: Process entire sequences simultaneously\n",
    "2. **Long-range dependencies**: Attend to any position in the sequence\n",
    "3. **Transfer learning**: Pre-train on massive corpora, fine-tune for specific tasks\n",
    "\n",
    "### Key Components:\n",
    "- **Self-Attention Mechanism**: Weighs importance of different tokens\n",
    "- **Positional Encoding**: Injects sequence order information\n",
    "- **Multi-Head Attention**: Multiple attention patterns in parallel\n",
    "- **Feed-Forward Networks**: Non-linear transformations at each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Self-Attention mechanism for educational purposes.\n",
    "    \n",
    "    Key Concepts:\n",
    "    - Query (Q): \"What am I looking for?\"\n",
    "    - Key (K): \"What do I contain?\"\n",
    "    - Value (V): \"What information do I provide?\"\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "# Demonstrate self-attention on a simple example\n",
    "print(\"=\" * 60)\n",
    "print(\"SELF-ATTENTION MECHANISM DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple embedding (3 tokens, 4-dimensional embeddings)\n",
    "embed_dim = 4\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Simulated token embeddings for: [\"stock\", \"price\", \"rises\"]\n",
    "sample_embeddings = torch.tensor([\n",
    "    [[0.5, 0.3, 0.1, 0.8],   # \"stock\"\n",
    "     [0.2, 0.9, 0.4, 0.1],   # \"price\"  \n",
    "     [0.7, 0.2, 0.6, 0.3]]   # \"rises\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Apply self-attention\n",
    "attention_layer = SelfAttention(embed_dim=embed_dim)\n",
    "output, attention_weights = attention_layer(sample_embeddings, return_attention=True)\n",
    "\n",
    "print(f\"\\nInput shape: {sample_embeddings.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (how much each token attends to others):\")\n",
    "print(attention_weights.squeeze().detach().numpy().round(3))\n",
    "\n",
    "# Visualize attention\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "tokens = [\"stock\", \"price\", \"rises\"]\n",
    "sns.heatmap(\n",
    "    attention_weights.squeeze().detach().numpy(),\n",
    "    xticklabels=tokens,\n",
    "    yticklabels=tokens,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='Blues',\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_xlabel('Key (attending to)')\n",
    "ax.set_ylabel('Query (from)')\n",
    "ax.set_title('Self-Attention Weights\\n(Row shows what each token attends to)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Interpretation:\")\n",
    "print(\"Each row shows how much attention a token pays to other tokens.\")\n",
    "print(\"This allows the model to create context-aware representations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding adds position information to embeddings.\n",
    "    \n",
    "    Since transformers process all tokens in parallel (no recurrence),\n",
    "    we need to explicitly inject position information.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Visualize positional encodings\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITIONAL ENCODING VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "pe_layer = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Get the positional encodings\n",
    "pe = pe_layer.pe.squeeze().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot first few dimensions\n",
    "ax1 = axes[0]\n",
    "for i in range(8):\n",
    "    ax1.plot(pe[:50, i], label=f'dim {i}', alpha=0.7)\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Encoding Value')\n",
    "ax1.set_title('Positional Encoding Values (First 8 Dimensions)')\n",
    "ax1.legend(loc='upper right', ncol=2, fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap of all encodings\n",
    "ax2 = axes[1]\n",
    "im = ax2.imshow(pe[:50, :32].T, aspect='auto', cmap='RdBu_r')\n",
    "ax2.set_xlabel('Position in Sequence')\n",
    "ax2.set_ylabel('Embedding Dimension')\n",
    "ax2.set_title('Positional Encoding Heatmap')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insight:\")\n",
    "print(\"- Different dimensions have different frequencies (sine/cosine waves)\")\n",
    "print(\"- The model can learn to attend to relative positions\")\n",
    "print(\"- Positions close together have similar encodings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd57ca3",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained BERT Model\n",
    "\n",
    "### BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "BERT was pre-trained on two tasks:\n",
    "1. **Masked Language Modeling (MLM)**: Predict masked tokens using bidirectional context\n",
    "2. **Next Sentence Prediction (NSP)**: Predict if sentence B follows sentence A\n",
    "\n",
    "Key characteristics:\n",
    "- **bert-base**: 12 layers, 768 hidden, 12 attention heads, 110M parameters\n",
    "- **bert-large**: 24 layers, 1024 hidden, 16 attention heads, 340M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc66501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base BERT model and tokenizer\n",
    "print(\"Loading BERT base model...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Move model to device\n",
    "bert_model = bert_model.to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "# Explore model architecture\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BERT BASE MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = bert_model.config\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Hidden size: {config.hidden_size}\")\n",
    "print(f\"  - Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"  - Attention heads: {config.num_attention_heads}\")\n",
    "print(f\"  - Intermediate size: {config.intermediate_size}\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  - Max position embeddings: {config.max_position_embeddings}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in bert_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in bert_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nParameter count:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6311e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process text through BERT\n",
    "sample_text = \"The Federal Reserve raised interest rates by 25 basis points.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = bert_tokenizer(\n",
    "    sample_text,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT TOKENIZATION AND OUTPUT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOriginal text: {sample_text}\")\n",
    "print(f\"\\nTokenized IDs: {inputs['input_ids'].tolist()}\")\n",
    "print(f\"Tokens: {bert_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "print(f\"Attention mask: {inputs['attention_mask'].tolist()}\")\n",
    "\n",
    "# Get BERT output\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = bert_model(**inputs)\n",
    "\n",
    "# BERT returns:\n",
    "# - last_hidden_state: (batch, seq_len, hidden_size) - contextual embeddings for each token\n",
    "# - pooler_output: (batch, hidden_size) - [CLS] token transformed for classification\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  - Last hidden state: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  - Pooler output: {outputs.pooler_output.shape}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Outputs:\")\n",
    "print(\"  - last_hidden_state: Contextual embeddings for ALL tokens\")\n",
    "print(\"  - pooler_output: [CLS] token embedding for classification tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2507cef",
   "metadata": {},
   "source": [
    "## 4. Load FinBERT for Financial Sentiment Analysis\n",
    "\n",
    "### FinBERT: Domain-Adapted BERT for Finance\n",
    "\n",
    "**FinBERT** (ProsusAI/finbert) is BERT fine-tuned on financial text:\n",
    "- Pre-trained on financial news and communications\n",
    "- Fine-tuned for sentiment classification (positive, negative, neutral)\n",
    "- Better understanding of financial jargon, company names, and market terminology\n",
    "\n",
    "### Why Domain-Specific Models Matter in Finance:\n",
    "- \"Bull\" in finance â‰  \"bull\" in general English\n",
    "- Numbers and percentages carry different meanings\n",
    "- Subtle language differences (\"missed expectations\" vs \"beat estimates\")\n",
    "- Temporal references to earnings, quarters, fiscal years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8fed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "print(\"Loading FinBERT model (ProsusAI/finbert)...\")\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# Move to device\n",
    "finbert_model = finbert_model.to(device)\n",
    "finbert_model.eval()\n",
    "\n",
    "# Model info\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINBERT MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "config = finbert_model.config\n",
    "print(f\"\\nModel type: {config.model_type}\")\n",
    "print(f\"Number of labels: {config.num_labels}\")\n",
    "print(f\"Label mapping: {config.id2label}\")\n",
    "\n",
    "# Parameter count\n",
    "total_params = sum(p.numel() for p in finbert_model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Create a sentiment analysis pipeline for convenience\n",
    "finbert_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=finbert_model,\n",
    "    tokenizer=finbert_tokenizer,\n",
    "    device=0 if device.type == 'cuda' else -1 if device.type == 'cpu' else 'mps'\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… FinBERT loaded and ready for sentiment analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61506f20",
   "metadata": {},
   "source": [
    "## 5. Tokenization for Financial Text\n",
    "\n",
    "Understanding tokenization is crucial for financial NLP:\n",
    "- Financial terms may be split into subwords\n",
    "- Numbers and percentages have special handling\n",
    "- Company names and tickers may be tokenized unexpectedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial text examples for tokenization analysis\n",
    "financial_texts = [\n",
    "    \"AAPL reported Q3 earnings of $1.26 EPS, beating estimates by 15%.\",\n",
    "    \"The Fed raised rates by 75bps amid persistent inflation concerns.\",\n",
    "    \"Goldman Sachs upgraded NVDA to 'Buy' with a $500 price target.\",\n",
    "    \"Credit Suisse faces $4.7B loss from Archegos margin call.\",\n",
    "    \"YoY revenue growth of 23% driven by strong SaaS adoption.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINANCIAL TEXT TOKENIZATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for text in financial_texts:\n",
    "    tokens = finbert_tokenizer.tokenize(text)\n",
    "    encoding = finbert_tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ“ Text: {text}\")\n",
    "    print(f\"   Tokens ({len(tokens)}): {tokens}\")\n",
    "    print(f\"   Token IDs: {encoding['input_ids'][0].tolist()[:15]}...\")\n",
    "    print(f\"   Attention Mask: {encoding['attention_mask'][0].tolist()[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive: Special tokens and padding\n",
    "print(\"=\" * 60)\n",
    "print(\"SPECIAL TOKENS IN BERT/FinBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n[CLS] token ID: {finbert_tokenizer.cls_token_id} -> '{finbert_tokenizer.cls_token}'\")\n",
    "print(f\"[SEP] token ID: {finbert_tokenizer.sep_token_id} -> '{finbert_tokenizer.sep_token}'\")\n",
    "print(f\"[PAD] token ID: {finbert_tokenizer.pad_token_id} -> '{finbert_tokenizer.pad_token}'\")\n",
    "print(f\"[UNK] token ID: {finbert_tokenizer.unk_token_id} -> '{finbert_tokenizer.unk_token}'\")\n",
    "print(f\"[MASK] token ID: {finbert_tokenizer.mask_token_id} -> '{finbert_tokenizer.mask_token}'\")\n",
    "\n",
    "# Demonstrate padding strategies\n",
    "short_text = \"Stock rises.\"\n",
    "long_text = \"Apple Inc. reported quarterly earnings that significantly exceeded Wall Street expectations.\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PADDING STRATEGIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pad to max length\n",
    "encoded_padded = finbert_tokenizer(\n",
    "    [short_text, long_text],\n",
    "    padding='max_length',\n",
    "    max_length=20,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(f\"\\nWith padding='max_length' (max_length=20):\")\n",
    "for i, text in enumerate([short_text, long_text]):\n",
    "    tokens = finbert_tokenizer.convert_ids_to_tokens(encoded_padded['input_ids'][i])\n",
    "    print(f\"\\n  Text: {text}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print(f\"  Attention: {encoded_padded['attention_mask'][i].tolist()}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Points:\")\n",
    "print(\"  - [CLS] always at the beginning (used for classification)\")\n",
    "print(\"  - [SEP] marks end of sentence\")\n",
    "print(\"  - [PAD] tokens have attention_mask=0 (ignored by model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ffd06",
   "metadata": {},
   "source": [
    "## 6. Financial Sentiment Classification\n",
    "\n",
    "Now let's use FinBERT for real financial sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807bf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial news headlines for sentiment analysis\n",
    "financial_headlines = [\n",
    "    # Positive sentiment\n",
    "    \"Apple beats earnings expectations with record iPhone sales\",\n",
    "    \"Tesla stock surges 15% after announcing breakthrough in battery technology\",\n",
    "    \"Microsoft announces $60 billion share buyback program\",\n",
    "    \"Amazon Web Services revenue grows 37% year-over-year\",\n",
    "    \"JPMorgan reports strongest quarterly profit in company history\",\n",
    "    \n",
    "    # Negative sentiment\n",
    "    \"Boeing faces new safety concerns after whistleblower revelations\",\n",
    "    \"Credit Suisse stock plunges amid liquidity crisis fears\",\n",
    "    \"Meta announces 11,000 layoffs as advertising revenue declines\",\n",
    "    \"FTX files for bankruptcy after $8 billion shortfall discovered\",\n",
    "    \"Evergrande misses bond payment deadline, default risk rises\",\n",
    "    \n",
    "    # Neutral sentiment\n",
    "    \"Federal Reserve holds interest rates steady at current levels\",\n",
    "    \"Quarterly earnings report scheduled for release next Tuesday\",\n",
    "    \"Company announces leadership transition plan for 2024\",\n",
    "    \"Stock market closes flat after mixed economic data\",\n",
    "    \"Analysts maintain neutral rating on retail sector stocks\"\n",
    "]\n",
    "\n",
    "def analyze_sentiment(texts, model, tokenizer, batch_size=8):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a list of texts using FinBERT.\n",
    "    \n",
    "    Returns DataFrame with text, sentiment label, and confidence scores.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            predictions = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        # Process results\n",
    "        for j, text in enumerate(batch_texts):\n",
    "            pred_label = model.config.id2label[predictions[j].item()]\n",
    "            confidence = probs[j][predictions[j]].item()\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'sentiment': pred_label,\n",
    "                'confidence': confidence,\n",
    "                'positive_prob': probs[j][2].item() if 2 in model.config.id2label else probs[j][0].item(),\n",
    "                'negative_prob': probs[j][0].item() if 0 in model.config.id2label else probs[j][1].item(),\n",
    "                'neutral_prob': probs[j][1].item() if 1 in model.config.id2label else probs[j][2].item()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze sentiment\n",
    "print(\"=\" * 70)\n",
    "print(\"FINANCIAL SENTIMENT ANALYSIS WITH FINBERT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sentiment_df = analyze_sentiment(financial_headlines, finbert_model, finbert_tokenizer)\n",
    "\n",
    "# Display results\n",
    "for idx, row in sentiment_df.iterrows():\n",
    "    emoji = \"ðŸŸ¢\" if row['sentiment'] == 'positive' else \"ðŸ”´\" if row['sentiment'] == 'negative' else \"âšª\"\n",
    "    print(f\"\\n{emoji} [{row['sentiment'].upper():^8}] (conf: {row['confidence']:.2%})\")\n",
    "    print(f\"   {row['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c920f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Sentiment counts\n",
    "ax1 = axes[0]\n",
    "sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#95a5a6'}\n",
    "bars = ax1.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "               color=[colors[s] for s in sentiment_counts.index])\n",
    "ax1.set_xlabel('Sentiment')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Sentiment Distribution')\n",
    "for bar, count in zip(bars, sentiment_counts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             str(count), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Confidence distribution\n",
    "ax2 = axes[1]\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    subset = sentiment_df[sentiment_df['sentiment'] == sentiment]['confidence']\n",
    "    if len(subset) > 0:\n",
    "        ax2.hist(subset, bins=10, alpha=0.6, label=sentiment, color=colors[sentiment])\n",
    "ax2.set_xlabel('Confidence Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Confidence Distribution by Sentiment')\n",
    "ax2.legend()\n",
    "\n",
    "# Probability breakdown for sample texts\n",
    "ax3 = axes[2]\n",
    "sample_indices = [0, 5, 10]  # One positive, one negative, one neutral\n",
    "x = np.arange(len(sample_indices))\n",
    "width = 0.25\n",
    "\n",
    "pos_probs = [sentiment_df.iloc[i]['positive_prob'] for i in sample_indices]\n",
    "neg_probs = [sentiment_df.iloc[i]['negative_prob'] for i in sample_indices]\n",
    "neu_probs = [sentiment_df.iloc[i]['neutral_prob'] for i in sample_indices]\n",
    "\n",
    "ax3.bar(x - width, pos_probs, width, label='Positive', color=colors['positive'])\n",
    "ax3.bar(x, neg_probs, width, label='Negative', color=colors['negative'])\n",
    "ax3.bar(x + width, neu_probs, width, label='Neutral', color=colors['neutral'])\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(['Headline 1', 'Headline 6', 'Headline 11'])\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.set_title('Probability Breakdown for Sample Headlines')\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal headlines analyzed: {len(sentiment_df)}\")\n",
    "print(f\"\\nSentiment breakdown:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    pct = count / len(sentiment_df) * 100\n",
    "    print(f\"  - {sentiment.capitalize()}: {count} ({pct:.1f}%)\")\n",
    "print(f\"\\nAverage confidence: {sentiment_df['confidence'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45079cfe",
   "metadata": {},
   "source": [
    "## 7. Batch Processing Financial News\n",
    "\n",
    "Efficient batch processing is essential for analyzing large volumes of financial text in production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be79e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNewsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for financial news text.\n",
    "    Handles tokenization and batching efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=256, labels=None):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            \n",
    "        return item\n",
    "\n",
    "def batch_inference(texts, model, tokenizer, batch_size=16):\n",
    "    \"\"\"\n",
    "    Efficient batch inference with progress bar.\n",
    "    \"\"\"\n",
    "    dataset = FinancialNewsDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            preds = torch.argmax(probs, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "# Generate larger synthetic dataset for demonstration\n",
    "np.random.seed(42)\n",
    "synthetic_news = []\n",
    "templates_positive = [\n",
    "    \"{company} reports record quarterly revenue of ${revenue}B\",\n",
    "    \"{company} stock jumps {pct}% on strong earnings beat\",\n",
    "    \"{company} raises full-year guidance amid robust demand\",\n",
    "    \"Analysts upgrade {company} after impressive product launch\",\n",
    "    \"{company} announces ${amount}B share buyback program\"\n",
    "]\n",
    "templates_negative = [\n",
    "    \"{company} misses earnings estimates, stock falls {pct}%\",\n",
    "    \"{company} faces regulatory probe over compliance issues\",\n",
    "    \"{company} announces {num} layoffs amid restructuring\",\n",
    "    \"{company} warns of supply chain disruptions affecting Q4\",\n",
    "    \"Credit rating agency downgrades {company} to junk status\"\n",
    "]\n",
    "templates_neutral = [\n",
    "    \"{company} schedules investor day for next month\",\n",
    "    \"{company} appoints new CFO effective January 1\",\n",
    "    \"{company} to release earnings report on Tuesday\",\n",
    "    \"Trading volume in {company} remains average\",\n",
    "    \"{company} maintains dividend at ${amount} per share\"\n",
    "]\n",
    "\n",
    "companies = ['Apple', 'Microsoft', 'Google', 'Amazon', 'Tesla', 'Meta', \n",
    "             'Netflix', 'NVIDIA', 'JPMorgan', 'Goldman Sachs']\n",
    "\n",
    "for _ in range(30):\n",
    "    company = np.random.choice(companies)\n",
    "    template = np.random.choice(templates_positive)\n",
    "    synthetic_news.append(template.format(\n",
    "        company=company, revenue=np.random.randint(20, 100),\n",
    "        pct=np.random.randint(5, 20), amount=np.random.randint(10, 60), num=1000\n",
    "    ))\n",
    "    \n",
    "for _ in range(30):\n",
    "    company = np.random.choice(companies)\n",
    "    template = np.random.choice(templates_negative)\n",
    "    synthetic_news.append(template.format(\n",
    "        company=company, revenue=np.random.randint(20, 100),\n",
    "        pct=np.random.randint(5, 20), amount=np.random.randint(10, 60),\n",
    "        num=np.random.randint(1000, 10000)\n",
    "    ))\n",
    "    \n",
    "for _ in range(30):\n",
    "    company = np.random.choice(companies)\n",
    "    template = np.random.choice(templates_neutral)\n",
    "    synthetic_news.append(template.format(\n",
    "        company=company, revenue=np.random.randint(20, 100),\n",
    "        pct=np.random.randint(5, 20), amount=f\"{np.random.uniform(0.5, 3):.2f}\", num=1000\n",
    "    ))\n",
    "\n",
    "print(f\"Generated {len(synthetic_news)} synthetic news headlines\")\n",
    "print(f\"\\nSample headlines:\")\n",
    "for i in [0, 30, 60]:\n",
    "    print(f\"  - {synthetic_news[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch inference\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH INFERENCE ON SYNTHETIC NEWS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions, probabilities = batch_inference(\n",
    "    synthetic_news, finbert_model, finbert_tokenizer, batch_size=16\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Processed {len(synthetic_news)} headlines in {elapsed_time:.2f} seconds\")\n",
    "print(f\"   Throughput: {len(synthetic_news)/elapsed_time:.1f} headlines/second\")\n",
    "\n",
    "# Create results DataFrame\n",
    "batch_results = pd.DataFrame({\n",
    "    'text': synthetic_news,\n",
    "    'prediction': [finbert_model.config.id2label[p] for p in predictions],\n",
    "    'positive_prob': probabilities[:, 2] if 2 in finbert_model.config.id2label else probabilities[:, 0],\n",
    "    'negative_prob': probabilities[:, 0] if 0 in finbert_model.config.id2label else probabilities[:, 1],\n",
    "    'neutral_prob': probabilities[:, 1] if 1 in finbert_model.config.id2label else probabilities[:, 2],\n",
    "    'confidence': np.max(probabilities, axis=1)\n",
    "})\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BATCH RESULTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prediction_counts = batch_results['prediction'].value_counts()\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "for pred, count in prediction_counts.items():\n",
    "    print(f\"  {pred.capitalize()}: {count} ({count/len(batch_results)*100:.1f}%)\")\n",
    "\n",
    "# Show some predictions with their expected sentiment\n",
    "print(\"\\nSample predictions (first 30 were designed as positive):\")\n",
    "for i in [0, 1, 2]:\n",
    "    row = batch_results.iloc[i]\n",
    "    print(f\"  ðŸ“° {row['text'][:60]}...\")\n",
    "    print(f\"     Predicted: {row['prediction']} (conf: {row['confidence']:.2%})\")\n",
    "\n",
    "print(\"\\nSample predictions (31-60 were designed as negative):\")\n",
    "for i in [30, 31, 32]:\n",
    "    row = batch_results.iloc[i]\n",
    "    print(f\"  ðŸ“° {row['text'][:60]}...\")\n",
    "    print(f\"     Predicted: {row['prediction']} (conf: {row['confidence']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa731c",
   "metadata": {},
   "source": [
    "## 8. Fine-tuning FinBERT on Custom Data\n",
    "\n",
    "Fine-tuning adapts the pre-trained model to your specific financial domain or task. Key considerations:\n",
    "- **Learning rate**: Lower than training from scratch (2e-5 to 5e-5)\n",
    "- **Epochs**: Few epochs to avoid overfitting (2-4 typically)\n",
    "- **Warmup**: Gradual learning rate increase prevents catastrophic forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd61f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a labeled dataset for fine-tuning demonstration\n",
    "# Labels: 0=negative, 1=neutral, 2=positive\n",
    "\n",
    "labeled_data = []\n",
    "\n",
    "# Positive examples (label=2)\n",
    "positive_texts = [\n",
    "    \"Revenue exceeded analyst expectations by 15%\",\n",
    "    \"Company announces special dividend of $2.50 per share\",\n",
    "    \"Strong forward guidance lifted investor confidence\",\n",
    "    \"Profit margins expanded to record levels\",\n",
    "    \"Customer acquisition costs declined significantly\",\n",
    "    \"Market share gains accelerated in key segments\",\n",
    "    \"Cash flow generation surpassed all forecasts\",\n",
    "    \"New product launch received exceptional reception\",\n",
    "    \"Strategic partnership expected to boost revenue\",\n",
    "    \"Operating efficiency improvements drove profit growth\"\n",
    "]\n",
    "labeled_data.extend([(text, 2) for text in positive_texts])\n",
    "\n",
    "# Negative examples (label=0)\n",
    "negative_texts = [\n",
    "    \"Revenue missed consensus estimates by 8%\",\n",
    "    \"Company suspends dividend amid cash concerns\",\n",
    "    \"Weak guidance disappointed investors\",\n",
    "    \"Profit margins contracted due to cost pressures\",\n",
    "    \"Customer churn rate increased substantially\",\n",
    "    \"Market share losses in core business\",\n",
    "    \"Cash burn rate raised liquidity concerns\",\n",
    "    \"Product recall will impact quarterly results\",\n",
    "    \"Key partnership terminated unexpectedly\",\n",
    "    \"Restructuring charges will pressure earnings\"\n",
    "]\n",
    "labeled_data.extend([(text, 0) for text in negative_texts])\n",
    "\n",
    "# Neutral examples (label=1)\n",
    "neutral_texts = [\n",
    "    \"Quarterly results in line with expectations\",\n",
    "    \"Management reiterated full-year guidance\",\n",
    "    \"Board approved routine capital expenditure\",\n",
    "    \"Annual shareholder meeting scheduled\",\n",
    "    \"Company filed regular SEC disclosures\",\n",
    "    \"Trading volume remained within normal range\",\n",
    "    \"Analyst coverage initiated with hold rating\",\n",
    "    \"Executive compensation disclosed in proxy\",\n",
    "    \"Credit facility renewed at existing terms\",\n",
    "    \"Industry conference presentation scheduled\"\n",
    "]\n",
    "labeled_data.extend([(text, 1) for text in neutral_texts])\n",
    "\n",
    "# Shuffle and split\n",
    "np.random.shuffle(labeled_data)\n",
    "texts = [item[0] for item in labeled_data]\n",
    "labels = [item[1] for item in labeled_data]\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_texts)} samples\")\n",
    "print(f\"Validation set: {len(val_texts)} samples\")\n",
    "print(f\"\\nLabel distribution (train):\")\n",
    "for label in [0, 1, 2]:\n",
    "    count = train_labels.count(label)\n",
    "    print(f\"  Label {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e95be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningDataset(Dataset):\n",
    "    \"\"\"Dataset class for fine-tuning with labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            total_loss += outputs.loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "# Fine-tuning setup\n",
    "print(\"=\" * 60)\n",
    "print(\"FINE-TUNING SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load a fresh model for fine-tuning\n",
    "finetune_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'ProsusAI/finbert',\n",
    "    num_labels=3\n",
    ")\n",
    "finetune_model = finetune_model.to(device)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FineTuningDataset(train_texts, train_labels, finbert_tokenizer)\n",
    "val_dataset = FineTuningDataset(val_texts, val_labels, finbert_tokenizer)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "warmup_ratio = 0.1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(finetune_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Learning rate: {learning_rate}\")\n",
    "print(f\"  - Epochs: {num_epochs}\")\n",
    "print(f\"  - Total steps: {total_steps}\")\n",
    "print(f\"  - Warmup steps: {warmup_steps}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
