{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a505ec82",
   "metadata": {},
   "source": [
    "# Day 5: Alternative Data Sources for Trading\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the landscape of alternative data in quantitative finance\n",
    "- Learn to source, process, and analyze various alternative data types\n",
    "- Build alpha signals from satellite imagery, web scraping, and social media\n",
    "- Implement data quality frameworks for alternative data\n",
    "- Understand regulatory and ethical considerations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec434e",
   "metadata": {},
   "source": [
    "## 1. Introduction to Alternative Data\n",
    "\n",
    "### What is Alternative Data?\n",
    "\n",
    "Alternative data refers to non-traditional data sources used by investors to gain insights beyond conventional financial statements and market data.\n",
    "\n",
    "### Categories of Alternative Data\n",
    "\n",
    "| Category | Examples | Use Cases |\n",
    "|----------|----------|----------|\n",
    "| **Satellite & Geolocation** | Parking lot imagery, ship tracking, crop monitoring | Retail sales, supply chain, commodities |\n",
    "| **Web & Social Media** | Twitter sentiment, Reddit discussions, web traffic | Market sentiment, product launches |\n",
    "| **Transaction Data** | Credit card purchases, POS data | Consumer spending, revenue nowcasting |\n",
    "| **Sensor & IoT** | Weather sensors, industrial sensors | Agriculture, energy, manufacturing |\n",
    "| **Text & NLP** | SEC filings, earnings calls, news | Event-driven strategies |\n",
    "| **App Usage** | Mobile app downloads, usage metrics | Company growth, user engagement |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40894b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "# For data processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Alternative Data Analysis Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b69a3",
   "metadata": {},
   "source": [
    "## 2. Satellite & Geolocation Data\n",
    "\n",
    "### 2.1 Parking Lot Analysis (Retail Nowcasting)\n",
    "\n",
    "Satellite imagery of retail parking lots can predict quarterly revenue before official announcements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParkingLotAnalyzer:\n",
    "    \"\"\"\n",
    "    Simulates satellite parking lot analysis for retail revenue prediction.\n",
    "    In production, this would use actual satellite imagery APIs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_stores: int = 100):\n",
    "        self.n_stores = n_stores\n",
    "        self.store_capacity = np.random.randint(50, 500, n_stores)\n",
    "        \n",
    "    def generate_parking_data(self, n_days: int = 90) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate simulated parking lot occupancy data.\n",
    "        \n",
    "        In reality, this comes from:\n",
    "        - Orbital Insight\n",
    "        - RS Metrics\n",
    "        - Geospatial Insight\n",
    "        \"\"\"\n",
    "        dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "        \n",
    "        data = []\n",
    "        for date in dates:\n",
    "            # Seasonal pattern (higher in Q4 for retail)\n",
    "            month = date.month\n",
    "            seasonal_factor = 1.0 + 0.3 * (month in [11, 12]) + 0.1 * (month in [6, 7])\n",
    "            \n",
    "            # Day of week pattern (weekends higher)\n",
    "            dow_factor = 1.2 if date.dayofweek >= 5 else 1.0\n",
    "            \n",
    "            # Random trend component\n",
    "            trend = 1.0 + 0.001 * (date - dates[0]).days\n",
    "            \n",
    "            for store_id in range(self.n_stores):\n",
    "                base_occupancy = 0.4 + np.random.normal(0, 0.1)\n",
    "                occupancy = base_occupancy * seasonal_factor * dow_factor * trend\n",
    "                occupancy = np.clip(occupancy + np.random.normal(0, 0.05), 0, 1)\n",
    "                \n",
    "                car_count = int(occupancy * self.store_capacity[store_id])\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'store_id': store_id,\n",
    "                    'car_count': car_count,\n",
    "                    'capacity': self.store_capacity[store_id],\n",
    "                    'occupancy_rate': occupancy\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_aggregate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate aggregate parking metrics for revenue prediction.\"\"\"\n",
    "        \n",
    "        daily_agg = df.groupby('date').agg({\n",
    "            'car_count': 'sum',\n",
    "            'occupancy_rate': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate rolling metrics\n",
    "        daily_agg['car_count_7d_avg'] = daily_agg['car_count'].rolling(7).mean()\n",
    "        daily_agg['occupancy_7d_avg'] = daily_agg['occupancy_rate'].rolling(7).mean()\n",
    "        daily_agg['yoy_change'] = daily_agg['car_count'].pct_change(365) if len(daily_agg) > 365 else np.nan\n",
    "        \n",
    "        # Week-over-week change\n",
    "        daily_agg['wow_change'] = daily_agg['car_count_7d_avg'].pct_change(7)\n",
    "        \n",
    "        return daily_agg\n",
    "\n",
    "\n",
    "# Generate and analyze parking lot data\n",
    "analyzer = ParkingLotAnalyzer(n_stores=100)\n",
    "parking_data = analyzer.generate_parking_data(n_days=180)\n",
    "agg_metrics = analyzer.calculate_aggregate_metrics(parking_data)\n",
    "\n",
    "print(\"Parking Lot Data Summary:\")\n",
    "print(f\"Total observations: {len(parking_data):,}\")\n",
    "print(f\"Date range: {parking_data['date'].min().date()} to {parking_data['date'].max().date()}\")\n",
    "print(f\"\\nAggregate Metrics (last 5 days):\")\n",
    "print(agg_metrics[['date', 'car_count', 'occupancy_rate', 'wow_change']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6de893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parking lot trends\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total car count over time\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(agg_metrics['date'], agg_metrics['car_count'], alpha=0.3, label='Daily')\n",
    "ax1.plot(agg_metrics['date'], agg_metrics['car_count_7d_avg'], linewidth=2, label='7-day Avg')\n",
    "ax1.set_title('Aggregate Parking Lot Traffic')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Total Car Count')\n",
    "ax1.legend()\n",
    "\n",
    "# Occupancy rate distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(parking_data['occupancy_rate'], bins=50, edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(parking_data['occupancy_rate'].mean(), color='red', linestyle='--', label=f\"Mean: {parking_data['occupancy_rate'].mean():.2f}\")\n",
    "ax2.set_title('Occupancy Rate Distribution')\n",
    "ax2.set_xlabel('Occupancy Rate')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "# Week-over-week changes\n",
    "ax3 = axes[1, 0]\n",
    "valid_wow = agg_metrics.dropna(subset=['wow_change'])\n",
    "colors = ['green' if x > 0 else 'red' for x in valid_wow['wow_change']]\n",
    "ax3.bar(valid_wow['date'], valid_wow['wow_change'] * 100, color=colors, alpha=0.7)\n",
    "ax3.axhline(0, color='black', linewidth=0.5)\n",
    "ax3.set_title('Week-over-Week Traffic Change')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('WoW Change (%)')\n",
    "\n",
    "# Day of week pattern\n",
    "ax4 = axes[1, 1]\n",
    "parking_data['dayofweek'] = parking_data['date'].dt.dayofweek\n",
    "dow_avg = parking_data.groupby('dayofweek')['occupancy_rate'].mean()\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "ax4.bar(days, dow_avg.values, color='steelblue', edgecolor='black')\n",
    "ax4.set_title('Average Occupancy by Day of Week')\n",
    "ax4.set_xlabel('Day of Week')\n",
    "ax4.set_ylabel('Avg Occupancy Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9474546",
   "metadata": {},
   "source": [
    "### 2.2 Ship Tracking (AIS Data)\n",
    "\n",
    "Automatic Identification System (AIS) data tracks global shipping, useful for:\n",
    "- Commodity flow analysis\n",
    "- Supply chain monitoring\n",
    "- Trade flow prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipTrackingAnalyzer:\n",
    "    \"\"\"\n",
    "    Simulates AIS ship tracking data analysis.\n",
    "    \n",
    "    Real providers include:\n",
    "    - MarineTraffic\n",
    "    - VesselFinder\n",
    "    - Spire Global\n",
    "    - Kpler\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vessel_types = ['Tanker', 'Bulk Carrier', 'Container', 'LNG Carrier']\n",
    "        self.routes = [\n",
    "            ('Shanghai', 'Los Angeles', 'Pacific'),\n",
    "            ('Rotterdam', 'New York', 'Atlantic'),\n",
    "            ('Singapore', 'Dubai', 'Indian Ocean'),\n",
    "            ('Houston', 'Amsterdam', 'Atlantic'),\n",
    "            ('Santos', 'Shanghai', 'Pacific')\n",
    "        ]\n",
    "        \n",
    "    def generate_vessel_data(self, n_vessels: int = 500, n_days: int = 90) -> pd.DataFrame:\n",
    "        \"\"\"Generate simulated vessel tracking data.\"\"\"\n",
    "        \n",
    "        data = []\n",
    "        dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "        \n",
    "        for vessel_id in range(n_vessels):\n",
    "            vessel_type = np.random.choice(self.vessel_types)\n",
    "            route = self.routes[np.random.randint(len(self.routes))]\n",
    "            \n",
    "            # Capacity based on vessel type (in DWT - deadweight tonnage)\n",
    "            capacity_ranges = {\n",
    "                'Tanker': (50000, 300000),\n",
    "                'Bulk Carrier': (30000, 200000),\n",
    "                'Container': (10000, 150000),\n",
    "                'LNG Carrier': (80000, 180000)\n",
    "            }\n",
    "            capacity = np.random.randint(*capacity_ranges[vessel_type])\n",
    "            \n",
    "            # Generate daily positions for this vessel\n",
    "            for i, date in enumerate(dates):\n",
    "                # Simulate cargo utilization\n",
    "                cargo_util = np.clip(np.random.normal(0.85, 0.1), 0.5, 1.0)\n",
    "                \n",
    "                # Simulate speed (knots)\n",
    "                avg_speed = np.random.normal(14, 2)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'vessel_id': f'V{vessel_id:04d}',\n",
    "                    'vessel_type': vessel_type,\n",
    "                    'origin': route[0],\n",
    "                    'destination': route[1],\n",
    "                    'ocean': route[2],\n",
    "                    'capacity_dwt': capacity,\n",
    "                    'cargo_utilization': cargo_util,\n",
    "                    'cargo_tonnes': int(capacity * cargo_util),\n",
    "                    'speed_knots': avg_speed\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_trade_flows(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate aggregate trade flow metrics.\"\"\"\n",
    "        \n",
    "        # Daily cargo by route and vessel type\n",
    "        daily_flows = df.groupby(['date', 'vessel_type', 'destination']).agg({\n",
    "            'cargo_tonnes': 'sum',\n",
    "            'vessel_id': 'count',\n",
    "            'cargo_utilization': 'mean'\n",
    "        }).reset_index()\n",
    "        daily_flows.columns = ['date', 'vessel_type', 'destination', 'total_cargo', 'vessel_count', 'avg_utilization']\n",
    "        \n",
    "        return daily_flows\n",
    "\n",
    "\n",
    "# Generate ship tracking data\n",
    "ship_analyzer = ShipTrackingAnalyzer()\n",
    "vessel_data = ship_analyzer.generate_vessel_data(n_vessels=200, n_days=60)\n",
    "trade_flows = ship_analyzer.calculate_trade_flows(vessel_data)\n",
    "\n",
    "print(\"Ship Tracking Data Summary:\")\n",
    "print(f\"\\nVessel Type Distribution:\")\n",
    "print(vessel_data.groupby('vessel_type').agg({\n",
    "    'vessel_id': 'nunique',\n",
    "    'cargo_tonnes': 'mean'\n",
    "}).round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e78a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize shipping data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Cargo flow by vessel type\n",
    "ax1 = axes[0, 0]\n",
    "vessel_type_cargo = trade_flows.groupby(['date', 'vessel_type'])['total_cargo'].sum().unstack()\n",
    "vessel_type_cargo.plot(ax=ax1, linewidth=2)\n",
    "ax1.set_title('Daily Cargo Flow by Vessel Type')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Total Cargo (tonnes)')\n",
    "ax1.legend(title='Vessel Type')\n",
    "\n",
    "# Destination analysis\n",
    "ax2 = axes[0, 1]\n",
    "dest_cargo = vessel_data.groupby('destination')['cargo_tonnes'].sum().sort_values(ascending=True)\n",
    "dest_cargo.plot(kind='barh', ax=ax2, color='steelblue', edgecolor='black')\n",
    "ax2.set_title('Total Cargo by Destination')\n",
    "ax2.set_xlabel('Total Cargo (tonnes)')\n",
    "\n",
    "# Utilization trends\n",
    "ax3 = axes[1, 0]\n",
    "daily_util = vessel_data.groupby('date')['cargo_utilization'].mean()\n",
    "ax3.plot(daily_util.index, daily_util.values, linewidth=2)\n",
    "ax3.axhline(daily_util.mean(), color='red', linestyle='--', label=f'Mean: {daily_util.mean():.2%}')\n",
    "ax3.set_title('Average Fleet Cargo Utilization')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Utilization Rate')\n",
    "ax3.legend()\n",
    "\n",
    "# Speed distribution by vessel type\n",
    "ax4 = axes[1, 1]\n",
    "vessel_data.boxplot(column='speed_knots', by='vessel_type', ax=ax4)\n",
    "ax4.set_title('Speed Distribution by Vessel Type')\n",
    "ax4.set_xlabel('Vessel Type')\n",
    "ax4.set_ylabel('Speed (knots)')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9105c0c",
   "metadata": {},
   "source": [
    "## 3. Web Scraping & Traffic Data\n",
    "\n",
    "### 3.1 Web Traffic Analysis\n",
    "\n",
    "Website traffic data can predict company performance, especially for e-commerce and SaaS businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebTrafficAnalyzer:\n",
    "    \"\"\"\n",
    "    Simulates web traffic analysis for trading signals.\n",
    "    \n",
    "    Real providers include:\n",
    "    - SimilarWeb\n",
    "    - SEMrush\n",
    "    - Alexa (discontinued)\n",
    "    - Cloudflare Radar\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.companies = {\n",
    "            'AMZN': {'base_visits': 2.5e9, 'volatility': 0.1},\n",
    "            'SHOP': {'base_visits': 500e6, 'volatility': 0.15},\n",
    "            'EBAY': {'base_visits': 800e6, 'volatility': 0.12},\n",
    "            'ETSY': {'base_visits': 400e6, 'volatility': 0.18},\n",
    "            'MELI': {'base_visits': 600e6, 'volatility': 0.14}\n",
    "        }\n",
    "    \n",
    "    def generate_traffic_data(self, n_days: int = 180) -> pd.DataFrame:\n",
    "        \"\"\"Generate simulated web traffic data.\"\"\"\n",
    "        \n",
    "        dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "        data = []\n",
    "        \n",
    "        for ticker, params in self.companies.items():\n",
    "            # Generate base traffic with trend and seasonality\n",
    "            for i, date in enumerate(dates):\n",
    "                # Trend component (slight growth)\n",
    "                trend = 1 + 0.0005 * i\n",
    "                \n",
    "                # Seasonal component (holiday boost)\n",
    "                month = date.month\n",
    "                seasonal = 1 + 0.4 * (month in [11, 12]) + 0.1 * (month in [6, 7])\n",
    "                \n",
    "                # Day of week effect\n",
    "                dow = date.dayofweek\n",
    "                dow_effect = 1.1 if dow < 5 else 0.9\n",
    "                \n",
    "                # Random noise\n",
    "                noise = np.random.normal(1, params['volatility'])\n",
    "                \n",
    "                visits = params['base_visits'] * trend * seasonal * dow_effect * noise\n",
    "                \n",
    "                # Engagement metrics\n",
    "                avg_duration = np.random.normal(180, 30)  # seconds\n",
    "                pages_per_visit = np.random.normal(5, 1)\n",
    "                bounce_rate = np.random.normal(0.4, 0.05)\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'ticker': ticker,\n",
    "                    'visits': int(visits),\n",
    "                    'unique_visitors': int(visits * np.random.uniform(0.6, 0.8)),\n",
    "                    'avg_duration_sec': max(60, avg_duration),\n",
    "                    'pages_per_visit': max(1, pages_per_visit),\n",
    "                    'bounce_rate': np.clip(bounce_rate, 0.2, 0.7)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_traffic_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate trading signals from web traffic data.\"\"\"\n",
    "        \n",
    "        signals = []\n",
    "        \n",
    "        for ticker in df['ticker'].unique():\n",
    "            ticker_data = df[df['ticker'] == ticker].sort_values('date').copy()\n",
    "            \n",
    "            # Calculate rolling metrics\n",
    "            ticker_data['visits_7d_avg'] = ticker_data['visits'].rolling(7).mean()\n",
    "            ticker_data['visits_30d_avg'] = ticker_data['visits'].rolling(30).mean()\n",
    "            \n",
    "            # Year-over-year comparison (or 90-day for shorter data)\n",
    "            ticker_data['visits_yoy'] = ticker_data['visits'].pct_change(90)\n",
    "            \n",
    "            # Momentum signal: short-term vs long-term\n",
    "            ticker_data['momentum_signal'] = (\n",
    "                ticker_data['visits_7d_avg'] / ticker_data['visits_30d_avg'] - 1\n",
    "            )\n",
    "            \n",
    "            # Engagement quality score\n",
    "            ticker_data['engagement_score'] = (\n",
    "                ticker_data['avg_duration_sec'] / 180 * 0.4 +\n",
    "                ticker_data['pages_per_visit'] / 5 * 0.4 +\n",
    "                (1 - ticker_data['bounce_rate']) * 0.2\n",
    "            )\n",
    "            \n",
    "            signals.append(ticker_data)\n",
    "        \n",
    "        return pd.concat(signals, ignore_index=True)\n",
    "\n",
    "\n",
    "# Generate web traffic data\n",
    "traffic_analyzer = WebTrafficAnalyzer()\n",
    "traffic_data = traffic_analyzer.generate_traffic_data(n_days=180)\n",
    "traffic_signals = traffic_analyzer.calculate_traffic_signals(traffic_data)\n",
    "\n",
    "print(\"Web Traffic Data Summary:\")\n",
    "print(traffic_data.groupby('ticker').agg({\n",
    "    'visits': ['mean', 'std'],\n",
    "    'bounce_rate': 'mean'\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize web traffic signals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Traffic trends by company\n",
    "ax1 = axes[0, 0]\n",
    "for ticker in traffic_signals['ticker'].unique():\n",
    "    ticker_data = traffic_signals[traffic_signals['ticker'] == ticker]\n",
    "    ax1.plot(ticker_data['date'], ticker_data['visits_7d_avg'] / 1e6, label=ticker)\n",
    "ax1.set_title('7-Day Average Web Traffic')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Visits (Millions)')\n",
    "ax1.legend()\n",
    "\n",
    "# Momentum signals\n",
    "ax2 = axes[0, 1]\n",
    "latest_signals = traffic_signals.groupby('ticker').last().reset_index()\n",
    "colors = ['green' if x > 0 else 'red' for x in latest_signals['momentum_signal']]\n",
    "ax2.barh(latest_signals['ticker'], latest_signals['momentum_signal'] * 100, color=colors)\n",
    "ax2.axvline(0, color='black', linewidth=0.5)\n",
    "ax2.set_title('Current Momentum Signal (7d vs 30d Avg)')\n",
    "ax2.set_xlabel('Momentum (%)')\n",
    "\n",
    "# Engagement score over time\n",
    "ax3 = axes[1, 0]\n",
    "for ticker in traffic_signals['ticker'].unique():\n",
    "    ticker_data = traffic_signals[traffic_signals['ticker'] == ticker]\n",
    "    ax3.plot(ticker_data['date'], ticker_data['engagement_score'].rolling(7).mean(), label=ticker)\n",
    "ax3.set_title('Engagement Quality Score (7-day Avg)')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_ylabel('Engagement Score')\n",
    "ax3.legend()\n",
    "\n",
    "# Bounce rate comparison\n",
    "ax4 = axes[1, 1]\n",
    "bounce_rates = traffic_data.groupby('ticker')['bounce_rate'].mean().sort_values()\n",
    "ax4.barh(bounce_rates.index, bounce_rates.values * 100, color='coral', edgecolor='black')\n",
    "ax4.set_title('Average Bounce Rate by Company')\n",
    "ax4.set_xlabel('Bounce Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59cca0",
   "metadata": {},
   "source": [
    "### 3.2 Web Scraping Best Practices\n",
    "\n",
    "Ethical web scraping considerations:\n",
    "- Respect `robots.txt`\n",
    "- Implement rate limiting\n",
    "- Identify your scraper with a proper User-Agent\n",
    "- Cache responses to minimize requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthicalWebScraper:\n",
    "    \"\"\"\n",
    "    A framework for ethical web scraping with rate limiting and caching.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_delay: float = 1.0, user_agent: str = None):\n",
    "        self.base_delay = base_delay\n",
    "        self.cache = {}\n",
    "        self.last_request_time = {}\n",
    "        \n",
    "        self.headers = {\n",
    "            'User-Agent': user_agent or 'ResearchBot/1.0 (Academic Research; contact@example.edu)'\n",
    "        }\n",
    "    \n",
    "    def check_robots_txt(self, url: str) -> dict:\n",
    "        \"\"\"Check robots.txt for the domain (simulated).\"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        \n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        \n",
    "        # In production, actually fetch and parse robots.txt\n",
    "        # This is a placeholder showing the structure\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'crawl_delay': self.base_delay,\n",
    "            'allowed': True,\n",
    "            'disallowed_paths': ['/admin', '/api/private']\n",
    "        }\n",
    "    \n",
    "    def rate_limit(self, domain: str):\n",
    "        \"\"\"Implement rate limiting per domain.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if domain in self.last_request_time:\n",
    "            elapsed = current_time - self.last_request_time[domain]\n",
    "            if elapsed < self.base_delay:\n",
    "                time.sleep(self.base_delay - elapsed)\n",
    "        \n",
    "        self.last_request_time[domain] = time.time()\n",
    "    \n",
    "    def get_cached_or_fetch(self, url: str, max_age_hours: int = 24) -> dict:\n",
    "        \"\"\"\n",
    "        Get from cache or fetch with rate limiting.\n",
    "        Returns simulated response for demonstration.\n",
    "        \"\"\"\n",
    "        from urllib.parse import urlparse\n",
    "        \n",
    "        # Check cache\n",
    "        if url in self.cache:\n",
    "            cached = self.cache[url]\n",
    "            age_hours = (datetime.now() - cached['timestamp']).total_seconds() / 3600\n",
    "            if age_hours < max_age_hours:\n",
    "                return {'source': 'cache', 'data': cached['data']}\n",
    "        \n",
    "        # Rate limit\n",
    "        domain = urlparse(url).netloc\n",
    "        self.rate_limit(domain)\n",
    "        \n",
    "        # In production, this would actually make the request\n",
    "        # response = requests.get(url, headers=self.headers)\n",
    "        \n",
    "        # Simulated response\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'status': 200,\n",
    "            'content': f'Simulated content from {url}'\n",
    "        }\n",
    "        \n",
    "        # Cache the response\n",
    "        self.cache[url] = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'data': data\n",
    "        }\n",
    "        \n",
    "        return {'source': 'fetch', 'data': data}\n",
    "\n",
    "\n",
    "# Demonstrate ethical scraping framework\n",
    "scraper = EthicalWebScraper(base_delay=2.0)\n",
    "\n",
    "# Check robots.txt\n",
    "robots_info = scraper.check_robots_txt('https://example.com/data')\n",
    "print(\"Robots.txt Information:\")\n",
    "print(json.dumps(robots_info, indent=2))\n",
    "\n",
    "# Demonstrate caching\n",
    "print(\"\\nFirst request (fetch):\")\n",
    "result1 = scraper.get_cached_or_fetch('https://example.com/data')\n",
    "print(f\"Source: {result1['source']}\")\n",
    "\n",
    "print(\"\\nSecond request (cache):\")\n",
    "result2 = scraper.get_cached_or_fetch('https://example.com/data')\n",
    "print(f\"Source: {result2['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672662f4",
   "metadata": {},
   "source": [
    "## 4. Social Media & Sentiment Data\n",
    "\n",
    "### 4.1 Social Media Volume & Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48850cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialMediaAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze social media data for trading signals.\n",
    "    \n",
    "    Real providers include:\n",
    "    - Stocktwits API\n",
    "    - Twitter/X API\n",
    "    - Reddit API\n",
    "    - Alternative.me (Crypto Fear & Greed)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tickers = ['AAPL', 'TSLA', 'NVDA', 'AMD', 'GME', 'AMC']\n",
    "        \n",
    "    def generate_social_data(self, n_days: int = 90) -> pd.DataFrame:\n",
    "        \"\"\"Generate simulated social media data.\"\"\"\n",
    "        \n",
    "        dates = pd.date_range(end=datetime.now(), periods=n_days, freq='D')\n",
    "        data = []\n",
    "        \n",
    "        for date in dates:\n",
    "            for ticker in self.tickers:\n",
    "                # Base mention volume varies by ticker popularity\n",
    "                base_mentions = {\n",
    "                    'AAPL': 50000, 'TSLA': 80000, 'NVDA': 60000,\n",
    "                    'AMD': 30000, 'GME': 40000, 'AMC': 35000\n",
    "                }\n",
    "                \n",
    "                # Add random events (earnings, news)\n",
    "                event_multiplier = 1 + np.random.exponential(0.3) if np.random.random() < 0.1 else 1\n",
    "                \n",
    "                mentions = int(base_mentions[ticker] * np.random.lognormal(0, 0.3) * event_multiplier)\n",
    "                \n",
    "                # Sentiment distribution (positive, negative, neutral)\n",
    "                base_sentiment = np.random.normal(0.55, 0.1)  # Slight positive bias\n",
    "                positive_ratio = np.clip(base_sentiment, 0.2, 0.8)\n",
    "                negative_ratio = np.clip(np.random.normal(0.25, 0.05), 0.1, 0.5)\n",
    "                neutral_ratio = 1 - positive_ratio - negative_ratio\n",
    "                neutral_ratio = max(0, neutral_ratio)\n",
    "                \n",
    "                # Normalize\n",
    "                total = positive_ratio + negative_ratio + neutral_ratio\n",
    "                positive_ratio /= total\n",
    "                negative_ratio /= total\n",
    "                neutral_ratio /= total\n",
    "                \n",
    "                data.append({\n",
    "                    'date': date,\n",
    "                    'ticker': ticker,\n",
    "                    'mentions': mentions,\n",
    "                    'positive_ratio': positive_ratio,\n",
    "                    'negative_ratio': negative_ratio,\n",
    "                    'neutral_ratio': neutral_ratio,\n",
    "                    'sentiment_score': positive_ratio - negative_ratio,  # Net sentiment\n",
    "                    'unique_users': int(mentions * np.random.uniform(0.3, 0.6)),\n",
    "                    'avg_engagement': np.random.lognormal(2, 0.5)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_sentiment_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate trading signals from social sentiment.\"\"\"\n",
    "        \n",
    "        signals = []\n",
    "        \n",
    "        for ticker in df['ticker'].unique():\n",
    "            ticker_data = df[df['ticker'] == ticker].sort_values('date').copy()\n",
    "            \n",
    "            # Rolling metrics\n",
    "            ticker_data['mentions_7d_avg'] = ticker_data['mentions'].rolling(7).mean()\n",
    "            ticker_data['sentiment_7d_avg'] = ticker_data['sentiment_score'].rolling(7).mean()\n",
    "            \n",
    "            # Z-score of mentions (unusual activity detector)\n",
    "            ticker_data['mentions_zscore'] = (\n",
    "                (ticker_data['mentions'] - ticker_data['mentions'].rolling(30).mean()) /\n",
    "                ticker_data['mentions'].rolling(30).std()\n",
    "            )\n",
    "            \n",
    "            # Sentiment momentum\n",
    "            ticker_data['sentiment_momentum'] = ticker_data['sentiment_7d_avg'].diff(7)\n",
    "            \n",
    "            # Combined signal: high sentiment + unusual volume\n",
    "            ticker_data['combined_signal'] = (\n",
    "                ticker_data['sentiment_score'] * 0.5 +\n",
    "                np.clip(ticker_data['mentions_zscore'], -2, 2) / 4 * 0.5\n",
    "            )\n",
    "            \n",
    "            signals.append(ticker_data)\n",
    "        \n",
    "        return pd.concat(signals, ignore_index=True)\n",
    "\n",
    "\n",
    "# Generate social media data\n",
    "social_analyzer = SocialMediaAnalyzer()\n",
    "social_data = social_analyzer.generate_social_data(n_days=90)\n",
    "social_signals = social_analyzer.calculate_sentiment_signals(social_data)\n",
    "\n",
    "print(\"Social Media Data Summary:\")\n",
    "print(social_data.groupby('ticker').agg({\n",
    "    'mentions': 'mean',\n",
    "    'sentiment_score': 'mean',\n",
    "    'unique_users': 'mean'\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdae1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize social sentiment signals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Mentions volume by ticker\n",
    "ax1 = axes[0, 0]\n",
    "for ticker in ['TSLA', 'NVDA', 'GME']:\n",
    "    ticker_data = social_signals[social_signals['ticker'] == ticker]\n",
    "    ax1.plot(ticker_data['date'], ticker_data['mentions_7d_avg'] / 1000, label=ticker)\n",
    "ax1.set_title('Social Media Mentions (7-day Avg)')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Mentions (Thousands)')\n",
    "ax1.legend()\n",
    "\n",
    "# Sentiment heatmap\n",
    "ax2 = axes[0, 1]\n",
    "pivot_sentiment = social_data.pivot_table(\n",
    "    index='ticker', \n",
    "    columns=social_data['date'].dt.week, \n",
    "    values='sentiment_score',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(pivot_sentiment, cmap='RdYlGn', center=0, ax=ax2, cbar_kws={'label': 'Sentiment'})\n",
    "ax2.set_title('Weekly Sentiment Heatmap by Ticker')\n",
    "ax2.set_xlabel('Week')\n",
    "\n",
    "# Unusual activity detection\n",
    "ax3 = axes[1, 0]\n",
    "recent_signals = social_signals.groupby('ticker').last().reset_index()\n",
    "colors = ['red' if abs(x) > 1.5 else 'gray' for x in recent_signals['mentions_zscore']]\n",
    "ax3.bar(recent_signals['ticker'], recent_signals['mentions_zscore'], color=colors)\n",
    "ax3.axhline(1.5, color='red', linestyle='--', alpha=0.5, label='Alert Threshold')\n",
    "ax3.axhline(-1.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax3.set_title('Mentions Z-Score (Unusual Activity)')\n",
    "ax3.set_xlabel('Ticker')\n",
    "ax3.set_ylabel('Z-Score')\n",
    "ax3.legend()\n",
    "\n",
    "# Combined signal distribution\n",
    "ax4 = axes[1, 1]\n",
    "for ticker in ['AAPL', 'TSLA', 'GME']:\n",
    "    ticker_data = social_signals[social_signals['ticker'] == ticker]\n",
    "    ax4.hist(ticker_data['combined_signal'].dropna(), alpha=0.5, bins=30, label=ticker)\n",
    "ax4.set_title('Combined Signal Distribution')\n",
    "ax4.set_xlabel('Signal Value')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe279ad1",
   "metadata": {},
   "source": [
    "## 5. Transaction & Credit Card Data\n",
    "\n",
    "### 5.1 Consumer Spending Analysis\n",
    "\n",
    "Aggregated credit card and transaction data provides real-time insights into consumer spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2dc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionDataAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze aggregated transaction data for revenue prediction.\n",
    "    \n",
    "    Real providers include:\n",
    "    - Second Measure\n",
    "    - Earnest Research\n",
    "    - Bloomberg Second Measure\n",
    "    - Mastercard SpendingPulse\n",
    "    - Visa Merchant Insights\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merchants = {\n",
    "            'AMZN': {'category': 'E-Commerce', 'base_txn': 10e6},\n",
    "            'WMT': {'category': 'Retail', 'base_txn': 15e6},\n",
    "            'SBUX': {'category': 'Restaurants', 'base_txn': 5e6},\n",
    "            'MCD': {'category': 'Restaurants', 'base_txn': 8e6},\n",
    "            'HD': {'category': 'Home Improvement', 'base_txn': 3e6},\n",
    "            'NKE': {'category': 'Apparel', 'base_txn': 2e6}\n",
    "        }\n",
    "    \n",
    "    def generate_transaction_data(self, n_weeks: int = 52) -> pd.DataFrame:\n",
    "        \"\"\"Generate simulated weekly transaction data.\"\"\"\n",
    "        \n",
    "        weeks = pd.date_range(end=datetime.now(), periods=n_weeks, freq='W')\n",
    "        data = []\n",
    "        \n",
    "        for week in weeks:\n",
    "            for ticker, params in self.merchants.items():\n",
    "                # Seasonal patterns\n",
    "                month = week.month\n",
    "                if params['category'] in ['E-Commerce', 'Retail']:\n",
    "                    seasonal = 1 + 0.5 * (month in [11, 12]) + 0.1 * (month in [6, 7])\n",
    "                elif params['category'] == 'Restaurants':\n",
    "                    seasonal = 1 + 0.1 * (month in [6, 7, 8])  # Summer boost\n",
    "                elif params['category'] == 'Home Improvement':\n",
    "                    seasonal = 1 + 0.2 * (month in [4, 5, 6])  # Spring boost\n",
    "                else:\n",
    "                    seasonal = 1.0\n",
    "                \n",
    "                # Trend (growth/decline)\n",
    "                trend = 1 + 0.001 * (week - weeks[0]).days\n",
    "                \n",
    "                # Random variation\n",
    "                noise = np.random.lognormal(0, 0.1)\n",
    "                \n",
    "                txn_count = int(params['base_txn'] * seasonal * trend * noise)\n",
    "                avg_ticket = np.random.normal(50, 10) if params['category'] != 'Restaurants' else np.random.normal(15, 5)\n",
    "                \n",
    "                data.append({\n",
    "                    'week': week,\n",
    "                    'ticker': ticker,\n",
    "                    'category': params['category'],\n",
    "                    'transaction_count': txn_count,\n",
    "                    'avg_ticket_size': max(5, avg_ticket),\n",
    "                    'total_spend': txn_count * avg_ticket,\n",
    "                    'unique_customers': int(txn_count * np.random.uniform(0.4, 0.7)),\n",
    "                    'repeat_rate': np.random.uniform(0.2, 0.5)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def predict_quarterly_revenue(self, df: pd.DataFrame, ticker: str) -> dict:\n",
    "        \"\"\"Predict quarterly revenue from transaction data.\"\"\"\n",
    "        \n",
    "        ticker_data = df[df['ticker'] == ticker].copy()\n",
    "        ticker_data['quarter'] = ticker_data['week'].dt.to_period('Q')\n",
    "        \n",
    "        # Aggregate by quarter\n",
    "        quarterly = ticker_data.groupby('quarter').agg({\n",
    "            'total_spend': 'sum',\n",
    "            'transaction_count': 'sum',\n",
    "            'unique_customers': 'sum',\n",
    "            'avg_ticket_size': 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Calculate growth rates\n",
    "        quarterly['spend_qoq'] = quarterly['total_spend'].pct_change()\n",
    "        quarterly['spend_yoy'] = quarterly['total_spend'].pct_change(4) if len(quarterly) > 4 else np.nan\n",
    "        \n",
    "        return {\n",
    "            'ticker': ticker,\n",
    "            'quarterly_data': quarterly,\n",
    "            'latest_quarter_spend': quarterly['total_spend'].iloc[-1],\n",
    "            'qoq_growth': quarterly['spend_qoq'].iloc[-1] if len(quarterly) > 1 else None\n",
    "        }\n",
    "\n",
    "\n",
    "# Generate transaction data\n",
    "txn_analyzer = TransactionDataAnalyzer()\n",
    "txn_data = txn_analyzer.generate_transaction_data(n_weeks=52)\n",
    "\n",
    "print(\"Transaction Data Summary by Category:\")\n",
    "print(txn_data.groupby('category').agg({\n",
    "    'total_spend': 'sum',\n",
    "    'transaction_count': 'sum',\n",
    "    'avg_ticket_size': 'mean'\n",
    "}).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae08255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transaction data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Total spend by company\n",
    "ax1 = axes[0, 0]\n",
    "for ticker in ['AMZN', 'WMT', 'SBUX']:\n",
    "    ticker_data = txn_data[txn_data['ticker'] == ticker]\n",
    "    ax1.plot(ticker_data['week'], ticker_data['total_spend'] / 1e9, label=ticker)\n",
    "ax1.set_title('Weekly Consumer Spend by Company')\n",
    "ax1.set_xlabel('Week')\n",
    "ax1.set_ylabel('Total Spend ($B)')\n",
    "ax1.legend()\n",
    "\n",
    "# Category comparison\n",
    "ax2 = axes[0, 1]\n",
    "category_spend = txn_data.groupby('category')['total_spend'].sum().sort_values()\n",
    "category_spend.plot(kind='barh', ax=ax2, color='steelblue', edgecolor='black')\n",
    "ax2.set_title('Total Annual Spend by Category')\n",
    "ax2.set_xlabel('Total Spend ($)')\n",
    "\n",
    "# Average ticket size trends\n",
    "ax3 = axes[1, 0]\n",
    "category_ticket = txn_data.groupby(['week', 'category'])['avg_ticket_size'].mean().unstack()\n",
    "category_ticket.plot(ax=ax3)\n",
    "ax3.set_title('Average Ticket Size by Category')\n",
    "ax3.set_xlabel('Week')\n",
    "ax3.set_ylabel('Avg Ticket ($)')\n",
    "ax3.legend(title='Category', loc='upper left')\n",
    "\n",
    "# Year-over-year growth by quarter\n",
    "ax4 = axes[1, 1]\n",
    "predictions = {}\n",
    "for ticker in ['AMZN', 'WMT', 'HD']:\n",
    "    pred = txn_analyzer.predict_quarterly_revenue(txn_data, ticker)\n",
    "    predictions[ticker] = pred\n",
    "\n",
    "tickers = list(predictions.keys())\n",
    "qoq_growth = [predictions[t]['qoq_growth'] * 100 if predictions[t]['qoq_growth'] else 0 for t in tickers]\n",
    "colors = ['green' if x > 0 else 'red' for x in qoq_growth]\n",
    "ax4.bar(tickers, qoq_growth, color=colors, edgecolor='black')\n",
    "ax4.axhline(0, color='black', linewidth=0.5)\n",
    "ax4.set_title('Quarter-over-Quarter Spend Growth')\n",
    "ax4.set_xlabel('Ticker')\n",
    "ax4.set_ylabel('QoQ Growth (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db579452",
   "metadata": {},
   "source": [
    "## 6. Alternative Data Quality Framework\n",
    "\n",
    "### 6.1 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db016912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AltDataQualityAssessor:\n",
    "    \"\"\"\n",
    "    Framework for assessing alternative data quality.\n",
    "    \n",
    "    Key dimensions:\n",
    "    1. Coverage - How representative is the sample?\n",
    "    2. Timeliness - How fresh is the data?\n",
    "    3. Accuracy - How reliable is the data?\n",
    "    4. Consistency - Is the data stable over time?\n",
    "    5. Completeness - Are there gaps or missing values?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, date_col: str, value_col: str):\n",
    "        self.df = df.copy()\n",
    "        self.date_col = date_col\n",
    "        self.value_col = value_col\n",
    "        \n",
    "    def assess_coverage(self, expected_entities: list = None) -> dict:\n",
    "        \"\"\"Assess data coverage.\"\"\"\n",
    "        \n",
    "        unique_dates = self.df[self.date_col].nunique()\n",
    "        date_range = (self.df[self.date_col].max() - self.df[self.date_col].min()).days\n",
    "        \n",
    "        return {\n",
    "            'unique_dates': unique_dates,\n",
    "            'date_range_days': date_range,\n",
    "            'coverage_ratio': unique_dates / max(date_range, 1),\n",
    "            'total_records': len(self.df)\n",
    "        }\n",
    "    \n",
    "    def assess_timeliness(self) -> dict:\n",
    "        \"\"\"Assess data timeliness.\"\"\"\n",
    "        \n",
    "        latest_date = self.df[self.date_col].max()\n",
    "        lag_days = (datetime.now() - pd.to_datetime(latest_date)).days\n",
    "        \n",
    "        return {\n",
    "            'latest_date': latest_date,\n",
    "            'lag_days': lag_days,\n",
    "            'is_stale': lag_days > 7  # Data older than 7 days is considered stale\n",
    "        }\n",
    "    \n",
    "    def assess_completeness(self) -> dict:\n",
    "        \"\"\"Assess data completeness.\"\"\"\n",
    "        \n",
    "        missing_pct = self.df[self.value_col].isna().mean() * 100\n",
    "        zero_pct = (self.df[self.value_col] == 0).mean() * 100\n",
    "        \n",
    "        return {\n",
    "            'missing_pct': missing_pct,\n",
    "            'zero_values_pct': zero_pct,\n",
    "            'completeness_score': 100 - missing_pct\n",
    "        }\n",
    "    \n",
    "    def assess_consistency(self, window: int = 30) -> dict:\n",
    "        \"\"\"Assess data consistency over time.\"\"\"\n",
    "        \n",
    "        daily_mean = self.df.groupby(self.date_col)[self.value_col].mean()\n",
    "        \n",
    "        # Calculate coefficient of variation\n",
    "        cv = daily_mean.std() / daily_mean.mean() if daily_mean.mean() != 0 else np.inf\n",
    "        \n",
    "        # Detect regime changes (large jumps)\n",
    "        pct_changes = daily_mean.pct_change().abs()\n",
    "        anomalies = (pct_changes > 0.5).sum()  # >50% daily change\n",
    "        \n",
    "        return {\n",
    "            'coefficient_of_variation': cv,\n",
    "            'anomaly_days': anomalies,\n",
    "            'is_stable': cv < 0.5 and anomalies < 5\n",
    "        }\n",
    "    \n",
    "    def full_assessment(self) -> pd.DataFrame:\n",
    "        \"\"\"Run full quality assessment.\"\"\"\n",
    "        \n",
    "        coverage = self.assess_coverage()\n",
    "        timeliness = self.assess_timeliness()\n",
    "        completeness = self.assess_completeness()\n",
    "        consistency = self.assess_consistency()\n",
    "        \n",
    "        # Calculate overall quality score\n",
    "        quality_score = (\n",
    "            min(coverage['coverage_ratio'], 1) * 25 +\n",
    "            (1 - min(timeliness['lag_days'] / 30, 1)) * 25 +\n",
    "            completeness['completeness_score'] / 100 * 25 +\n",
    "            (1 if consistency['is_stable'] else 0.5) * 25\n",
    "        )\n",
    "        \n",
    "        assessment = pd.DataFrame([\n",
    "            {'Dimension': 'Coverage', 'Score': coverage['coverage_ratio'] * 100, 'Details': f\"{coverage['unique_dates']} unique dates\"},\n",
    "            {'Dimension': 'Timeliness', 'Score': max(0, 100 - timeliness['lag_days'] * 5), 'Details': f\"{timeliness['lag_days']} days lag\"},\n",
    "            {'Dimension': 'Completeness', 'Score': completeness['completeness_score'], 'Details': f\"{completeness['missing_pct']:.1f}% missing\"},\n",
    "            {'Dimension': 'Consistency', 'Score': 80 if consistency['is_stable'] else 50, 'Details': f\"CV: {consistency['coefficient_of_variation']:.2f}\"},\n",
    "            {'Dimension': 'Overall', 'Score': quality_score, 'Details': 'Weighted average'}\n",
    "        ])\n",
    "        \n",
    "        return assessment\n",
    "\n",
    "\n",
    "# Assess parking data quality\n",
    "assessor = AltDataQualityAssessor(agg_metrics, 'date', 'car_count')\n",
    "quality_report = assessor.full_assessment()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Alternative Data Quality Assessment Report\")\n",
    "print(\"=\"*50)\n",
    "print(quality_report.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe62e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality assessment\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "dimensions = quality_report['Dimension'][:-1].tolist()  # Exclude 'Overall'\n",
    "scores = quality_report['Score'][:-1].tolist()\n",
    "\n",
    "# Create radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()\n",
    "scores_plot = scores + [scores[0]]  # Close the polygon\n",
    "angles += angles[:1]\n",
    "\n",
    "ax = plt.subplot(111, polar=True)\n",
    "ax.plot(angles, scores_plot, 'o-', linewidth=2, color='steelblue')\n",
    "ax.fill(angles, scores_plot, alpha=0.25, color='steelblue')\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(dimensions)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_title('Data Quality Assessment Radar', size=14, y=1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print overall score interpretation\n",
    "overall_score = quality_report[quality_report['Dimension'] == 'Overall']['Score'].values[0]\n",
    "if overall_score >= 80:\n",
    "    quality_grade = 'HIGH QUALITY - Suitable for trading signals'\n",
    "elif overall_score >= 60:\n",
    "    quality_grade = 'MEDIUM QUALITY - Use with caution'\n",
    "else:\n",
    "    quality_grade = 'LOW QUALITY - Not recommended for trading'\n",
    "\n",
    "print(f\"\\nOverall Quality Score: {overall_score:.1f}/100\")\n",
    "print(f\"Assessment: {quality_grade}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b7b73f",
   "metadata": {},
   "source": [
    "## 7. Combining Alternative Data Sources\n",
    "\n",
    "### 7.1 Multi-Source Alpha Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77545fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSourceAlphaGenerator:\n",
    "    \"\"\"\n",
    "    Combine multiple alternative data sources for alpha generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.source_weights = {\n",
    "            'web_traffic': 0.25,\n",
    "            'social_sentiment': 0.25,\n",
    "            'transaction_data': 0.30,\n",
    "            'satellite_data': 0.20\n",
    "        }\n",
    "    \n",
    "    def generate_combined_signal(self, \n",
    "                                  web_signal: float,\n",
    "                                  social_signal: float,\n",
    "                                  txn_signal: float,\n",
    "                                  satellite_signal: float) -> dict:\n",
    "        \"\"\"Generate combined alpha signal from multiple sources.\"\"\"\n",
    "        \n",
    "        # Normalize signals to [-1, 1]\n",
    "        signals = {\n",
    "            'web_traffic': np.clip(web_signal, -1, 1),\n",
    "            'social_sentiment': np.clip(social_signal, -1, 1),\n",
    "            'transaction_data': np.clip(txn_signal, -1, 1),\n",
    "            'satellite_data': np.clip(satellite_signal, -1, 1)\n",
    "        }\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined = sum(\n",
    "            signals[source] * weight \n",
    "            for source, weight in self.source_weights.items()\n",
    "        )\n",
    "        \n",
    "        # Calculate signal strength (agreement across sources)\n",
    "        signal_signs = [np.sign(s) for s in signals.values()]\n",
    "        agreement = abs(sum(signal_signs)) / len(signal_signs)\n",
    "        \n",
    "        # Confidence score based on agreement\n",
    "        confidence = agreement * abs(combined)\n",
    "        \n",
    "        return {\n",
    "            'combined_signal': combined,\n",
    "            'agreement': agreement,\n",
    "            'confidence': confidence,\n",
    "            'individual_signals': signals,\n",
    "            'recommendation': 'LONG' if combined > 0.2 else 'SHORT' if combined < -0.2 else 'NEUTRAL'\n",
    "        }\n",
    "    \n",
    "    def backtest_combined_signal(self, n_periods: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Backtest the combined signal approach.\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i in range(n_periods):\n",
    "            # Generate random signals (in reality, these come from actual data)\n",
    "            web = np.random.normal(0, 0.3)\n",
    "            social = np.random.normal(0, 0.4)\n",
    "            txn = np.random.normal(0.05, 0.2)  # Slight positive bias\n",
    "            satellite = np.random.normal(0, 0.25)\n",
    "            \n",
    "            signal = self.generate_combined_signal(web, social, txn, satellite)\n",
    "            \n",
    "            # Simulate forward return (correlated with signal)\n",
    "            forward_return = (\n",
    "                signal['combined_signal'] * 0.02 +  # Signal has predictive power\n",
    "                np.random.normal(0, 0.02)  # Random noise\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'period': i,\n",
    "                'combined_signal': signal['combined_signal'],\n",
    "                'confidence': signal['confidence'],\n",
    "                'recommendation': signal['recommendation'],\n",
    "                'forward_return': forward_return\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Demonstrate multi-source alpha generation\n",
    "alpha_gen = MultiSourceAlphaGenerator()\n",
    "\n",
    "# Example signal combination\n",
    "example_signal = alpha_gen.generate_combined_signal(\n",
    "    web_signal=0.3,      # Positive web traffic momentum\n",
    "    social_signal=0.5,   # Bullish social sentiment\n",
    "    txn_signal=0.2,      # Above-average transaction growth\n",
    "    satellite_signal=-0.1 # Slightly negative parking data\n",
    ")\n",
    "\n",
    "print(\"Multi-Source Alpha Signal Analysis\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nIndividual Signals:\")\n",
    "for source, value in example_signal['individual_signals'].items():\n",
    "    print(f\"  {source}: {value:+.2f}\")\n",
    "print(f\"\\nCombined Signal: {example_signal['combined_signal']:+.3f}\")\n",
    "print(f\"Source Agreement: {example_signal['agreement']:.2%}\")\n",
    "print(f\"Confidence Score: {example_signal['confidence']:.3f}\")\n",
    "print(f\"Recommendation: {example_signal['recommendation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest the combined signal\n",
    "backtest_results = alpha_gen.backtest_combined_signal(n_periods=200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Signal vs Return scatter\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(backtest_results['combined_signal'], \n",
    "            backtest_results['forward_return'] * 100,\n",
    "            alpha=0.5, c=backtest_results['confidence'], cmap='viridis')\n",
    "ax1.axhline(0, color='black', linewidth=0.5)\n",
    "ax1.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(backtest_results['combined_signal'], backtest_results['forward_return'] * 100, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(backtest_results['combined_signal'].min(), backtest_results['combined_signal'].max(), 100)\n",
    "ax1.plot(x_line, p(x_line), 'r--', label=f'Regression (slope={z[0]:.2f})')\n",
    "ax1.set_title('Signal vs Forward Return')\n",
    "ax1.set_xlabel('Combined Signal')\n",
    "ax1.set_ylabel('Forward Return (%)')\n",
    "ax1.legend()\n",
    "\n",
    "# Return by recommendation\n",
    "ax2 = axes[0, 1]\n",
    "rec_returns = backtest_results.groupby('recommendation')['forward_return'].mean() * 100\n",
    "colors = ['green' if x > 0 else 'red' for x in rec_returns.values]\n",
    "ax2.bar(rec_returns.index, rec_returns.values, color=colors, edgecolor='black')\n",
    "ax2.axhline(0, color='black', linewidth=0.5)\n",
    "ax2.set_title('Average Return by Recommendation')\n",
    "ax2.set_xlabel('Recommendation')\n",
    "ax2.set_ylabel('Avg Return (%)')\n",
    "\n",
    "# Cumulative returns\n",
    "ax3 = axes[1, 0]\n",
    "backtest_results['strategy_return'] = np.sign(backtest_results['combined_signal']) * backtest_results['forward_return']\n",
    "backtest_results['cum_strategy'] = (1 + backtest_results['strategy_return']).cumprod()\n",
    "backtest_results['cum_market'] = (1 + backtest_results['forward_return']).cumprod()\n",
    "\n",
    "ax3.plot(backtest_results['period'], backtest_results['cum_strategy'], label='Strategy', linewidth=2)\n",
    "ax3.plot(backtest_results['period'], backtest_results['cum_market'], label='Market', linewidth=2, linestyle='--')\n",
    "ax3.set_title('Cumulative Returns: Strategy vs Market')\n",
    "ax3.set_xlabel('Period')\n",
    "ax3.set_ylabel('Cumulative Return')\n",
    "ax3.legend()\n",
    "\n",
    "# Confidence distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(backtest_results['confidence'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax4.axvline(backtest_results['confidence'].mean(), color='red', linestyle='--', \n",
    "            label=f\"Mean: {backtest_results['confidence'].mean():.3f}\")\n",
    "ax4.set_title('Confidence Score Distribution')\n",
    "ax4.set_xlabel('Confidence')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"\\nBacktest Performance Metrics:\")\n",
    "print(f\"Strategy Sharpe Ratio: {backtest_results['strategy_return'].mean() / backtest_results['strategy_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"Hit Rate: {(np.sign(backtest_results['combined_signal']) == np.sign(backtest_results['forward_return'])).mean():.1%}\")\n",
    "print(f\"Information Coefficient: {stats.pearsonr(backtest_results['combined_signal'], backtest_results['forward_return'])[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea08c73",
   "metadata": {},
   "source": [
    "## 8. Regulatory & Ethical Considerations\n",
    "\n",
    "### Key Compliance Areas\n",
    "\n",
    "1. **Material Non-Public Information (MNPI)**\n",
    "   - Alternative data must not constitute insider information\n",
    "   - Data providers should have proper consent and aggregation\n",
    "\n",
    "2. **Personal Data Protection**\n",
    "   - GDPR (Europe), CCPA (California)\n",
    "   - Proper anonymization and aggregation required\n",
    "\n",
    "3. **Data Licensing**\n",
    "   - Ensure proper rights to use data for trading\n",
    "   - Redistribution restrictions\n",
    "\n",
    "4. **Web Scraping Legality**\n",
    "   - Terms of Service compliance\n",
    "   - Computer Fraud and Abuse Act considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de95c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceChecker:\n",
    "    \"\"\"\n",
    "    Framework for checking alternative data compliance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.compliance_checklist = {\n",
    "            'data_source': [\n",
    "                'Source has documented data collection methodology',\n",
    "                'Data is properly anonymized/aggregated',\n",
    "                'No direct individual identification possible',\n",
    "                'Source has necessary licenses and permissions'\n",
    "            ],\n",
    "            'mnpi_risk': [\n",
    "                'Data is not obtained from company insiders',\n",
    "                'Information is derived from public observations',\n",
    "                'Data is available to any willing buyer',\n",
    "                'No special access or relationships involved'\n",
    "            ],\n",
    "            'usage_rights': [\n",
    "                'License permits use for trading signals',\n",
    "                'Redistribution terms are understood',\n",
    "                'Derivatives work creation is permitted',\n",
    "                'Data retention policy is compliant'\n",
    "            ],\n",
    "            'privacy': [\n",
    "                'GDPR requirements satisfied (if EU data)',\n",
    "                'CCPA requirements satisfied (if CA data)',\n",
    "                'Consent obtained for data collection',\n",
    "                'Data minimization principles followed'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_checklist(self, data_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Generate compliance checklist for a data type.\"\"\"\n",
    "        \n",
    "        rows = []\n",
    "        for category, items in self.compliance_checklist.items():\n",
    "            for item in items:\n",
    "                rows.append({\n",
    "                    'Category': category.replace('_', ' ').title(),\n",
    "                    'Requirement': item,\n",
    "                    'Status': ' Pending',\n",
    "                    'Notes': ''\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Generate compliance checklist\n",
    "compliance = ComplianceChecker()\n",
    "checklist = compliance.generate_checklist('web_scraping')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALTERNATIVE DATA COMPLIANCE CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "print(checklist[['Category', 'Requirement', 'Status']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef297e8",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways & Best Practices\n",
    "\n",
    "### Alternative Data Integration Framework\n",
    "\n",
    "1. **Data Sourcing**\n",
    "   - Evaluate multiple providers for each data type\n",
    "   - Conduct thorough due diligence on data provenance\n",
    "   - Ensure compliance with all regulations\n",
    "\n",
    "2. **Data Quality**\n",
    "   - Implement robust quality assessment frameworks\n",
    "   - Monitor for coverage changes and data drift\n",
    "   - Validate against known benchmarks\n",
    "\n",
    "3. **Signal Construction**\n",
    "   - Combine multiple orthogonal data sources\n",
    "   - Account for signal decay and crowding\n",
    "   - Implement proper backtesting methodology\n",
    "\n",
    "4. **Production Considerations**\n",
    "   - Build robust data pipelines with monitoring\n",
    "   - Handle missing data and delays gracefully\n",
    "   - Document all data transformations\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- **Survivorship bias**: Ensure historical data includes failed companies\n",
    "- **Look-ahead bias**: Respect data availability timestamps\n",
    "- **Overfitting**: Use out-of-sample validation rigorously\n",
    "- **Signal decay**: Monitor for alpha erosion as data becomes commoditized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d315dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of alternative data landscape\n",
    "alt_data_summary = pd.DataFrame({\n",
    "    'Data Type': ['Satellite/Geo', 'Web Traffic', 'Social Media', 'Transaction', 'App Usage', 'Sensor/IoT'],\n",
    "    'Latency': ['Days-Weeks', 'Hours-Days', 'Minutes-Hours', 'Days', 'Days-Weeks', 'Real-time'],\n",
    "    'Cost': ['$$$$$', '$$$', '$$', '$$$$', '$$$', '$$$$'],\n",
    "    'Alpha Decay': ['Medium', 'High', 'Very High', 'Medium', 'High', 'Low'],\n",
    "    'Coverage': ['Global', 'Online only', 'Tech-savvy', 'Card users', 'App users', 'Specific sectors'],\n",
    "    'Complexity': ['High', 'Medium', 'Medium', 'Low', 'Low', 'High']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALTERNATIVE DATA LANDSCAPE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(alt_data_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY VENDORS BY DATA TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "vendors = {\n",
    "    'Satellite': ['Orbital Insight', 'RS Metrics', 'Descartes Labs', 'Planet Labs'],\n",
    "    'Web/Social': ['SimilarWeb', 'Thinknum', 'Eagle Alpha', 'Quandl'],\n",
    "    'Transaction': ['Second Measure', 'Earnest Research', 'Bloomberg Second Measure'],\n",
    "    'Shipping': ['MarineTraffic', 'Kpler', 'Spire Global', 'VesselsValue']\n",
    "}\n",
    "\n",
    "for category, provider_list in vendors.items():\n",
    "    print(f\"\\n{category}: {', '.join(provider_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bec48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "1. **Satellite Data Analysis**: Extend the parking lot analyzer to include weather data adjustments\n",
    "2. **Web Scraping Project**: Build a scraper for job postings to predict company growth\n",
    "3. **Multi-Source Integration**: Combine 3+ alternative data sources for a single stock\n",
    "4. **Quality Framework**: Implement automated data quality alerts\n",
    "5. **Compliance Review**: Create a compliance checklist for a new data source\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- \"Alternative Data: A Guide to Intelligent Use\" - CFA Institute\n",
    "- \"The Rise of Alternative Data\" - JPMorgan Quantitative Research\n",
    "- \"Big Data and Machine Learning in Quantitative Investment\" - Lpez de Prado\n",
    "- Alternative Data sources: [alternativedata.org](https://alternativedata.org)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
