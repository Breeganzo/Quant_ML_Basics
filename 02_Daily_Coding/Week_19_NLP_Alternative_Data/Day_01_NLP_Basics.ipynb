{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd6f2044",
   "metadata": {},
   "source": [
    "# Day 01: NLP Basics for Financial Documents\n",
    "\n",
    "## Week 19: NLP & Alternative Data\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master text preprocessing techniques for financial documents\n",
    "- Understand tokenization strategies for financial text\n",
    "- Implement TF-IDF for document analysis and similarity\n",
    "- Apply NLP techniques to earnings calls, news, and SEC filings\n",
    "\n",
    "**Interview Topics Covered:**\n",
    "- Text preprocessing pipelines\n",
    "- Bag-of-words vs TF-IDF representations\n",
    "- Document similarity and retrieval\n",
    "- Domain-specific NLP challenges in finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4004398",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc35df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Scikit-learn NLP tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment ready for NLP analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7bc722",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Financial Text Data\n",
    "\n",
    "Let's create sample financial documents representing different types of financial text:\n",
    "- Earnings call transcripts\n",
    "- Financial news articles\n",
    "- SEC filing excerpts\n",
    "- Analyst reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d000502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample financial documents\n",
    "financial_documents = {\n",
    "    'earnings_call_1': \"\"\"\n",
    "    Good morning, and welcome to TechCorp's Q3 2025 earnings call. Our revenue \n",
    "    increased 15% year-over-year to $2.3 billion, exceeding analyst expectations. \n",
    "    EBITDA margins expanded by 200 basis points to 28.5%. We're raising our \n",
    "    full-year guidance due to strong demand in cloud services. Free cash flow \n",
    "    was $450 million, and we repurchased $200 million in shares during the quarter.\n",
    "    \"\"\",\n",
    "    \n",
    "    'earnings_call_2': \"\"\"\n",
    "    Thank you for joining GlobalBank's quarterly earnings presentation. Net interest \n",
    "    income declined 8% as the yield curve remained inverted. Provisions for credit \n",
    "    losses increased to $1.2 billion amid rising defaults in commercial real estate. \n",
    "    Our CET1 ratio stands at 12.5%, well above regulatory requirements. We're \n",
    "    implementing cost-cutting measures to improve efficiency ratios.\n",
    "    \"\"\",\n",
    "    \n",
    "    'news_positive': \"\"\"\n",
    "    Apple Inc. shares surged 5% in after-hours trading following better-than-expected \n",
    "    iPhone sales. The tech giant reported record services revenue, beating Wall Street \n",
    "    estimates by a wide margin. Analysts upgraded their price targets, citing strong \n",
    "    momentum in emerging markets and the successful launch of new AI features.\n",
    "    \"\"\",\n",
    "    \n",
    "    'news_negative': \"\"\"\n",
    "    Oil prices plummeted 8% today as OPEC+ failed to reach agreement on production \n",
    "    cuts. Energy stocks faced significant selling pressure, with Exxon and Chevron \n",
    "    down over 4%. Traders fear oversupply concerns could push crude below $60 per \n",
    "    barrel. The bearish sentiment extended to related sectors including oilfield services.\n",
    "    \"\"\",\n",
    "    \n",
    "    'sec_filing': \"\"\"\n",
    "    RISK FACTORS: Our business is subject to various risks including market volatility, \n",
    "    regulatory changes, and cybersecurity threats. We face intense competition in our \n",
    "    primary markets. Currency fluctuations may adversely affect our international \n",
    "    operations. The company has significant debt obligations totaling $5.2 billion \n",
    "    maturing over the next five years.\n",
    "    \"\"\",\n",
    "    \n",
    "    'analyst_report': \"\"\"\n",
    "    We initiate coverage of MegaRetail Corp with an OVERWEIGHT rating and $150 price \n",
    "    target, implying 25% upside. Key catalysts include margin expansion from supply \n",
    "    chain optimization and accelerating e-commerce growth. Valuation is attractive at \n",
    "    15x forward P/E versus 20x for peers. Main risks include consumer spending weakness \n",
    "    and competitive pressures from discount retailers.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "docs_df = pd.DataFrame([\n",
    "    {'doc_id': k, 'text': v.strip(), 'category': k.split('_')[0]} \n",
    "    for k, v in financial_documents.items()\n",
    "])\n",
    "\n",
    "print(f\"Created {len(docs_df)} financial documents\")\n",
    "docs_df[['doc_id', 'category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068ffd7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Text Preprocessing Pipeline\n",
    "\n",
    "### 3.1 Basic Text Cleaning\n",
    "\n",
    "Financial text often contains special characters, numbers, and formatting that need handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing pipeline optimized for financial documents.\n",
    "    \n",
    "    Key considerations for financial text:\n",
    "    - Preserve financial numbers (percentages, currency)\n",
    "    - Handle financial acronyms (EBITDA, P/E, CET1)\n",
    "    - Keep sentiment-bearing words\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Standard English stopwords\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Financial stopwords to add\n",
    "        self.financial_stopwords = {\n",
    "            'company', 'corporation', 'inc', 'corp', 'llc', 'ltd',\n",
    "            'quarter', 'year', 'fiscal', 'annual', 'period'\n",
    "        }\n",
    "        \n",
    "        # Words to preserve (financially meaningful)\n",
    "        self.preserve_words = {\n",
    "            'up', 'down', 'above', 'below', 'high', 'low',\n",
    "            'increase', 'decrease', 'rise', 'fall', 'gain', 'loss',\n",
    "            'bullish', 'bearish', 'overweight', 'underweight'\n",
    "        }\n",
    "        \n",
    "        # Remove preserved words from stopwords\n",
    "        self.stop_words = self.stop_words - self.preserve_words\n",
    "        self.stop_words = self.stop_words | self.financial_stopwords\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace currency symbols with tokens\n",
    "        text = re.sub(r'\\$([\\d,.]+)\\s*(billion|million|thousand)?', \n",
    "                      r'CURRENCY_\\2 ', text)\n",
    "        \n",
    "        # Replace percentages with tokens\n",
    "        text = re.sub(r'([\\d.]+)\\s*%', r'PERCENT ', text)\n",
    "        \n",
    "        # Replace basis points\n",
    "        text = re.sub(r'(\\d+)\\s*basis\\s*points?', r'BASIS_POINTS ', text)\n",
    "        text = re.sub(r'(\\d+)\\s*bps', r'BASIS_POINTS ', text)\n",
    "        \n",
    "        # Remove remaining numbers (optional - may want to keep some)\n",
    "        text = re.sub(r'\\b\\d+\\.?\\d*\\b', '', text)\n",
    "        \n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s_]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords while preserving financial terms.\"\"\"\n",
    "        return [t for t in tokens if t not in self.stop_words and len(t) > 2]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"Apply Porter stemming.\"\"\"\n",
    "        return [self.stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Apply lemmatization (better for financial text).\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    def preprocess(self, text, use_lemmatization=True, remove_stops=True):\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        # Clean text\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(cleaned)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if remove_stops:\n",
    "            tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Lemmatize or stem\n",
    "        if use_lemmatization:\n",
    "            tokens = self.lemmatize_tokens(tokens)\n",
    "        else:\n",
    "            tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FinancialTextPreprocessor()\n",
    "\n",
    "# Example preprocessing\n",
    "sample_text = financial_documents['earnings_call_1']\n",
    "print(\"Original text:\")\n",
    "print(sample_text[:200], \"...\")\n",
    "print(\"\\nCleaned text:\")\n",
    "print(preprocessor.clean_text(sample_text)[:200], \"...\")\n",
    "print(\"\\nTokens:\")\n",
    "print(preprocessor.preprocess(sample_text)[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41826d",
   "metadata": {},
   "source": [
    "### 3.2 Stemming vs Lemmatization\n",
    "\n",
    "**Interview Question:** What's the difference between stemming and lemmatization? When would you prefer one over the other in financial NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47415256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stemming vs lemmatization on financial terms\n",
    "financial_words = [\n",
    "    'trading', 'traded', 'trades', 'trader',\n",
    "    'increasing', 'increased', 'increases',\n",
    "    'volatility', 'volatile',\n",
    "    'earnings', 'earned', 'earning',\n",
    "    'better', 'best', 'good',\n",
    "    'running', 'ran', 'runs'\n",
    "]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': financial_words,\n",
    "    'Stemmed': [stemmer.stem(w) for w in financial_words],\n",
    "    'Lemmatized': [lemmatizer.lemmatize(w) for w in financial_words]\n",
    "})\n",
    "\n",
    "print(\"Stemming vs Lemmatization Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"- Stemming is faster but can create non-words (e.g., 'volat')\")\n",
    "print(\"- Lemmatization preserves real words, better for interpretation\")\n",
    "print(\"- For financial text, lemmatization often preferred for readability\")\n",
    "print(\"- Stemming may be better for pure ML tasks where interpretability is less critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2853319",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tokenization Strategies\n",
    "\n",
    "### 4.1 Word-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokenization(text, doc_name):\n",
    "    \"\"\"\n",
    "    Analyze different tokenization approaches.\n",
    "    \"\"\"\n",
    "    # Word tokenization\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Simple whitespace split\n",
    "    simple_tokens = text.lower().split()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Document: {doc_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nSentences ({len(sentences)}):\")\n",
    "    for i, sent in enumerate(sentences[:3]):\n",
    "        print(f\"  {i+1}. {sent[:80]}...\" if len(sent) > 80 else f\"  {i+1}. {sent}\")\n",
    "    \n",
    "    print(f\"\\nWord tokens (NLTK): {len(word_tokens)} tokens\")\n",
    "    print(f\"Simple split: {len(simple_tokens)} tokens\")\n",
    "    \n",
    "    # Show difference\n",
    "    nltk_set = set(word_tokens)\n",
    "    simple_set = set(simple_tokens)\n",
    "    \n",
    "    print(f\"\\nTokens unique to NLTK: {list(nltk_set - simple_set)[:10]}\")\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "# Analyze tokenization for different document types\n",
    "for doc_id in ['earnings_call_1', 'sec_filing']:\n",
    "    analyze_tokenization(financial_documents[doc_id], doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32665c01",
   "metadata": {},
   "source": [
    "### 4.2 N-gram Tokenization\n",
    "\n",
    "N-grams capture multi-word financial phrases like \"interest rate\", \"market volatility\", \"price target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abecb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(text, n=2):\n",
    "    \"\"\"\n",
    "    Extract n-grams from text.\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    tokens = preprocessor.preprocess(text, use_lemmatization=True, remove_stops=False)\n",
    "    \n",
    "    # Generate n-grams\n",
    "    n_grams = list(ngrams(tokens, n))\n",
    "    \n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "\n",
    "\n",
    "# Extract bigrams from all documents\n",
    "all_text = ' '.join(financial_documents.values())\n",
    "\n",
    "# Bigrams\n",
    "bigrams = extract_ngrams(all_text, n=2)\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "print(\"Top 15 Bigrams in Financial Documents:\")\n",
    "print(\"-\" * 40)\n",
    "for phrase, count in bigram_freq.most_common(15):\n",
    "    print(f\"{phrase:30} | {count}\")\n",
    "\n",
    "# Trigrams\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "print(\"-\" * 40)\n",
    "trigrams = extract_ngrams(all_text, n=3)\n",
    "trigram_freq = Counter(trigrams)\n",
    "for phrase, count in trigram_freq.most_common(10):\n",
    "    print(f\"{phrase:40} | {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa809fc5",
   "metadata": {},
   "source": [
    "### 4.3 Financial Domain-Specific Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTokenizer:\n",
    "    \"\"\"\n",
    "    Custom tokenizer for financial text that handles:\n",
    "    - Financial acronyms (EBITDA, P/E, ROE)\n",
    "    - Ticker symbols ($AAPL, $TSLA)\n",
    "    - Currency amounts\n",
    "    - Percentages and basis points\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common financial acronyms to preserve\n",
    "        self.financial_acronyms = {\n",
    "            'ebitda', 'ebit', 'eps', 'pe', 'pb', 'roe', 'roa', 'roi',\n",
    "            'cet1', 'rwa', 'nii', 'nim', 'npv', 'irr', 'wacc', 'capm',\n",
    "            'yoy', 'qoq', 'mom', 'ytd', 'mtd', 'cagr', 'fcf', 'dcf',\n",
    "            'ipo', 'sec', 'fed', 'fomc', 'gdp', 'cpi', 'pmi'\n",
    "        }\n",
    "        \n",
    "        # Patterns for financial entities\n",
    "        self.patterns = {\n",
    "            'ticker': r'\\$[A-Z]{1,5}\\b',\n",
    "            'currency': r'\\$[\\d,.]+\\s*(billion|million|thousand|B|M|K)?',\n",
    "            'percentage': r'[\\d.]+\\s*%',\n",
    "            'basis_points': r'\\d+\\s*(basis\\s*points?|bps)',\n",
    "            'ratio': r'\\b\\d+\\.?\\d*x\\b',\n",
    "            'date': r'Q[1-4]\\s*\\'?\\d{2,4}|FY\\s*\\'?\\d{2,4}'\n",
    "        }\n",
    "    \n",
    "    def extract_financial_entities(self, text):\n",
    "        \"\"\"\n",
    "        Extract financial entities from text.\n",
    "        \"\"\"\n",
    "        entities = {}\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                entities[entity_type] = matches\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize with financial entity preservation.\n",
    "        \"\"\"\n",
    "        # Extract entities first\n",
    "        entities = self.extract_financial_entities(text)\n",
    "        \n",
    "        # Replace entities with placeholders\n",
    "        modified_text = text\n",
    "        placeholders = {}\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            placeholder = f'__{entity_type.upper()}__'\n",
    "            modified_text = re.sub(pattern, placeholder, modified_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Standard tokenization\n",
    "        tokens = word_tokenize(modified_text.lower())\n",
    "        \n",
    "        return tokens, entities\n",
    "\n",
    "\n",
    "# Test financial tokenizer\n",
    "fin_tokenizer = FinancialTokenizer()\n",
    "\n",
    "test_text = \"\"\"\n",
    "$AAPL reported Q3'25 earnings with EPS of $2.50, beating estimates by 15%. \n",
    "Revenue was $95.2 billion, up 12% YoY. EBITDA margin expanded 200 basis points.\n",
    "The stock trades at 25x forward P/E ratio.\n",
    "\"\"\"\n",
    "\n",
    "tokens, entities = fin_tokenizer.tokenize(test_text)\n",
    "\n",
    "print(\"Extracted Financial Entities:\")\n",
    "for entity_type, values in entities.items():\n",
    "    print(f\"  {entity_type}: {values}\")\n",
    "\n",
    "print(f\"\\nTokens (sample): {tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f1256",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Bag-of-Words (BoW) Representation\n",
    "\n",
    "### 5.1 Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdd5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all documents\n",
    "processed_docs = []\n",
    "for text in docs_df['text']:\n",
    "    tokens = preprocessor.preprocess(text)\n",
    "    processed_docs.append(' '.join(tokens))\n",
    "\n",
    "docs_df['processed_text'] = processed_docs\n",
    "\n",
    "# Create Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=1,            # Minimum document frequency\n",
    "    max_df=0.9           # Maximum document frequency (remove very common terms)\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "bow_matrix = count_vectorizer.fit_transform(docs_df['processed_text'])\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=count_vectorizer.get_feature_names_out(),\n",
    "    index=docs_df['doc_id']\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(f\"Document-term matrix shape: {bow_matrix.shape}\")\n",
    "print(\"\\nSample of BoW representation:\")\n",
    "bow_df.iloc[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26de053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize term frequencies across documents\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Get top terms by total frequency\n",
    "term_freq = bow_df.sum().sort_values(ascending=False).head(20)\n",
    "\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(term_freq)))\n",
    "bars = ax.bar(range(len(term_freq)), term_freq.values, color=colors)\n",
    "ax.set_xticks(range(len(term_freq)))\n",
    "ax.set_xticklabels(term_freq.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Total Count')\n",
    "ax.set_title('Top 20 Terms by Frequency (Bag-of-Words)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fae97a",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "### 6.1 TF-IDF Theory\n",
    "\n",
    "**Interview Question:** Explain TF-IDF and why it's better than raw term counts for document analysis.\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(t, d)$ = Term frequency of term $t$ in document $d$\n",
    "- $\\text{IDF}(t, D) = \\log\\frac{N}{|\\{d \\in D : t \\in d\\}|}$\n",
    "- $N$ = Total number of documents\n",
    "\n",
    "**Key Insight:** TF-IDF downweights common terms and highlights distinctive terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd84484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,  # Apply sublinear TF scaling (1 + log(tf))\n",
    "    norm='l2'           # L2 normalization\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(docs_df['processed_text'])\n",
    "\n",
    "# Create DataFrame\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=docs_df['doc_id']\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(\"\\nTF-IDF scores (sample):\")\n",
    "tfidf_df.iloc[:, :8].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_terms(doc_index, n_terms=10):\n",
    "    \"\"\"\n",
    "    Get top TF-IDF terms for a document.\n",
    "    \"\"\"\n",
    "    doc_name = docs_df['doc_id'].iloc[doc_index]\n",
    "    scores = tfidf_df.iloc[doc_index].sort_values(ascending=False)\n",
    "    \n",
    "    return doc_name, scores.head(n_terms)\n",
    "\n",
    "\n",
    "# Analyze top terms for each document\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(len(docs_df)):\n",
    "    doc_name, top_terms = get_top_tfidf_terms(i, 8)\n",
    "    \n",
    "    ax = axes[i]\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.8, len(top_terms)))\n",
    "    ax.barh(range(len(top_terms)), top_terms.values, color=colors)\n",
    "    ax.set_yticks(range(len(top_terms)))\n",
    "    ax.set_yticklabels(top_terms.index)\n",
    "    ax.set_xlabel('TF-IDF Score')\n",
    "    ax.set_title(f'{doc_name}', fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.suptitle('Top TF-IDF Terms by Document', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b37011",
   "metadata": {},
   "source": [
    "### 6.2 Comparing BoW vs TF-IDF\n",
    "\n",
    "**Interview Question:** When would you use BoW vs TF-IDF for financial text analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb200b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare rankings between BoW and TF-IDF\n",
    "print(\"Comparison: BoW vs TF-IDF Term Rankings\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall top terms\n",
    "bow_top = bow_df.sum().sort_values(ascending=False).head(10)\n",
    "tfidf_top = tfidf_df.sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'BoW_Term': bow_top.index,\n",
    "    'BoW_Score': bow_top.values,\n",
    "    'TF-IDF_Term': tfidf_top.index,\n",
    "    'TF-IDF_Score': tfidf_top.values.round(3)\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY DIFFERENCES:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"BoW: Raw counts favor frequent terms across all documents\")\n",
    "print(\"TF-IDF: Highlights distinctive terms for each document\")\n",
    "print(\"\\nUse BoW when: Frequency matters (e.g., keyword detection)\")\n",
    "print(\"Use TF-IDF when: Finding distinctive content (e.g., document retrieval)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03f66d1",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Document Similarity with TF-IDF\n",
    "\n",
    "### 7.1 Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document similarity\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity_matrix,\n",
    "    index=docs_df['doc_id'],\n",
    "    columns=docs_df['doc_id']\n",
    ")\n",
    "\n",
    "print(\"Document Similarity Matrix (Cosine Similarity):\")\n",
    "print(similarity_df.round(3))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    similarity_df, \n",
    "    annot=True, \n",
    "    fmt='.2f', \n",
    "    cmap='RdYlGn',\n",
    "    center=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Document Similarity Matrix (TF-IDF + Cosine Similarity)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595603dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most similar document pairs\n",
    "print(\"Most Similar Document Pairs:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(similarity_df)):\n",
    "    for j in range(i+1, len(similarity_df)):\n",
    "        pairs.append({\n",
    "            'doc1': similarity_df.index[i],\n",
    "            'doc2': similarity_df.columns[j],\n",
    "            'similarity': similarity_df.iloc[i, j]\n",
    "        })\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs).sort_values('similarity', ascending=False)\n",
    "print(pairs_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Earnings calls are similar to each other (corporate language)\")\n",
    "print(\"- News articles have distinct vocabulary based on sentiment\")\n",
    "print(\"- SEC filings use different legal/regulatory language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f96bb1a",
   "metadata": {},
   "source": [
    "### 7.2 Query-Based Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa723dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_documents(query, vectorizer, doc_matrix, doc_names, top_n=3):\n",
    "    \"\"\"\n",
    "    Find documents most similar to a query using TF-IDF.\n",
    "    \n",
    "    This is commonly used in:\n",
    "    - Document search systems\n",
    "    - News article retrieval\n",
    "    - SEC filing analysis\n",
    "    \"\"\"\n",
    "    # Preprocess query\n",
    "    query_processed = ' '.join(preprocessor.preprocess(query))\n",
    "    \n",
    "    # Transform query to TF-IDF vector\n",
    "    query_vector = vectorizer.transform([query_processed])\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarities = cosine_similarity(query_vector, doc_matrix)[0]\n",
    "    \n",
    "    # Rank documents\n",
    "    results = pd.DataFrame({\n",
    "        'document': doc_names,\n",
    "        'similarity': similarities\n",
    "    }).sort_values('similarity', ascending=False)\n",
    "    \n",
    "    return results.head(top_n)\n",
    "\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"company earnings revenue growth profit\",\n",
    "    \"stock price target analyst recommendation\",\n",
    "    \"risk factors debt regulatory concerns\",\n",
    "    \"oil energy prices market decline\"\n",
    "]\n",
    "\n",
    "print(\"Document Retrieval Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    results = find_similar_documents(\n",
    "        query, \n",
    "        tfidf_vectorizer, \n",
    "        tfidf_matrix, \n",
    "        docs_df['doc_id'].values\n",
    "    )\n",
    "    for _, row in results.iterrows():\n",
    "        print(f\"  {row['document']:20} | Similarity: {row['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4aa8e9",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Advanced TF-IDF Applications\n",
    "\n",
    "### 8.1 Extracting Key Financial Topics (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895eab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Analysis (LSA) using SVD on TF-IDF\n",
    "n_topics = 3\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_topics, random_state=42)\n",
    "doc_topics = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Get top terms for each topic\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Latent Topics from Financial Documents (LSA):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for topic_idx, topic in enumerate(svd.components_):\n",
    "    top_term_indices = topic.argsort()[-8:][::-1]\n",
    "    top_terms = [terms[i] for i in top_term_indices]\n",
    "    \n",
    "    print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "    print(f\"  Terms: {', '.join(top_terms)}\")\n",
    "    \n",
    "    # Documents most associated with this topic\n",
    "    top_doc_idx = doc_topics[:, topic_idx].argsort()[-2:][::-1]\n",
    "    top_docs = [docs_df['doc_id'].iloc[i] for i in top_doc_idx]\n",
    "    print(f\"  Top docs: {', '.join(top_docs)}\")\n",
    "\n",
    "# Visualize document-topic matrix\n",
    "topic_df = pd.DataFrame(\n",
    "    doc_topics,\n",
    "    columns=[f'Topic_{i+1}' for i in range(n_topics)],\n",
    "    index=docs_df['doc_id']\n",
    ")\n",
    "\n",
    "print(\"\\nDocument-Topic Matrix:\")\n",
    "print(topic_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f6477",
   "metadata": {},
   "source": [
    "### 8.2 Financial Sentiment Lexicon Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple financial sentiment lexicon (Loughran-McDonald style)\n",
    "financial_sentiment = {\n",
    "    'positive': [\n",
    "        'growth', 'increase', 'gain', 'profit', 'surge', 'strong', 'beat',\n",
    "        'exceed', 'upgrade', 'bullish', 'opportunity', 'momentum', 'record',\n",
    "        'optimistic', 'outperform', 'attractive', 'upside', 'expansion'\n",
    "    ],\n",
    "    'negative': [\n",
    "        'loss', 'decline', 'risk', 'weak', 'fall', 'plummet', 'bearish',\n",
    "        'concern', 'fear', 'default', 'downgrade', 'pressure', 'threat',\n",
    "        'adverse', 'underperform', 'volatile', 'recession', 'uncertainty'\n",
    "    ],\n",
    "    'uncertainty': [\n",
    "        'may', 'could', 'might', 'possible', 'uncertain', 'unclear',\n",
    "        'depending', 'subject', 'approximate', 'estimate', 'expect'\n",
    "    ],\n",
    "    'litigious': [\n",
    "        'litigation', 'lawsuit', 'regulatory', 'compliance', 'investigation',\n",
    "        'penalty', 'settlement', 'violation', 'enforcement'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def calculate_sentiment_scores(text):\n",
    "    \"\"\"\n",
    "    Calculate sentiment scores based on financial lexicon.\n",
    "    \"\"\"\n",
    "    tokens = preprocessor.preprocess(text, remove_stops=False)\n",
    "    token_set = set(tokens)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    scores = {}\n",
    "    for sentiment, words in financial_sentiment.items():\n",
    "        count = sum(1 for t in tokens if t in words)\n",
    "        scores[sentiment] = count / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# Calculate sentiment for all documents\n",
    "sentiment_results = []\n",
    "for idx, row in docs_df.iterrows():\n",
    "    scores = calculate_sentiment_scores(row['text'])\n",
    "    scores['doc_id'] = row['doc_id']\n",
    "    sentiment_results.append(scores)\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_results).set_index('doc_id')\n",
    "\n",
    "print(\"Financial Sentiment Scores:\")\n",
    "print(sentiment_df.round(4))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sentiment_df.plot(kind='bar', ax=ax, colormap='RdYlGn')\n",
    "ax.set_ylabel('Sentiment Score (word frequency)')\n",
    "ax.set_title('Financial Sentiment Analysis by Document')\n",
    "ax.legend(title='Sentiment', bbox_to_anchor=(1.02, 1))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0896a2",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Interview Practice Questions\n",
    "\n",
    "### Q1: Text Preprocessing Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW QUESTION 1: Text Preprocessing\n",
    "=========================================\n",
    "\n",
    "Q: You're building an NLP model to analyze earnings call transcripts.\n",
    "   What preprocessing steps would you take and why?\n",
    "\n",
    "ANSWER:\n",
    "-------\n",
    "1. CASE NORMALIZATION: Convert to lowercase for consistency,\n",
    "   BUT preserve acronyms like EBITDA, EPS (common in finance)\n",
    "\n",
    "2. NUMBER HANDLING: \n",
    "   - Replace specific numbers with tokens (e.g., $2.3B â†’ CURRENCY_BILLION)\n",
    "   - Preserves semantic meaning without vocabulary explosion\n",
    "   - Percentages â†’ PERCENT token\n",
    "\n",
    "3. STOPWORDS:\n",
    "   - Remove standard stopwords\n",
    "   - BUT keep sentiment words (up, down, above, below)\n",
    "   - Add domain stopwords (quarter, fiscal, company)\n",
    "\n",
    "4. LEMMATIZATION over STEMMING:\n",
    "   - Preserves real words for interpretability\n",
    "   - Important for financial reports where terms matter\n",
    "\n",
    "5. N-GRAMS:\n",
    "   - Include bigrams to capture phrases like \"interest rate\"\n",
    "   - Financial language is phrase-heavy\n",
    "\n",
    "6. ENTITY RECOGNITION:\n",
    "   - Identify and normalize ticker symbols, dates, currencies\n",
    "   - May need custom NER for financial entities\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2da5f",
   "metadata": {},
   "source": [
    "### Q2: TF-IDF Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601719c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW QUESTION 2: TF-IDF\n",
    "============================\n",
    "\n",
    "Q: Explain TF-IDF and when you'd use it vs. raw word counts.\n",
    "\n",
    "ANSWER:\n",
    "-------\n",
    "TF-IDF = Term Frequency Ã— Inverse Document Frequency\n",
    "\n",
    "TF (Term Frequency):\n",
    "  - How often a term appears in a document\n",
    "  - Higher TF = term is important to this document\n",
    "\n",
    "IDF (Inverse Document Frequency):\n",
    "  - log(N / df), where df = docs containing the term\n",
    "  - Downweights common terms (\"the\", \"company\")\n",
    "  - Upweights rare, distinctive terms\n",
    "\n",
    "WHY USE TF-IDF?\n",
    "  - Raw counts favor common words\n",
    "  - TF-IDF highlights what makes documents UNIQUE\n",
    "  - Better for: document similarity, search, classification\n",
    "\n",
    "WHEN TO USE RAW COUNTS?\n",
    "  - When frequency itself is meaningful (keyword detection)\n",
    "  - As input to some models (Naive Bayes)\n",
    "  - Topic modeling (LDA prefers counts)\n",
    "\n",
    "PRACTICAL EXAMPLE:\n",
    "  In SEC filings, \"risk\" appears in every document.\n",
    "  TF-IDF will downweight \"risk\" and highlight\n",
    "  specific risks like \"cybersecurity\" or \"litigation\".\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573564f5",
   "metadata": {},
   "source": [
    "### Q3: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "INTERVIEW QUESTION 3: Document Similarity\n",
    "=========================================\n",
    "\n",
    "Q: Why use cosine similarity instead of Euclidean distance\n",
    "   for comparing documents?\n",
    "\n",
    "ANSWER:\n",
    "-------\n",
    "COSINE SIMILARITY:\n",
    "  cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)\n",
    "  \n",
    "  - Measures angle between vectors (direction)\n",
    "  - Normalized: ignores document length\n",
    "  - Range: [-1, 1] (or [0, 1] for TF-IDF since no negatives)\n",
    "\n",
    "EUCLIDEAN DISTANCE:\n",
    "  d = sqrt(Î£(a_i - b_i)Â²)\n",
    "  \n",
    "  - Measures absolute distance\n",
    "  - Affected by document length\n",
    "\n",
    "WHY COSINE FOR DOCUMENTS?\n",
    "  1. Document length varies wildly\n",
    "     - 10-K filing: 50,000 words\n",
    "     - News headline: 10 words\n",
    "     - Euclidean would say they're always different\n",
    "  \n",
    "  2. We care about topic similarity, not length\n",
    "     - Two articles about same topic should be similar\n",
    "     - Even if one is 3x longer\n",
    "  \n",
    "  3. Sparse high-dimensional vectors\n",
    "     - TF-IDF vectors are very sparse\n",
    "     - Cosine handles this well\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate with example\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Compare cosine vs euclidean\n",
    "print(\"\\nComparison: Cosine vs Euclidean Similarity\")\n",
    "print(\"=\"*50)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "euclidean_dist = euclidean_distances(tfidf_matrix)\n",
    "\n",
    "print(\"\\nCosine Similarity (first 3 docs):\")\n",
    "print(cosine_sim[:3, :3].round(3))\n",
    "print(\"\\nEuclidean Distance (first 3 docs):\")\n",
    "print(euclidean_dist[:3, :3].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072cf7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Practical Exercise: News Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNewsAnalyzer:\n",
    "    \"\"\"\n",
    "    Complete NLP pipeline for financial news analysis.\n",
    "    \n",
    "    Features:\n",
    "    - Text preprocessing\n",
    "    - TF-IDF vectorization\n",
    "    - Document similarity\n",
    "    - Sentiment scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = FinancialTextPreprocessor()\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=500,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=1,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        self.documents = []\n",
    "        self.tfidf_matrix = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"\n",
    "        Fit the analyzer on a corpus of documents.\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Preprocess\n",
    "        processed = [\n",
    "            ' '.join(self.preprocessor.preprocess(doc)) \n",
    "            for doc in documents\n",
    "        ]\n",
    "        \n",
    "        # Fit TF-IDF\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(processed)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def find_similar(self, query, top_n=3):\n",
    "        \"\"\"\n",
    "        Find documents similar to a query.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Analyzer not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Process query\n",
    "        query_processed = ' '.join(self.preprocessor.preprocess(query))\n",
    "        query_vector = self.vectorizer.transform([query_processed])\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]\n",
    "        \n",
    "        # Get top results\n",
    "        top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx][:100] + '...',\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_key_terms(self, doc_index, n_terms=10):\n",
    "        \"\"\"\n",
    "        Get most important terms for a document.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Analyzer not fitted. Call fit() first.\")\n",
    "        \n",
    "        terms = self.vectorizer.get_feature_names_out()\n",
    "        scores = self.tfidf_matrix[doc_index].toarray()[0]\n",
    "        \n",
    "        top_indices = scores.argsort()[-n_terms:][::-1]\n",
    "        \n",
    "        return [(terms[i], scores[i]) for i in top_indices]\n",
    "\n",
    "\n",
    "# Test the analyzer\n",
    "analyzer = FinancialNewsAnalyzer()\n",
    "analyzer.fit(list(financial_documents.values()))\n",
    "\n",
    "print(\"Financial News Analyzer Demo\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test query\n",
    "query = \"stock market decline bearish sentiment\"\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nMost Similar Documents:\")\n",
    "\n",
    "for i, result in enumerate(analyzer.find_similar(query)):\n",
    "    print(f\"\\n{i+1}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"   {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298d676",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary & Key Takeaways\n",
    "\n",
    "### What We Covered Today:\n",
    "\n",
    "1. **Text Preprocessing for Finance**\n",
    "   - Domain-specific considerations (numbers, acronyms, sentiment words)\n",
    "   - Stemming vs lemmatization trade-offs\n",
    "   - Custom tokenization for financial entities\n",
    "\n",
    "2. **Tokenization Strategies**\n",
    "   - Word-level tokenization\n",
    "   - N-grams for phrase capture\n",
    "   - Financial entity extraction\n",
    "\n",
    "3. **Document Representations**\n",
    "   - Bag-of-Words (BoW)\n",
    "   - TF-IDF and its advantages\n",
    "   - When to use each approach\n",
    "\n",
    "4. **Applications**\n",
    "   - Document similarity with cosine similarity\n",
    "   - Query-based document retrieval\n",
    "   - Topic extraction with LSA\n",
    "   - Sentiment analysis with financial lexicons\n",
    "\n",
    "### Interview Prep Checklist:\n",
    "- [ ] Explain TF-IDF formula and intuition\n",
    "- [ ] Compare stemming vs lemmatization\n",
    "- [ ] Why cosine similarity for documents?\n",
    "- [ ] Design a text preprocessing pipeline for financial data\n",
    "- [ ] Explain challenges of NLP in finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "ðŸŽ¯ NEXT STEPS:\n",
    "==============\n",
    "\n",
    "Day 02: Word Embeddings (Word2Vec, GloVe)\n",
    "Day 03: Sentiment Analysis with ML Models\n",
    "Day 04: Named Entity Recognition for Finance\n",
    "Day 05: Topic Modeling (LDA)\n",
    "Day 06: Alternative Data Sources\n",
    "Day 07: Interview Review & Practice\n",
    "\n",
    "ðŸ“š RECOMMENDED READING:\n",
    "- \"Textual Analysis in Finance\" - Loughran & McDonald\n",
    "- NLTK Book (free online)\n",
    "- Scikit-learn text feature extraction docs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
