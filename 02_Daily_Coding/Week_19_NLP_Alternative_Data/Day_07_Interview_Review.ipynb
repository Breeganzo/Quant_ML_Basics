{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8769d16a",
   "metadata": {},
   "source": [
    "# Day 7: NLP & Alternative Data - Interview Questions & Week Review\n",
    "\n",
    "## Week 19 Summary & Interview Preparation\n",
    "\n",
    "**Date:** Week 19, Day 7\n",
    "\n",
    "**Topics Covered This Week:**\n",
    "- Text preprocessing and tokenization for financial documents\n",
    "- Sentiment analysis using traditional ML and deep learning\n",
    "- Named Entity Recognition (NER) for financial entities\n",
    "- Topic modeling (LDA, NMF) on earnings calls and news\n",
    "- Word embeddings (Word2Vec, GloVe, FinBERT)\n",
    "- Transformer-based models for finance (BERT, FinBERT)\n",
    "- Alternative data sources and alpha generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa61b9",
   "metadata": {},
   "source": [
    "## Interview Questions Overview\n",
    "\n",
    "| # | Topic | Difficulty | Key Concepts |\n",
    "|---|-------|------------|-------------|\n",
    "| 1 | Text Preprocessing | Medium | Tokenization, Lemmatization, Financial Text |\n",
    "| 2 | Sentiment Analysis | Medium | Lexicon vs ML approaches, Domain adaptation |\n",
    "| 3 | Word Embeddings | Medium | Word2Vec, GloVe, Contextual embeddings |\n",
    "| 4 | FinBERT & Transformers | Hard | Transfer learning, Fine-tuning |\n",
    "| 5 | Topic Modeling | Medium | LDA, Coherence, Interpretability |\n",
    "| 6 | NER for Finance | Medium | Entity extraction, Relation extraction |\n",
    "| 7 | Event-Driven Strategies | Hard | News impact, Event detection |\n",
    "| 8 | Alternative Data | Medium | Data sources, Alpha decay |\n",
    "| 9 | NLP Feature Engineering | Hard | Text-to-signal pipeline |\n",
    "| 10 | Production NLP Systems | Hard | Latency, Scalability, Monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f4d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Week 19 Day 7: NLP & Alternative Data Interview Review\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29b901",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Text Preprocessing for Financial Documents\n",
    "\n",
    "### Interview Question:\n",
    "**\"Describe your approach to preprocessing financial text data (earnings calls, SEC filings, news). What unique challenges exist compared to general NLP?\"**\n",
    "\n",
    "### Key Discussion Points:\n",
    "1. Financial-specific tokenization challenges\n",
    "2. Handling numbers, percentages, and financial terms\n",
    "3. Domain-specific stop words\n",
    "4. Preserving semantic meaning (negations, comparatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Interview Answer: Financial Text Preprocessing Pipeline\n",
    "    \n",
    "    Key differences from general NLP:\n",
    "    1. Numbers carry meaning (10% vs 5% matters)\n",
    "    2. Financial jargon (EBITDA, YoY, QoQ)\n",
    "    3. Negations are critical (\"not expecting growth\" vs \"expecting growth\")\n",
    "    4. Comparative language matters (\"better than\", \"worse than\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Financial-specific patterns\n",
    "        self.financial_terms = {\n",
    "            'ebitda', 'eps', 'p/e', 'yoy', 'qoq', 'mom', 'cagr',\n",
    "            'roi', 'roe', 'roa', 'wacc', 'dcf', 'fcf', 'capex'\n",
    "        }\n",
    "        \n",
    "        # Numbers to preserve with context\n",
    "        self.number_patterns = [\n",
    "            (r'\\$[\\d,]+\\.?\\d*[BMK]?', 'DOLLAR_AMOUNT'),\n",
    "            (r'\\d+\\.?\\d*%', 'PERCENTAGE'),\n",
    "            (r'\\d+\\.?\\d*x', 'MULTIPLE'),\n",
    "            (r'Q[1-4]', 'QUARTER'),\n",
    "            (r'FY\\d{2,4}', 'FISCAL_YEAR'),\n",
    "        ]\n",
    "        \n",
    "        # Negation handling\n",
    "        self.negation_words = {'not', 'no', \"n't\", 'never', 'neither', 'without'}\n",
    "        \n",
    "    def normalize_numbers(self, text: str) -> str:\n",
    "        \"\"\"Replace numbers with semantic tokens while preserving meaning.\"\"\"\n",
    "        for pattern, token in self.number_patterns:\n",
    "            text = re.sub(pattern, f' {token} ', text)\n",
    "        return text\n",
    "    \n",
    "    def handle_negations(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Mark tokens following negations.\n",
    "        'not good' -> 'not' 'NEG_good'\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        negate_next = False\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.lower() in self.negation_words:\n",
    "                negate_next = True\n",
    "                result.append(token)\n",
    "            elif negate_next:\n",
    "                result.append(f'NEG_{token}')\n",
    "                negate_next = False\n",
    "            else:\n",
    "                result.append(token)\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        \"\"\"Full preprocessing pipeline.\"\"\"\n",
    "        # 1. Lowercase (but preserve acronyms)\n",
    "        text = self._smart_lowercase(text)\n",
    "        \n",
    "        # 2. Normalize numbers\n",
    "        text = self.normalize_numbers(text)\n",
    "        \n",
    "        # 3. Basic tokenization\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # 4. Handle negations\n",
    "        tokens = self.handle_negations(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _smart_lowercase(self, text: str) -> str:\n",
    "        \"\"\"Lowercase while preserving financial acronyms.\"\"\"\n",
    "        words = text.split()\n",
    "        result = []\n",
    "        for word in words:\n",
    "            if word.upper() == word and len(word) <= 5:  # Likely acronym\n",
    "                result.append(word)\n",
    "            else:\n",
    "                result.append(word.lower())\n",
    "        return ' '.join(result)\n",
    "\n",
    "# Demonstration\n",
    "preprocessor = FinancialTextPreprocessor()\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Our Q3 EBITDA grew by 15% YoY to $2.5B. However, we are not expecting \n",
    "similar growth in Q4 due to increased CAPEX of $500M.\n",
    "\"\"\"\n",
    "\n",
    "tokens = preprocessor.preprocess(sample_text)\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nProcessed tokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab1331",
   "metadata": {},
   "source": [
    "### Answer Framework:\n",
    "\n",
    "**Financial text preprocessing differs from general NLP in several key ways:**\n",
    "\n",
    "1. **Number Handling**: Unlike general NLP where numbers are often removed, financial numbers carry critical meaning. We normalize them to semantic tokens while preserving context.\n",
    "\n",
    "2. **Domain Vocabulary**: Financial acronyms (EBITDA, EPS, P/E) must be preserved. Standard lemmatizers may corrupt these.\n",
    "\n",
    "3. **Negation Sensitivity**: In finance, \"exceeding expectations\" vs \"not exceeding expectations\" has opposite trading implications. We explicitly mark negation scope.\n",
    "\n",
    "4. **Temporal References**: Q1/Q2/Q3/Q4, YoY, MoM are critical for time-series alignment.\n",
    "\n",
    "5. **Comparative Language**: \"Better than expected\" vs \"worse than expected\" signals require special handling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb6517",
   "metadata": {},
   "source": [
    "## Question 2: Sentiment Analysis Approaches\n",
    "\n",
    "### Interview Question:\n",
    "**\"Compare lexicon-based vs machine learning approaches for financial sentiment analysis. When would you use each?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Interview Answer: Comparison of Sentiment Analysis Approaches\n",
    "    \n",
    "    1. Lexicon-Based (Loughran-McDonald)\n",
    "       - Pros: Interpretable, no training needed, domain-specific\n",
    "       - Cons: Misses context, limited vocabulary\n",
    "       \n",
    "    2. Machine Learning (Traditional)\n",
    "       - Pros: Learns patterns, better accuracy\n",
    "       - Cons: Needs labeled data, may overfit\n",
    "       \n",
    "    3. Deep Learning (FinBERT)\n",
    "       - Pros: Contextual understanding, state-of-the-art\n",
    "       - Cons: Computationally expensive, black box\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Loughran-McDonald Financial Sentiment Lexicon (subset)\n",
    "        self.positive_words = {\n",
    "            'achieve', 'attain', 'benefit', 'better', 'breakthrough',\n",
    "            'gain', 'improve', 'increase', 'opportunity', 'outperform',\n",
    "            'positive', 'profit', 'progress', 'strength', 'success',\n",
    "            'surpass', 'upturn', 'upgrade', 'growth', 'exceeded'\n",
    "        }\n",
    "        \n",
    "        self.negative_words = {\n",
    "            'adverse', 'against', 'decline', 'decrease', 'deficit',\n",
    "            'deteriorate', 'difficulty', 'fail', 'failure', 'loss',\n",
    "            'negative', 'risk', 'threat', 'unable', 'uncertain',\n",
    "            'unfavorable', 'weak', 'worsen', 'downturn', 'missed'\n",
    "        }\n",
    "        \n",
    "        self.uncertainty_words = {\n",
    "            'approximately', 'assume', 'believe', 'could', 'depend',\n",
    "            'estimate', 'expect', 'fluctuate', 'may', 'might',\n",
    "            'possible', 'predict', 'probably', 'risk', 'uncertain'\n",
    "        }\n",
    "        \n",
    "        self.litigious_words = {\n",
    "            'alleged', 'attorney', 'claim', 'court', 'defendant',\n",
    "            'lawsuit', 'legal', 'litigation', 'penalty', 'plaintiff'\n",
    "        }\n",
    "    \n",
    "    def lexicon_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Loughran-McDonald lexicon-based sentiment.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        total_words = len(words)\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return {'positive': 0, 'negative': 0, 'uncertainty': 0, 'litigious': 0}\n",
    "        \n",
    "        scores = {\n",
    "            'positive': sum(1 for w in words if w in self.positive_words) / total_words,\n",
    "            'negative': sum(1 for w in words if w in self.negative_words) / total_words,\n",
    "            'uncertainty': sum(1 for w in words if w in self.uncertainty_words) / total_words,\n",
    "            'litigious': sum(1 for w in words if w in self.litigious_words) / total_words\n",
    "        }\n",
    "        \n",
    "        # Net sentiment score\n",
    "        scores['net_sentiment'] = scores['positive'] - scores['negative']\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def compare_approaches(self, texts: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Compare lexicon sentiment across multiple texts.\"\"\"\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            scores = self.lexicon_sentiment(text)\n",
    "            scores['text_preview'] = text[:50] + '...'\n",
    "            results.append(scores)\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Demonstration\n",
    "analyzer = FinancialSentimentAnalyzer()\n",
    "\n",
    "sample_texts = [\n",
    "    \"Strong earnings growth exceeded expectations with profit up 25% and positive outlook for next quarter.\",\n",
    "    \"The company faces significant risk and uncertainty due to declining sales and potential litigation.\",\n",
    "    \"We believe revenue may increase but there could be some difficulty in achieving targets.\"\n",
    "]\n",
    "\n",
    "results = analyzer.compare_approaches(sample_texts)\n",
    "print(\"Lexicon-Based Sentiment Analysis Results:\")\n",
    "print(results.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of sentiment comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Approach comparison\n",
    "approaches = ['Lexicon\\n(L-M)', 'ML\\n(Traditional)', 'Deep Learning\\n(FinBERT)']\n",
    "metrics = {\n",
    "    'Accuracy': [0.65, 0.78, 0.89],\n",
    "    'Interpretability': [0.95, 0.60, 0.30],\n",
    "    'Speed': [0.95, 0.80, 0.40],\n",
    "    'Context Understanding': [0.30, 0.55, 0.90]\n",
    "}\n",
    "\n",
    "x = np.arange(len(approaches))\n",
    "width = 0.2\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    axes[0].bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "axes[0].set_xlabel('Approach')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_title('Sentiment Analysis Approaches Comparison')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels(approaches)\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "\n",
    "# Use case decision tree\n",
    "use_cases = ['Real-time\\nTrading', 'Research\\nAnalysis', 'Risk\\nMonitoring', 'Regulatory\\nCompliance']\n",
    "recommended = {\n",
    "    'Lexicon': [0.3, 0.5, 0.8, 0.9],\n",
    "    'ML': [0.6, 0.7, 0.6, 0.5],\n",
    "    'FinBERT': [0.4, 0.9, 0.5, 0.3]\n",
    "}\n",
    "\n",
    "x2 = np.arange(len(use_cases))\n",
    "for i, (approach, scores) in enumerate(recommended.items()):\n",
    "    axes[1].bar(x2 + i*0.25, scores, 0.25, label=approach)\n",
    "\n",
    "axes[1].set_xlabel('Use Case')\n",
    "axes[1].set_ylabel('Recommendation Score')\n",
    "axes[1].set_title('Recommended Approach by Use Case')\n",
    "axes[1].set_xticks(x2 + 0.25)\n",
    "axes[1].set_xticklabels(use_cases)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9aebad",
   "metadata": {},
   "source": [
    "### Answer Framework:\n",
    "\n",
    "**Use Lexicon-Based (Loughran-McDonald) when:**\n",
    "- Need interpretability for compliance/audit\n",
    "- Low latency requirements\n",
    "- Limited labeled training data\n",
    "- Baseline sentiment tracking\n",
    "\n",
    "**Use ML-Based when:**\n",
    "- Have labeled training data\n",
    "- Need moderate accuracy improvement\n",
    "- Can accept some latency\n",
    "\n",
    "**Use Deep Learning (FinBERT) when:**\n",
    "- Maximum accuracy is critical\n",
    "- Context understanding matters (negations, sarcasm)\n",
    "- Batch processing acceptable\n",
    "- Research/analysis (not real-time trading)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d6b90",
   "metadata": {},
   "source": [
    "## Question 3: Word Embeddings for Finance\n",
    "\n",
    "### Interview Question:\n",
    "**\"Explain the difference between Word2Vec, GloVe, and contextual embeddings. How would you train domain-specific embeddings for financial text?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingComparison:\n",
    "    \"\"\"\n",
    "    Interview Answer: Word Embeddings Comparison\n",
    "    \n",
    "    Static Embeddings:\n",
    "    - Word2Vec: Predicts context (Skip-gram) or word (CBOW)\n",
    "    - GloVe: Global co-occurrence matrix factorization\n",
    "    - Both produce ONE vector per word regardless of context\n",
    "    \n",
    "    Contextual Embeddings:\n",
    "    - ELMo: Bidirectional LSTM\n",
    "    - BERT: Transformer-based, different vector per context\n",
    "    - \"Bank\" in \"river bank\" vs \"bank account\" → different vectors\n",
    "    \n",
    "    Financial Domain Adaptation:\n",
    "    1. Train Word2Vec on SEC filings corpus\n",
    "    2. Fine-tune BERT on financial text → FinBERT\n",
    "    3. Domain-specific vocabulary handling\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def demonstrate_word2vec_concept():\n",
    "        \"\"\"\n",
    "        Simplified Word2Vec training demonstration.\n",
    "        \n",
    "        Skip-gram: Given word, predict context\n",
    "        CBOW: Given context, predict word\n",
    "        \"\"\"\n",
    "        # Financial context windows\n",
    "        financial_corpus = [\n",
    "            \"stock price increased after earnings beat\",\n",
    "            \"revenue growth exceeded analyst expectations\",\n",
    "            \"market volatility increased during crisis\",\n",
    "            \"earnings beat drove stock rally\",\n",
    "            \"profit margin improved quarter over quarter\",\n",
    "            \"risk exposure reduced through hedging\",\n",
    "            \"dividend yield attracted income investors\",\n",
    "            \"acquisition synergies boosted earnings\"\n",
    "        ]\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = set()\n",
    "        for sentence in financial_corpus:\n",
    "            vocab.update(sentence.split())\n",
    "        \n",
    "        word_to_idx = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "        \n",
    "        # Create skip-gram training pairs (simplified)\n",
    "        window_size = 2\n",
    "        training_pairs = []\n",
    "        \n",
    "        for sentence in financial_corpus:\n",
    "            words = sentence.split()\n",
    "            for i, target in enumerate(words):\n",
    "                context_start = max(0, i - window_size)\n",
    "                context_end = min(len(words), i + window_size + 1)\n",
    "                \n",
    "                for j in range(context_start, context_end):\n",
    "                    if i != j:\n",
    "                        training_pairs.append((target, words[j]))\n",
    "        \n",
    "        return {\n",
    "            'vocab_size': len(vocab),\n",
    "            'training_pairs_sample': training_pairs[:10],\n",
    "            'total_pairs': len(training_pairs)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def embedding_properties():\n",
    "        \"\"\"Compare embedding properties.\"\"\"\n",
    "        comparison = pd.DataFrame({\n",
    "            'Property': [\n",
    "                'Training Objective',\n",
    "                'Context Handling',\n",
    "                'Polysemy',\n",
    "                'Training Data',\n",
    "                'Computation',\n",
    "                'Finance Use Case'\n",
    "            ],\n",
    "            'Word2Vec': [\n",
    "                'Local context prediction',\n",
    "                'Fixed window',\n",
    "                'Single vector per word',\n",
    "                'Can train on domain corpus',\n",
    "                'Fast training',\n",
    "                'Document similarity, clustering'\n",
    "            ],\n",
    "            'GloVe': [\n",
    "                'Global co-occurrence',\n",
    "                'Whole corpus statistics',\n",
    "                'Single vector per word',\n",
    "                'Pre-trained + fine-tune',\n",
    "                'Moderate',\n",
    "                'Analogy tasks, visualization'\n",
    "            ],\n",
    "            'BERT/FinBERT': [\n",
    "                'Masked LM + NSP',\n",
    "                'Full bidirectional',\n",
    "                'Different vector per context',\n",
    "                'Requires fine-tuning',\n",
    "                'Computationally expensive',\n",
    "                'Sentiment, classification, QA'\n",
    "            ]\n",
    "        })\n",
    "        return comparison\n",
    "\n",
    "# Demonstration\n",
    "comparison = EmbeddingComparison()\n",
    "\n",
    "print(\"Skip-gram Training Pairs Example:\")\n",
    "w2v_demo = comparison.demonstrate_word2vec_concept()\n",
    "print(f\"Vocabulary size: {w2v_demo['vocab_size']}\")\n",
    "print(f\"Total training pairs: {w2v_demo['total_pairs']}\")\n",
    "print(f\"\\nSample pairs (target, context):\")\n",
    "for pair in w2v_demo['training_pairs_sample'][:5]:\n",
    "    print(f\"  {pair}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nEmbedding Comparison:\")\n",
    "print(comparison.embedding_properties().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef066d42",
   "metadata": {},
   "source": [
    "### Answer Framework:\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Word2Vec | GloVe | BERT/FinBERT |\n",
    "|--------|----------|-------|-------------|\n",
    "| Type | Static | Static | Contextual |\n",
    "| Training | Predict context | Matrix factorization | Masked LM |\n",
    "| Polysemy | ❌ | ❌ | ✅ |\n",
    "| Speed | Fast | Moderate | Slow |\n",
    "\n",
    "**Domain Adaptation for Finance:**\n",
    "1. **Corpus Collection**: SEC filings, earnings calls, analyst reports\n",
    "2. **Pre-training**: Train Word2Vec/FastText on financial corpus\n",
    "3. **Fine-tuning**: Start from pre-trained BERT, continue training on financial text\n",
    "4. **Vocabulary Extension**: Add financial terms to tokenizer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85537d50",
   "metadata": {},
   "source": [
    "## Question 4: FinBERT and Transformer Models\n",
    "\n",
    "### Interview Question:\n",
    "**\"Explain how FinBERT works and when you would use it over simpler approaches. What are the trade-offs?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f57ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinBERTExplainer:\n",
    "    \"\"\"\n",
    "    Interview Answer: FinBERT for Financial NLP\n",
    "    \n",
    "    FinBERT Architecture:\n",
    "    1. Pre-trained BERT base (110M parameters)\n",
    "    2. Further pre-trained on financial corpus\n",
    "    3. Fine-tuned on financial sentiment (PhraseBank dataset)\n",
    "    \n",
    "    Key Advantages:\n",
    "    - Contextual understanding of financial language\n",
    "    - Handles negations and complex sentences\n",
    "    - Transfer learning from massive pre-training\n",
    "    \n",
    "    Trade-offs:\n",
    "    - Computational cost (GPU required)\n",
    "    - Latency (not suitable for HFT)\n",
    "    - Less interpretable than lexicon methods\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformer_architecture_overview():\n",
    "        \"\"\"Key components of transformer architecture.\"\"\"\n",
    "        components = {\n",
    "            'Input Embedding': {\n",
    "                'purpose': 'Convert tokens to vectors',\n",
    "                'includes': ['Token embeddings', 'Position embeddings', 'Segment embeddings']\n",
    "            },\n",
    "            'Self-Attention': {\n",
    "                'purpose': 'Weigh importance of each token to others',\n",
    "                'formula': 'Attention(Q,K,V) = softmax(QK^T/√d_k)V',\n",
    "                'key_insight': 'Captures long-range dependencies'\n",
    "            },\n",
    "            'Multi-Head Attention': {\n",
    "                'purpose': 'Multiple attention patterns simultaneously',\n",
    "                'heads': '12 in BERT-base',\n",
    "                'key_insight': 'Different heads learn different relationships'\n",
    "            },\n",
    "            'Feed-Forward': {\n",
    "                'purpose': 'Non-linear transformation per position',\n",
    "                'architecture': 'Two linear layers with GELU activation'\n",
    "            },\n",
    "            '[CLS] Token': {\n",
    "                'purpose': 'Aggregate representation for classification',\n",
    "                'usage': 'Extract this for sentiment classification'\n",
    "            }\n",
    "        }\n",
    "        return components\n",
    "    \n",
    "    @staticmethod\n",
    "    def finbert_vs_bert_differences():\n",
    "        \"\"\"Key differences between BERT and FinBERT.\"\"\"\n",
    "        differences = pd.DataFrame({\n",
    "            'Aspect': [\n",
    "                'Pre-training Corpus',\n",
    "                'Vocabulary',\n",
    "                'Fine-tuning Data',\n",
    "                'Output Classes',\n",
    "                'Domain Terms'\n",
    "            ],\n",
    "            'BERT': [\n",
    "                'Wikipedia + BookCorpus',\n",
    "                'General (30K tokens)',\n",
    "                'General sentiment',\n",
    "                'Positive/Negative',\n",
    "                'May not understand'\n",
    "            ],\n",
    "            'FinBERT': [\n",
    "                '+ Reuters, SEC filings',\n",
    "                'Extended with financial terms',\n",
    "                'Financial PhraseBank',\n",
    "                'Positive/Negative/Neutral',\n",
    "                'Properly represented'\n",
    "            ]\n",
    "        })\n",
    "        return differences\n",
    "    \n",
    "    @staticmethod\n",
    "    def when_to_use_finbert():\n",
    "        \"\"\"Decision framework for using FinBERT.\"\"\"\n",
    "        decision_matrix = {\n",
    "            'Use FinBERT': [\n",
    "                'Complex financial documents',\n",
    "                'Negation-heavy text',\n",
    "                'Research/analysis (batch)',\n",
    "                'Accuracy is paramount',\n",
    "                'GPU resources available'\n",
    "            ],\n",
    "            'Use Simpler Approach': [\n",
    "                'Real-time trading signals',\n",
    "                'Simple sentiment monitoring',\n",
    "                'Limited compute resources',\n",
    "                'Need interpretability',\n",
    "                'High volume, low latency'\n",
    "            ]\n",
    "        }\n",
    "        return decision_matrix\n",
    "\n",
    "explainer = FinBERTExplainer()\n",
    "\n",
    "print(\"Transformer Architecture Components:\")\n",
    "print(\"=\" * 60)\n",
    "for component, details in explainer.transformer_architecture_overview().items():\n",
    "    print(f\"\\n{component}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nBERT vs FinBERT:\")\n",
    "print(explainer.finbert_vs_bert_differences().to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nDecision Framework:\")\n",
    "for category, items in explainer.when_to_use_finbert().items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  • {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3800277",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Topic Modeling for Financial Documents\n",
    "\n",
    "### Interview Question:\n",
    "**\"How would you apply topic modeling to earnings calls? What insights can you extract and how would you validate the topics?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModelingFinance:\n",
    "    \"\"\"\n",
    "    Interview Answer: Topic Modeling for Financial Documents\n",
    "    \n",
    "    Applications:\n",
    "    1. Identify themes in earnings calls over time\n",
    "    2. Track management focus areas\n",
    "    3. Compare topic distributions across companies/sectors\n",
    "    4. Event detection (new topics emerging)\n",
    "    \n",
    "    Methods:\n",
    "    - LDA: Probabilistic, interpretable\n",
    "    - NMF: Non-negative, faster\n",
    "    - BERTopic: Neural, better coherence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_topics: int = 5):\n",
    "        self.n_topics = n_topics\n",
    "        \n",
    "    def demonstrate_lda_concept(self, documents: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Simplified LDA concept demonstration.\n",
    "        \n",
    "        LDA Assumptions:\n",
    "        1. Documents are mixtures of topics\n",
    "        2. Topics are distributions over words\n",
    "        3. Each word assigned to one topic\n",
    "        \"\"\"\n",
    "        # Build vocabulary\n",
    "        all_words = []\n",
    "        for doc in documents:\n",
    "            all_words.extend(doc.lower().split())\n",
    "        \n",
    "        word_freq = Counter(all_words)\n",
    "        \n",
    "        # Simulated topic distributions (what LDA would learn)\n",
    "        simulated_topics = {\n",
    "            'Topic 1 - Growth': ['revenue', 'growth', 'increase', 'expansion', 'market'],\n",
    "            'Topic 2 - Risk': ['risk', 'uncertainty', 'volatility', 'exposure', 'hedge'],\n",
    "            'Topic 3 - Operations': ['cost', 'efficiency', 'margin', 'operations', 'productivity'],\n",
    "            'Topic 4 - Strategy': ['acquisition', 'investment', 'strategic', 'partnership', 'innovation'],\n",
    "            'Topic 5 - Guidance': ['expect', 'outlook', 'forecast', 'guidance', 'target']\n",
    "        }\n",
    "        \n",
    "        # Document-topic distribution (simulated)\n",
    "        doc_topics = []\n",
    "        for doc in documents:\n",
    "            words = set(doc.lower().split())\n",
    "            topic_scores = {}\n",
    "            for topic_name, topic_words in simulated_topics.items():\n",
    "                overlap = len(words.intersection(topic_words))\n",
    "                topic_scores[topic_name] = overlap\n",
    "            doc_topics.append(topic_scores)\n",
    "        \n",
    "        return {\n",
    "            'topics': simulated_topics,\n",
    "            'doc_topics': doc_topics,\n",
    "            'vocab_size': len(word_freq)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def topic_validation_metrics():\n",
    "        \"\"\"\n",
    "        How to validate topic quality.\n",
    "        \n",
    "        1. Coherence Score: Do top words co-occur?\n",
    "        2. Perplexity: How well model predicts held-out docs?\n",
    "        3. Human Evaluation: Are topics interpretable?\n",
    "        4. Downstream Task: Does it improve prediction?\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'Coherence (C_v)': {\n",
    "                'range': '0 to 1 (higher better)',\n",
    "                'interpretation': 'Semantic similarity of top words',\n",
    "                'good_score': '> 0.5'\n",
    "            },\n",
    "            'Perplexity': {\n",
    "                'range': 'Lower is better',\n",
    "                'interpretation': 'Model fit on held-out data',\n",
    "                'warning': 'Can decrease even with bad topics'\n",
    "            },\n",
    "            'Topic Diversity': {\n",
    "                'range': '0 to 1',\n",
    "                'interpretation': 'Unique words across topics',\n",
    "                'good_score': '> 0.6'\n",
    "            }\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "# Demonstration\n",
    "topic_modeler = TopicModelingFinance()\n",
    "\n",
    "sample_earnings_excerpts = [\n",
    "    \"Our revenue growth exceeded expectations with strong market expansion in Asia.\",\n",
    "    \"We are managing risk exposure through hedging strategies amid market volatility.\",\n",
    "    \"Cost efficiency initiatives improved our operating margin significantly.\",\n",
    "    \"The strategic acquisition will drive growth and create synergies.\",\n",
    "    \"We expect continued growth and are raising our full-year guidance.\"\n",
    "]\n",
    "\n",
    "results = topic_modeler.demonstrate_lda_concept(sample_earnings_excerpts)\n",
    "\n",
    "print(\"Discovered Topics:\")\n",
    "print(\"=\" * 60)\n",
    "for topic_name, words in results['topics'].items():\n",
    "    print(f\"\\n{topic_name}:\")\n",
    "    print(f\"  Top words: {', '.join(words)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, details in topic_modeler.topic_validation_metrics().items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cda43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6: Named Entity Recognition for Finance\n",
    "\n",
    "### Interview Question:\n",
    "**\"How would you build a Named Entity Recognition system for financial documents? What entities are most valuable for trading signals?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d144f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNER:\n",
    "    \"\"\"\n",
    "    Interview Answer: Named Entity Recognition for Finance\n",
    "    \n",
    "    Financial Entity Types:\n",
    "    1. ORG: Companies, exchanges, regulators\n",
    "    2. MONEY: Dollar amounts, percentages\n",
    "    3. DATE: Earnings dates, fiscal periods\n",
    "    4. PERSON: Executives, analysts\n",
    "    5. TICKER: Stock symbols\n",
    "    6. METRIC: EPS, EBITDA, P/E\n",
    "    7. EVENT: M&A, IPO, bankruptcy\n",
    "    \n",
    "    Trading Signal Value:\n",
    "    - Link entities to time for event-driven trading\n",
    "    - Track executive changes\n",
    "    - Identify M&A targets\n",
    "    - Extract numerical guidance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Pattern-based entity recognition (simplified)\n",
    "        self.patterns = {\n",
    "            'TICKER': r'\\b[A-Z]{1,5}\\b(?=\\s|,|\\.|$)',\n",
    "            'MONEY': r'\\$[\\d,]+\\.?\\d*[BMK]?',\n",
    "            'PERCENTAGE': r'\\d+\\.?\\d*%',\n",
    "            'DATE': r'(?:Q[1-4]|FY)\\s?\\d{2,4}|\\d{1,2}/\\d{1,2}/\\d{2,4}',\n",
    "            'METRIC': r'\\b(?:EPS|EBITDA|P/E|ROE|ROA|CAGR)\\b'\n",
    "        }\n",
    "        \n",
    "        # Company name indicators\n",
    "        self.org_suffixes = ['Inc', 'Corp', 'LLC', 'Ltd', 'Co', 'Company', 'Group']\n",
    "        \n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract financial entities using patterns.\"\"\"\n",
    "        entities = {}\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                entities[entity_type] = matches\n",
    "        \n",
    "        # Extract company names\n",
    "        org_pattern = r'([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\s(?:' + '|'.join(self.org_suffixes) + r'))'\n",
    "        orgs = re.findall(org_pattern, text)\n",
    "        if orgs:\n",
    "            entities['ORGANIZATION'] = orgs\n",
    "            \n",
    "        return entities\n",
    "    \n",
    "    def entity_linking(self, entities: Dict[str, List[str]]) -> Dict:\n",
    "        \"\"\"\n",
    "        Link entities to knowledge base (simplified).\n",
    "        \n",
    "        In production:\n",
    "        - Link tickers to company database\n",
    "        - Normalize dates to timestamp\n",
    "        - Link executives to roles\n",
    "        \"\"\"\n",
    "        linked = {}\n",
    "        \n",
    "        # Example: normalize money amounts\n",
    "        if 'MONEY' in entities:\n",
    "            normalized = []\n",
    "            for amount in entities['MONEY']:\n",
    "                # Parse and normalize\n",
    "                value = amount.replace('$', '').replace(',', '')\n",
    "                if 'B' in value:\n",
    "                    normalized.append(float(value.replace('B', '')) * 1e9)\n",
    "                elif 'M' in value:\n",
    "                    normalized.append(float(value.replace('M', '')) * 1e6)\n",
    "                elif 'K' in value:\n",
    "                    normalized.append(float(value.replace('K', '')) * 1e3)\n",
    "                else:\n",
    "                    try:\n",
    "                        normalized.append(float(value))\n",
    "                    except:\n",
    "                        normalized.append(value)\n",
    "            linked['MONEY_NORMALIZED'] = normalized\n",
    "            \n",
    "        return linked\n",
    "\n",
    "# Demonstration\n",
    "ner = FinancialNER()\n",
    "\n",
    "sample_news = \"\"\"\n",
    "Apple Inc reported Q3 2024 EPS of $1.26, beating estimates by 5%. \n",
    "Revenue reached $85.8B, up 8% YoY. Microsoft Corp and GOOGL also \n",
    "reported strong results. The P/E ratio expanded to 28x as AAPL \n",
    "raised FY2025 guidance.\n",
    "\"\"\"\n",
    "\n",
    "entities = ner.extract_entities(sample_news)\n",
    "linked = ner.entity_linking(entities)\n",
    "\n",
    "print(\"Extracted Entities:\")\n",
    "print(\"=\" * 60)\n",
    "for entity_type, values in entities.items():\n",
    "    print(f\"\\n{entity_type}:\")\n",
    "    for value in values:\n",
    "        print(f\"  • {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nLinked/Normalized:\")\n",
    "print(linked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5d5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7: Event-Driven Trading with NLP\n",
    "\n",
    "### Interview Question:\n",
    "**\"Design an event-driven trading system using NLP. How would you handle news latency and signal decay?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2438170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventDrivenNLPSystem:\n",
    "    \"\"\"\n",
    "    Interview Answer: Event-Driven Trading with NLP\n",
    "    \n",
    "    System Components:\n",
    "    1. News Ingestion: Real-time feeds (Reuters, Bloomberg)\n",
    "    2. Entity Extraction: Identify affected securities\n",
    "    3. Event Classification: Earnings, M&A, regulatory, etc.\n",
    "    4. Sentiment/Impact Scoring: Predict price direction\n",
    "    5. Signal Generation: Trade decision\n",
    "    6. Execution: Speed is critical\n",
    "    \n",
    "    Latency Handling:\n",
    "    - Use fast models (not FinBERT for trading)\n",
    "    - Pre-compute entity mappings\n",
    "    - Cache model predictions\n",
    "    \n",
    "    Signal Decay:\n",
    "    - Signals have half-life (minutes to hours)\n",
    "    - Early movers capture most alpha\n",
    "    - Model decay function and adjust sizing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.event_categories = [\n",
    "            'EARNINGS', 'M&A', 'GUIDANCE', 'EXECUTIVE',\n",
    "            'REGULATORY', 'PRODUCT', 'LEGAL', 'MACRO'\n",
    "        ]\n",
    "        \n",
    "    def classify_event(self, headline: str) -> Tuple[str, float]:\n",
    "        \"\"\"Classify news event type (simplified).\"\"\"\n",
    "        headline_lower = headline.lower()\n",
    "        \n",
    "        event_keywords = {\n",
    "            'EARNINGS': ['earnings', 'eps', 'revenue', 'profit', 'quarter'],\n",
    "            'M&A': ['acquire', 'merger', 'buyout', 'takeover', 'bid'],\n",
    "            'GUIDANCE': ['guidance', 'outlook', 'forecast', 'expect', 'raise'],\n",
    "            'EXECUTIVE': ['ceo', 'cfo', 'resign', 'appoint', 'executive'],\n",
    "            'REGULATORY': ['sec', 'fda', 'approved', 'regulatory', 'fine'],\n",
    "            'PRODUCT': ['launch', 'product', 'release', 'innovation'],\n",
    "            'LEGAL': ['lawsuit', 'settlement', 'litigation', 'court'],\n",
    "            'MACRO': ['fed', 'rates', 'inflation', 'gdp', 'unemployment']\n",
    "        }\n",
    "        \n",
    "        scores = {}\n",
    "        for event_type, keywords in event_keywords.items():\n",
    "            score = sum(1 for kw in keywords if kw in headline_lower)\n",
    "            scores[event_type] = score\n",
    "        \n",
    "        best_event = max(scores, key=scores.get)\n",
    "        confidence = scores[best_event] / 5.0  # Normalize\n",
    "        \n",
    "        return best_event, min(confidence, 1.0)\n",
    "    \n",
    "    def calculate_signal_decay(self, \n",
    "                               time_since_event: float,\n",
    "                               event_type: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate signal strength decay over time.\n",
    "        \n",
    "        Different events have different decay rates:\n",
    "        - Earnings: Fast decay (minutes)\n",
    "        - M&A: Slow decay (hours/days)\n",
    "        - Macro: Medium decay (hours)\n",
    "        \"\"\"\n",
    "        # Half-life in minutes\n",
    "        half_lives = {\n",
    "            'EARNINGS': 10,\n",
    "            'M&A': 120,\n",
    "            'GUIDANCE': 30,\n",
    "            'EXECUTIVE': 60,\n",
    "            'REGULATORY': 45,\n",
    "            'PRODUCT': 60,\n",
    "            'LEGAL': 90,\n",
    "            'MACRO': 60\n",
    "        }\n",
    "        \n",
    "        half_life = half_lives.get(event_type, 60)\n",
    "        \n",
    "        # Exponential decay\n",
    "        decay_factor = np.exp(-0.693 * time_since_event / half_life)\n",
    "        \n",
    "        return decay_factor\n",
    "    \n",
    "    def generate_signal(self,\n",
    "                        headline: str,\n",
    "                        sentiment_score: float,\n",
    "                        time_since_event: float) -> Dict:\n",
    "        \"\"\"Generate trading signal from news event.\"\"\"\n",
    "        # Classify event\n",
    "        event_type, event_confidence = self.classify_event(headline)\n",
    "        \n",
    "        # Calculate decay\n",
    "        decay = self.calculate_signal_decay(time_since_event, event_type)\n",
    "        \n",
    "        # Adjusted signal strength\n",
    "        raw_signal = sentiment_score * event_confidence\n",
    "        adjusted_signal = raw_signal * decay\n",
    "        \n",
    "        # Position sizing based on signal strength\n",
    "        if abs(adjusted_signal) > 0.5:\n",
    "            position_size = 'FULL'\n",
    "        elif abs(adjusted_signal) > 0.3:\n",
    "            position_size = 'HALF'\n",
    "        elif abs(adjusted_signal) > 0.1:\n",
    "            position_size = 'QUARTER'\n",
    "        else:\n",
    "            position_size = 'NONE'\n",
    "        \n",
    "        return {\n",
    "            'event_type': event_type,\n",
    "            'event_confidence': event_confidence,\n",
    "            'raw_signal': raw_signal,\n",
    "            'decay_factor': decay,\n",
    "            'adjusted_signal': adjusted_signal,\n",
    "            'direction': 'LONG' if adjusted_signal > 0 else 'SHORT',\n",
    "            'position_size': position_size\n",
    "        }\n",
    "\n",
    "# Demonstration\n",
    "system = EventDrivenNLPSystem()\n",
    "\n",
    "test_headlines = [\n",
    "    (\"Apple beats earnings expectations, raises guidance for Q4\", 0.8, 2),\n",
    "    (\"Microsoft to acquire gaming company in $10B deal\", 0.6, 30),\n",
    "    (\"Fed signals potential rate cut amid slowing inflation\", 0.4, 15),\n",
    "    (\"CEO resigns amid accounting investigation\", -0.9, 5)\n",
    "]\n",
    "\n",
    "print(\"Event-Driven Signal Generation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for headline, sentiment, time_mins in test_headlines:\n",
    "    signal = system.generate_signal(headline, sentiment, time_mins)\n",
    "    print(f\"\\nHeadline: {headline[:50]}...\")\n",
    "    print(f\"  Event Type: {signal['event_type']}\")\n",
    "    print(f\"  Raw Signal: {signal['raw_signal']:.3f}\")\n",
    "    print(f\"  Decay ({time_mins} min): {signal['decay_factor']:.3f}\")\n",
    "    print(f\"  Adjusted Signal: {signal['adjusted_signal']:.3f}\")\n",
    "    print(f\"  Action: {signal['direction']} - {signal['position_size']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b837d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signal decay\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Signal decay curves\n",
    "time_range = np.linspace(0, 180, 100)  # 0 to 180 minutes\n",
    "event_types = ['EARNINGS', 'M&A', 'GUIDANCE', 'MACRO']\n",
    "\n",
    "for event_type in event_types:\n",
    "    decay_values = [system.calculate_signal_decay(t, event_type) for t in time_range]\n",
    "    axes[0].plot(time_range, decay_values, label=event_type, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Time Since Event (minutes)')\n",
    "axes[0].set_ylabel('Signal Strength')\n",
    "axes[0].set_title('Signal Decay by Event Type')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='50% decay')\n",
    "\n",
    "# Alpha capture timeline\n",
    "alpha_timeline = {\n",
    "    'HFT (< 1s)': 40,\n",
    "    'Fast Quant (1-60s)': 30,\n",
    "    'Quant (1-5min)': 15,\n",
    "    'Discretionary (5-60min)': 10,\n",
    "    'Slow (> 1hr)': 5\n",
    "}\n",
    "\n",
    "categories = list(alpha_timeline.keys())\n",
    "values = list(alpha_timeline.values())\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.8, 0.2, len(categories)))\n",
    "\n",
    "axes[1].barh(categories, values, color=colors)\n",
    "axes[1].set_xlabel('% of Total Alpha Captured')\n",
    "axes[1].set_title('Alpha Capture by Speed (News Events)')\n",
    "\n",
    "for i, v in enumerate(values):\n",
    "    axes[1].text(v + 1, i, f'{v}%', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe29925",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 8: Alternative Data Sources\n",
    "\n",
    "### Interview Question:\n",
    "**\"What alternative data sources have you worked with? How do you evaluate the alpha potential and decay of alternative data?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4baf6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlternativeDataEvaluator:\n",
    "    \"\"\"\n",
    "    Interview Answer: Alternative Data Evaluation Framework\n",
    "    \n",
    "    Alternative Data Categories:\n",
    "    1. Sentiment: News, social media, reviews\n",
    "    2. Geolocation: Foot traffic, satellite imagery\n",
    "    3. Transaction: Credit card, web traffic\n",
    "    4. Expert: Analyst estimates, insider trades\n",
    "    5. Government: Patents, regulatory filings\n",
    "    \n",
    "    Evaluation Criteria:\n",
    "    1. Coverage: What securities/sectors?\n",
    "    2. History: How far back?\n",
    "    3. Frequency: Real-time to monthly\n",
    "    4. Exclusivity: How many users?\n",
    "    5. Alpha: Predictive power\n",
    "    6. Decay: How fast does signal degrade?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_sources = {\n",
    "            'Social Media Sentiment': {\n",
    "                'coverage': 'Broad (public companies)',\n",
    "                'frequency': 'Real-time',\n",
    "                'exclusivity': 'Low (widely available)',\n",
    "                'alpha_potential': 0.6,\n",
    "                'decay_months': 6,\n",
    "                'cost': 'Medium',\n",
    "                'effort': 'High (NLP pipeline)'\n",
    "            },\n",
    "            'Satellite Imagery': {\n",
    "                'coverage': 'Limited (retail, oil, agriculture)',\n",
    "                'frequency': 'Daily to weekly',\n",
    "                'exclusivity': 'High',\n",
    "                'alpha_potential': 0.8,\n",
    "                'decay_months': 18,\n",
    "                'cost': 'Very High',\n",
    "                'effort': 'Very High (CV pipeline)'\n",
    "            },\n",
    "            'Credit Card Data': {\n",
    "                'coverage': 'Consumer companies',\n",
    "                'frequency': 'Daily',\n",
    "                'exclusivity': 'Medium',\n",
    "                'alpha_potential': 0.85,\n",
    "                'decay_months': 12,\n",
    "                'cost': 'High',\n",
    "                'effort': 'Medium'\n",
    "            },\n",
    "            'Web Scraping': {\n",
    "                'coverage': 'Depends on target',\n",
    "                'frequency': 'Real-time possible',\n",
    "                'exclusivity': 'Can be high',\n",
    "                'alpha_potential': 0.7,\n",
    "                'decay_months': 3,\n",
    "                'cost': 'Low',\n",
    "                'effort': 'Medium-High'\n",
    "            },\n",
    "            'SEC Filings NLP': {\n",
    "                'coverage': 'All US public companies',\n",
    "                'frequency': 'Quarterly/Event',\n",
    "                'exclusivity': 'Low (public data)',\n",
    "                'alpha_potential': 0.65,\n",
    "                'decay_months': 24,\n",
    "                'cost': 'Low',\n",
    "                'effort': 'Medium'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def evaluate_roi(self, data_source: str) -> Dict:\n",
    "        \"\"\"Calculate estimated ROI for data source.\"\"\"\n",
    "        if data_source not in self.data_sources:\n",
    "            return None\n",
    "        \n",
    "        source = self.data_sources[data_source]\n",
    "        \n",
    "        # Cost mapping\n",
    "        cost_map = {'Low': 50000, 'Medium': 200000, 'High': 500000, 'Very High': 1000000}\n",
    "        effort_map = {'Low': 1, 'Medium': 2, 'Medium-High': 3, 'High': 4, 'Very High': 5}\n",
    "        \n",
    "        # Simplified ROI calculation\n",
    "        alpha_value = source['alpha_potential'] * 1000000  # Assume $1M alpha at 100%\n",
    "        cost = cost_map.get(source['cost'], 200000)\n",
    "        decay_factor = source['decay_months'] / 24  # Normalize to 2 years\n",
    "        effort = effort_map.get(source['effort'], 3)\n",
    "        \n",
    "        lifetime_value = alpha_value * decay_factor\n",
    "        roi = (lifetime_value - cost) / cost\n",
    "        time_to_value = effort * 3  # months\n",
    "        \n",
    "        return {\n",
    "            'estimated_annual_alpha': alpha_value,\n",
    "            'setup_cost': cost,\n",
    "            'decay_adjusted_value': lifetime_value,\n",
    "            'roi': roi,\n",
    "            'time_to_production_months': time_to_value\n",
    "        }\n",
    "    \n",
    "    def comparison_matrix(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate comparison matrix of all sources.\"\"\"\n",
    "        data = []\n",
    "        for name, attrs in self.data_sources.items():\n",
    "            row = {'Source': name}\n",
    "            row.update(attrs)\n",
    "            roi = self.evaluate_roi(name)\n",
    "            row['roi'] = roi['roi']\n",
    "            data.append(row)\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "# Demonstration\n",
    "evaluator = AlternativeDataEvaluator()\n",
    "\n",
    "print(\"Alternative Data Source Evaluation:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison = evaluator.comparison_matrix()\n",
    "print(comparison[['Source', 'alpha_potential', 'decay_months', 'cost', 'roi']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nDetailed ROI Analysis - Credit Card Data:\")\n",
    "roi_analysis = evaluator.evaluate_roi('Credit Card Data')\n",
    "for key, value in roi_analysis.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:,.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b45f42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 9: NLP Feature Engineering for Alpha\n",
    "\n",
    "### Interview Question:\n",
    "**\"Walk me through your process of converting raw text data into features for a predictive model. What features have you found most predictive?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPFeatureEngineering:\n",
    "    \"\"\"\n",
    "    Interview Answer: Text-to-Feature Pipeline for Alpha\n",
    "    \n",
    "    Feature Categories:\n",
    "    1. Sentiment Features: Score, magnitude, uncertainty\n",
    "    2. Linguistic Features: Complexity, tone shift\n",
    "    3. Entity Features: Counts, types, relationships\n",
    "    4. Temporal Features: Change vs history\n",
    "    5. Cross-sectional: vs peers, vs market\n",
    "    \n",
    "    Most Predictive (in my experience):\n",
    "    1. Sentiment CHANGE (not level)\n",
    "    2. Uncertainty increase\n",
    "    3. Executive tone shift\n",
    "    4. Unusual word frequency\n",
    "    5. Q&A session sentiment divergence\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract_features(self, text: str, historical_texts: List[str] = None) -> Dict:\n",
    "        \"\"\"Extract comprehensive NLP features.\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Basic text statistics\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        features['word_count'] = len(words)\n",
    "        features['avg_word_length'] = np.mean([len(w) for w in words])\n",
    "        features['sentence_count'] = len(sentences)\n",
    "        features['avg_sentence_length'] = len(words) / max(len(sentences), 1)\n",
    "        \n",
    "        # Readability (simplified Flesch-Kincaid)\n",
    "        syllables = sum(self._count_syllables(w) for w in words)\n",
    "        features['readability'] = 206.835 - 1.015 * (len(words) / max(len(sentences), 1)) - 84.6 * (syllables / max(len(words), 1))\n",
    "        \n",
    "        # Sentiment features\n",
    "        positive_words = ['growth', 'increase', 'profit', 'success', 'strong', 'exceeded', 'positive']\n",
    "        negative_words = ['decline', 'decrease', 'loss', 'risk', 'weak', 'missed', 'negative']\n",
    "        uncertain_words = ['may', 'might', 'could', 'expect', 'believe', 'estimate', 'uncertain']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        features['positive_ratio'] = sum(1 for w in positive_words if w in text_lower) / len(words)\n",
    "        features['negative_ratio'] = sum(1 for w in negative_words if w in text_lower) / len(words)\n",
    "        features['uncertainty_ratio'] = sum(1 for w in uncertain_words if w in text_lower) / len(words)\n",
    "        features['net_sentiment'] = features['positive_ratio'] - features['negative_ratio']\n",
    "        \n",
    "        # Numerical mentions (often predictive)\n",
    "        features['number_count'] = len(re.findall(r'\\d+\\.?\\d*%?', text))\n",
    "        features['dollar_count'] = len(re.findall(r'\\$[\\d,]+', text))\n",
    "        \n",
    "        # Forward-looking language (critical for guidance)\n",
    "        forward_words = ['expect', 'anticipate', 'outlook', 'guidance', 'forecast', 'project']\n",
    "        features['forward_looking_ratio'] = sum(1 for w in forward_words if w in text_lower) / len(words)\n",
    "        \n",
    "        # Calculate changes if historical data available\n",
    "        if historical_texts:\n",
    "            historical_features = self.extract_features(historical_texts[-1])\n",
    "            features['sentiment_change'] = features['net_sentiment'] - historical_features.get('net_sentiment', 0)\n",
    "            features['uncertainty_change'] = features['uncertainty_ratio'] - historical_features.get('uncertainty_ratio', 0)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _count_syllables(self, word: str) -> int:\n",
    "        \"\"\"Simple syllable counter.\"\"\"\n",
    "        vowels = 'aeiou'\n",
    "        count = 0\n",
    "        prev_is_vowel = False\n",
    "        for char in word.lower():\n",
    "            is_vowel = char in vowels\n",
    "            if is_vowel and not prev_is_vowel:\n",
    "                count += 1\n",
    "            prev_is_vowel = is_vowel\n",
    "        return max(count, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def feature_importance_analysis():\n",
    "        \"\"\"Typical feature importance from earnings call prediction.\"\"\"\n",
    "        importance = {\n",
    "            'sentiment_change': 0.18,\n",
    "            'uncertainty_change': 0.15,\n",
    "            'forward_looking_ratio': 0.12,\n",
    "            'net_sentiment': 0.10,\n",
    "            'number_count': 0.09,\n",
    "            'readability_change': 0.08,\n",
    "            'negative_ratio': 0.07,\n",
    "            'positive_ratio': 0.06,\n",
    "            'word_count_change': 0.05,\n",
    "            'dollar_mentions': 0.05,\n",
    "            'other': 0.05\n",
    "        }\n",
    "        return importance\n",
    "\n",
    "# Demonstration\n",
    "fe = NLPFeatureEngineering()\n",
    "\n",
    "sample_earnings = \"\"\"\n",
    "We are pleased to report strong quarterly results with revenue growth of 15% year-over-year.\n",
    "Our operating margin improved to 22%, exceeding expectations. Looking ahead, we expect \n",
    "continued momentum and are raising our full-year guidance. However, we remain cautious about\n",
    "potential macroeconomic headwinds and may adjust our investment timeline if conditions change.\n",
    "\"\"\"\n",
    "\n",
    "features = fe.extract_features(sample_earnings)\n",
    "\n",
    "print(\"Extracted NLP Features:\")\n",
    "print(\"=\" * 60)\n",
    "for feature, value in features.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {feature}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {feature}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nTypical Feature Importance (Earnings Prediction):\")\n",
    "importance = fe.feature_importance_analysis()\n",
    "for feat, imp in sorted(importance.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feat}: {imp:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b53c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 10: Production NLP Systems\n",
    "\n",
    "### Interview Question:\n",
    "**\"How would you deploy an NLP-based trading signal in production? What are the key monitoring and validation considerations?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9413ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionNLPSystem:\n",
    "    \"\"\"\n",
    "    Interview Answer: Production NLP System Architecture\n",
    "    \n",
    "    Key Components:\n",
    "    1. Data Ingestion: Real-time feeds, deduplication\n",
    "    2. Processing Pipeline: Async, fault-tolerant\n",
    "    3. Model Serving: Low latency, A/B testing\n",
    "    4. Signal Generation: Aggregation, filtering\n",
    "    5. Monitoring: Drift detection, quality metrics\n",
    "    6. Alerting: Anomaly detection, circuit breakers\n",
    "    \n",
    "    Critical Considerations:\n",
    "    - Latency budget by use case\n",
    "    - Model staleness and retraining\n",
    "    - Data quality monitoring\n",
    "    - Position risk from NLP errors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.latency_budget_ms = 100  # Target latency\n",
    "        self.model_version = '1.0'\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def system_architecture(self) -> Dict:\n",
    "        \"\"\"Production system architecture.\"\"\"\n",
    "        architecture = {\n",
    "            'Data Layer': {\n",
    "                'News Feeds': ['Reuters', 'Bloomberg', 'SEC EDGAR'],\n",
    "                'Message Queue': 'Kafka',\n",
    "                'Deduplication': 'Redis',\n",
    "                'Storage': 'TimescaleDB'\n",
    "            },\n",
    "            'Processing Layer': {\n",
    "                'Orchestration': 'Kubernetes',\n",
    "                'NLP Service': 'FastAPI + async',\n",
    "                'Model Serving': 'TorchServe / TF Serving',\n",
    "                'Caching': 'Redis (entity lookups)'\n",
    "            },\n",
    "            'Signal Layer': {\n",
    "                'Aggregation': 'Time-weighted sentiment',\n",
    "                'Filtering': 'Confidence threshold',\n",
    "                'Output': 'Signal API / PMS integration'\n",
    "            },\n",
    "            'Monitoring Layer': {\n",
    "                'Metrics': 'Prometheus + Grafana',\n",
    "                'Logging': 'ELK Stack',\n",
    "                'Alerting': 'PagerDuty',\n",
    "                'Drift Detection': 'Custom ML monitors'\n",
    "            }\n",
    "        }\n",
    "        return architecture\n",
    "    \n",
    "    def monitoring_metrics(self) -> Dict:\n",
    "        \"\"\"Key metrics to monitor in production.\"\"\"\n",
    "        metrics = {\n",
    "            'Operational': {\n",
    "                'latency_p50': 'Target < 50ms',\n",
    "                'latency_p99': 'Target < 200ms',\n",
    "                'throughput': 'Messages/second',\n",
    "                'error_rate': 'Target < 0.1%',\n",
    "                'queue_depth': 'Backlog indicator'\n",
    "            },\n",
    "            'Model Quality': {\n",
    "                'sentiment_distribution': 'Detect drift from baseline',\n",
    "                'confidence_distribution': 'Monitor uncertainty',\n",
    "                'entity_coverage': '% tickers matched',\n",
    "                'prediction_vs_actual': 'Rolling accuracy'\n",
    "            },\n",
    "            'Business': {\n",
    "                'signal_count': 'Daily signals generated',\n",
    "                'signal_hit_rate': 'Correct direction %',\n",
    "                'pnl_attribution': 'P&L from NLP signals',\n",
    "                'decay_rate': 'Signal degradation over time'\n",
    "            }\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "    def drift_detection(self, current_dist: Dict, baseline_dist: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Detect distribution drift in sentiment scores.\n",
    "        \n",
    "        Methods:\n",
    "        - KL Divergence\n",
    "        - Population Stability Index (PSI)\n",
    "        - Kolmogorov-Smirnov test\n",
    "        \"\"\"\n",
    "        # Simplified PSI calculation\n",
    "        psi = 0\n",
    "        for bucket in current_dist:\n",
    "            if bucket in baseline_dist:\n",
    "                curr = current_dist[bucket]\n",
    "                base = baseline_dist[bucket]\n",
    "                if curr > 0 and base > 0:\n",
    "                    psi += (curr - base) * np.log(curr / base)\n",
    "        \n",
    "        # PSI interpretation\n",
    "        if psi < 0.1:\n",
    "            status = 'STABLE'\n",
    "        elif psi < 0.25:\n",
    "            status = 'WARNING'\n",
    "        else:\n",
    "            status = 'CRITICAL'\n",
    "        \n",
    "        return {\n",
    "            'psi': psi,\n",
    "            'status': status,\n",
    "            'action': 'Investigate model' if status != 'STABLE' else 'None'\n",
    "        }\n",
    "    \n",
    "    def failsafe_rules(self) -> List[Dict]:\n",
    "        \"\"\"Circuit breaker rules for production safety.\"\"\"\n",
    "        rules = [\n",
    "            {\n",
    "                'rule': 'Latency Circuit Breaker',\n",
    "                'trigger': 'p99 > 500ms for 5 minutes',\n",
    "                'action': 'Switch to fallback model'\n",
    "            },\n",
    "            {\n",
    "                'rule': 'Error Rate Circuit Breaker',\n",
    "                'trigger': 'Error rate > 5%',\n",
    "                'action': 'Halt signal generation'\n",
    "            },\n",
    "            {\n",
    "                'rule': 'Drift Alert',\n",
    "                'trigger': 'PSI > 0.25',\n",
    "                'action': 'Page on-call, reduce position sizes'\n",
    "            },\n",
    "            {\n",
    "                'rule': 'Anomaly Detection',\n",
    "                'trigger': 'Sentiment outside 3-sigma',\n",
    "                'action': 'Manual review before trading'\n",
    "            },\n",
    "            {\n",
    "                'rule': 'Data Quality',\n",
    "                'trigger': 'Missing feeds > 10 minutes',\n",
    "                'action': 'Fallback to stale data, alert'\n",
    "            }\n",
    "        ]\n",
    "        return rules\n",
    "\n",
    "# Demonstration\n",
    "production = ProductionNLPSystem()\n",
    "\n",
    "print(\"Production NLP System Architecture:\")\n",
    "print(\"=\" * 70)\n",
    "for layer, components in production.system_architecture().items():\n",
    "    print(f\"\\n{layer}:\")\n",
    "    for component, value in components.items():\n",
    "        print(f\"  {component}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nMonitoring Metrics:\")\n",
    "for category, metrics in production.monitoring_metrics().items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric, target in metrics.items():\n",
    "        print(f\"  {metric}: {target}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nFailsafe Rules:\")\n",
    "for rule in production.failsafe_rules():\n",
    "    print(f\"\\n  {rule['rule']}:\")\n",
    "    print(f\"    Trigger: {rule['trigger']}\")\n",
    "    print(f\"    Action: {rule['action']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fe780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production dashboard visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Latency distribution\n",
    "np.random.seed(42)\n",
    "latencies = np.concatenate([\n",
    "    np.random.exponential(20, 900),  # Normal\n",
    "    np.random.exponential(100, 100)  # Some slow ones\n",
    "])\n",
    "\n",
    "axes[0, 0].hist(latencies, bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "axes[0, 0].axvline(np.percentile(latencies, 50), color='green', linestyle='--', label=f'p50: {np.percentile(latencies, 50):.0f}ms')\n",
    "axes[0, 0].axvline(np.percentile(latencies, 99), color='red', linestyle='--', label=f'p99: {np.percentile(latencies, 99):.0f}ms')\n",
    "axes[0, 0].set_xlabel('Latency (ms)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('NLP Service Latency Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Sentiment drift over time\n",
    "days = np.arange(30)\n",
    "baseline_sentiment = 0.05\n",
    "sentiment_drift = baseline_sentiment + 0.02 * np.sin(days/10) + np.random.normal(0, 0.01, 30)\n",
    "sentiment_drift[20:] += 0.03  # Sudden drift\n",
    "\n",
    "axes[0, 1].plot(days, sentiment_drift, 'b-', linewidth=2, label='Observed')\n",
    "axes[0, 1].axhline(baseline_sentiment, color='green', linestyle='--', label='Baseline')\n",
    "axes[0, 1].axhline(baseline_sentiment + 0.05, color='orange', linestyle=':', label='Warning')\n",
    "axes[0, 1].axhline(baseline_sentiment + 0.08, color='red', linestyle=':', label='Critical')\n",
    "axes[0, 1].fill_between(days[20:], baseline_sentiment - 0.1, sentiment_drift[20:], alpha=0.3, color='red')\n",
    "axes[0, 1].set_xlabel('Day')\n",
    "axes[0, 1].set_ylabel('Average Sentiment')\n",
    "axes[0, 1].set_title('Sentiment Drift Monitoring')\n",
    "axes[0, 1].legend(loc='upper left')\n",
    "axes[0, 1].annotate('Drift Detected!', xy=(22, 0.1), fontsize=10, color='red')\n",
    "\n",
    "# Signal accuracy over time\n",
    "accuracy = 0.6 + 0.05 * np.random.randn(30).cumsum() / 10\n",
    "accuracy = np.clip(accuracy, 0.45, 0.75)\n",
    "\n",
    "axes[1, 0].plot(days, accuracy, 'g-', linewidth=2)\n",
    "axes[1, 0].axhline(0.5, color='red', linestyle='--', label='Random (50%)')\n",
    "axes[1, 0].fill_between(days, 0.5, accuracy, where=accuracy > 0.5, alpha=0.3, color='green')\n",
    "axes[1, 0].fill_between(days, 0.5, accuracy, where=accuracy <= 0.5, alpha=0.3, color='red')\n",
    "axes[1, 0].set_xlabel('Day')\n",
    "axes[1, 0].set_ylabel('Signal Accuracy')\n",
    "axes[1, 0].set_title('Rolling Signal Accuracy (7-day)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylim(0.4, 0.8)\n",
    "\n",
    "# System health dashboard\n",
    "health_metrics = {\n",
    "    'News Feed': 0.99,\n",
    "    'NLP Service': 0.98,\n",
    "    'Model Server': 0.97,\n",
    "    'Signal API': 0.99,\n",
    "    'Database': 0.999\n",
    "}\n",
    "\n",
    "names = list(health_metrics.keys())\n",
    "values = list(health_metrics.values())\n",
    "colors = ['green' if v > 0.99 else 'orange' if v > 0.95 else 'red' for v in values]\n",
    "\n",
    "bars = axes[1, 1].barh(names, values, color=colors)\n",
    "axes[1, 1].set_xlim(0.9, 1.01)\n",
    "axes[1, 1].set_xlabel('Uptime')\n",
    "axes[1, 1].set_title('System Health Dashboard')\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    axes[1, 1].text(val + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{val:.1%}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5c554",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Week 19 Summary & Key Takeaways\n",
    "\n",
    "### Topics Mastered:\n",
    "\n",
    "| Day | Topic | Key Concepts |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Text Preprocessing | Tokenization, financial vocab, negation handling |\n",
    "| 2 | Sentiment Analysis | Lexicon vs ML, Loughran-McDonald |\n",
    "| 3 | Word Embeddings | Word2Vec, GloVe, domain adaptation |\n",
    "| 4 | Transformers | BERT, FinBERT, fine-tuning |\n",
    "| 5 | Topic Modeling | LDA, NMF, coherence validation |\n",
    "| 6 | Alternative Data | Data sources, alpha evaluation |\n",
    "| 7 | Interview Review | Production systems, best practices |\n",
    "\n",
    "### Interview Readiness Checklist:\n",
    "\n",
    "✅ Can explain text preprocessing for financial documents  \n",
    "✅ Understand sentiment analysis approaches and trade-offs  \n",
    "✅ Know difference between static and contextual embeddings  \n",
    "✅ Can discuss FinBERT architecture and use cases  \n",
    "✅ Understand topic modeling validation  \n",
    "✅ Can evaluate alternative data sources  \n",
    "✅ Know NLP feature engineering for alpha  \n",
    "✅ Understand production NLP system design  \n",
    "\n",
    "### Common Interview Mistakes to Avoid:\n",
    "\n",
    "1. **Using FinBERT for real-time trading** - Too slow, use lexicon or simple ML\n",
    "2. **Ignoring negations** - \"Not good\" ≠ \"Good\" in finance\n",
    "3. **Using general sentiment lexicons** - Financial language is different\n",
    "4. **Overfitting to backtest** - Alternative data decays fast\n",
    "5. **Ignoring latency** - Speed matters for alpha capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7748a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "print(\"=\" * 70)\n",
    "print(\"WEEK 19: NLP & ALTERNATIVE DATA - COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "skills_matrix = {\n",
    "    'Text Preprocessing': ['Tokenization', 'Financial vocab', 'Negation handling'],\n",
    "    'Sentiment Analysis': ['Loughran-McDonald', 'ML approaches', 'FinBERT'],\n",
    "    'Feature Engineering': ['Sentiment scores', 'Topic features', 'Change metrics'],\n",
    "    'Production Systems': ['Latency optimization', 'Drift monitoring', 'Circuit breakers'],\n",
    "    'Alternative Data': ['Source evaluation', 'Alpha decay', 'ROI analysis']\n",
    "}\n",
    "\n",
    "print(\"\\n📚 Skills Acquired:\")\n",
    "for category, skills in skills_matrix.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for skill in skills:\n",
    "        print(f\"  ✓ {skill}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n🎯 Ready for NLP & Alternative Data Interview Questions!\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
