{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3676b1",
   "metadata": {},
   "source": [
    "# ðŸ” Data Quality Checks for Quantitative Finance\n",
    "\n",
    "## Point-in-Time Data Validation\n",
    "\n",
    "This notebook performs comprehensive data quality checks essential for quantitative research. Poor data quality is one of the **primary causes of backtest failures** in production.\n",
    "\n",
    "### Why Data Quality Matters\n",
    "\n",
    "> \"Garbage in, garbage out\" - In quant finance, this means:\n",
    "> - **P&L miscalculations** from bad prices\n",
    "> - **False alpha signals** from data errors\n",
    "> - **Look-ahead bias** from improperly timestamped data\n",
    "> - **Survivorship bias** from incomplete universes\n",
    "\n",
    "### Quality Checks Performed\n",
    "1. **Missing Values**: Identify gaps in time series\n",
    "2. **Date Continuity**: Check for unexpected trading day gaps\n",
    "3. **Returns Sanity**: Flag unrealistic price movements\n",
    "4. **Future Data Leakage**: Ensure point-in-time compliance\n",
    "5. **Outlier Detection**: Identify potential data errors\n",
    "\n",
    "---\n",
    "**Author**: ML Quant Finance Mastery  \n",
    "**Last Updated**: 2026-01-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa07c18",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e1e01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully\n",
      "   Shape: (1836, 63)\n",
      "   Date range: 2019-01-01 to 2026-01-19\n",
      "   Instruments: ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'META']...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = Path(\"./raw_data\")\n",
    "REFERENCE_DATE = datetime(2026, 1, 20)  # \"Today\" for point-in-time checks\n",
    "\n",
    "# Load combined dataset\n",
    "prices = pd.read_csv(DATA_DIR / \"combined_adjusted_close.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"âœ… Data loaded successfully\")\n",
    "print(f\"   Shape: {prices.shape}\")\n",
    "print(f\"   Date range: {prices.index.min().strftime('%Y-%m-%d')} to {prices.index.max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   Instruments: {prices.columns.tolist()[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd164d9d",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis\n",
    "\n",
    "Missing values can indicate:\n",
    "- Trading halts\n",
    "- Delistings\n",
    "- Data provider issues\n",
    "- Weekend/holiday gaps (expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff6822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š MISSING VALUES REPORT\n",
      "============================================================\n",
      "\n",
      "ðŸ”´ Instruments with >5% missing data:\n",
      "   None - all instruments have >95% data coverage!\n",
      "\n",
      "ðŸ“ˆ Overall data quality:\n",
      "   Average coverage: 96.68%\n",
      "   Minimum coverage: 96.46% (AAPL)\n",
      "   Maximum coverage: 99.89%\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze missing values in each column.\n",
    "    \n",
    "    Returns DataFrame with missing value statistics per instrument.\n",
    "    \"\"\"\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'total_rows': len(df),\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'first_valid': df.apply(lambda x: x.first_valid_index()),\n",
    "        'last_valid': df.apply(lambda x: x.last_valid_index()),\n",
    "    })\n",
    "    \n",
    "    # Calculate valid data range\n",
    "    missing_stats['data_coverage'] = (\n",
    "        (missing_stats['total_rows'] - missing_stats['missing_count']) / \n",
    "        missing_stats['total_rows'] * 100\n",
    "    ).round(2)\n",
    "    \n",
    "    return missing_stats.sort_values('missing_pct', ascending=False)\n",
    "\n",
    "# Run missing value check\n",
    "missing_report = check_missing_values(prices)\n",
    "\n",
    "print(\"ðŸ“Š MISSING VALUES REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ”´ Instruments with >5% missing data:\")\n",
    "high_missing = missing_report[missing_report['missing_pct'] > 5]\n",
    "if len(high_missing) > 0:\n",
    "    print(high_missing[['missing_count', 'missing_pct', 'data_coverage']].head(10))\n",
    "else:\n",
    "    print(\"   None - all instruments have >95% data coverage!\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Overall data quality:\")\n",
    "print(f\"   Average coverage: {missing_report['data_coverage'].mean():.2f}%\")\n",
    "print(f\"   Minimum coverage: {missing_report['data_coverage'].min():.2f}% ({missing_report['data_coverage'].idxmin()})\")\n",
    "print(f\"   Maximum coverage: {missing_report['data_coverage'].max():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db37ac",
   "metadata": {},
   "source": [
    "## 3. Returns Sanity Check\n",
    "\n",
    "Flag potentially erroneous data by checking for:\n",
    "- Returns exceeding Â±50% in a single day (highly unusual)\n",
    "- Zero returns for extended periods (data staleness)\n",
    "- Negative prices (data error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a61ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RETURNS SANITY CHECK\n",
      "============================================================\n",
      "\n",
      "ðŸ”´ Extreme returns (>Â±50% daily):\n",
      "        date ticker     return return_pct\n",
      "0 2020-03-18    IRX  -0.981818     -98.2%\n",
      "1 2020-03-19    IRX -10.333334   -1033.3%\n",
      "2 2020-03-25    IRX   1.121212     112.1%\n",
      "3 2020-03-30    IRX  -1.224138    -122.4%\n",
      "4 2020-03-31    IRX   1.307692     130.8%\n",
      "5 2020-04-01    IRX   1.100000     110.0%\n",
      "6 2020-04-07    IRX   1.133333     113.3%\n",
      "7 2021-02-09    IRX   0.650000      65.0%\n",
      "8 2021-03-18    IRX  -0.625000     -62.5%\n",
      "9 2021-03-19    IRX   0.666667      66.7%\n",
      "\n",
      "âŒ Negative prices found: 7\n",
      "âŒ Zero prices found: 0\n",
      "\n",
      "ðŸ“ˆ Returns statistics (sample):\n",
      "         mean     std     min     max\n",
      "AAPL   0.0012  0.0192 -0.1286  0.1533\n",
      "MSFT   0.0010  0.0175 -0.1474  0.1422\n",
      "GOOGL  0.0012  0.0194 -0.1163  0.1022\n",
      "NVDA   0.0027  0.0318 -0.1845  0.2437\n",
      "META   0.0012  0.0259 -0.2639  0.2328\n"
     ]
    }
   ],
   "source": [
    "def check_returns_sanity(df: pd.DataFrame, threshold: float = 0.50) -> dict:\n",
    "    \"\"\"\n",
    "    Check for unrealistic daily returns that may indicate data errors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Price data\n",
    "    threshold : float\n",
    "        Return threshold to flag (default 50%)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with flagged instances\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = df.pct_change()\n",
    "    \n",
    "    # Check for extreme returns\n",
    "    extreme_returns = []\n",
    "    for col in returns.columns:\n",
    "        extremes = returns[col][np.abs(returns[col]) > threshold].dropna()\n",
    "        for date, ret in extremes.items():\n",
    "            extreme_returns.append({\n",
    "                'date': date,\n",
    "                'ticker': col,\n",
    "                'return': ret,\n",
    "                'return_pct': f\"{ret*100:.1f}%\"\n",
    "            })\n",
    "    \n",
    "    # Check for negative prices\n",
    "    negative_prices = []\n",
    "    for col in df.columns:\n",
    "        negatives = df[col][df[col] < 0].dropna()\n",
    "        for date, price in negatives.items():\n",
    "            negative_prices.append({\n",
    "                'date': date,\n",
    "                'ticker': col,\n",
    "                'price': price\n",
    "            })\n",
    "    \n",
    "    # Check for zero prices (excluding NaN)\n",
    "    zero_prices = (df == 0).sum().sum()\n",
    "    \n",
    "    return {\n",
    "        'extreme_returns': pd.DataFrame(extreme_returns) if extreme_returns else None,\n",
    "        'negative_prices': pd.DataFrame(negative_prices) if negative_prices else None,\n",
    "        'zero_prices_count': zero_prices,\n",
    "        'returns_stats': returns.describe().T[['mean', 'std', 'min', 'max']]\n",
    "    }\n",
    "\n",
    "# Run returns sanity check\n",
    "sanity_results = check_returns_sanity(prices)\n",
    "\n",
    "print(\"ðŸ“Š RETURNS SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ”´ Extreme returns (>Â±50% daily):\")\n",
    "if sanity_results['extreme_returns'] is not None and len(sanity_results['extreme_returns']) > 0:\n",
    "    print(sanity_results['extreme_returns'].head(10))\n",
    "else:\n",
    "    print(\"   None found - returns are within normal bounds\")\n",
    "\n",
    "print(f\"\\nâŒ Negative prices found: {len(sanity_results['negative_prices']) if sanity_results['negative_prices'] is not None else 0}\")\n",
    "print(f\"âŒ Zero prices found: {sanity_results['zero_prices_count']}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Returns statistics (sample):\")\n",
    "print(sanity_results['returns_stats'].head(5).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa175d97",
   "metadata": {},
   "source": [
    "### âš ï¸ IRX (3-Month Treasury) Data Note\n",
    "\n",
    "The extreme returns in IRX are **expected** during the COVID-19 crisis (March 2020) when short-term rates briefly went negative. This is a known historical event, not a data error.\n",
    "\n",
    "**Action**: For equity analysis, consider excluding or carefully handling fixed income data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850690dd",
   "metadata": {},
   "source": [
    "## 4. Point-in-Time Data Validation\n",
    "\n",
    "**Critical for backtesting**: Ensure no future data leakage. We verify that:\n",
    "1. No data points exist beyond our reference date\n",
    "2. Data timestamps represent close-of-day (not intraday peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164ea68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š POINT-IN-TIME VALIDATION\n",
      "============================================================\n",
      "\n",
      "ðŸ“… Reference date: 2026-01-20\n",
      "ðŸ“… Latest data: 2026-01-19\n",
      "ðŸ“… Days from reference: -1\n",
      "\n",
      "âœ… PASS: No future data leakage detected\n"
     ]
    }
   ],
   "source": [
    "def check_point_in_time(df: pd.DataFrame, reference_date: datetime) -> dict:\n",
    "    \"\"\"\n",
    "    Validate point-in-time data integrity.\n",
    "    \n",
    "    Checks for future data leakage - a critical backtesting error.\n",
    "    \"\"\"\n",
    "    # Check for future dates\n",
    "    max_date = df.index.max()\n",
    "    future_data = df[df.index > reference_date]\n",
    "    \n",
    "    # Check if max date is reasonable (not in the future)\n",
    "    days_from_ref = (max_date - reference_date).days\n",
    "    \n",
    "    return {\n",
    "        'reference_date': reference_date,\n",
    "        'max_data_date': max_date,\n",
    "        'days_from_reference': days_from_ref,\n",
    "        'future_rows': len(future_data),\n",
    "        'is_point_in_time_valid': days_from_ref <= 0\n",
    "    }\n",
    "\n",
    "# Run point-in-time check\n",
    "pit_results = check_point_in_time(prices, REFERENCE_DATE)\n",
    "\n",
    "print(\"ðŸ“Š POINT-IN-TIME VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“… Reference date: {pit_results['reference_date'].strftime('%Y-%m-%d')}\")\n",
    "print(f\"ðŸ“… Latest data: {pit_results['max_data_date'].strftime('%Y-%m-%d')}\")\n",
    "print(f\"ðŸ“… Days from reference: {pit_results['days_from_reference']}\")\n",
    "\n",
    "if pit_results['is_point_in_time_valid']:\n",
    "    print(f\"\\nâœ… PASS: No future data leakage detected\")\n",
    "else:\n",
    "    print(f\"\\nâŒ FAIL: {pit_results['future_rows']} rows contain future data!\")\n",
    "    print(\"   This would cause look-ahead bias in backtests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786994ae",
   "metadata": {},
   "source": [
    "## 5. Trading Day Gap Analysis\n",
    "\n",
    "Check for unexpected gaps that might indicate missing data vs. legitimate market closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "985093a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š TRADING GAP ANALYSIS\n",
      "============================================================\n",
      "\n",
      "ðŸ“… Expected gaps: weekends (2-3 days), holidays (3-4 days)\n",
      "\n",
      "âœ… No unexpected gaps found\n",
      "\n",
      "ðŸ“ˆ Gap distribution:\n",
      "   1 day (normal trading): 1464 occurrences\n",
      "   2-3 days (weekends): 370 occurrences\n",
      "   4+ days (holidays): 1 occurrences\n"
     ]
    }
   ],
   "source": [
    "def analyze_trading_gaps(df: pd.DataFrame, max_expected_gap: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Identify gaps in trading data that exceed expected duration.\n",
    "    \n",
    "    Normal gaps: weekends (2 days), holidays (3-4 days with weekends)\n",
    "    Suspicious gaps: >4 days (may indicate data issues)\n",
    "    \"\"\"\n",
    "    # Calculate days between observations\n",
    "    date_diffs = pd.Series(df.index).diff().dt.days\n",
    "    \n",
    "    # Find suspicious gaps\n",
    "    gaps = []\n",
    "    for i, diff in enumerate(date_diffs):\n",
    "        if pd.notna(diff) and diff > max_expected_gap:\n",
    "            gaps.append({\n",
    "                'gap_start': df.index[i-1] if i > 0 else None,\n",
    "                'gap_end': df.index[i],\n",
    "                'days': int(diff),\n",
    "                'reason': 'Holiday period' if diff <= 7 else 'Investigate'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(gaps) if gaps else None\n",
    "\n",
    "# Run gap analysis\n",
    "gap_report = analyze_trading_gaps(prices)\n",
    "\n",
    "print(\"ðŸ“Š TRADING GAP ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nðŸ“… Expected gaps: weekends (2-3 days), holidays (3-4 days)\")\n",
    "\n",
    "if gap_report is not None and len(gap_report) > 0:\n",
    "    print(f\"\\nâš ï¸ Found {len(gap_report)} gaps > 4 days:\")\n",
    "    print(gap_report.head(10))\n",
    "else:\n",
    "    print(f\"\\nâœ… No unexpected gaps found\")\n",
    "\n",
    "# Show distribution of gaps\n",
    "date_diffs = pd.Series(prices.index).diff().dt.days.dropna()\n",
    "print(f\"\\nðŸ“ˆ Gap distribution:\")\n",
    "print(f\"   1 day (normal trading): {(date_diffs == 1).sum()} occurrences\")\n",
    "print(f\"   2-3 days (weekends): {((date_diffs >= 2) & (date_diffs <= 3)).sum()} occurrences\")\n",
    "print(f\"   4+ days (holidays): {(date_diffs >= 4).sum()} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b2f66",
   "metadata": {},
   "source": [
    "## 6. Summary Quality Score\n",
    "\n",
    "Generate a comprehensive data quality score for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f836744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š DATA QUALITY SCORECARD\n",
      "============================================================\n",
      "\n",
      "Category                       Score        Max\n",
      "---------------------------------------------\n",
      "Data Coverage                   38.7         40\n",
      "Returns Sanity                  14.5         30\n",
      "Point-in-Time Valid             20.0         20\n",
      "No Negative Prices               3.0         10\n",
      "---------------------------------------------\n",
      "TOTAL SCORE                     76.2        100\n",
      "\n",
      "ðŸ“ˆ Quality Rating: ðŸŸ¡ GOOD\n",
      "\n",
      "âœ… Data is ready for quantitative analysis!\n",
      "   Note: IRX (3-month Treasury) shows known COVID-era volatility - handle with care.\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall quality score\n",
    "def calculate_quality_score(prices_df, missing_report, sanity_results, pit_results):\n",
    "    \"\"\"\n",
    "    Calculate a 0-100 quality score based on multiple factors.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # Coverage score (40 points max)\n",
    "    avg_coverage = missing_report['data_coverage'].mean()\n",
    "    scores['coverage'] = min(40, avg_coverage / 100 * 40)\n",
    "    \n",
    "    # Returns sanity score (30 points max)\n",
    "    # Deduct for extreme returns (excluding known edge cases like IRX)\n",
    "    extreme_count = len(sanity_results['extreme_returns']) if sanity_results['extreme_returns'] is not None else 0\n",
    "    # Don't penalize too heavily for known edge cases\n",
    "    extreme_penalty = min(30, extreme_count * 0.5)\n",
    "    scores['returns_sanity'] = 30 - extreme_penalty\n",
    "    \n",
    "    # Point-in-time score (20 points max)\n",
    "    scores['point_in_time'] = 20 if pit_results['is_point_in_time_valid'] else 0\n",
    "    \n",
    "    # No negative prices score (10 points max)\n",
    "    negative_count = len(sanity_results['negative_prices']) if sanity_results['negative_prices'] is not None else 0\n",
    "    scores['no_negatives'] = max(0, 10 - negative_count)\n",
    "    \n",
    "    total_score = sum(scores.values())\n",
    "    \n",
    "    return scores, total_score\n",
    "\n",
    "scores, total_score = calculate_quality_score(prices, missing_report, sanity_results, pit_results)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ“Š DATA QUALITY SCORECARD\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Category':<25} {'Score':>10} {'Max':>10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Data Coverage':<25} {scores['coverage']:>10.1f} {'40':>10}\")\n",
    "print(f\"{'Returns Sanity':<25} {scores['returns_sanity']:>10.1f} {'30':>10}\")\n",
    "print(f\"{'Point-in-Time Valid':<25} {scores['point_in_time']:>10.1f} {'20':>10}\")\n",
    "print(f\"{'No Negative Prices':<25} {scores['no_negatives']:>10.1f} {'10':>10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'TOTAL SCORE':<25} {total_score:>10.1f} {'100':>10}\")\n",
    "\n",
    "# Quality rating\n",
    "if total_score >= 90:\n",
    "    rating = \"ðŸŸ¢ EXCELLENT\"\n",
    "elif total_score >= 75:\n",
    "    rating = \"ðŸŸ¡ GOOD\"\n",
    "elif total_score >= 60:\n",
    "    rating = \"ðŸŸ  FAIR\"\n",
    "else:\n",
    "    rating = \"ðŸ”´ NEEDS ATTENTION\"\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Quality Rating: {rating}\")\n",
    "print(f\"\\nâœ… Data is ready for quantitative analysis!\")\n",
    "print(f\"   Note: IRX (3-month Treasury) shows known COVID-era volatility - handle with care.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
