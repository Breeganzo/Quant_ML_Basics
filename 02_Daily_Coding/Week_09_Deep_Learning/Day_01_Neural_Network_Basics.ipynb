{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdf4672",
   "metadata": {},
   "source": [
    "# Week 9, Day 1: Neural Network Basics\n",
    "\n",
    "## ğŸ¯ Today's Objectives\n",
    "- Understand the perceptron (single neuron)\n",
    "- Learn PyTorch tensor operations\n",
    "- Build your first neural network from scratch\n",
    "- Understand forward propagation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3d8146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded!\n",
      "ğŸ”§ PyTorch version: 2.9.1\n",
      "ğŸ’» Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(f\"ğŸ”§ PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸ’» Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c83449",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Perceptron\n",
    "\n",
    "The simplest neural network is a **single neuron** (perceptron):\n",
    "\n",
    "$$y = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weight vector\n",
    "- $b$ = bias\n",
    "- $\\sigma$ = activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcda9bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCEPTRON:\n",
      "Perceptron(weights=[ 0.005  -0.0014  0.0065], bias=0.0000)\n",
      "\n",
      "Input: [ 0.5 -0.2  0.8]\n",
      "Weights: [ 0.005  -0.0014  0.0065]\n",
      "Bias: 0\n",
      "Linear: wÂ·x + b = 0.0079\n",
      "Output (after sigmoid): 0.5020\n"
     ]
    }
   ],
   "source": [
    "# Manual perceptron implementation\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    A single neuron implementation from scratch.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs):\n",
    "        # Initialize weights randomly\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute output for input x.\"\"\"\n",
    "        # Linear transformation\n",
    "        linear = np.dot(x, self.weights) + self.bias\n",
    "        # Apply activation\n",
    "        return self.sigmoid(linear)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Perceptron(weights={self.weights.round(4)}, bias={self.bias:.4f})\"\n",
    "\n",
    "# Create perceptron\n",
    "neuron = Perceptron(n_inputs=3)\n",
    "print(\"PERCEPTRON:\")\n",
    "print(neuron)\n",
    "\n",
    "# Test with sample input\n",
    "x = np.array([0.5, -0.2, 0.8])\n",
    "output = neuron.forward(x)\n",
    "\n",
    "print(f\"\\nInput: {x}\")\n",
    "print(f\"Weights: {neuron.weights.round(4)}\")\n",
    "print(f\"Bias: {neuron.bias}\")\n",
    "print(f\"Linear: wÂ·x + b = {np.dot(x, neuron.weights) + neuron.bias:.4f}\")\n",
    "print(f\"Output (after sigmoid): {output:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32257be",
   "metadata": {},
   "source": [
    "### ğŸ“Š OUTPUT EXPLANATION:\n",
    "\n",
    "- **Weights**: Randomly initialized small values (to break symmetry)\n",
    "- **Bias**: Starts at 0\n",
    "- **Linear output**: The weighted sum of inputs\n",
    "- **Final output**: Squashed between 0-1 by sigmoid\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bdd9e",
   "metadata": {},
   "source": [
    "## Part 2: PyTorch Tensors\n",
    "\n",
    "PyTorch tensors are like NumPy arrays but with:\n",
    "- GPU acceleration\n",
    "- Automatic differentiation (autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e32d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH TENSORS\n",
      "============================================================\n",
      "From list: tensor([1, 2, 3, 4])\n",
      "From numpy:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Random (randn):\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0., -1., -0.],\n",
      "        [ 2., -1.,  0.]])\n",
      "\n",
      "Zeros: torch.Size([2, 3]), Ones: torch.Size([2, 3])\n",
      "\n",
      "Tensor Properties:\n",
      "  Shape: torch.Size([3, 3])\n",
      "  Dtype: torch.float32\n",
      "  Device: cpu\n",
      "  Requires grad: False\n"
     ]
    }
   ],
   "source": [
    "# Creating tensors\n",
    "print(\"PYTORCH TENSORS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# From list\n",
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"From list: {t1}\")\n",
    "\n",
    "# From numpy\n",
    "np_arr = np.array([[1, 2], [3, 4]])\n",
    "t2 = torch.from_numpy(np_arr)\n",
    "print(f\"From numpy:\\n{t2}\")\n",
    "\n",
    "# Random tensors\n",
    "t3 = torch.randn(3, 3)  # Standard normal distribution\n",
    "print(f\"\\nRandom (randn):\\n{t3.round()}\")\n",
    "\n",
    "# Zeros and ones\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"\\nZeros: {zeros.shape}, Ones: {ones.shape}\")\n",
    "\n",
    "# Tensor properties\n",
    "print(f\"\\nTensor Properties:\")\n",
    "print(f\"  Shape: {t3.shape}\")\n",
    "print(f\"  Dtype: {t3.dtype}\")\n",
    "print(f\"  Device: {t3.device}\")\n",
    "print(f\"  Requires grad: {t3.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8feba7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR OPERATIONS\n",
      "============================================================\n",
      "a:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "b:\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "a + b:\n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n",
      "\n",
      "a * b (element-wise):\n",
      "tensor([[ 5., 12.],\n",
      "        [21., 32.]])\n",
      "\n",
      "a @ b (matrix mult):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "Same as torch.mm(a, b):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "a.view(4): tensor([1., 2., 3., 4.])\n",
      "a.view(1, 4): tensor([[1., 2., 3., 4.]])\n",
      "a.T (transpose):\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor operations\n",
    "print(\"TENSOR OPERATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "print(f\"a:\\n{a}\")\n",
    "print(f\"\\nb:\\n{b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"\\na + b:\\n{a + b}\")\n",
    "print(f\"\\na * b (element-wise):\\n{a * b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\na @ b (matrix mult):\\n{a @ b}\")\n",
    "print(f\"Same as torch.mm(a, b):\\n{torch.mm(a, b)}\")\n",
    "\n",
    "# Reshaping\n",
    "print(f\"\\na.view(4): {a.view(4)}\")\n",
    "print(f\"a.view(1, 4): {a.view(1, 4)}\")\n",
    "print(f\"a.T (transpose):\\n{a.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832b0268",
   "metadata": {},
   "source": [
    "### ğŸ“Š OUTPUT EXPLANATION:\n",
    "\n",
    "- **Element-wise (`*`)**: Multiply corresponding elements\n",
    "- **Matrix multiplication (`@` or `mm`)**: True matrix product\n",
    "- **view()**: Reshape without copying (like numpy reshape)\n",
    "- **T**: Transpose (swap rows and columns)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d2100",
   "metadata": {},
   "source": [
    "## Part 3: Building a Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e481bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL NETWORK ARCHITECTURE\n",
      "============================================================\n",
      "SimpleNN(\n",
      "  (hidden): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "ğŸ“Š Parameter Count:\n",
      "  hidden.weight: torch.Size([64, 10]) = 640 params\n",
      "  hidden.bias: torch.Size([64]) = 64 params\n",
      "  output.weight: torch.Size([1, 64]) = 64 params\n",
      "  output.bias: torch.Size([1]) = 1 params\n",
      "\n",
      "  Total: 769 parameters\n"
     ]
    }
   ],
   "source": [
    "# Simple neural network with nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network.\n",
    "    \n",
    "    Architecture: Input(10) -> Hidden(64) -> ReLU -> Output(1) -> Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=10, hidden_size=64, output_size=1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Activations\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.hidden(x)    # Linear transformation\n",
    "        x = self.relu(x)      # Non-linearity\n",
    "        x = self.output(x)    # Second linear transformation\n",
    "        x = self.sigmoid(x)   # Output activation (0-1)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNN(input_size=10, hidden_size=64)\n",
    "\n",
    "print(\"NEURAL NETWORK ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "print(\"\\nğŸ“Š Parameter Count:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape} = {param.numel()} params\")\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n  Total: {total:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a5bb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORWARD PASS TEST\n",
      "============================================================\n",
      "Input shape: torch.Size([5, 10])\n",
      "Output shape: torch.Size([5, 1])\n",
      "\n",
      "Outputs (probabilities):\n",
      "  Sample 1: 0.4485\n",
      "  Sample 2: 0.4418\n",
      "  Sample 3: 0.4527\n",
      "  Sample 4: 0.4159\n",
      "  Sample 5: 0.4290\n",
      "\n",
      "ğŸ“Š OUTPUT EXPLANATION:\n",
      "  â€¢ Values between 0-1 (sigmoid activation)\n",
      "  â€¢ Each output is a probability\n",
      "  â€¢ > 0.5 â†’ Predict class 1, < 0.5 â†’ Predict class 0\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "print(\"FORWARD PASS TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create sample input (batch of 5 samples, 10 features each)\n",
    "x = torch.randn(5, 10)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():  # Don't track gradients\n",
    "    output = model(x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutputs (probabilities):\")\n",
    "for i, prob in enumerate(output.squeeze()):\n",
    "    print(f\"  Sample {i+1}: {prob.item():.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š OUTPUT EXPLANATION:\")\n",
    "print(\"  â€¢ Values between 0-1 (sigmoid activation)\")\n",
    "print(\"  â€¢ Each output is a probability\")\n",
    "print(\"  â€¢ > 0.5 â†’ Predict class 1, < 0.5 â†’ Predict class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab8c57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Using nn.Sequential (Shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123b69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQUENTIAL MODEL\n",
      "============================================================\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "\n",
      "Input: torch.Size([3, 10]) â†’ Output: torch.Size([3, 1])\n",
      "Outputs: [0.45738762617111206, 0.466450572013855, 0.4609096944332123]\n"
     ]
    }
   ],
   "source": [
    "# Faster way to define simple networks\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "print(\"SEQUENTIAL MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(model_seq)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(3, 10)\n",
    "with torch.no_grad():\n",
    "    out = model_seq(x)\n",
    "print(f\"\\nInput: {x.shape} â†’ Output: {out.shape}\")\n",
    "print(f\"Outputs: {out.squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3796099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Financial Application - Stock Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97feee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCK FEATURE DATA\n",
      "============================================================\n",
      "       momentum_5d  momentum_20d  volatility  volume_ratio      rsi     macd  \\\n",
      "count      1000.00       1000.00     1000.00       1000.00  1000.00  1000.00   \n",
      "mean          0.00          0.00        0.24          0.97    49.93    -0.00   \n",
      "std           0.02          0.05        0.18          0.98    17.38     0.01   \n",
      "min          -0.06         -0.15        0.00          0.00    20.06    -0.03   \n",
      "25%          -0.01         -0.03        0.10          0.28    35.09    -0.01   \n",
      "50%           0.00          0.00        0.20          0.66    49.13    -0.00   \n",
      "75%           0.01          0.04        0.33          1.31    64.92     0.01   \n",
      "max           0.08          0.16        1.18          6.10    79.96     0.03   \n",
      "\n",
      "       bb_position  \n",
      "count      1000.00  \n",
      "mean         -0.04  \n",
      "std           0.57  \n",
      "min          -1.00  \n",
      "25%          -0.53  \n",
      "50%          -0.04  \n",
      "75%           0.43  \n",
      "max           1.00  \n",
      "\n",
      "ğŸ“Š After Normalization (StandardScaler):\n",
      "Mean: [-0.  0.  0.  0. -0.  0. -0.]\n",
      "Std:  [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic stock features\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features: momentum, volatility, volume, RSI, MACD, etc.\n",
    "features = {\n",
    "    'momentum_5d': np.random.randn(n_samples) * 0.02,\n",
    "    'momentum_20d': np.random.randn(n_samples) * 0.05,\n",
    "    'volatility': np.abs(np.random.randn(n_samples)) * 0.3,\n",
    "    'volume_ratio': np.random.exponential(1, n_samples),\n",
    "    'rsi': np.random.uniform(20, 80, n_samples),\n",
    "    'macd': np.random.randn(n_samples) * 0.01,\n",
    "    'bb_position': np.random.uniform(-1, 1, n_samples),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "# Normalize features (important for neural networks!)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "print(\"STOCK FEATURE DATA\")\n",
    "print(\"=\"*60)\n",
    "print(df.describe().round(2))\n",
    "\n",
    "print(\"\\nğŸ“Š After Normalization (StandardScaler):\")\n",
    "print(f\"Mean: {X_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Std:  {X_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a24f7ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCK PREDICTION MODEL\n",
      "============================================================\n",
      "Model: StockPredictionNN(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=7, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "ğŸ“Š Sample Predictions:\n",
      "Sample      Probability  Direction\n",
      "-----------------------------------\n",
      "1                0.4474       DOWN\n",
      "2                0.4458       DOWN\n",
      "3                0.4542       DOWN\n",
      "4                0.4617       DOWN\n",
      "5                0.4215       DOWN\n",
      "6                0.4587       DOWN\n",
      "7                0.4489       DOWN\n",
      "8                0.4522       DOWN\n",
      "9                0.4595       DOWN\n",
      "10               0.4570       DOWN\n"
     ]
    }
   ],
   "source": [
    "# Build stock prediction model\n",
    "class StockPredictionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for stock direction prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features):\n",
    "        super(StockPredictionNN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # Probability of up move\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def predict_direction(self, x, threshold=0.5):\n",
    "        \"\"\"Predict direction (1=up, 0=down).\"\"\"\n",
    "        prob = self.forward(x)\n",
    "        return (prob > threshold).float()\n",
    "\n",
    "# Create model\n",
    "stock_model = StockPredictionNN(n_features=7)\n",
    "\n",
    "# Test with real data\n",
    "X_tensor = torch.FloatTensor(X_scaled[:10])\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs = stock_model(X_tensor)\n",
    "    directions = stock_model.predict_direction(X_tensor)\n",
    "\n",
    "print(\"STOCK PREDICTION MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {stock_model}\")\n",
    "print(f\"\\nğŸ“Š Sample Predictions:\")\n",
    "print(f\"{'Sample':<10} {'Probability':>12} {'Direction':>10}\")\n",
    "print(\"-\"*35)\n",
    "for i in range(10):\n",
    "    dir_str = \"UP\" if directions[i] == 1 else \"DOWN\"\n",
    "    print(f\"{i+1:<10} {probs[i].item():>12.4f} {dir_str:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8416d9f9",
   "metadata": {},
   "source": [
    "### ğŸ“Š OUTPUT EXPLANATION:\n",
    "\n",
    "- **Probability**: Confidence in UP direction (0-1)\n",
    "- **Direction**: Binary prediction based on 0.5 threshold\n",
    "- âš ï¸ Model is UNTRAINED! Predictions are essentially random\n",
    "- Tomorrow we'll learn how to train the model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c164d95",
   "metadata": {},
   "source": [
    "## ğŸ“ Day 1 Summary\n",
    "\n",
    "### Key Concepts Learned:\n",
    "\n",
    "1. **Perceptron**: Single neuron with weights, bias, activation\n",
    "2. **PyTorch Tensors**: GPU-accelerated arrays with autograd\n",
    "3. **nn.Module**: Base class for neural networks\n",
    "4. **nn.Sequential**: Quick way to stack layers\n",
    "5. **Forward Pass**: Data flows through network layers\n",
    "\n",
    "### Important Points:\n",
    "- Always normalize input features (StandardScaler)\n",
    "- Use `model.eval()` for inference\n",
    "- Use `torch.no_grad()` when not training\n",
    "\n",
    "### Tomorrow:\n",
    "- Activation functions in depth\n",
    "- Loss functions (MSE, BCE, CE)\n",
    "- Introduction to backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘              DAY 1 COMPLETE: NEURAL NETWORK BASICS               â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  âœ“ Perceptron (single neuron)                                   â•‘\n",
    "â•‘  âœ“ PyTorch tensors and operations                               â•‘\n",
    "â•‘  âœ“ Building networks with nn.Module                             â•‘\n",
    "â•‘  âœ“ Forward pass through network                                 â•‘\n",
    "â•‘  âœ“ Stock feature processing example                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
