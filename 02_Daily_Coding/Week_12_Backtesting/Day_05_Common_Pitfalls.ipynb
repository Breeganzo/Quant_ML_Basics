{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7fb3fa",
   "metadata": {},
   "source": [
    "# Day 5: Common Pitfalls & Overfitting\n",
    "\n",
    "## Week 12 - Backtesting & Validation\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Identify and avoid lookahead bias\n",
    "- Understand overfitting in financial ML\n",
    "- Apply multiple testing corrections\n",
    "- Detect strategy decay\n",
    "\n",
    "### ‚è±Ô∏è Time Allocation\n",
    "- Theory review: 30 min\n",
    "- Guided exercises: 90 min\n",
    "- Practice problems: 60 min\n",
    "- Interview prep: 30 min\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: ML Quant Finance Mastery  \n",
    "**Difficulty**: Intermediate  \n",
    "**Prerequisites**: Day 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0320e",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate financial data\n",
    "n_samples = 1500  # ~6 years\n",
    "\n",
    "# Create features with different predictive power\n",
    "df = pd.DataFrame({\n",
    "    'momentum': np.random.randn(n_samples) * 0.01,\n",
    "    'volatility': np.abs(np.random.randn(n_samples) * 0.015),\n",
    "    'volume': np.random.randn(n_samples) * 0.02,\n",
    "    'sentiment': np.random.randn(n_samples) * 0.01\n",
    "})\n",
    "\n",
    "# Target with weak predictability\n",
    "df['target'] = (0.02 * df['momentum'] + \n",
    "                -0.01 * df['volatility'] + \n",
    "                np.random.randn(n_samples) * 0.015)\n",
    "\n",
    "print(f\"üìä Generated {n_samples} samples\")\n",
    "print(f\"   Actual signal in target: ~2% from momentum, ~1% from volatility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa2308",
   "metadata": {},
   "source": [
    "## 2. Lookahead Bias Demonstration\n",
    "\n",
    "### Common Sources of Lookahead\n",
    "\n",
    "1. **Future data in features**: Using tomorrow's data today\n",
    "2. **Revised data**: Using final revisions, not initial releases\n",
    "3. **Survivorship**: Only testing on stocks that survived\n",
    "4. **Perfect foresight**: Labels computed using future prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_lookahead_bias():\n",
    "    \"\"\"\n",
    "    Show how lookahead bias inflates performance\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n = 1000\n",
    "    \n",
    "    # True returns (unpredictable random walk)\n",
    "    true_returns = np.random.randn(n) * 0.01\n",
    "    \n",
    "    # Feature WITHOUT lookahead: lagged return\n",
    "    feature_correct = np.roll(true_returns, 1)\n",
    "    feature_correct[0] = 0\n",
    "    \n",
    "    # Feature WITH lookahead: includes current return\n",
    "    feature_biased = true_returns * 0.5 + np.random.randn(n) * 0.005\n",
    "    \n",
    "    # Train models\n",
    "    X_correct = feature_correct.reshape(-1, 1)[:-1]\n",
    "    X_biased = feature_biased.reshape(-1, 1)[:-1]\n",
    "    y = true_returns[1:]\n",
    "    \n",
    "    # Time series split\n",
    "    split = int(len(y) * 0.8)\n",
    "    \n",
    "    # Correct model\n",
    "    model_correct = Ridge(alpha=1.0)\n",
    "    model_correct.fit(X_correct[:split], y[:split])\n",
    "    pred_correct = model_correct.predict(X_correct[split:])\n",
    "    r2_correct = r2_score(y[split:], pred_correct)\n",
    "    \n",
    "    # Biased model\n",
    "    model_biased = Ridge(alpha=1.0)\n",
    "    model_biased.fit(X_biased[:split], y[:split])\n",
    "    pred_biased = model_biased.predict(X_biased[split:])\n",
    "    r2_biased = r2_score(y[split:], pred_biased)\n",
    "    \n",
    "    return r2_correct, r2_biased\n",
    "\n",
    "r2_correct, r2_biased = demonstrate_lookahead_bias()\n",
    "\n",
    "print(\"üìä LOOKAHEAD BIAS DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Correct features (lagged): R¬≤ = {r2_correct:.4f}\")\n",
    "print(f\"Biased features (current):  R¬≤ = {r2_biased:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è Biased model appears {r2_biased/max(r2_correct, 0.0001):.0f}x better!\")\n",
    "print(\"   But it's CHEATING by using future information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d083a",
   "metadata": {},
   "source": [
    "## 3. Overfitting in Financial ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d22e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overfitting():\n",
    "    \"\"\"\n",
    "    Show how model complexity leads to overfitting\n",
    "    \"\"\"\n",
    "    X = df[['momentum', 'volatility', 'volume', 'sentiment']].values\n",
    "    y = df['target'].values\n",
    "    \n",
    "    # Split: 80% train, 20% test (time-based)\n",
    "    split = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    \n",
    "    # Test different model complexities\n",
    "    complexities = [1, 5, 10, 20, 50, 100, 200]\n",
    "    results = []\n",
    "    \n",
    "    for n_trees in complexities:\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_trees,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # In-sample performance\n",
    "        pred_train = model.predict(X_train)\n",
    "        r2_train = r2_score(y_train, pred_train)\n",
    "        \n",
    "        # Out-of-sample performance\n",
    "        pred_test = model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, pred_test)\n",
    "        \n",
    "        results.append({\n",
    "            'complexity': n_trees,\n",
    "            'r2_train': r2_train,\n",
    "            'r2_test': r2_test,\n",
    "            'gap': r2_train - r2_test\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "results = demonstrate_overfitting()\n",
    "\n",
    "print(\"üìä OVERFITTING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(results['complexity'], results['r2_train'], 'b-o', label='In-Sample (Train)', linewidth=2)\n",
    "ax.plot(results['complexity'], results['r2_test'], 'r-o', label='Out-of-Sample (Test)', linewidth=2)\n",
    "ax.fill_between(results['complexity'], results['r2_train'], results['r2_test'], alpha=0.3, color='orange', label='Overfitting Gap')\n",
    "ax.set_xlabel('Model Complexity (# Trees)')\n",
    "ax.set_ylabel('R¬≤ Score')\n",
    "ax.set_title('Overfitting: Gap Between Train and Test Performance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHT: More complexity ‚Üí better train, worse test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2de7e",
   "metadata": {},
   "source": [
    "## 4. Multiple Testing Problem\n",
    "\n",
    "### The Issue\n",
    "\n",
    "When you test many strategies, some will be \"significant\" by chance.\n",
    "\n",
    "**Example:**\n",
    "- Test 100 random strategies\n",
    "- 5% significance level\n",
    "- Expect ~5 to look significant even if all are random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_testing_simulation(n_strategies=100, n_days=252):\n",
    "    \"\"\"\n",
    "    Simulate the multiple testing problem\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate random strategies (all are actually noise)\n",
    "    sharpe_ratios = []\n",
    "    p_values = []\n",
    "    \n",
    "    for _ in range(n_strategies):\n",
    "        # Random returns (no real signal)\n",
    "        returns = np.random.randn(n_days) * 0.01\n",
    "        \n",
    "        # Calculate Sharpe\n",
    "        sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        sharpe_ratios.append(sharpe)\n",
    "        \n",
    "        # t-test for Sharpe > 0\n",
    "        t_stat = sharpe * np.sqrt(n_days) / np.sqrt(252)\n",
    "        p_value = 1 - stats.t.cdf(t_stat, n_days - 1)\n",
    "        p_values.append(p_value)\n",
    "    \n",
    "    # Count \"significant\" strategies\n",
    "    n_significant_5pct = sum(1 for p in p_values if p < 0.05)\n",
    "    n_significant_1pct = sum(1 for p in p_values if p < 0.01)\n",
    "    \n",
    "    return {\n",
    "        'n_strategies': n_strategies,\n",
    "        'n_significant_5pct': n_significant_5pct,\n",
    "        'n_significant_1pct': n_significant_1pct,\n",
    "        'best_sharpe': max(sharpe_ratios),\n",
    "        'sharpe_ratios': sharpe_ratios,\n",
    "        'p_values': p_values\n",
    "    }\n",
    "\n",
    "results = multiple_testing_simulation(n_strategies=100)\n",
    "\n",
    "print(\"üìä MULTIPLE TESTING PROBLEM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Strategies tested: {results['n_strategies']} (ALL ARE RANDOM!)\")\n",
    "print(f\"\\n'Significant' at 5% level: {results['n_significant_5pct']} strategies\")\n",
    "print(f\"'Significant' at 1% level: {results['n_significant_1pct']} strategies\")\n",
    "print(f\"\\nBest 'strategy' Sharpe: {results['best_sharpe']:.2f}\")\n",
    "print(\"\\n‚ö†Ô∏è These 'significant' results are all due to CHANCE!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sharpe distribution\n",
    "axes[0].hist(results['sharpe_ratios'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', label='Zero')\n",
    "axes[0].axvline(x=results['best_sharpe'], color='g', linestyle='--', label=f'Best: {results[\"best_sharpe\"]:.2f}')\n",
    "axes[0].set_xlabel('Sharpe Ratio')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Sharpe Ratios (All Random!)')\n",
    "axes[0].legend()\n",
    "\n",
    "# P-value distribution\n",
    "axes[1].hist(results['p_values'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0.05, color='r', linestyle='--', label='5% threshold')\n",
    "axes[1].set_xlabel('P-Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('P-Values Should Be Uniform (Random Strategies)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonferroni_correction(p_values, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction for multiple testing\n",
    "    \"\"\"\n",
    "    n_tests = len(p_values)\n",
    "    corrected_alpha = alpha / n_tests\n",
    "    \n",
    "    # Significant after correction\n",
    "    significant = [p < corrected_alpha for p in p_values]\n",
    "    \n",
    "    return {\n",
    "        'original_alpha': alpha,\n",
    "        'corrected_alpha': corrected_alpha,\n",
    "        'n_significant_original': sum(1 for p in p_values if p < alpha),\n",
    "        'n_significant_corrected': sum(significant)\n",
    "    }\n",
    "\n",
    "correction = bonferroni_correction(results['p_values'])\n",
    "\n",
    "print(\"üìä BONFERRONI CORRECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original alpha: {correction['original_alpha']:.2%}\")\n",
    "print(f\"Corrected alpha: {correction['corrected_alpha']:.4%}\")\n",
    "print(f\"\\nSignificant without correction: {correction['n_significant_original']}\")\n",
    "print(f\"Significant with correction: {correction['n_significant_corrected']}\")\n",
    "print(\"\\n‚úÖ Correction properly filters out false discoveries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8055c",
   "metadata": {},
   "source": [
    "## 5. Strategy Decay Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_strategy_decay():\n",
    "    \"\"\"\n",
    "    Simulate a strategy that decays over time (alpha decay)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_years = 5\n",
    "    n_days = 252 * n_years\n",
    "    \n",
    "    # Strategy alpha decays exponentially\n",
    "    time = np.arange(n_days)\n",
    "    alpha_decay = 0.01 * np.exp(-time / (252 * 2))  # Half-life of 2 years\n",
    "    \n",
    "    # Generate returns with decaying alpha\n",
    "    noise = np.random.randn(n_days) * 0.015\n",
    "    strategy_returns = alpha_decay + noise\n",
    "    \n",
    "    # Calculate rolling metrics\n",
    "    window = 63  # 3 months\n",
    "    \n",
    "    rolling_sharpe = pd.Series(strategy_returns).rolling(window).apply(\n",
    "        lambda x: x.mean() / x.std() * np.sqrt(252) if x.std() > 0 else 0\n",
    "    )\n",
    "    \n",
    "    rolling_alpha = pd.Series(strategy_returns).rolling(window).mean() * 252\n",
    "    \n",
    "    return {\n",
    "        'returns': strategy_returns,\n",
    "        'true_alpha': alpha_decay,\n",
    "        'rolling_sharpe': rolling_sharpe,\n",
    "        'rolling_alpha': rolling_alpha\n",
    "    }\n",
    "\n",
    "decay_results = simulate_strategy_decay()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# Cumulative returns\n",
    "cumulative = (1 + pd.Series(decay_results['returns'])).cumprod()\n",
    "axes[0].plot(cumulative.values)\n",
    "axes[0].set_ylabel('Portfolio Value')\n",
    "axes[0].set_title('Strategy with Alpha Decay')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling Sharpe\n",
    "axes[1].plot(decay_results['rolling_sharpe'].values)\n",
    "axes[1].axhline(y=1, color='g', linestyle='--', label='Target Sharpe')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_ylabel('Rolling Sharpe (63d)')\n",
    "axes[1].set_title('Strategy Stability Declining')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# True vs estimated alpha\n",
    "axes[2].plot(decay_results['true_alpha'] * 252, label='True Alpha', alpha=0.7)\n",
    "axes[2].plot(decay_results['rolling_alpha'].values, label='Estimated Alpha', alpha=0.7)\n",
    "axes[2].set_xlabel('Days')\n",
    "axes[2].set_ylabel('Annualized Alpha')\n",
    "axes[2].set_title('Alpha Decay Over Time')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° KEY INSIGHT: Strategies decay as edge gets arbitraged away!\")\n",
    "print(\"   Monitor rolling metrics to detect decay early.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044618a1",
   "metadata": {},
   "source": [
    "## 6. ‚è±Ô∏è TIMED CODING CHALLENGE (30 minutes)\n",
    "\n",
    "**Challenge:** Build a function that:\n",
    "1. Takes a set of strategy returns\n",
    "2. Tests for statistical significance with multiple testing correction\n",
    "3. Detects alpha decay using structural break tests\n",
    "4. Returns a comprehensive diagnostic report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def strategy_diagnostic(returns, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Comprehensive strategy diagnostic\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : array-like\n",
    "        Strategy returns\n",
    "    confidence_level : float\n",
    "        Confidence level for statistical tests\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Diagnostic results\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    # 1. Test statistical significance\n",
    "    # 2. Check for alpha decay\n",
    "    # 3. Identify structural breaks\n",
    "    # 4. Generate report\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# diagnostic = strategy_diagnostic(decay_results['returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369358d",
   "metadata": {},
   "source": [
    "## 7. Interview Question of the Day\n",
    "\n",
    "**Q: How would you distinguish between a strategy that has decayed vs one that was overfitted from the start?**\n",
    "\n",
    "Think about:\n",
    "1. Out-of-sample vs post-deployment performance\n",
    "2. Pattern of decay (sudden vs gradual)\n",
    "3. Market conditions during backtest vs live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ff63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä OVERFITTING vs DECAY: KEY DIFFERENCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Initial live performance',\n",
    "        'Decay pattern',\n",
    "        'Recovery possibility',\n",
    "        'Backtest characteristics',\n",
    "        'Fix'\n",
    "    ],\n",
    "    'Overfitting': [\n",
    "        'Poor immediately',\n",
    "        'N/A - never worked',\n",
    "        'Very unlikely',\n",
    "        'Too good to be true',\n",
    "        'Simplify model, more data'\n",
    "    ],\n",
    "    'Alpha Decay': [\n",
    "        'Good initially',\n",
    "        'Gradual decline',\n",
    "        'Possible with adaptation',\n",
    "        'Reasonable, matched reality',\n",
    "        'Find new signals, adapt'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f83a3d",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "| Pitfall | Description | Prevention |\n",
    "|---------|-------------|------------|\n",
    "| Lookahead | Using future data | Strict point-in-time |\n",
    "| Overfitting | Fitting noise | Walk-forward, regularization |\n",
    "| Multiple Testing | False discoveries | Bonferroni, FDR correction |\n",
    "| Alpha Decay | Edge disappears | Monitor, adapt |\n",
    "| Survivorship | Ignoring failures | Include dead stocks |\n",
    "\n",
    "---\n",
    "\n",
    "**Tomorrow:** Full Backtest Pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
