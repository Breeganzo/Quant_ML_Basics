{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85181cf7",
   "metadata": {},
   "source": [
    "# üìà Week 7.1 Trading Strategy: Tree-Based Ensemble Methods\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Strategy Overview\n",
    "\n",
    "**Building on Weeks 1-7 + Adding Week 7.1 Concepts:**\n",
    "- Foundation (returns, volatility, correlation)\n",
    "- Statistics (distributions, hypothesis testing)\n",
    "- Time series (stationarity, GARCH)\n",
    "- ML foundations (regression, regularization)\n",
    "- Portfolio optimization (MPT)\n",
    "- Factor models (CAPM, Fama-French)\n",
    "- Classification (logistic, SVM)\n",
    "- **NEW:** Decision Trees for interpretable rules\n",
    "- **NEW:** Random Forest for robust predictions\n",
    "- **NEW:** XGBoost/LightGBM for high accuracy\n",
    "- **NEW:** Stacking for optimal ensemble\n",
    "\n",
    "**Strategy Logic:**\n",
    "1. Engineer comprehensive features\n",
    "2. Train multiple tree-based models\n",
    "3. Compare and ensemble predictions\n",
    "4. Generate robust trading signals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META']\n",
    "BENCHMARK = 'SPY'\n",
    "LOOKBACK_DAYS = 252 * 2\n",
    "PREDICTION_HORIZON = 5\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä WEEK 7.1 TRADING STRATEGY: TREE ENSEMBLES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüéØ Analyzing: {TICKERS}\")\n",
    "print(f\"üìÖ Prediction horizon: {PREDICTION_HORIZON} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772cb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS & DATA\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Try importing XGBoost and LightGBM\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except ImportError:\n",
    "    HAS_XGB = False\n",
    "    \n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except ImportError:\n",
    "    HAS_LGB = False\n",
    "\n",
    "# Fetch data\n",
    "all_tickers = TICKERS + [BENCHMARK]\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=int(LOOKBACK_DAYS * 1.5))\n",
    "\n",
    "print(\"\\nüì• Downloading market data...\")\n",
    "data = yf.download(all_tickers, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "prices = data['Close'].dropna().tail(LOOKBACK_DAYS)\n",
    "volumes = data['Volume'].dropna().tail(LOOKBACK_DAYS)\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(prices)} trading days\")\n",
    "print(f\"   XGBoost available: {HAS_XGB}\")\n",
    "print(f\"   LightGBM available: {HAS_LGB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0650d35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Section 1: Feature Engineering\n",
    "\n",
    "**Creating comprehensive features for tree models:**\n",
    "- Momentum features\n",
    "- Volatility features\n",
    "- Technical indicators\n",
    "- Volume-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ddbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "def create_tree_features(prices, returns, volumes, ticker):\n",
    "    \"\"\"Create features optimized for tree-based models.\"\"\"\n",
    "    df = pd.DataFrame(index=returns.index)\n",
    "    \n",
    "    close = prices[ticker]\n",
    "    ret = returns[ticker]\n",
    "    \n",
    "    # Momentum features\n",
    "    df['ret_1d'] = ret.shift(1)\n",
    "    df['ret_5d'] = ret.shift(1).rolling(5).sum()\n",
    "    df['ret_10d'] = ret.shift(1).rolling(10).sum()\n",
    "    df['ret_20d'] = ret.shift(1).rolling(20).sum()\n",
    "    \n",
    "    # Volatility\n",
    "    df['vol_5d'] = ret.rolling(5).std().shift(1)\n",
    "    df['vol_20d'] = ret.rolling(20).std().shift(1)\n",
    "    df['vol_ratio'] = df['vol_5d'] / df['vol_20d']\n",
    "    \n",
    "    # Moving averages\n",
    "    df['ma_ratio_5_20'] = (close.rolling(5).mean() / close.rolling(20).mean() - 1).shift(1)\n",
    "    df['ma_ratio_10_50'] = (close.rolling(10).mean() / close.rolling(50).mean() - 1).shift(1)\n",
    "    \n",
    "    # RSI\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    df['rsi'] = (100 - (100 / (1 + gain / loss))).shift(1)\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    ma20 = close.rolling(20).mean()\n",
    "    std20 = close.rolling(20).std()\n",
    "    df['bb_upper_dist'] = ((close - (ma20 + 2*std20)) / close).shift(1)\n",
    "    df['bb_lower_dist'] = ((close - (ma20 - 2*std20)) / close).shift(1)\n",
    "    \n",
    "    # MACD\n",
    "    ema12 = close.ewm(span=12).mean()\n",
    "    ema26 = close.ewm(span=26).mean()\n",
    "    df['macd'] = (ema12 - ema26).shift(1)\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
    "    \n",
    "    # Volume\n",
    "    if ticker in volumes.columns:\n",
    "        df['volume_ratio'] = (volumes[ticker] / volumes[ticker].rolling(20).mean()).shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create labels (multi-class: Down, Neutral, Up)\n",
    "def create_labels(returns, ticker, horizon=PREDICTION_HORIZON, threshold=0.02):\n",
    "    \"\"\"Create classification labels.\"\"\"\n",
    "    future_ret = returns[ticker].shift(-horizon).rolling(horizon).sum()\n",
    "    labels = pd.Series(index=returns.index, dtype=int)\n",
    "    labels[future_ret > threshold] = 1   # Up\n",
    "    labels[future_ret < -threshold] = -1  # Down\n",
    "    labels[(future_ret >= -threshold) & (future_ret <= threshold)] = 0  # Neutral\n",
    "    return labels\n",
    "\n",
    "# Create features and labels for all tickers\n",
    "features_dict = {}\n",
    "labels_dict = {}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    features_dict[ticker] = create_tree_features(prices, returns, volumes, ticker)\n",
    "    labels_dict[ticker] = create_labels(returns, ticker)\n",
    "\n",
    "print(f\"‚úÖ Created {len(features_dict[TICKERS[0]].columns)} features per ticker\")\n",
    "print(f\"Features: {list(features_dict[TICKERS[0]].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1165a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Section 2: Model Training & Comparison\n",
    "\n",
    "**Comparing tree-based models:**\n",
    "- Decision Tree (baseline)\n",
    "- Random Forest (bagging)\n",
    "- Gradient Boosting\n",
    "- XGBoost (if available)\n",
    "- LightGBM (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL TRAINING\n",
    "# ============================================================\n",
    "\n",
    "def train_tree_models(features, labels):\n",
    "    \"\"\"Train and compare tree-based models.\"\"\"\n",
    "    \n",
    "    # Align and clean\n",
    "    common_idx = features.index.intersection(labels.index)\n",
    "    X = features.loc[common_idx].dropna()\n",
    "    y = labels.loc[X.index].dropna()\n",
    "    \n",
    "    common = X.index.intersection(y.index)\n",
    "    X = X.loc[common]\n",
    "    y = y.loc[common]\n",
    "    \n",
    "    if len(X) < 200:\n",
    "        return None\n",
    "    \n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Train/test split\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    # Scale (not required for trees but helps with some models)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Models to compare\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42),\n",
    "        'Gradient Boost': GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)\n",
    "    }\n",
    "    \n",
    "    if HAS_XGB:\n",
    "        models['XGBoost'] = xgb.XGBClassifier(n_estimators=100, max_depth=4, \n",
    "                                               random_state=42, use_label_encoder=False,\n",
    "                                               eval_metric='mlogloss', verbosity=0)\n",
    "    \n",
    "    if HAS_LGB:\n",
    "        models['LightGBM'] = lgb.LGBMClassifier(n_estimators=100, max_depth=4,\n",
    "                                                 random_state=42, verbose=-1)\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv, scoring='f1_weighted')\n",
    "        \n",
    "        # Fit and predict\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "        else:\n",
    "            importance = None\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'cv_f1': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'test_acc': accuracy_score(y_test, y_pred),\n",
    "            'test_f1': f1_score(y_test, y_pred, average='weighted'),\n",
    "            'predictions': pd.Series(y_pred, index=y_test.index),\n",
    "            'importance': importance,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train for all tickers\n",
    "print(\"üìä TREE MODEL COMPARISON\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\n{'Ticker':<10} {'Model':<18} {'CV F1':>10} {'Test Acc':>12} {'Test F1':>10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "all_results = {}\n",
    "for ticker in TICKERS:\n",
    "    results = train_tree_models(features_dict[ticker], labels_dict[ticker])\n",
    "    \n",
    "    if results:\n",
    "        all_results[ticker] = results\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"{ticker:<10} {model_name:<18} {metrics['cv_f1']:>9.3f} {metrics['test_acc']:>11.2%} {metrics['test_f1']:>9.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä AVERAGE FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Aggregate importance across tickers\n",
    "all_importance = pd.DataFrame()\n",
    "for ticker in TICKERS:\n",
    "    if ticker in all_results and 'Random Forest' in all_results[ticker]:\n",
    "        imp = all_results[ticker]['Random Forest']['importance']\n",
    "        if imp is not None:\n",
    "            all_importance[ticker] = imp\n",
    "\n",
    "if len(all_importance.columns) > 0:\n",
    "    avg_importance = all_importance.mean(axis=1).sort_values(ascending=False)\n",
    "    print(avg_importance.head(10).to_string())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ticker in enumerate(TICKERS):\n",
    "    ax = axes[i]\n",
    "    if ticker in all_results and 'Random Forest' in all_results[ticker]:\n",
    "        imp = all_results[ticker]['Random Forest']['importance']\n",
    "        if imp is not None:\n",
    "            imp.sort_values().plot(kind='barh', ax=ax, color='steelblue', alpha=0.7)\n",
    "            ax.set_title(f'{ticker} Feature Importance', fontsize=11, fontweight='bold')\n",
    "            ax.set_xlabel('Importance')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8311578",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Section 3: Ensemble Stacking\n",
    "\n",
    "**Combining models for robust predictions:**\n",
    "- Use best tree models as base learners\n",
    "- Logistic regression as meta-learner\n",
    "- Cross-validated stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b44781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STACKING ENSEMBLE\n",
    "# ============================================================\n",
    "\n",
    "def create_stacking_model(features, labels):\n",
    "    \"\"\"Create a stacking ensemble.\"\"\"\n",
    "    \n",
    "    common_idx = features.index.intersection(labels.index)\n",
    "    X = features.loc[common_idx].dropna()\n",
    "    y = labels.loc[X.index].dropna()\n",
    "    common = X.index.intersection(y.index)\n",
    "    X = X.loc[common]\n",
    "    y = y.loc[common]\n",
    "    \n",
    "    if len(X) < 200:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Base models\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, max_depth=4, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    if HAS_XGB:\n",
    "        base_models.append(('xgb', xgb.XGBClassifier(n_estimators=50, max_depth=3, \n",
    "                                                      random_state=42, use_label_encoder=False,\n",
    "                                                      eval_metric='mlogloss', verbosity=0)))\n",
    "    \n",
    "    # Stacking\n",
    "    stack = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5\n",
    "    )\n",
    "    \n",
    "    stack.fit(X_train_scaled, y_train)\n",
    "    y_pred = stack.predict(X_test_scaled)\n",
    "    \n",
    "    return stack, scaler, accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"üìä STACKING ENSEMBLE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stack_results = {}\n",
    "for ticker in TICKERS:\n",
    "    stack, scaler, acc, f1 = create_stacking_model(features_dict[ticker], labels_dict[ticker])\n",
    "    if stack:\n",
    "        stack_results[ticker] = {'model': stack, 'scaler': scaler, 'acc': acc, 'f1': f1}\n",
    "        print(f\"{ticker}: Accuracy={acc:.2%}, F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. F1 Score by Model and Ticker\n",
    "ax1 = axes[0, 0]\n",
    "model_names = ['Decision Tree', 'Random Forest', 'Gradient Boost']\n",
    "if HAS_XGB:\n",
    "    model_names.append('XGBoost')\n",
    "if HAS_LGB:\n",
    "    model_names.append('LightGBM')\n",
    "\n",
    "x = np.arange(len(TICKERS))\n",
    "width = 0.15\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    f1_scores = []\n",
    "    for ticker in TICKERS:\n",
    "        if ticker in all_results and model in all_results[ticker]:\n",
    "            f1_scores.append(all_results[ticker][model]['test_f1'])\n",
    "        else:\n",
    "            f1_scores.append(0)\n",
    "    ax1.bar(x + i*width, f1_scores, width, label=model, alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x + width * len(model_names) / 2)\n",
    "ax1.set_xticklabels(TICKERS)\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('Model Performance Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=8, loc='upper right')\n",
    "\n",
    "# 2. Best Model per Ticker\n",
    "ax2 = axes[0, 1]\n",
    "best_models = []\n",
    "best_f1s = []\n",
    "for ticker in TICKERS:\n",
    "    if ticker in all_results:\n",
    "        best = max(all_results[ticker].items(), key=lambda x: x[1]['test_f1'])\n",
    "        best_models.append(best[0])\n",
    "        best_f1s.append(best[1]['test_f1'])\n",
    "    else:\n",
    "        best_models.append('N/A')\n",
    "        best_f1s.append(0)\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(set(best_models))))\n",
    "color_map = dict(zip(set(best_models), colors))\n",
    "bar_colors = [color_map[m] for m in best_models]\n",
    "\n",
    "ax2.bar(TICKERS, best_f1s, color=bar_colors, alpha=0.7)\n",
    "for i, (t, m, f) in enumerate(zip(TICKERS, best_models, best_f1s)):\n",
    "    ax2.text(i, f + 0.01, m[:8], ha='center', fontsize=8, rotation=45)\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('Best Model by Ticker', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 3. Stacking vs Best Individual\n",
    "ax3 = axes[1, 0]\n",
    "individual_best = [max([all_results[t][m]['test_f1'] for m in all_results[t]]) \n",
    "                   if t in all_results else 0 for t in TICKERS]\n",
    "stacking_f1 = [stack_results[t]['f1'] if t in stack_results else 0 for t in TICKERS]\n",
    "\n",
    "x = np.arange(len(TICKERS))\n",
    "ax3.bar(x - 0.2, individual_best, 0.4, label='Best Individual', alpha=0.7)\n",
    "ax3.bar(x + 0.2, stacking_f1, 0.4, label='Stacking', alpha=0.7)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(TICKERS)\n",
    "ax3.set_ylabel('F1 Score')\n",
    "ax3.set_title('Individual vs Stacking', fontsize=12, fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. CV vs Test Performance\n",
    "ax4 = axes[1, 1]\n",
    "for ticker in TICKERS[:3]:\n",
    "    if ticker in all_results:\n",
    "        cv_scores = [all_results[ticker][m]['cv_f1'] for m in model_names if m in all_results[ticker]]\n",
    "        test_scores = [all_results[ticker][m]['test_f1'] for m in model_names if m in all_results[ticker]]\n",
    "        ax4.scatter(cv_scores, test_scores, s=80, label=ticker, alpha=0.7)\n",
    "\n",
    "ax4.plot([0.2, 0.6], [0.2, 0.6], 'k--', alpha=0.5)\n",
    "ax4.set_xlabel('CV F1 Score')\n",
    "ax4.set_ylabel('Test F1 Score')\n",
    "ax4.set_title('Cross-Validation vs Test', fontsize=12, fontweight='bold')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70848348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRADING SIGNALS\n",
    "# ============================================================\n",
    "\n",
    "def get_ensemble_signal(ticker, all_results, stack_results, features_dict):\n",
    "    \"\"\"Get trading signal from ensemble.\"\"\"\n",
    "    if ticker not in stack_results:\n",
    "        if ticker not in all_results:\n",
    "            return {'signal': 'N/A', 'confidence': 0}\n",
    "        # Use best individual model\n",
    "        best = max(all_results[ticker].items(), key=lambda x: x[1]['test_f1'])\n",
    "        model = best[1]['model']\n",
    "        scaler = best[1]['scaler']\n",
    "        model_name = best[0]\n",
    "    else:\n",
    "        model = stack_results[ticker]['model']\n",
    "        scaler = stack_results[ticker]['scaler']\n",
    "        model_name = 'Stacking'\n",
    "    \n",
    "    # Get latest features\n",
    "    features = features_dict[ticker].dropna().iloc[[-1]]\n",
    "    if len(features) == 0:\n",
    "        return {'signal': 'N/A', 'confidence': 0, 'model': model_name}\n",
    "    \n",
    "    X_scaled = scaler.transform(features)\n",
    "    pred = model.predict(X_scaled)[0]\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probs = model.predict_proba(X_scaled)[0]\n",
    "        confidence = max(probs)\n",
    "    else:\n",
    "        confidence = 0.5\n",
    "    \n",
    "    signal = 'BUY' if pred == 1 else ('SELL' if pred == -1 else 'HOLD')\n",
    "    \n",
    "    return {\n",
    "        'signal': signal,\n",
    "        'confidence': confidence,\n",
    "        'model': model_name,\n",
    "        'prediction': pred\n",
    "    }\n",
    "\n",
    "print(\"üìä CURRENT TRADING SIGNALS\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"\\n{'Ticker':<10} {'Model':<18} {'Signal':>10} {'Confidence':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "current_signals = {}\n",
    "for ticker in TICKERS:\n",
    "    signal_info = get_ensemble_signal(ticker, all_results, stack_results, features_dict)\n",
    "    current_signals[ticker] = signal_info\n",
    "    \n",
    "    emoji = \"üìà\" if signal_info['signal'] == 'BUY' else (\"üìâ\" if signal_info['signal'] == 'SELL' else \"‚ö™\")\n",
    "    print(f\"{ticker:<10} {signal_info['model']:<18} {emoji} {signal_info['signal']:>6} {signal_info['confidence']:>11.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471aeb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL RECOMMENDATIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä WEEK 7.1 STRATEGY - FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary\n",
    "summary = pd.DataFrame({\n",
    "    'Model': [current_signals[t]['model'] for t in TICKERS],\n",
    "    'Signal': [current_signals[t]['signal'] for t in TICKERS],\n",
    "    'Confidence': [current_signals[t]['confidence'] for t in TICKERS]\n",
    "}, index=TICKERS)\n",
    "\n",
    "print(\"\\nüìã SIGNAL SUMMARY:\")\n",
    "print(summary.to_string())\n",
    "\n",
    "# Recommendations\n",
    "buys = [t for t in TICKERS if current_signals[t]['signal'] == 'BUY']\n",
    "sells = [t for t in TICKERS if current_signals[t]['signal'] == 'SELL']\n",
    "holds = [t for t in TICKERS if current_signals[t]['signal'] == 'HOLD']\n",
    "\n",
    "print(\"\\nüèÜ RECOMMENDATIONS:\")\n",
    "print(f\"\\nüìà BUY: {buys if buys else 'None'}\")\n",
    "print(f\"üìâ SELL: {sells if sells else 'None'}\")\n",
    "print(f\"‚ö™ HOLD: {holds if holds else 'None'}\")\n",
    "\n",
    "# High confidence\n",
    "high_conf = [(t, current_signals[t]) for t in TICKERS \n",
    "             if current_signals[t]['confidence'] > 0.5 and current_signals[t]['signal'] != 'HOLD']\n",
    "\n",
    "if high_conf:\n",
    "    print(\"\\n‚≠ê HIGH CONFIDENCE TRADES:\")\n",
    "    for ticker, info in high_conf:\n",
    "        print(f\"   {ticker}: {info['signal']} ({info['model']}, Confidence: {info['confidence']:.1%})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ö†Ô∏è DISCLAIMER: Educational purposes only. Not financial advice!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d4f61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Key Concepts Used (Week 1-7.1)\n",
    "\n",
    "| Week | Concept | Application |\n",
    "|------|---------|-------------|\n",
    "| 1 | Returns & Volatility | Feature engineering |\n",
    "| 2 | Statistical Testing | Model validation |\n",
    "| 3 | Time Series | Temporal features |\n",
    "| 4 | ML Regression | Baseline models |\n",
    "| 5 | Portfolio Theory | Position sizing |\n",
    "| 6 | Factor Models | Factor features |\n",
    "| 6.1 | Classification | Signal generation |\n",
    "| **7.1** | **Decision Trees** | **Interpretable rules** |\n",
    "| **7.1** | **Random Forest** | **Robust predictions** |\n",
    "| **7.1** | **XGBoost/LightGBM** | **High accuracy** |\n",
    "| **7.1** | **Stacking** | **Ensemble signals** |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
