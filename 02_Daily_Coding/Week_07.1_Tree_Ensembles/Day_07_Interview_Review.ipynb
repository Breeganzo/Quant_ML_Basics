{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cae7a39",
   "metadata": {},
   "source": [
    "# Day 7: Week 7.1 Review - Tree Ensembles Interview Prep\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Consolidate all tree ensemble methods\n",
    "- Build comprehensive comparison pipeline\n",
    "- Practice interview questions\n",
    "- Final trading strategy showdown\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Week Summary\n",
    "\n",
    "| Day | Topic | Key Concept |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Decision Trees | Splitting, interpretability, overfitting |\n",
    "| 2 | Random Forest | Bagging, OOB error, feature importance |\n",
    "| 3 | XGBoost | Gradient boosting, regularization |\n",
    "| 4 | LightGBM | Leaf-wise growth, speed, categoricals |\n",
    "| 5 | CatBoost | Ordered boosting, categorical handling |\n",
    "| 6 | Stacking | Meta-learning, out-of-fold predictions |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25f1cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesSplit, cross_val_predict\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score, roc_auc_score, classification_report\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "TRADING_DAYS = 252\n",
    "RISK_FREE_RATE = 0.05\n",
    "\n",
    "# Download data\n",
    "ticker = 'AAPL'\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(\"üì• Downloading data...\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date, progress=False, auto_adjust=True)\n",
    "prices = data['Close']\n",
    "volume = data['Volume']\n",
    "returns = prices.pct_change().dropna()\n",
    "\n",
    "print(f\"‚úÖ Data: {len(prices)} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Feature Engineering\n",
    "df = pd.DataFrame(index=prices.index)\n",
    "df['price'] = prices\n",
    "df['return'] = returns\n",
    "\n",
    "# Momentum features\n",
    "for lag in [1, 5, 10, 20, 60]:\n",
    "    df[f'momentum_{lag}'] = prices.pct_change(lag)\n",
    "\n",
    "# Volatility\n",
    "for window in [5, 10, 20]:\n",
    "    df[f'volatility_{window}'] = returns.rolling(window).std()\n",
    "\n",
    "# Volume\n",
    "df['volume_ratio'] = volume / volume.rolling(20).mean()\n",
    "\n",
    "# MA Ratios\n",
    "df['ma_5_20'] = prices.rolling(5).mean() / prices.rolling(20).mean() - 1\n",
    "df['ma_20_50'] = prices.rolling(20).mean() / prices.rolling(50).mean() - 1\n",
    "\n",
    "# RSI\n",
    "delta = prices.diff()\n",
    "gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "df['rsi'] = 100 - (100 / (1 + gain / loss))\n",
    "\n",
    "# MACD\n",
    "ema_12 = prices.ewm(span=12).mean()\n",
    "ema_26 = prices.ewm(span=26).mean()\n",
    "df['macd'] = (ema_12 - ema_26) / prices\n",
    "\n",
    "# Target\n",
    "df['next_return'] = returns.shift(-1)\n",
    "df['target'] = (df['next_return'] > 0).astype(int)\n",
    "\n",
    "df = df.dropna()\n",
    "print(f\"üìä Features: {len([c for c in df.columns if c not in ['price', 'return', 'next_return', 'target']])}, Samples: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data with 3-way split\n",
    "feature_cols = [c for c in df.columns if c not in ['price', 'return', 'next_return', 'target']]\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "# 70% train, 15% val, 15% test\n",
    "train_idx = int(len(df) * 0.7)\n",
    "val_idx = int(len(df) * 0.85)\n",
    "\n",
    "X_train = X.iloc[:train_idx]\n",
    "X_val = X.iloc[train_idx:val_idx]\n",
    "X_test = X.iloc[val_idx:]\n",
    "\n",
    "y_train = y.iloc[:train_idx]\n",
    "y_val = y.iloc[train_idx:val_idx]\n",
    "y_test = y.iloc[val_idx:]\n",
    "\n",
    "returns_test = df['next_return'].iloc[val_idx:]\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all tree ensemble models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=5, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42, verbosity=-1),\n",
    "    'CatBoost': CatBoostClassifier(iterations=200, depth=5, learning_rate=0.1, random_seed=42, verbose=False)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "predictions = {}\n",
    "probabilities = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    predictions[name] = y_pred\n",
    "    probabilities[name] = y_proba\n",
    "    \n",
    "    # Validation performance (for model selection)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Time': f'{train_time:.2f}s',\n",
    "        'Val Acc': accuracy_score(y_val, y_val_pred),\n",
    "        'Val F1': f1_score(y_val, y_val_pred),\n",
    "        'Test Acc': accuracy_score(y_test, y_pred),\n",
    "        'Test F1': f1_score(y_test, y_pred),\n",
    "        'Test AUC': roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6060ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Ensemble methods\n",
    "# Simple Averaging\n",
    "avg_proba = np.mean([probabilities[name] for name in probabilities], axis=0)\n",
    "avg_pred = (avg_proba > 0.5).astype(int)\n",
    "predictions['Ensemble (Avg)'] = avg_pred\n",
    "probabilities['Ensemble (Avg)'] = avg_proba\n",
    "\n",
    "# Stacking\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, max_depth=5, random_state=42, eval_metric='logloss')),\n",
    "        ('lgbm', lgb.LGBMClassifier(n_estimators=100, max_depth=5, random_state=42, verbosity=-1))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=TimeSeriesSplit(n_splits=3)\n",
    ")\n",
    "\n",
    "stacking.fit(X_train, y_train)\n",
    "stack_pred = stacking.predict(X_test)\n",
    "stack_proba = stacking.predict_proba(X_test)[:, 1]\n",
    "predictions['Ensemble (Stack)'] = stack_pred\n",
    "probabilities['Ensemble (Stack)'] = stack_proba\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Method':<20} {'Accuracy':>12} {'F1':>12} {'AUC':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Ensemble (Avg)':<20} {accuracy_score(y_test, avg_pred):>12.2%} {f1_score(y_test, avg_pred):>12.2%} {roc_auc_score(y_test, avg_proba):>12.4f}\")\n",
    "print(f\"{'Ensemble (Stack)':<20} {accuracy_score(y_test, stack_pred):>12.2%} {f1_score(y_test, stack_pred):>12.2%} {roc_auc_score(y_test, stack_proba):>12.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd25b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "importance_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "\n",
    "for i, name in enumerate(importance_models):\n",
    "    model = models[name]\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True).tail(8)\n",
    "    \n",
    "    axes[i].barh(importance['Feature'], importance['Importance'], color='steelblue')\n",
    "    axes[i].set_xlabel('Importance')\n",
    "    axes[i].set_title(f'{name} Top Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Strategy Backtest - All Models\n",
    "backtest = pd.DataFrame(index=y_test.index)\n",
    "backtest['actual_return'] = returns_test.values\n",
    "\n",
    "for name in predictions.keys():\n",
    "    safe_name = name.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    backtest[f'{safe_name}_signal'] = predictions[name]\n",
    "    backtest[f'{safe_name}_return'] = predictions[name] * backtest['actual_return']\n",
    "    backtest[f'{safe_name}_cum'] = (1 + backtest[f'{safe_name}_return']).cumprod()\n",
    "\n",
    "backtest['buy_hold_cum'] = (1 + backtest['actual_return']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e14a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Top Models\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(backtest.index, backtest['buy_hold_cum'], label='Buy & Hold', linewidth=2.5, color='black')\n",
    "\n",
    "top_models = ['Random_Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Ensemble_Stack']\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(top_models)))\n",
    "\n",
    "for name, color in zip(top_models, colors):\n",
    "    plt.plot(backtest.index, backtest[f'{name}_cum'], label=name.replace('_', ' '), \n",
    "             linewidth=2 if 'Ensemble' in name else 1.5, color=color)\n",
    "\n",
    "plt.title(f'Tree Ensemble Trading Strategies ({ticker})', fontsize=14)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aee179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Performance Summary\n",
    "def calc_metrics(returns, cumulative):\n",
    "    total = cumulative.iloc[-1] - 1\n",
    "    sharpe = (returns.mean() * TRADING_DAYS - RISK_FREE_RATE) / (returns.std() * np.sqrt(TRADING_DAYS)) if returns.std() > 0 else 0\n",
    "    peak = cumulative.cummax()\n",
    "    mdd = ((cumulative - peak) / peak).min()\n",
    "    trades = (returns != 0).sum()\n",
    "    win_rate = (returns > 0).sum() / trades if trades > 0 else 0\n",
    "    return total, sharpe, mdd, trades, win_rate\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL STRATEGY PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'Strategy':<20} {'Total Ret':>12} {'Sharpe':>10} {'Max DD':>10} {'Trades':>8} {'Win Rate':>10}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Buy and hold\n",
    "total, sharpe, mdd, _, _ = calc_metrics(backtest['actual_return'], backtest['buy_hold_cum'])\n",
    "print(f\"{'Buy & Hold':<20} {total:>12.2%} {sharpe:>10.2f} {mdd:>10.2%} {len(backtest):>8} {'N/A':>10}\")\n",
    "\n",
    "# All models\n",
    "for name in predictions.keys():\n",
    "    safe_name = name.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    total, sharpe, mdd, trades, win_rate = calc_metrics(backtest[f'{safe_name}_return'], backtest[f'{safe_name}_cum'])\n",
    "    print(f\"{name:<20} {total:>12.2%} {sharpe:>10.2f} {mdd:>10.2%} {trades:>8} {win_rate:>10.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Day Predictions - All Models\n",
    "latest = X.iloc[-1:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üìä NEXT DAY PREDICTIONS FOR {ticker}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDate: {df.index[-1].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(f\"\\n{'Model':<20} {'Prediction':>15} {'Confidence':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "bullish_count = 0\n",
    "for name, model in models.items():\n",
    "    pred = model.predict(latest)[0]\n",
    "    proba = model.predict_proba(latest)[0, 1]\n",
    "    direction = 'üìà UP' if pred == 1 else 'üìâ DOWN'\n",
    "    bullish_count += pred\n",
    "    print(f\"{name:<20} {direction:>15} {proba:>12.1%}\")\n",
    "\n",
    "# Ensemble\n",
    "stack_pred = stacking.predict(latest)[0]\n",
    "stack_proba = stacking.predict_proba(latest)[0, 1]\n",
    "print(f\"{'Ensemble (Stack)':<20} {'üìà UP' if stack_pred == 1 else 'üìâ DOWN':>15} {stack_proba:>12.1%}\")\n",
    "\n",
    "print(f\"\\nüéØ Consensus: {bullish_count}/{len(models)} base models bullish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c71d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Interview Questions & Answers\n",
    "\n",
    "### Q1: Compare Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost\n",
    "**A:** \n",
    "- **Decision Tree**: Interpretable but high variance\n",
    "- **Random Forest**: Bagging + feature sampling, reduces variance\n",
    "- **XGBoost**: Gradient boosting + regularization, reduces bias\n",
    "- **LightGBM**: Leaf-wise growth, faster, histogram binning\n",
    "- **CatBoost**: Ordered boosting, native categorical support\n",
    "\n",
    "### Q2: Why does Random Forest reduce variance?\n",
    "**A:** By averaging multiple trees trained on bootstrap samples with random feature subsets, individual tree errors cancel out. The correlation between trees is reduced by feature subsampling.\n",
    "\n",
    "### Q3: XGBoost regularization?\n",
    "**A:** L1 (alpha) on leaf weights for sparsity, L2 (lambda) for smoothing, gamma for minimum loss reduction to split, and learning rate shrinkage.\n",
    "\n",
    "### Q4: LightGBM vs XGBoost?\n",
    "**A:** LightGBM uses leaf-wise growth (faster but can overfit), histogram binning (memory efficient), and native categorical support. XGBoost uses level-wise growth (more balanced trees).\n",
    "\n",
    "### Q5: When to use stacking?\n",
    "**A:** When you have diverse base models with similar performance. Use out-of-fold predictions to prevent leakage. Simple meta-models often work best.\n",
    "\n",
    "---\n",
    "\n",
    "## üè¢ Company Use Cases\n",
    "\n",
    "| Company | Tree Ensemble Application |\n",
    "|---------|-------------------------|\n",
    "| Two Sigma | Alpha factor generation |\n",
    "| Citadel | High-frequency signals |\n",
    "| DE Shaw | Cross-asset prediction |\n",
    "| WorldQuant | Alpha mining |\n",
    "| AQR | Factor timing |\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ Next Week: Instance-Based Methods (KNN, Kernel Methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
