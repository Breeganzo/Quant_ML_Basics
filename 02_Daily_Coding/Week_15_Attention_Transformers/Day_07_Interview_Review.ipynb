{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c230eb99",
   "metadata": {},
   "source": [
    "# Week 15 - Day 7: Interview Review - Transformers & Attention\n",
    "\n",
    "## Overview\n",
    "Final day of Week 15 covering:\n",
    "- **10 Transformer Interview Questions** with detailed answers\n",
    "- **Common Mistakes** and how to avoid them\n",
    "- **Mini-Project**: Complete Transformer-based stock predictor\n",
    "- **Week Summary** consolidating all concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da85795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0128",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: 10 Transformer Interview Questions\n",
    "\n",
    "## Question 1: What is Self-Attention and Why is it Important?\n",
    "\n",
    "**Answer:**\n",
    "Self-attention allows each position in a sequence to attend to all other positions, computing relevance scores dynamically. Unlike RNNs that process sequentially, self-attention captures long-range dependencies in O(1) sequential operations.\n",
    "\n",
    "**Key Formula:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Finance Application:** In time series, self-attention can directly relate today's price to events from 100 days ago without information degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Self-Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Single-head self-attention for interview demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.scale = math.sqrt(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output, attn_weights\n",
    "\n",
    "# Test self-attention\n",
    "embed_dim = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "attn = SelfAttention(embed_dim)\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "output, weights = attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights sum (should be 1.0): {weights[0, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92fe7a6",
   "metadata": {},
   "source": [
    "## Question 2: Explain Multi-Head Attention\n",
    "\n",
    "**Answer:**\n",
    "Multi-head attention runs multiple attention operations in parallel, each learning different relationship patterns. Heads are concatenated and projected back.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "**Why Multiple Heads?**\n",
    "- Head 1 might learn short-term momentum\n",
    "- Head 2 might capture mean reversion patterns\n",
    "- Head 3 might identify volatility clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18271845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Implementation\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention for interview demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "        \n",
    "        self.W_qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Combined projection\n",
    "        qkv = self.W_qkv(x)  # (batch, seq, 3*embed)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq, head_dim)\n",
    "        Q, K, V = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(embed_dim=64, num_heads=8)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, weights = mha(x)\n",
    "\n",
    "print(f\"Multi-Head Attention:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {weights.shape} (batch, heads, seq, seq)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd081a5",
   "metadata": {},
   "source": [
    "## Question 3: Why Scale by ‚àöd_k in Attention?\n",
    "\n",
    "**Answer:**\n",
    "Without scaling, dot products grow with dimension size, pushing softmax into regions with extremely small gradients.\n",
    "\n",
    "**Mathematical Explanation:**\n",
    "- If Q and K have components with mean 0 and variance 1\n",
    "- Their dot product has variance d_k\n",
    "- Scaling by ‚àöd_k normalizes variance back to 1\n",
    "- This keeps softmax in a region with meaningful gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate scaling importance\n",
    "def compare_scaling(d_k_values=[16, 64, 256, 1024]):\n",
    "    \"\"\"Show how scaling affects attention distribution.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(d_k_values), figsize=(14, 6))\n",
    "    \n",
    "    for i, d_k in enumerate(d_k_values):\n",
    "        # Random Q and K\n",
    "        Q = torch.randn(1, 10, d_k)\n",
    "        K = torch.randn(1, 10, d_k)\n",
    "        \n",
    "        # Unscaled\n",
    "        scores_unscaled = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attn_unscaled = torch.softmax(scores_unscaled, dim=-1)\n",
    "        \n",
    "        # Scaled\n",
    "        scores_scaled = scores_unscaled / math.sqrt(d_k)\n",
    "        attn_scaled = torch.softmax(scores_scaled, dim=-1)\n",
    "        \n",
    "        # Plot unscaled\n",
    "        axes[0, i].imshow(attn_unscaled[0].detach().numpy(), cmap='Blues')\n",
    "        axes[0, i].set_title(f'd_k={d_k} (unscaled)')\n",
    "        axes[0, i].set_xlabel(f'Max: {attn_unscaled.max():.3f}')\n",
    "        \n",
    "        # Plot scaled\n",
    "        axes[1, i].imshow(attn_scaled[0].detach().numpy(), cmap='Blues')\n",
    "        axes[1, i].set_title(f'd_k={d_k} (scaled)')\n",
    "        axes[1, i].set_xlabel(f'Max: {attn_scaled.max():.3f}')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Unscaled')\n",
    "    axes[1, 0].set_ylabel('Scaled')\n",
    "    plt.suptitle('Effect of Scaling on Attention Distributions', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_scaling()\n",
    "print(\"\\nObservation: Without scaling, larger d_k leads to more 'peaky' attention\")\n",
    "print(\"(one position dominates), reducing model's ability to attend broadly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f054348",
   "metadata": {},
   "source": [
    "## Question 4: What is Positional Encoding and Why is it Needed?\n",
    "\n",
    "**Answer:**\n",
    "Self-attention is permutation invariant - it treats sequences as sets. Positional encoding injects position information so the model knows the order of elements.\n",
    "\n",
    "**Sinusoidal Encoding Formula:**\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d})$$\n",
    "\n",
    "**Finance Application:** Critical for time series where sequence order represents temporal dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ae559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Implementation\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 64\n",
    "pe = PositionalEncoding(d_model, max_len=100, dropout=0.0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "pe_matrix = pe.pe[0, :50, :32].numpy()\n",
    "im = axes[0].imshow(pe_matrix, aspect='auto', cmap='RdBu')\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "axes[0].set_title('Positional Encoding Heatmap')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Individual dimensions\n",
    "positions = range(50)\n",
    "for dim in [0, 1, 4, 5, 10, 11]:\n",
    "    axes[1].plot(positions, pe.pe[0, :50, dim].numpy(), label=f'dim {dim}')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Encoding Value')\n",
    "axes[1].set_title('Positional Encoding by Dimension')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb38005",
   "metadata": {},
   "source": [
    "## Question 5: What is the Transformer Architecture?\n",
    "\n",
    "**Answer:**\n",
    "The Transformer consists of:\n",
    "1. **Encoder**: Processes input sequence with self-attention + feed-forward layers\n",
    "2. **Decoder**: Generates output with masked self-attention + cross-attention + feed-forward\n",
    "\n",
    "**Key Components:**\n",
    "- Multi-head attention\n",
    "- Position-wise feed-forward networks\n",
    "- Layer normalization\n",
    "- Residual connections\n",
    "\n",
    "**For Time Series (Encoder-only):** Often use just the encoder for regression/classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b95325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"Single transformer encoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual\n",
    "        attn_output, attn_weights = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# Test encoder layer\n",
    "enc_layer = TransformerEncoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 20, 64)\n",
    "output, weights = enc_layer(x)\n",
    "\n",
    "print(f\"Transformer Encoder Layer:\")\n",
    "print(f\"  Input: {x.shape}\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce08dba",
   "metadata": {},
   "source": [
    "## Question 6: Layer Normalization vs Batch Normalization?\n",
    "\n",
    "**Answer:**\n",
    "| Aspect | LayerNorm | BatchNorm |\n",
    "|--------|-----------|------------|\n",
    "| Normalizes across | Features | Batch |\n",
    "| Batch size dependency | No | Yes |\n",
    "| Sequence length handling | Independent | Problematic |\n",
    "| Preferred for | Transformers, NLP | CNNs, fixed-size inputs |\n",
    "\n",
    "**Why LayerNorm for Transformers?**\n",
    "- Works with variable sequence lengths\n",
    "- No running statistics to maintain\n",
    "- Each sample normalized independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55044ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare normalization methods\n",
    "batch_size, seq_len, features = 4, 10, 64\n",
    "x = torch.randn(batch_size, seq_len, features) * 3 + 2  # Non-standard distribution\n",
    "\n",
    "# Layer Normalization (across features)\n",
    "layer_norm = nn.LayerNorm(features)\n",
    "x_ln = layer_norm(x)\n",
    "\n",
    "# Batch Normalization (across batch)\n",
    "batch_norm = nn.BatchNorm1d(features)\n",
    "x_bn = batch_norm(x.transpose(1, 2)).transpose(1, 2)  # BN expects (N, C, L)\n",
    "\n",
    "print(\"Normalization Comparison:\")\n",
    "print(f\"\\nOriginal - Mean: {x.mean():.4f}, Std: {x.std():.4f}\")\n",
    "print(f\"LayerNorm - Mean: {x_ln.mean():.4f}, Std: {x_ln.std():.4f}\")\n",
    "print(f\"BatchNorm - Mean: {x_bn.mean():.4f}, Std: {x_bn.std():.4f}\")\n",
    "\n",
    "# Per-sample statistics (LayerNorm normalizes each sample independently)\n",
    "print(f\"\\nPer-sample mean after LayerNorm (should be ~0):\")\n",
    "for i in range(batch_size):\n",
    "    print(f\"  Sample {i}: {x_ln[i].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb98ff7",
   "metadata": {},
   "source": [
    "## Question 7: What is Causal (Masked) Attention?\n",
    "\n",
    "**Answer:**\n",
    "Causal attention prevents positions from attending to future positions, essential for:\n",
    "- Autoregressive generation\n",
    "- Time series forecasting (no future information leakage)\n",
    "\n",
    "**Implementation:** Apply a triangular mask where future positions have -‚àû before softmax.\n",
    "\n",
    "**Finance Critical:** Prevents look-ahead bias in backtesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Mask Implementation\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create causal mask for self-attention.\"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "seq_len = 10\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Show mask\n",
    "mask_visual = torch.where(causal_mask == float('-inf'), torch.tensor(0.0), torch.tensor(1.0))\n",
    "axes[0].imshow(mask_visual.numpy(), cmap='RdYlGn')\n",
    "axes[0].set_xlabel('Key Position (can attend to)')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "axes[0].set_title('Causal Mask (Green = can attend)')\n",
    "\n",
    "# Show resulting attention pattern\n",
    "Q = K = torch.randn(1, seq_len, 64)\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(64)\n",
    "scores_masked = scores + causal_mask.unsqueeze(0)\n",
    "attn = torch.softmax(scores_masked, dim=-1)\n",
    "\n",
    "axes[1].imshow(attn[0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "axes[1].set_title('Resulting Attention Weights')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Position 0 can only attend to itself\")\n",
    "print(\"Position 5 can attend to positions 0-5\")\n",
    "print(\"This prevents future information leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053692b3",
   "metadata": {},
   "source": [
    "## Question 8: Transformer Complexity - Time and Space?\n",
    "\n",
    "**Answer:**\n",
    "| Component | Time Complexity | Space Complexity |\n",
    "|-----------|-----------------|------------------|\n",
    "| Self-Attention | O(n¬≤d) | O(n¬≤) |\n",
    "| Feed-Forward | O(nd¬≤) | O(d) |\n",
    "| Total per Layer | O(n¬≤d + nd¬≤) | O(n¬≤ + d¬≤) |\n",
    "\n",
    "**Key Insight:** Quadratic in sequence length n is the main bottleneck for long sequences.\n",
    "\n",
    "**Solutions:**\n",
    "- Sparse attention (Longformer, BigBird)\n",
    "- Linear attention approximations\n",
    "- Chunked processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e70003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity Analysis\n",
    "import time\n",
    "\n",
    "def measure_attention_time(seq_lengths, d_model=64, num_heads=8):\n",
    "    \"\"\"Measure attention computation time for different sequence lengths.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        x = torch.randn(1, seq_len, d_model)\n",
    "        \n",
    "        # Warm up\n",
    "        _ = mha(x, x, x)\n",
    "        \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            _ = mha(x, x, x)\n",
    "        elapsed = (time.time() - start) / 10\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return times\n",
    "\n",
    "seq_lengths = [32, 64, 128, 256, 512]\n",
    "times = measure_attention_time(seq_lengths)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(seq_lengths, times, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Sequence Length')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Attention Computation Time')\n",
    "\n",
    "# Theoretical O(n¬≤) curve\n",
    "theoretical = [t * (seq_lengths[0]**2 / s**2) for s, t in zip(seq_lengths, times)]\n",
    "axes[1].plot(seq_lengths, [t/times[0] for t in times], 'bo-', label='Measured', linewidth=2)\n",
    "axes[1].plot(seq_lengths, [(s/seq_lengths[0])**2 for s in seq_lengths], 'r--', label='O(n¬≤) theoretical')\n",
    "axes[1].set_xlabel('Sequence Length')\n",
    "axes[1].set_ylabel('Relative Time')\n",
    "axes[1].set_title('Complexity Scaling')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e155a1",
   "metadata": {},
   "source": [
    "## Question 9: Pre-Norm vs Post-Norm Architecture?\n",
    "\n",
    "**Answer:**\n",
    "- **Post-Norm (Original):** LayerNorm after residual addition\n",
    "  - `x = LayerNorm(x + Sublayer(x))`\n",
    "  - Requires careful learning rate warmup\n",
    "  \n",
    "- **Pre-Norm (Preferred):** LayerNorm before sublayer\n",
    "  - `x = x + Sublayer(LayerNorm(x))`\n",
    "  - More stable training\n",
    "  - Better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4878ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Norm vs Post-Norm Comparison\n",
    "class PreNormEncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer with Pre-LayerNorm (preferred).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm: normalize BEFORE sublayer\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, _ = self.self_attn(x_norm, x_norm, x_norm, attn_mask=mask)\n",
    "        x = x + attn_output  # Residual\n",
    "        \n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + ffn_output  # Residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PostNormEncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder layer with Post-LayerNorm (original).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Post-norm: normalize AFTER residual addition\n",
    "        attn_output, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + attn_output)  # Norm after residual\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)  # Norm after residual\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Pre-Norm Advantages:\")\n",
    "print(\"  ‚úì More stable training\")\n",
    "print(\"  ‚úì Doesn't require learning rate warmup\")\n",
    "print(\"  ‚úì Better gradient flow through residual paths\")\n",
    "print(\"\\nPost-Norm Disadvantages:\")\n",
    "print(\"  ‚úó Can have gradient explosion without warmup\")\n",
    "print(\"  ‚úó Sensitive to initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e700ba",
   "metadata": {},
   "source": [
    "## Question 10: How to Adapt Transformers for Time Series?\n",
    "\n",
    "**Answer:**\n",
    "Key adaptations:\n",
    "1. **Input Embedding:** Linear projection or Conv1D for continuous values\n",
    "2. **Positional Encoding:** Learnable or sinusoidal for temporal order\n",
    "3. **Causal Masking:** Prevent future information leakage\n",
    "4. **Output Head:** Regression head for price prediction\n",
    "5. **Temporal Features:** Add time-based features (day of week, month, etc.)\n",
    "\n",
    "**Finance-Specific:**\n",
    "- Use returns instead of raw prices\n",
    "- Include volume, volatility as features\n",
    "- Consider market regime indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c957a",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Common Mistakes & How to Avoid Them\n",
    "\n",
    "## Mistake 1: Information Leakage (Look-Ahead Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc14ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: No causal mask - can see future\n",
    "class LeakyTransformer(nn.Module):\n",
    "    \"\"\"BAD: This transformer can see the future!\"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)  # No mask!\n",
    "\n",
    "# CORRECT: With causal mask\n",
    "class CausalTransformer(nn.Module):\n",
    "    \"\"\"GOOD: Properly masked transformer.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        mask = mask.to(x.device)\n",
    "        return self.transformer(x, mask=mask)\n",
    "\n",
    "print(\"‚ùå WRONG: Transformer without causal mask can 'cheat' by seeing future values\")\n",
    "print(\"‚úÖ CORRECT: Always use causal mask for time series prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927a0ee",
   "metadata": {},
   "source": [
    "## Mistake 2: Forgetting Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1469cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate importance of positional encoding\n",
    "class TransformerNoPE(nn.Module):\n",
    "    \"\"\"BAD: No positional encoding.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)  # No positional info!\n",
    "        return self.output(x[:, -1])\n",
    "\n",
    "class TransformerWithPE(nn.Module):\n",
    "    \"\"\"GOOD: With positional encoding.\"\"\"\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.output = nn.Linear(d_model, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)  # Add position info!\n",
    "        x = self.transformer(x)\n",
    "        return self.output(x[:, -1])\n",
    "\n",
    "# Test: shuffle input and check if output changes\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, 10, 4)\n",
    "x_shuffled = x[:, torch.randperm(10)]\n",
    "\n",
    "model_no_pe = TransformerNoPE(4, 32, 4, 2)\n",
    "model_with_pe = TransformerWithPE(4, 32, 4, 2)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_no_pe = model_no_pe(x)\n",
    "    out_no_pe_shuffled = model_no_pe(x_shuffled)\n",
    "    \n",
    "    out_with_pe = model_with_pe(x)\n",
    "    out_with_pe_shuffled = model_with_pe(x_shuffled)\n",
    "\n",
    "print(\"Test: Does shuffling input change output?\")\n",
    "print(f\"\\nNo PE - Original: {out_no_pe.item():.4f}, Shuffled: {out_no_pe_shuffled.item():.4f}\")\n",
    "print(f\"  Difference: {abs(out_no_pe.item() - out_no_pe_shuffled.item()):.6f}\")\n",
    "print(f\"\\nWith PE - Original: {out_with_pe.item():.4f}, Shuffled: {out_with_pe_shuffled.item():.4f}\")\n",
    "print(f\"  Difference: {abs(out_with_pe.item() - out_with_pe_shuffled.item()):.6f}\")\n",
    "print(\"\\n‚Üí Without PE, order doesn't matter! This is wrong for time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf83a96",
   "metadata": {},
   "source": [
    "## Mistake 3: Improper Scaling of Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common scaling mistakes\n",
    "print(\"Common Scaling Mistakes in Finance ML:\\n\")\n",
    "\n",
    "# Example data\n",
    "prices = np.array([100, 105, 102, 108, 110, 115, 112, 120])\n",
    "\n",
    "# WRONG: Scaling entire dataset (data leakage)\n",
    "print(\"‚ùå WRONG: Fit scaler on ALL data\")\n",
    "scaler_wrong = MinMaxScaler()\n",
    "prices_wrong = scaler_wrong.fit_transform(prices.reshape(-1, 1))\n",
    "print(f\"   Train sample scaled with future info: {prices_wrong[3, 0]:.4f}\")\n",
    "\n",
    "# CORRECT: Fit only on training data\n",
    "print(\"\\n‚úÖ CORRECT: Fit scaler only on TRAINING data\")\n",
    "train_prices = prices[:5]\n",
    "test_prices = prices[5:]\n",
    "\n",
    "scaler_correct = MinMaxScaler()\n",
    "scaler_correct.fit(train_prices.reshape(-1, 1))\n",
    "train_scaled = scaler_correct.transform(train_prices.reshape(-1, 1))\n",
    "test_scaled = scaler_correct.transform(test_prices.reshape(-1, 1))\n",
    "print(f\"   Train sample (no future info): {train_scaled[3, 0]:.4f}\")\n",
    "\n",
    "# Best practice: Use returns instead of prices\n",
    "print(\"\\n‚úÖ BETTER: Use returns (naturally bounded)\")\n",
    "returns = np.diff(prices) / prices[:-1]\n",
    "print(f\"   Returns: {returns}\")\n",
    "print(f\"   Returns range naturally from -1 to +inf, typically small numbers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1242587e",
   "metadata": {},
   "source": [
    "## Mistake 4: Wrong Attention Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554290b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common dimension mistakes\n",
    "print(\"Dimension Mistakes:\\n\")\n",
    "\n",
    "# WRONG: d_model not divisible by num_heads\n",
    "try:\n",
    "    mha_wrong = nn.MultiheadAttention(embed_dim=65, num_heads=8)\n",
    "    print(\"This would fail...\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Error: embed_dim=65 not divisible by num_heads=8\")\n",
    "\n",
    "# CORRECT: d_model divisible by num_heads\n",
    "mha_correct = nn.MultiheadAttention(embed_dim=64, num_heads=8)\n",
    "print(f\"\\n‚úÖ Correct: embed_dim=64 / num_heads=8 = head_dim=8\")\n",
    "\n",
    "# Common head_dim values\n",
    "print(\"\\nTypical configurations:\")\n",
    "configs = [\n",
    "    (64, 4, 16),\n",
    "    (128, 8, 16),\n",
    "    (256, 8, 32),\n",
    "    (512, 8, 64),\n",
    "    (768, 12, 64),  # BERT-base\n",
    "]\n",
    "for d_model, heads, head_dim in configs:\n",
    "    print(f\"  d_model={d_model}, heads={heads} ‚Üí head_dim={head_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26e805",
   "metadata": {},
   "source": [
    "## Mistake 5: Overfitting Due to Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer capacity vs dataset size\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Different model sizes\n",
    "configs = [\n",
    "    {\"d_model\": 32, \"num_heads\": 4, \"num_layers\": 2, \"d_ff\": 64},\n",
    "    {\"d_model\": 64, \"num_heads\": 8, \"num_layers\": 4, \"d_ff\": 256},\n",
    "    {\"d_model\": 128, \"num_heads\": 8, \"num_layers\": 6, \"d_ff\": 512},\n",
    "    {\"d_model\": 256, \"num_heads\": 8, \"num_layers\": 8, \"d_ff\": 1024},\n",
    "]\n",
    "\n",
    "print(\"Model Size vs Recommended Data Size:\\n\")\n",
    "print(f\"{'Config':<35} {'Params':<12} {'Min Samples':<15}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for cfg in configs:\n",
    "    model = TransformerWithPE(\n",
    "        input_dim=5,\n",
    "        d_model=cfg[\"d_model\"],\n",
    "        num_heads=cfg[\"num_heads\"],\n",
    "        num_layers=cfg[\"num_layers\"]\n",
    "    )\n",
    "    params = count_parameters(model)\n",
    "    # Rule of thumb: 10-100x more samples than parameters for good generalization\n",
    "    min_samples = params * 10\n",
    "    \n",
    "    cfg_str = f\"d={cfg['d_model']}, h={cfg['num_heads']}, L={cfg['num_layers']}\"\n",
    "    print(f\"{cfg_str:<35} {params:>10,} {min_samples:>13,}\")\n",
    "\n",
    "print(\"\\nüí° Tip: Start small! For finance data (~5 years = 1,260 trading days),\")\n",
    "print(\"   use compact transformers (d_model=32-64, layers=2-4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1466db4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Mini-Project - Complete Transformer Stock Predictor\n",
    "\n",
    "Build a production-ready transformer model for stock price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Stock Data\n",
    "print(\"Downloading stock data...\")\n",
    "ticker = \"AAPL\"\n",
    "data = yf.download(ticker, start=\"2019-01-01\", end=\"2024-01-01\", progress=False)\n",
    "prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "print(f\"Downloaded {len(prices)} days of {ticker} data\")\n",
    "print(f\"Date range: {data.index[0].date()} to {data.index[-1].date()}\")\n",
    "print(f\"Price range: ${prices.min():.2f} - ${prices.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def create_features(prices_series, window=20):\n",
    "    \"\"\"Create features from price series.\"\"\"\n",
    "    df = pd.DataFrame({'close': prices_series.flatten()})\n",
    "    \n",
    "    # Returns\n",
    "    df['returns'] = df['close'].pct_change()\n",
    "    \n",
    "    # Moving averages\n",
    "    df['ma_5'] = df['close'].rolling(5).mean() / df['close'] - 1\n",
    "    df['ma_20'] = df['close'].rolling(20).mean() / df['close'] - 1\n",
    "    \n",
    "    # Volatility\n",
    "    df['volatility'] = df['returns'].rolling(window).std()\n",
    "    \n",
    "    # Momentum\n",
    "    df['momentum_5'] = df['close'].pct_change(5)\n",
    "    df['momentum_20'] = df['close'].pct_change(20)\n",
    "    \n",
    "    # Drop NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "df = create_features(prices)\n",
    "print(f\"Features created: {df.columns.tolist()}\")\n",
    "print(f\"Samples after feature engineering: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Sequences\n",
    "def create_sequences(data, target_col, seq_length=30, pred_horizon=1):\n",
    "    \"\"\"Create sequences for transformer input.\"\"\"\n",
    "    features = data.drop(columns=[target_col]).values\n",
    "    target = data[target_col].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(target[i+seq_length+pred_horizon-1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "SEQ_LENGTH = 30\n",
    "PRED_HORIZON = 1\n",
    "\n",
    "X, y = create_sequences(df, target_col='returns', seq_length=SEQ_LENGTH, pred_horizon=PRED_HORIZON)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Train/Val/Test split (time-based)\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features (fit only on training data!)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape for scaling\n",
    "X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler.fit(X_train_flat)  # Only fit on training!\n",
    "\n",
    "# Transform all sets\n",
    "X_train_scaled = scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "X_val_t = torch.FloatTensor(X_val_scaled)\n",
    "y_val_t = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "\n",
    "# DataLoaders\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a58d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Transformer Model for Stock Prediction\n",
    "class StockTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Production-ready Transformer for stock prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Pre-LayerNorm architecture\n",
    "    - Causal masking\n",
    "    - Sinusoidal positional encoding\n",
    "    - Dropout regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        d_model=64,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        d_ff=128,\n",
    "        dropout=0.1,\n",
    "        max_len=500\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer encoder layers (Pre-Norm)\n",
    "        self.layers = nn.ModuleList([\n",
    "            self._make_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _make_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        return nn.ModuleDict({\n",
    "            'attn': nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True),\n",
    "            'ffn': nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_ff, d_model),\n",
    "                nn.Dropout(dropout)\n",
    "            ),\n",
    "            'norm1': nn.LayerNorm(d_model),\n",
    "            'norm2': nn.LayerNorm(d_model)\n",
    "        })\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier uniform.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def _create_causal_mask(self, seq_len, device):\n",
    "        \"\"\"Create causal attention mask.\"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
    "        return mask.bool()\n",
    "    \n",
    "    def forward(self, x, return_attention=False):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = self._create_causal_mask(seq_len, x.device)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        attention_weights = []\n",
    "        \n",
    "        # Transformer layers (Pre-Norm)\n",
    "        for layer in self.layers:\n",
    "            # Self-attention\n",
    "            x_norm = layer['norm1'](x)\n",
    "            attn_out, attn_w = layer['attn'](x_norm, x_norm, x_norm, attn_mask=mask)\n",
    "            x = x + attn_out\n",
    "            attention_weights.append(attn_w)\n",
    "            \n",
    "            # Feed-forward\n",
    "            x_norm = layer['norm2'](x)\n",
    "            x = x + layer['ffn'](x_norm)\n",
    "        \n",
    "        # Final norm\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Output (use last position)\n",
    "        output = self.output_head(x[:, -1])\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "INPUT_DIM = X_train.shape[-1]\n",
    "model = StockTransformer(\n",
    "    input_dim=INPUT_DIM,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    d_ff=128,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model Parameters: {count_parameters(model):,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, actuals = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            actuals.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(loader), np.array(predictions), np.array(actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "EPOCHS = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "print(\"Training Transformer...\")\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<15} {'Val Loss':<15} {'LR':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, _, _ = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_transformer.pth')\n",
    "        patience_counter = 0\n",
    "        marker = \"*\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = \"\"\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"{epoch+1:<8} {train_loss:<15.6f} {val_loss:<15.6f} {current_lr:<12.6f} {marker}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f28ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "axes[1].semilogy(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[1].semilogy(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss (log scale)')\n",
    "axes[1].set_title('Training & Validation Loss (Log Scale)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29cdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model and Evaluate\n",
    "model.load_state_dict(torch.load('best_transformer.pth'))\n",
    "test_loss, predictions, actuals = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "# Direction accuracy\n",
    "pred_direction = (predictions.flatten() > 0).astype(int)\n",
    "actual_direction = (actuals.flatten() > 0).astype(int)\n",
    "direction_accuracy = (pred_direction == actual_direction).mean()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "print(f\"R¬≤:   {r2:.4f}\")\n",
    "print(f\"Direction Accuracy: {direction_accuracy:.2%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37655e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Predictions vs Actuals\n",
    "axes[0, 0].plot(actuals[:100], label='Actual', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].plot(predictions[:100], label='Predicted', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Time Step')\n",
    "axes[0, 0].set_ylabel('Return')\n",
    "axes[0, 0].set_title('Predictions vs Actuals (First 100 samples)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot\n",
    "axes[0, 1].scatter(actuals, predictions, alpha=0.3, s=10)\n",
    "axes[0, 1].plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Actual Return')\n",
    "axes[0, 1].set_ylabel('Predicted Return')\n",
    "axes[0, 1].set_title(f'Scatter Plot (R¬≤ = {r2:.4f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "errors = predictions.flatten() - actuals.flatten()\n",
    "axes[1, 0].hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Prediction Error')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Error Distribution (Mean: {errors.mean():.6f})')\n",
    "\n",
    "# Cumulative return (trading simulation)\n",
    "# Simple strategy: go long when predicted return > 0\n",
    "strategy_returns = np.where(predictions.flatten() > 0, actuals.flatten(), -actuals.flatten())\n",
    "cumulative_strategy = np.cumprod(1 + strategy_returns) - 1\n",
    "cumulative_buy_hold = np.cumprod(1 + actuals.flatten()) - 1\n",
    "\n",
    "axes[1, 1].plot(cumulative_buy_hold * 100, label='Buy & Hold', linewidth=2)\n",
    "axes[1, 1].plot(cumulative_strategy * 100, label='Transformer Strategy', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Time Step')\n",
    "axes[1, 1].set_ylabel('Cumulative Return (%)')\n",
    "axes[1, 1].set_title('Strategy Performance (Test Set)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStrategy Total Return: {cumulative_strategy[-1]*100:.2f}%\")\n",
    "print(f\"Buy & Hold Return: {cumulative_buy_hold[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Attention Patterns\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = X_test_t[:1].to(device)\n",
    "    _, attention_weights = model(sample, return_attention=True)\n",
    "\n",
    "# Plot attention from last layer\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "attn = attention_weights[-1][0].cpu().numpy()  # Last layer, first sample\n",
    "for head in range(min(4, attn.shape[0])):\n",
    "    im = axes[head].imshow(attn[head], cmap='Blues', aspect='auto')\n",
    "    axes[head].set_xlabel('Key Position')\n",
    "    axes[head].set_ylabel('Query Position')\n",
    "    axes[head].set_title(f'Head {head + 1}')\n",
    "    plt.colorbar(im, ax=axes[head])\n",
    "\n",
    "plt.suptitle('Attention Patterns (Last Layer)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAttention Interpretation:\")\n",
    "print(\"- Diagonal patterns: focus on recent positions\")\n",
    "print(\"- Horizontal bands: certain positions attended by all queries\")\n",
    "print(\"- Lower triangle only: causal masking working correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0cde6c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Week 15 Summary\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "### Day 1: Attention Mechanisms\n",
    "- Self-attention fundamentals\n",
    "- Query, Key, Value projections\n",
    "- Scaled dot-product attention\n",
    "\n",
    "### Day 2: Multi-Head Attention\n",
    "- Parallel attention heads\n",
    "- Head concatenation and projection\n",
    "- Different heads learn different patterns\n",
    "\n",
    "### Day 3: Transformer Architecture\n",
    "- Encoder-decoder structure\n",
    "- Positional encoding\n",
    "- Layer normalization and residual connections\n",
    "\n",
    "### Day 4: Transformers for Time Series\n",
    "- Adapting transformers for continuous data\n",
    "- Causal masking for forecasting\n",
    "- Feature engineering for financial data\n",
    "\n",
    "### Day 5: Advanced Topics\n",
    "- Efficient attention variants\n",
    "- Pre-norm vs Post-norm\n",
    "- Hyperparameter tuning\n",
    "\n",
    "### Day 6: Practical Implementation\n",
    "- Training strategies\n",
    "- Regularization techniques\n",
    "- Model evaluation\n",
    "\n",
    "### Day 7: Interview Review\n",
    "- 10 key interview questions\n",
    "- Common mistakes\n",
    "- Complete mini-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0615060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 15 Quick Reference Card\n",
    "reference_card = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                WEEK 15 QUICK REFERENCE CARD                      ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  ATTENTION FORMULA:                                              ‚ïë\n",
    "‚ïë  Attention(Q,K,V) = softmax(QK^T / ‚àöd_k) √ó V                     ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  MULTI-HEAD:                                                     ‚ïë\n",
    "‚ïë  MultiHead = Concat(head_1,...,head_h) √ó W_O                     ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  POSITIONAL ENCODING:                                            ‚ïë\n",
    "‚ïë  PE(pos,2i) = sin(pos/10000^(2i/d))                              ‚ïë\n",
    "‚ïë  PE(pos,2i+1) = cos(pos/10000^(2i/d))                            ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  COMPLEXITY: O(n¬≤d) time, O(n¬≤) space                            ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  KEY HYPERPARAMETERS:                                            ‚ïë\n",
    "‚ïë  ‚Ä¢ d_model: 64-512 (must be divisible by num_heads)              ‚ïë\n",
    "‚ïë  ‚Ä¢ num_heads: 4-16                                               ‚ïë\n",
    "‚ïë  ‚Ä¢ num_layers: 2-12                                              ‚ïë\n",
    "‚ïë  ‚Ä¢ d_ff: 2-4 √ó d_model                                           ‚ïë\n",
    "‚ïë  ‚Ä¢ dropout: 0.1-0.3                                              ‚ïë\n",
    "‚ïë                                                                  ‚ïë\n",
    "‚ïë  FINANCE CHECKLIST:                                              ‚ïë\n",
    "‚ïë  ‚ñ° Use causal masking (no look-ahead)                            ‚ïë\n",
    "‚ïë  ‚ñ° Scale data (fit only on training)                             ‚ïë\n",
    "‚ïë  ‚ñ° Use returns not raw prices                                    ‚ïë\n",
    "‚ïë  ‚ñ° Time-based train/val/test split                               ‚ïë\n",
    "‚ïë  ‚ñ° Include positional encoding                                   ‚ïë\n",
    "‚ïë  ‚ñ° Start with small model, scale up                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "print(reference_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ba429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WEEK 15 COMPLETION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = {\n",
    "    \"Topics Covered\": 7,\n",
    "    \"Interview Questions\": 10,\n",
    "    \"Common Mistakes Addressed\": 5,\n",
    "    \"Model Parameters\": f\"{count_parameters(model):,}\",\n",
    "    \"Training Samples\": len(X_train),\n",
    "    \"Test MSE\": f\"{mse:.6f}\",\n",
    "    \"Test R¬≤\": f\"{r2:.4f}\",\n",
    "    \"Direction Accuracy\": f\"{direction_accuracy:.2%}\"\n",
    "}\n",
    "\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Week 15: Attention & Transformers - COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  ‚Üí Week 16: Reinforcement Learning for Trading\")\n",
    "print(\"  ‚Üí Apply transformers to multi-asset portfolios\")\n",
    "print(\"  ‚Üí Experiment with efficient attention variants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f6b23",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional Resources\n",
    "\n",
    "### Papers\n",
    "1. \"Attention Is All You Need\" - Vaswani et al. (2017)\n",
    "2. \"Temporal Fusion Transformers\" - Lim et al. (2019)\n",
    "3. \"Informer: Beyond Efficient Transformer\" - Zhou et al. (2021)\n",
    "\n",
    "### Books\n",
    "- \"Advances in Financial Machine Learning\" - Marcos L√≥pez de Prado\n",
    "- \"Deep Learning for Finance\" - Jannes Klaas\n",
    "\n",
    "### Practice Problems\n",
    "1. Implement linear attention approximation\n",
    "2. Add learnable positional encoding\n",
    "3. Build cross-attention for multi-asset prediction\n",
    "4. Implement Temporal Fusion Transformer\n",
    "\n",
    "---\n",
    "*Week 15 Day 7 Complete - Interview Review & Week Summary*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
