{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c85a3e2",
   "metadata": {},
   "source": [
    "# Day 6: Transformer Trading System\n",
    "\n",
    "## Week 15: Attention & Transformers in Finance\n",
    "\n",
    "### Learning Objectives\n",
    "- Build a complete transformer-based trading model from scratch\n",
    "- Implement feature engineering specifically designed for transformers\n",
    "- Create a full backtesting framework with realistic assumptions\n",
    "- Analyze trading performance with comprehensive metrics\n",
    "\n",
    "### Topics Covered\n",
    "1. **Transformer Architecture** - Multi-head attention for financial time series\n",
    "2. **Feature Engineering** - Technical indicators, returns, and volatility features\n",
    "3. **Backtesting Engine** - Walk-forward validation with transaction costs\n",
    "4. **Performance Analysis** - Sharpe ratio, drawdowns, and risk metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a05902",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f107d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd521e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download financial data\n",
    "tickers = ['SPY', 'QQQ', 'IWM']  # S&P 500, Nasdaq 100, Russell 2000\n",
    "start_date = '2015-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# Download data\n",
    "data_dict = {}\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    data_dict[ticker] = df['Close']\n",
    "\n",
    "# Combine into single DataFrame\n",
    "prices = pd.DataFrame(data_dict)\n",
    "prices.columns = prices.columns.droplevel(1) if isinstance(prices.columns, pd.MultiIndex) else prices.columns\n",
    "prices = prices.dropna()\n",
    "\n",
    "print(f\"Data shape: {prices.shape}\")\n",
    "print(f\"Date range: {prices.index[0]} to {prices.index[-1]}\")\n",
    "prices.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2d8c9",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Feature Engineering for Transformers\n",
    "\n",
    "Transformers benefit from rich feature sets. We'll create:\n",
    "- **Price-based features**: Returns, log returns\n",
    "- **Technical indicators**: Moving averages, RSI, MACD, Bollinger Bands\n",
    "- **Volatility features**: Rolling std, ATR proxies\n",
    "- **Cross-asset features**: Relative strength, correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe89849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering pipeline for transformer trading models.\n",
    "    Creates technical indicators and derived features from price data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_ticker='SPY'):\n",
    "        self.target_ticker = target_ticker\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def calculate_returns(self, prices, periods=[1, 5, 10, 20]):\n",
    "        \"\"\"Calculate returns over multiple periods.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            for period in periods:\n",
    "                features[f'{ticker}_ret_{period}d'] = prices[ticker].pct_change(period)\n",
    "                features[f'{ticker}_log_ret_{period}d'] = np.log(prices[ticker] / prices[ticker].shift(period))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_moving_averages(self, prices, windows=[5, 10, 20, 50]):\n",
    "        \"\"\"Calculate moving averages and MA crossover signals.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            for window in windows:\n",
    "                ma = prices[ticker].rolling(window).mean()\n",
    "                features[f'{ticker}_ma_{window}'] = (prices[ticker] - ma) / ma  # Normalized distance\n",
    "            \n",
    "            # MA crossover signals\n",
    "            features[f'{ticker}_ma_5_20_cross'] = (\n",
    "                prices[ticker].rolling(5).mean() - prices[ticker].rolling(20).mean()\n",
    "            ) / prices[ticker]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_rsi(self, prices, periods=[14, 28]):\n",
    "        \"\"\"Calculate Relative Strength Index.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            for period in periods:\n",
    "                delta = prices[ticker].diff()\n",
    "                gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "                loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "                rs = gain / loss\n",
    "                rsi = 100 - (100 / (1 + rs))\n",
    "                features[f'{ticker}_rsi_{period}'] = (rsi - 50) / 50  # Normalized to [-1, 1]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_macd(self, prices):\n",
    "        \"\"\"Calculate MACD indicator.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            ema_12 = prices[ticker].ewm(span=12, adjust=False).mean()\n",
    "            ema_26 = prices[ticker].ewm(span=26, adjust=False).mean()\n",
    "            macd = ema_12 - ema_26\n",
    "            signal = macd.ewm(span=9, adjust=False).mean()\n",
    "            \n",
    "            features[f'{ticker}_macd'] = macd / prices[ticker]  # Normalized\n",
    "            features[f'{ticker}_macd_signal'] = (macd - signal) / prices[ticker]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_bollinger_bands(self, prices, window=20, num_std=2):\n",
    "        \"\"\"Calculate Bollinger Bands position.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            ma = prices[ticker].rolling(window).mean()\n",
    "            std = prices[ticker].rolling(window).std()\n",
    "            upper = ma + num_std * std\n",
    "            lower = ma - num_std * std\n",
    "            \n",
    "            # Position within bands (-1 to 1)\n",
    "            features[f'{ticker}_bb_position'] = (prices[ticker] - ma) / (num_std * std)\n",
    "            features[f'{ticker}_bb_width'] = (upper - lower) / ma\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_volatility(self, prices, windows=[5, 10, 20]):\n",
    "        \"\"\"Calculate volatility features.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        for ticker in prices.columns:\n",
    "            returns = prices[ticker].pct_change()\n",
    "            \n",
    "            for window in windows:\n",
    "                features[f'{ticker}_vol_{window}d'] = returns.rolling(window).std() * np.sqrt(252)\n",
    "            \n",
    "            # Volatility ratio (short/long)\n",
    "            features[f'{ticker}_vol_ratio'] = (\n",
    "                returns.rolling(5).std() / returns.rolling(20).std()\n",
    "            )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def calculate_cross_asset_features(self, prices):\n",
    "        \"\"\"Calculate cross-asset relative features.\"\"\"\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "        \n",
    "        if len(prices.columns) > 1:\n",
    "            # Relative strength vs other assets\n",
    "            for i, ticker1 in enumerate(prices.columns):\n",
    "                for ticker2 in prices.columns[i+1:]:\n",
    "                    ratio = prices[ticker1] / prices[ticker2]\n",
    "                    features[f'{ticker1}_{ticker2}_ratio_ret'] = ratio.pct_change(5)\n",
    "            \n",
    "            # Rolling correlation\n",
    "            returns = prices.pct_change()\n",
    "            for i, ticker1 in enumerate(prices.columns):\n",
    "                for ticker2 in prices.columns[i+1:]:\n",
    "                    features[f'{ticker1}_{ticker2}_corr_20'] = (\n",
    "                        returns[ticker1].rolling(20).corr(returns[ticker2])\n",
    "                    )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_target(self, prices, horizon=1, threshold=0.0):\n",
    "        \"\"\"\n",
    "        Create classification target.\n",
    "        1 = positive return (buy signal)\n",
    "        0 = negative return (sell/hold signal)\n",
    "        \"\"\"\n",
    "        future_returns = prices[self.target_ticker].pct_change(horizon).shift(-horizon)\n",
    "        target = (future_returns > threshold).astype(int)\n",
    "        return target\n",
    "    \n",
    "    def build_features(self, prices):\n",
    "        \"\"\"Build complete feature set.\"\"\"\n",
    "        print(\"Building features...\")\n",
    "        \n",
    "        feature_dfs = [\n",
    "            self.calculate_returns(prices),\n",
    "            self.calculate_moving_averages(prices),\n",
    "            self.calculate_rsi(prices),\n",
    "            self.calculate_macd(prices),\n",
    "            self.calculate_bollinger_bands(prices),\n",
    "            self.calculate_volatility(prices),\n",
    "            self.calculate_cross_asset_features(prices)\n",
    "        ]\n",
    "        \n",
    "        features = pd.concat(feature_dfs, axis=1)\n",
    "        self.feature_names = features.columns.tolist()\n",
    "        \n",
    "        print(f\"Created {len(self.feature_names)} features\")\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721064fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "fe = FeatureEngineer(target_ticker='SPY')\n",
    "features = fe.build_features(prices)\n",
    "target = fe.create_target(prices, horizon=1)\n",
    "\n",
    "# Combine and clean data\n",
    "data = features.copy()\n",
    "data['target'] = target\n",
    "data = data.dropna()\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {data.shape}\")\n",
    "print(f\"Target distribution:\\n{data['target'].value_counts(normalize=True)}\")\n",
    "\n",
    "# Display sample features\n",
    "print(f\"\\nSample features:\")\n",
    "data.iloc[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487edc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Select subset of features for visualization\n",
    "spy_features = [col for col in data.columns if col.startswith('SPY_') and not col.endswith('target')][:12]\n",
    "\n",
    "# Correlation heatmap\n",
    "corr_matrix = data[spy_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.2f', ax=axes[0], annot_kws={'size': 8})\n",
    "axes[0].set_title('Feature Correlation Matrix (SPY Features)', fontsize=12)\n",
    "\n",
    "# Feature importance (correlation with target)\n",
    "target_corr = data.drop('target', axis=1).corrwith(data['target']).sort_values()\n",
    "top_features = pd.concat([target_corr.head(10), target_corr.tail(10)])\n",
    "colors = ['red' if x < 0 else 'green' for x in top_features.values]\n",
    "axes[1].barh(range(len(top_features)), top_features.values, color=colors)\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels(top_features.index, fontsize=8)\n",
    "axes[1].set_xlabel('Correlation with Target')\n",
    "axes[1].set_title('Top 20 Features by Target Correlation')\n",
    "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09713ca0",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Transformer Model Architecture\n",
    "\n",
    "We'll build a custom Transformer model optimized for financial time series:\n",
    "- **Positional Encoding**: Inject temporal information\n",
    "- **Multi-Head Self-Attention**: Capture complex dependencies\n",
    "- **Feed-Forward Networks**: Learn non-linear transformations\n",
    "- **Classification Head**: Output trading signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for sequence position information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=500, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Add positional encoding to input.\"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder block with multi-head attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward pass with residual connections.\"\"\"\n",
    "        # Self-attention with residual\n",
    "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class TransformerTradingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model for trading signal prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input projection layer\n",
    "    2. Positional encoding\n",
    "    3. Stack of transformer blocks\n",
    "    4. Global average pooling\n",
    "    5. Classification head\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, d_model=64, n_heads=4, n_layers=3, \n",
    "                 d_ff=256, dropout=0.1, seq_len=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len, dropout=dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Store attention weights for analysis\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Output probabilities of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        attention_weights_list = []\n",
    "        for block in self.transformer_blocks:\n",
    "            x, attn_weights = block(x)\n",
    "            attention_weights_list.append(attn_weights)\n",
    "        \n",
    "        self.attention_weights = attention_weights_list\n",
    "        \n",
    "        # Global average pooling over sequence dimension\n",
    "        x = x.mean(dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        \"\"\"Return stored attention weights.\"\"\"\n",
    "        return self.attention_weights\n",
    "\n",
    "\n",
    "# Test model architecture\n",
    "test_model = TransformerTradingModel(\n",
    "    input_dim=len(fe.feature_names),\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=3,\n",
    "    seq_len=20\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in test_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  Input dimension: {len(fe.feature_names)}\")\n",
    "print(f\"  Model dimension: 64\")\n",
    "print(f\"  Attention heads: 4\")\n",
    "print(f\"  Transformer layers: 3\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f74afe1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf83ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for trading data with sequence windowing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, targets, seq_len=20):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        X = self.features[idx:idx + self.seq_len]\n",
    "        # Target is the label at the end of the sequence\n",
    "        y = self.targets[idx + self.seq_len - 1]\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def prepare_data(data, train_ratio=0.7, val_ratio=0.15, seq_len=20):\n",
    "    \"\"\"\n",
    "    Prepare data with walk-forward split (no look-ahead bias).\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    feature_cols = [col for col in data.columns if col != 'target']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['target'].values\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data chronologically\n",
    "    X_train, y_train = X[:train_end], y[:train_end]\n",
    "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "    X_test, y_test = X[val_end:], y[val_end:]\n",
    "    \n",
    "    # Normalize features (fit only on training data)\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Handle any remaining NaN/Inf values\n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TradingDataset(X_train, y_train, seq_len)\n",
    "    val_dataset = TradingDataset(X_val, y_val, seq_len)\n",
    "    test_dataset = TradingDataset(X_test, y_test, seq_len)\n",
    "    \n",
    "    # Store dates for backtest\n",
    "    dates = {\n",
    "        'train': data.index[:train_end],\n",
    "        'val': data.index[train_end:val_end],\n",
    "        'test': data.index[val_end:]\n",
    "    }\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, scaler, dates, feature_cols\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "seq_len = 20\n",
    "train_dataset, val_dataset, test_dataset, scaler, dates, feature_cols = prepare_data(\n",
    "    data, train_ratio=0.7, val_ratio=0.15, seq_len=seq_len\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Training: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "print(f\"\\nSequence length: {seq_len}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {dates['train'][0]} to {dates['train'][-1]}\")\n",
    "print(f\"  Val: {dates['val'][0]} to {dates['val'][-1]}\")\n",
    "print(f\"  Test: {dates['test'][0]} to {dates['test'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa62fd6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ac6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    \"\"\"\n",
    "    Training class for the Transformer trading model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, learning_rate=1e-4, weight_decay=1e-5):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function with class weights for imbalanced data\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "        # Optimizer with weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_acc': [], 'val_acc': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X).squeeze()\n",
    "            loss = self.criterion(outputs, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        return total_loss / total, correct / total\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(X).squeeze()\n",
    "                loss = self.criterion(outputs, y)\n",
    "                \n",
    "                total_loss += loss.item() * X.size(0)\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                correct += (predictions == y).sum().item()\n",
    "                total += y.size(0)\n",
    "        \n",
    "        return total_loss / total, correct / total\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=50, early_stopping_patience=10):\n",
    "        \"\"\"Full training loop with early stopping.\"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        print(f\"Starting training for {epochs} epochs...\\n\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = self.model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "            print(f\"\\nRestored best model with validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = TransformerTradingModel(\n",
    "    input_dim=len(feature_cols),\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.2,\n",
    "    seq_len=seq_len\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = TransformerTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=50,\n",
    "    early_stopping_patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].axhline(y=0.5, color='r', linestyle='--', label='Random Baseline')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68755876",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Backtesting Engine\n",
    "\n",
    "Implement a realistic backtesting framework with:\n",
    "- Transaction costs\n",
    "- Position sizing\n",
    "- Walk-forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtester:\n",
    "    \"\"\"\n",
    "    Backtesting engine for trading strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, transaction_cost=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Trained transformer model\n",
    "            device: PyTorch device\n",
    "            transaction_cost: Round-trip transaction cost (default 0.1%)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.transaction_cost = transaction_cost\n",
    "    \n",
    "    def generate_signals(self, data_loader, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Generate trading signals from model predictions.\n",
    "        \n",
    "        Returns:\n",
    "            signals: 1 for long, 0 for flat\n",
    "            probabilities: Raw model output probabilities\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        all_probs = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in data_loader:\n",
    "                X = X.to(self.device)\n",
    "                probs = self.model(X).squeeze().cpu().numpy()\n",
    "                all_probs.extend(probs if len(probs.shape) > 0 else [probs])\n",
    "                all_targets.extend(y.numpy())\n",
    "        \n",
    "        probabilities = np.array(all_probs)\n",
    "        targets = np.array(all_targets)\n",
    "        signals = (probabilities > threshold).astype(int)\n",
    "        \n",
    "        return signals, probabilities, targets\n",
    "    \n",
    "    def calculate_returns(self, prices, signals):\n",
    "        \"\"\"\n",
    "        Calculate strategy returns with transaction costs.\n",
    "        \n",
    "        Args:\n",
    "            prices: Price series\n",
    "            signals: Trading signals (1 for long, 0 for flat)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with strategy metrics\n",
    "        \"\"\"\n",
    "        # Calculate market returns\n",
    "        returns = prices.pct_change().fillna(0)\n",
    "        \n",
    "        # Align signals with returns (signal at t determines position at t+1)\n",
    "        positions = pd.Series(signals, index=returns.index[:len(signals)])\n",
    "        positions = positions.shift(1).fillna(0)  # Avoid look-ahead bias\n",
    "        \n",
    "        # Calculate position changes for transaction costs\n",
    "        position_changes = positions.diff().abs().fillna(0)\n",
    "        transaction_costs = position_changes * self.transaction_cost\n",
    "        \n",
    "        # Strategy returns\n",
    "        strategy_returns = (positions * returns.iloc[:len(positions)]) - transaction_costs\n",
    "        \n",
    "        # Buy and hold returns\n",
    "        buy_hold_returns = returns.iloc[:len(positions)]\n",
    "        \n",
    "        # Cumulative returns\n",
    "        strategy_cumulative = (1 + strategy_returns).cumprod()\n",
    "        buy_hold_cumulative = (1 + buy_hold_returns).cumprod()\n",
    "        \n",
    "        return {\n",
    "            'strategy_returns': strategy_returns,\n",
    "            'buy_hold_returns': buy_hold_returns,\n",
    "            'strategy_cumulative': strategy_cumulative,\n",
    "            'buy_hold_cumulative': buy_hold_cumulative,\n",
    "            'positions': positions,\n",
    "            'transaction_costs': transaction_costs.sum()\n",
    "        }\n",
    "    \n",
    "    def calculate_metrics(self, results):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive performance metrics.\n",
    "        \"\"\"\n",
    "        strategy_returns = results['strategy_returns']\n",
    "        buy_hold_returns = results['buy_hold_returns']\n",
    "        \n",
    "        def calc_metrics(returns, name):\n",
    "            total_return = (1 + returns).prod() - 1\n",
    "            annual_return = (1 + total_return) ** (252 / len(returns)) - 1\n",
    "            annual_vol = returns.std() * np.sqrt(252)\n",
    "            sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "            \n",
    "            # Maximum drawdown\n",
    "            cumulative = (1 + returns).cumprod()\n",
    "            rolling_max = cumulative.expanding().max()\n",
    "            drawdowns = cumulative / rolling_max - 1\n",
    "            max_drawdown = drawdowns.min()\n",
    "            \n",
    "            # Sortino ratio\n",
    "            downside_returns = returns[returns < 0]\n",
    "            downside_std = downside_returns.std() * np.sqrt(252)\n",
    "            sortino = annual_return / downside_std if downside_std > 0 else 0\n",
    "            \n",
    "            # Calmar ratio\n",
    "            calmar = annual_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "            \n",
    "            # Win rate\n",
    "            win_rate = (returns > 0).sum() / len(returns)\n",
    "            \n",
    "            return {\n",
    "                f'{name}_total_return': total_return,\n",
    "                f'{name}_annual_return': annual_return,\n",
    "                f'{name}_annual_vol': annual_vol,\n",
    "                f'{name}_sharpe': sharpe,\n",
    "                f'{name}_sortino': sortino,\n",
    "                f'{name}_calmar': calmar,\n",
    "                f'{name}_max_drawdown': max_drawdown,\n",
    "                f'{name}_win_rate': win_rate\n",
    "            }\n",
    "        \n",
    "        strategy_metrics = calc_metrics(strategy_returns, 'strategy')\n",
    "        benchmark_metrics = calc_metrics(buy_hold_returns, 'benchmark')\n",
    "        \n",
    "        # Additional metrics\n",
    "        positions = results['positions']\n",
    "        additional = {\n",
    "            'avg_position': positions.mean(),\n",
    "            'num_trades': (positions.diff().abs() > 0).sum(),\n",
    "            'total_transaction_costs': results['transaction_costs'],\n",
    "            'time_in_market': (positions > 0).sum() / len(positions)\n",
    "        }\n",
    "        \n",
    "        return {**strategy_metrics, **benchmark_metrics, **additional}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a98fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest on test set\n",
    "backtester = Backtester(model, device, transaction_cost=0.001)\n",
    "\n",
    "# Generate signals\n",
    "signals, probabilities, targets = backtester.generate_signals(test_loader, threshold=0.5)\n",
    "\n",
    "# Get test period prices\n",
    "test_dates = dates['test'][seq_len:]  # Adjust for sequence length\n",
    "test_prices = prices.loc[test_dates[:len(signals)], 'SPY']\n",
    "\n",
    "# Calculate returns\n",
    "results = backtester.calculate_returns(test_prices, signals)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = backtester.calculate_metrics(results)\n",
    "\n",
    "# Display metrics\n",
    "print(\"=\"*60)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Period: {test_dates[0]} to {test_dates[-1]}\")\n",
    "print(f\"Number of trading days: {len(test_prices)}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Strategy Performance:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Total Return:      {metrics['strategy_total_return']*100:>10.2f}%\")\n",
    "print(f\"  Annual Return:     {metrics['strategy_annual_return']*100:>10.2f}%\")\n",
    "print(f\"  Annual Volatility: {metrics['strategy_annual_vol']*100:>10.2f}%\")\n",
    "print(f\"  Sharpe Ratio:      {metrics['strategy_sharpe']:>10.3f}\")\n",
    "print(f\"  Sortino Ratio:     {metrics['strategy_sortino']:>10.3f}\")\n",
    "print(f\"  Calmar Ratio:      {metrics['strategy_calmar']:>10.3f}\")\n",
    "print(f\"  Max Drawdown:      {metrics['strategy_max_drawdown']*100:>10.2f}%\")\n",
    "print(f\"  Win Rate:          {metrics['strategy_win_rate']*100:>10.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Benchmark (Buy & Hold) Performance:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Total Return:      {metrics['benchmark_total_return']*100:>10.2f}%\")\n",
    "print(f\"  Annual Return:     {metrics['benchmark_annual_return']*100:>10.2f}%\")\n",
    "print(f\"  Sharpe Ratio:      {metrics['benchmark_sharpe']:>10.3f}\")\n",
    "print(f\"  Max Drawdown:      {metrics['benchmark_max_drawdown']*100:>10.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Trading Statistics:\")\n",
    "print(\"-\"*60)\n",
    "print(f\"  Number of Trades:  {metrics['num_trades']:>10.0f}\")\n",
    "print(f\"  Avg Position:      {metrics['avg_position']*100:>10.2f}%\")\n",
    "print(f\"  Time in Market:    {metrics['time_in_market']*100:>10.2f}%\")\n",
    "print(f\"  Transaction Costs: {metrics['total_transaction_costs']*100:>10.4f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec032429",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Performance Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15acfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Cumulative Returns\n",
    "ax1 = axes[0, 0]\n",
    "results['strategy_cumulative'].plot(ax=ax1, label='Transformer Strategy', linewidth=2, color='blue')\n",
    "results['buy_hold_cumulative'].plot(ax=ax1, label='Buy & Hold', linewidth=2, color='gray', alpha=0.7)\n",
    "ax1.fill_between(results['strategy_cumulative'].index, 1, results['strategy_cumulative'], \n",
    "                 where=results['strategy_cumulative'] > 1, alpha=0.3, color='green')\n",
    "ax1.fill_between(results['strategy_cumulative'].index, 1, results['strategy_cumulative'], \n",
    "                 where=results['strategy_cumulative'] < 1, alpha=0.3, color='red')\n",
    "ax1.axhline(y=1, color='black', linestyle='--', linewidth=0.5)\n",
    "ax1.set_title('Cumulative Returns', fontsize=14)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Cumulative Return')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Drawdown Analysis\n",
    "ax2 = axes[0, 1]\n",
    "strategy_cumulative = results['strategy_cumulative']\n",
    "rolling_max = strategy_cumulative.expanding().max()\n",
    "drawdowns = strategy_cumulative / rolling_max - 1\n",
    "\n",
    "ax2.fill_between(drawdowns.index, 0, drawdowns * 100, color='red', alpha=0.6)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax2.set_title('Strategy Drawdown', fontsize=14)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Drawdown (%)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Signal Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(probabilities, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax3.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "ax3.set_title('Prediction Probability Distribution', fontsize=14)\n",
    "ax3.set_xlabel('Predicted Probability')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Monthly Returns Heatmap\n",
    "ax4 = axes[1, 1]\n",
    "monthly_returns = results['strategy_returns'].copy()\n",
    "monthly_returns.index = pd.to_datetime(monthly_returns.index)\n",
    "monthly_returns = monthly_returns.resample('M').sum() * 100\n",
    "\n",
    "# Create year-month pivot\n",
    "monthly_df = pd.DataFrame({\n",
    "    'Year': monthly_returns.index.year,\n",
    "    'Month': monthly_returns.index.month,\n",
    "    'Return': monthly_returns.values\n",
    "})\n",
    "monthly_pivot = monthly_df.pivot(index='Year', columns='Month', values='Return')\n",
    "\n",
    "sns.heatmap(monthly_pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=0, \n",
    "            ax=ax4, annot_kws={'size': 8}, cbar_kws={'label': 'Return (%)'})\n",
    "ax4.set_title('Monthly Returns Heatmap (%)', fontsize=14)\n",
    "ax4.set_xlabel('Month')\n",
    "ax4.set_ylabel('Year')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification performance analysis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Confusion Matrix Style Analysis\n",
    "ax1 = axes[0]\n",
    "predictions = (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(targets, predictions)\n",
    "precision = precision_score(targets, predictions, zero_division=0)\n",
    "recall = recall_score(targets, predictions, zero_division=0)\n",
    "f1 = f1_score(targets, predictions, zero_division=0)\n",
    "\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "metrics_values = [accuracy, precision, recall, f1]\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(metrics_names)))\n",
    "\n",
    "bars = ax1.bar(metrics_names, metrics_values, color=colors, edgecolor='black')\n",
    "ax1.axhline(y=0.5, color='red', linestyle='--', label='Random Baseline')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('Classification Metrics', fontsize=14)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. Rolling Accuracy\n",
    "ax2 = axes[1]\n",
    "correct = (predictions == targets).astype(int)\n",
    "rolling_accuracy = pd.Series(correct).rolling(window=50).mean()\n",
    "\n",
    "ax2.plot(rolling_accuracy, linewidth=2, color='blue')\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', label='Random Baseline')\n",
    "ax2.fill_between(range(len(rolling_accuracy)), 0.5, rolling_accuracy, \n",
    "                 where=rolling_accuracy > 0.5, alpha=0.3, color='green')\n",
    "ax2.fill_between(range(len(rolling_accuracy)), 0.5, rolling_accuracy, \n",
    "                 where=rolling_accuracy < 0.5, alpha=0.3, color='red')\n",
    "ax2.set_title('Rolling Accuracy (50-day window)', fontsize=14)\n",
    "ax2.set_xlabel('Trading Days')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Prediction vs Actual Returns\n",
    "ax3 = axes[2]\n",
    "actual_returns = test_prices.pct_change().dropna().values[:len(probabilities)]\n",
    "\n",
    "# Color points by prediction accuracy\n",
    "colors = ['green' if (p > 0.5 and r > 0) or (p <= 0.5 and r <= 0) else 'red' \n",
    "          for p, r in zip(probabilities, actual_returns)]\n",
    "\n",
    "ax3.scatter(probabilities, actual_returns * 100, c=colors, alpha=0.5, s=10)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.axvline(x=0.5, color='black', linestyle='-', linewidth=0.5)\n",
    "ax3.set_title('Prediction Probability vs Actual Return', fontsize=14)\n",
    "ax3.set_xlabel('Predicted Probability (Long)')\n",
    "ax3.set_ylabel('Actual Return (%)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax3.text(0.75, ax3.get_ylim()[1]*0.8, 'True Positive', ha='center', fontsize=10, color='green')\n",
    "ax3.text(0.25, ax3.get_ylim()[1]*0.8, 'False Negative', ha='center', fontsize=10, color='red')\n",
    "ax3.text(0.25, ax3.get_ylim()[0]*0.8, 'True Negative', ha='center', fontsize=10, color='green')\n",
    "ax3.text(0.75, ax3.get_ylim()[0]*0.8, 'False Positive', ha='center', fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClassification Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76302a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention weights visualization\n",
    "def visualize_attention(model, data_loader, device, sample_idx=0):\n",
    "    \"\"\"\n",
    "    Visualize attention weights for a sample.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a sample batch\n",
    "    for X, y in data_loader:\n",
    "        X = X.to(device)\n",
    "        _ = model(X)\n",
    "        break\n",
    "    \n",
    "    # Get attention weights from last layer\n",
    "    attention_weights = model.get_attention_weights()\n",
    "    if attention_weights is None:\n",
    "        print(\"No attention weights available\")\n",
    "        return\n",
    "    \n",
    "    # Take attention from the last transformer block\n",
    "    last_attn = attention_weights[-1][sample_idx].detach().cpu().numpy()\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(last_attn, cmap='Blues', ax=ax)\n",
    "    ax.set_title('Attention Weights (Last Transformer Layer)', fontsize=14)\n",
    "    ax.set_xlabel('Key Position (Past Time Steps)')\n",
    "    ax.set_ylabel('Query Position (Time Steps)')\n",
    "    \n",
    "    # Add time labels\n",
    "    time_labels = [f't-{seq_len-1-i}' for i in range(seq_len)]\n",
    "    ax.set_xticklabels(time_labels, rotation=45)\n",
    "    ax.set_yticklabels(time_labels, rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show which time steps get the most attention\n",
    "    avg_attention = last_attn.mean(axis=0)\n",
    "    print(\"\\nAverage attention by time step (most recent = t-0):\")\n",
    "    for i, att in enumerate(avg_attention):\n",
    "        print(f\"  t-{seq_len-1-i}: {att:.4f}\")\n",
    "\n",
    "# Visualize attention\n",
    "visualize_attention(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ed2f6",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_risk(results, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Perform comprehensive risk analysis.\n",
    "    \"\"\"\n",
    "    strategy_returns = results['strategy_returns']\n",
    "    \n",
    "    # Value at Risk (VaR)\n",
    "    var_95 = np.percentile(strategy_returns, (1 - confidence_level) * 100)\n",
    "    var_99 = np.percentile(strategy_returns, 1)\n",
    "    \n",
    "    # Conditional VaR (Expected Shortfall)\n",
    "    cvar_95 = strategy_returns[strategy_returns <= var_95].mean()\n",
    "    cvar_99 = strategy_returns[strategy_returns <= var_99].mean()\n",
    "    \n",
    "    # Tail ratio\n",
    "    right_tail = np.percentile(strategy_returns, 95)\n",
    "    left_tail = abs(np.percentile(strategy_returns, 5))\n",
    "    tail_ratio = right_tail / left_tail if left_tail != 0 else np.inf\n",
    "    \n",
    "    # Skewness and Kurtosis\n",
    "    skewness = strategy_returns.skew()\n",
    "    kurtosis = strategy_returns.kurtosis()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"RISK ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nValue at Risk (VaR):\")\n",
    "    print(f\"  VaR 95%: {var_95*100:>10.4f}%\")\n",
    "    print(f\"  VaR 99%: {var_99*100:>10.4f}%\")\n",
    "    print(f\"\\nConditional VaR (Expected Shortfall):\")\n",
    "    print(f\"  CVaR 95%: {cvar_95*100:>10.4f}%\")\n",
    "    print(f\"  CVaR 99%: {cvar_99*100:>10.4f}%\")\n",
    "    print(f\"\\nDistribution Statistics:\")\n",
    "    print(f\"  Skewness: {skewness:>10.4f}\")\n",
    "    print(f\"  Kurtosis: {kurtosis:>10.4f}\")\n",
    "    print(f\"  Tail Ratio: {tail_ratio:>10.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return {\n",
    "        'var_95': var_95, 'var_99': var_99,\n",
    "        'cvar_95': cvar_95, 'cvar_99': cvar_99,\n",
    "        'skewness': skewness, 'kurtosis': kurtosis,\n",
    "        'tail_ratio': tail_ratio\n",
    "    }\n",
    "\n",
    "# Perform risk analysis\n",
    "risk_metrics = analyze_risk(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Return Distribution with VaR\n",
    "ax1 = axes[0]\n",
    "strategy_returns = results['strategy_returns']\n",
    "\n",
    "ax1.hist(strategy_returns * 100, bins=50, density=True, alpha=0.7, \n",
    "         color='steelblue', edgecolor='black', label='Strategy Returns')\n",
    "\n",
    "# VaR lines\n",
    "var_95 = risk_metrics['var_95'] * 100\n",
    "var_99 = risk_metrics['var_99'] * 100\n",
    "ax1.axvline(x=var_95, color='orange', linestyle='--', linewidth=2, label=f'VaR 95% ({var_95:.2f}%)')\n",
    "ax1.axvline(x=var_99, color='red', linestyle='--', linewidth=2, label=f'VaR 99% ({var_99:.2f}%)')\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax1.set_title('Return Distribution with Value at Risk', fontsize=14)\n",
    "ax1.set_xlabel('Daily Return (%)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Rolling Risk Metrics\n",
    "ax2 = axes[1]\n",
    "rolling_window = 50\n",
    "\n",
    "rolling_vol = strategy_returns.rolling(rolling_window).std() * np.sqrt(252) * 100\n",
    "rolling_sharpe = (\n",
    "    strategy_returns.rolling(rolling_window).mean() * 252 / \n",
    "    (strategy_returns.rolling(rolling_window).std() * np.sqrt(252))\n",
    ")\n",
    "\n",
    "ax2.plot(rolling_vol.index, rolling_vol, label='Rolling Volatility (%)', linewidth=2, color='blue')\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(rolling_sharpe.index, rolling_sharpe, label='Rolling Sharpe', linewidth=2, color='green', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Annualized Volatility (%)', color='blue')\n",
    "ax2_twin.set_ylabel('Sharpe Ratio', color='green')\n",
    "ax2.set_title(f'Rolling Risk Metrics ({rolling_window}-day window)', fontsize=14)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08453afe",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Summary & Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "1. **Complete Transformer Trading System**: End-to-end pipeline from data to trading signals\n",
    "2. **Rich Feature Engineering**: Technical indicators, volatility, cross-asset features\n",
    "3. **Custom Transformer Architecture**: Multi-head attention for financial time series\n",
    "4. **Realistic Backtesting**: Walk-forward validation with transaction costs\n",
    "5. **Comprehensive Analysis**: Performance metrics, risk analysis, attention visualization\n",
    "\n",
    "### Key Insights\n",
    "- **Transformers can capture complex temporal dependencies** in financial data\n",
    "- **Feature engineering remains crucial** even with attention mechanisms\n",
    "- **Walk-forward validation prevents look-ahead bias**\n",
    "- **Transaction costs significantly impact strategy performance**\n",
    "- **Attention weights reveal which time steps the model focuses on**\n",
    "\n",
    "### Future Improvements\n",
    "- Add more sophisticated position sizing (Kelly criterion, risk parity)\n",
    "- Implement ensemble methods with multiple transformers\n",
    "- Include regime detection for adaptive strategies\n",
    "- Add alternative data sources (sentiment, fundamentals)\n",
    "- Implement online learning for model adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"TRANSFORMER TRADING SYSTEM - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n DATA\")\n",
    "print(f\"   Assets: {tickers}\")\n",
    "print(f\"   Period: {start_date} to {end_date}\")\n",
    "print(f\"   Features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n MODEL ARCHITECTURE\")\n",
    "print(f\"   Type: Transformer Encoder\")\n",
    "print(f\"   Layers: 3\")\n",
    "print(f\"   Attention Heads: 4\")\n",
    "print(f\"   Model Dimension: 64\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\n STRATEGY PERFORMANCE (Test Period)\")\n",
    "print(f\"   Total Return: {metrics['strategy_total_return']*100:.2f}%\")\n",
    "print(f\"   Sharpe Ratio: {metrics['strategy_sharpe']:.3f}\")\n",
    "print(f\"   Max Drawdown: {metrics['strategy_max_drawdown']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\n BENCHMARK (Buy & Hold)\")\n",
    "print(f\"   Total Return: {metrics['benchmark_total_return']*100:.2f}%\")\n",
    "print(f\"   Sharpe Ratio: {metrics['benchmark_sharpe']:.3f}\")\n",
    "\n",
    "print(f\"\\n RISK METRICS\")\n",
    "print(f\"   VaR 95%: {risk_metrics['var_95']*100:.4f}%\")\n",
    "print(f\"   CVaR 95%: {risk_metrics['cvar_95']*100:.4f}%\")\n",
    "\n",
    "print(f\"\\n MODEL ACCURACY\")\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Notebook completed successfully!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
