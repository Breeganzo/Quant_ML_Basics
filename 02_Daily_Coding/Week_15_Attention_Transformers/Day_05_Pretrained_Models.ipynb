{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea57c40",
   "metadata": {},
   "source": [
    "# Week 15 - Day 5: Pre-trained Models for Finance\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand transfer learning concepts for financial applications\n",
    "- Learn to use FinBERT for financial sentiment analysis\n",
    "- Implement practical sentiment analysis on financial texts\n",
    "- Extract and utilize embeddings for downstream tasks\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Transfer Learning for Finance](#1-transfer-learning-for-finance)\n",
    "2. [FinBERT Introduction](#2-finbert-introduction)\n",
    "3. [Sentiment Analysis Practical](#3-sentiment-analysis-practical)\n",
    "4. [Using Embeddings](#4-using-embeddings)\n",
    "5. [Trading Signal Generation](#5-trading-signal-generation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers torch pandas numpy scikit-learn matplotlib seaborn yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232266bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7be8b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Transfer Learning for Finance\n",
    "\n",
    "### What is Transfer Learning?\n",
    "\n",
    "Transfer learning is a machine learning technique where a model trained on one task is reused as the starting point for a model on a different task. In NLP, this typically means:\n",
    "\n",
    "1. **Pre-training**: A large language model is trained on massive amounts of text data\n",
    "2. **Fine-tuning**: The pre-trained model is adapted to a specific downstream task\n",
    "\n",
    "### Why Transfer Learning for Finance?\n",
    "\n",
    "| Challenge | Solution via Transfer Learning |\n",
    "|-----------|-------------------------------|\n",
    "| Limited labeled financial data | Leverage knowledge from general text |\n",
    "| Domain-specific vocabulary | Use finance-specific pre-trained models |\n",
    "| Expensive annotation | Reduce need for large labeled datasets |\n",
    "| Complex financial context | Utilize contextual embeddings |\n",
    "\n",
    "### Pre-trained Models Landscape\n",
    "\n",
    "```\n",
    "General Models          Finance-Specific Models\n",
    "â”œâ”€â”€ BERT               â”œâ”€â”€ FinBERT\n",
    "â”œâ”€â”€ RoBERTa            â”œâ”€â”€ FinancialBERT\n",
    "â”œâ”€â”€ DistilBERT         â”œâ”€â”€ BloombergGPT\n",
    "â”œâ”€â”€ GPT-2/3            â””â”€â”€ SEC-BERT\n",
    "â””â”€â”€ XLNet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the concept of transfer learning\n",
    "# Compare general BERT vs FinBERT representations\n",
    "\n",
    "class TransferLearningDemo:\n",
    "    \"\"\"\n",
    "    Demonstrates why domain-specific pre-training matters.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.financial_terms = [\n",
    "            \"bull market\",      # Positive market sentiment\n",
    "            \"bear market\",      # Negative market sentiment  \n",
    "            \"puts\",             # Options terminology\n",
    "            \"calls\",            # Options terminology\n",
    "            \"short squeeze\",    # Trading phenomenon\n",
    "            \"dividend yield\",   # Investment metric\n",
    "            \"market cap\",       # Company valuation\n",
    "            \"P/E ratio\",        # Valuation metric\n",
    "        ]\n",
    "        \n",
    "        self.financial_sentences = [\n",
    "            \"The Fed raised interest rates by 25 basis points.\",\n",
    "            \"Apple reported earnings beat with strong iPhone sales.\",\n",
    "            \"The yield curve inverted, signaling recession fears.\",\n",
    "            \"Short sellers are covering their positions aggressively.\",\n",
    "            \"The stock is trading at a significant discount to NAV.\",\n",
    "        ]\n",
    "    \n",
    "    def explain_transfer_learning(self):\n",
    "        \"\"\"Visual explanation of transfer learning concept.\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"TRANSFER LEARNING IN FINANCE\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        print(\"Stage 1: PRE-TRAINING (done by researchers)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"ðŸ“š General BERT: Trained on Wikipedia + BookCorpus\")\n",
    "        print(\"   â†’ Learns general language understanding\")\n",
    "        print()\n",
    "        print(\"ðŸ“ˆ FinBERT: Further trained on financial texts\")\n",
    "        print(\"   â†’ Financial news, SEC filings, analyst reports\")\n",
    "        print(\"   â†’ Learns financial domain knowledge\")\n",
    "        print()\n",
    "        print(\"Stage 2: FINE-TUNING (what we do)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"ðŸŽ¯ Task-specific adaptation:\")\n",
    "        print(\"   â†’ Sentiment analysis\")\n",
    "        print(\"   â†’ Named entity recognition\")\n",
    "        print(\"   â†’ Question answering\")\n",
    "        print(\"   â†’ Trading signal generation\")\n",
    "        print()\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return self.financial_terms, self.financial_sentences\n",
    "\n",
    "# Run demonstration\n",
    "demo = TransferLearningDemo()\n",
    "terms, sentences = demo.explain_transfer_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9674a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. FinBERT Introduction\n",
    "\n",
    "### What is FinBERT?\n",
    "\n",
    "**FinBERT** is a BERT model pre-trained on financial communication text. There are several variants:\n",
    "\n",
    "1. **ProsusAI/finbert** - Trained for financial sentiment analysis\n",
    "2. **yiyanghkust/finbert-tone** - Trained on analyst reports\n",
    "3. **ahmedrachid/FinancialBERT** - Trained on financial news\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "FinBERT Architecture:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Classification Head         â”‚  â† Sentiment: Positive/Negative/Neutral\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚         [CLS] Token Output          â”‚  â† Sentence representation\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚      12 Transformer Layers          â”‚  â† Contextual encoding\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚         Token Embeddings            â”‚  â† Word + Position + Segment\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚           Input Text                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "- **Hidden size**: 768\n",
    "- **Attention heads**: 12\n",
    "- **Layers**: 12\n",
    "- **Max sequence length**: 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT model and tokenizer\n",
    "print(\"Loading FinBERT model...\")\n",
    "\n",
    "# Using ProsusAI/finbert - most popular for sentiment analysis\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "finbert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "finbert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "finbert_model.to(device)\n",
    "finbert_model.eval()\n",
    "\n",
    "print(f\"âœ“ Model loaded: {MODEL_NAME}\")\n",
    "print(f\"âœ“ Model parameters: {sum(p.numel() for p in finbert_model.parameters()):,}\")\n",
    "print(f\"âœ“ Number of labels: {finbert_model.config.num_labels}\")\n",
    "print(f\"âœ“ Label mapping: {finbert_model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore FinBERT architecture\n",
    "def explore_model_architecture(model):\n",
    "    \"\"\"Display model architecture details.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"FINBERT MODEL ARCHITECTURE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Count parameters by layer type\n",
    "    param_counts = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_type = name.split('.')[0]\n",
    "        if layer_type not in param_counts:\n",
    "            param_counts[layer_type] = 0\n",
    "        param_counts[layer_type] += param.numel()\n",
    "    \n",
    "    print(\"\\nParameter distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    for layer_type, count in param_counts.items():\n",
    "        print(f\"  {layer_type}: {count:,} parameters\")\n",
    "    \n",
    "    print(\"\\nModel configuration:\")\n",
    "    print(\"-\" * 40)\n",
    "    config = model.config\n",
    "    print(f\"  Hidden size: {config.hidden_size}\")\n",
    "    print(f\"  Intermediate size: {config.intermediate_size}\")\n",
    "    print(f\"  Attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"  Hidden layers: {config.num_hidden_layers}\")\n",
    "    print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "    print(f\"  Max position embeddings: {config.max_position_embeddings}\")\n",
    "    \n",
    "    return param_counts\n",
    "\n",
    "param_counts = explore_model_architecture(finbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization demonstration\n",
    "def demonstrate_tokenization(tokenizer, texts):\n",
    "    \"\"\"Show how FinBERT tokenizes financial text.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TOKENIZATION EXAMPLES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for text in texts:\n",
    "        print(f\"\\nOriginal: {text}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Number of tokens: {len(tokens)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Test tokenization\n",
    "sample_texts = [\n",
    "    \"The stock surged 15% on earnings beat.\",\n",
    "    \"Fed signals hawkish stance on inflation.\",\n",
    "    \"Company announces $2B share buyback program.\"\n",
    "]\n",
    "\n",
    "demonstrate_tokenization(finbert_tokenizer, sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086f42d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Sentiment Analysis Practical\n",
    "\n",
    "### Financial Sentiment Categories\n",
    "\n",
    "| Sentiment | Description | Trading Signal |\n",
    "|-----------|-------------|----------------|\n",
    "| **Positive** | Bullish news, good earnings, upgrades | Long / Buy |\n",
    "| **Negative** | Bearish news, misses, downgrades | Short / Sell |\n",
    "| **Neutral** | Factual, no clear direction | Hold / No action |\n",
    "\n",
    "### Use Cases in Trading\n",
    "- News sentiment scoring\n",
    "- Earnings call analysis\n",
    "- Social media monitoring\n",
    "- Analyst report parsing\n",
    "- SEC filing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create financial news dataset\n",
    "financial_news = pd.DataFrame({\n",
    "    'headline': [\n",
    "        # Positive headlines\n",
    "        \"Apple reports record quarterly revenue, beating analyst expectations\",\n",
    "        \"Tesla shares surge after better-than-expected delivery numbers\",\n",
    "        \"Microsoft announces $60 billion share buyback program\",\n",
    "        \"Amazon Web Services growth accelerates, driving stock higher\",\n",
    "        \"Goldman Sachs upgrades NVIDIA to buy on AI demand\",\n",
    "        \"JP Morgan reports strong trading revenue amid market volatility\",\n",
    "        \"Retail sales rise more than expected, boosting consumer stocks\",\n",
    "        \"Biotech firm announces breakthrough cancer treatment results\",\n",
    "        \n",
    "        # Negative headlines\n",
    "        \"Bank of America faces $500 million fine over regulatory violations\",\n",
    "        \"Meta shares plunge on weak advertising revenue guidance\",\n",
    "        \"Boeing reports wider-than-expected quarterly loss\",\n",
    "        \"Intel warns of continued chip demand weakness\",\n",
    "        \"Retail giant announces 10,000 layoffs amid slowing sales\",\n",
    "        \"Oil prices crash on recession fears and demand concerns\",\n",
    "        \"Credit Suisse shares hit record low on banking crisis fears\",\n",
    "        \"Pharmaceutical company recalls popular drug over safety concerns\",\n",
    "        \n",
    "        # Neutral headlines\n",
    "        \"Federal Reserve holds interest rates steady as expected\",\n",
    "        \"Company completes previously announced acquisition\",\n",
    "        \"CEO to present at upcoming investor conference\",\n",
    "        \"Board of directors appoints new chief financial officer\",\n",
    "        \"Company releases annual sustainability report\",\n",
    "        \"Stock split takes effect at market open today\",\n",
    "        \"Quarterly dividend declared at unchanged rate\",\n",
    "        \"Company announces date for next earnings release\",\n",
    "    ],\n",
    "    'expected_sentiment': [\n",
    "        'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive',\n",
    "        'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative',\n",
    "        'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {len(financial_news)} headlines\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(financial_news['expected_sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b63cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FinBERT Sentiment Analyzer\n",
    "class FinBERTSentimentAnalyzer:\n",
    "    \"\"\"\n",
    "    Financial sentiment analyzer using FinBERT.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.labels = ['positive', 'negative', 'neutral']\n",
    "    \n",
    "    def analyze_single(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of a single text.\n",
    "        \n",
    "        Returns:\n",
    "            dict with sentiment label and probabilities\n",
    "        \"\"\"\n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Get results\n",
    "        probs_np = probs.cpu().numpy()[0]\n",
    "        pred_idx = np.argmax(probs_np)\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'sentiment': self.labels[pred_idx],\n",
    "            'confidence': float(probs_np[pred_idx]),\n",
    "            'probabilities': {\n",
    "                label: float(prob) \n",
    "                for label, prob in zip(self.labels, probs_np)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, texts, batch_size=8):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of multiple texts.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with sentiment results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # Process results\n",
    "            probs_np = probs.cpu().numpy()\n",
    "            \n",
    "            for j, text in enumerate(batch_texts):\n",
    "                pred_idx = np.argmax(probs_np[j])\n",
    "                results.append({\n",
    "                    'text': text,\n",
    "                    'sentiment': self.labels[pred_idx],\n",
    "                    'confidence': float(probs_np[j][pred_idx]),\n",
    "                    'positive_prob': float(probs_np[j][0]),\n",
    "                    'negative_prob': float(probs_np[j][1]),\n",
    "                    'neutral_prob': float(probs_np[j][2]),\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def get_sentiment_score(self, text):\n",
    "        \"\"\"\n",
    "        Get a continuous sentiment score (-1 to 1).\n",
    "        Useful for trading signals.\n",
    "        \"\"\"\n",
    "        result = self.analyze_single(text)\n",
    "        probs = result['probabilities']\n",
    "        \n",
    "        # Score: positive - negative (neutral doesn't affect)\n",
    "        score = probs['positive'] - probs['negative']\n",
    "        \n",
    "        return score\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = FinBERTSentimentAnalyzer(finbert_model, finbert_tokenizer, device)\n",
    "print(\"âœ“ FinBERT Sentiment Analyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9486467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze financial news sentiment\n",
    "print(\"Analyzing sentiment for financial headlines...\\n\")\n",
    "\n",
    "# Analyze all headlines\n",
    "results_df = analyzer.analyze_batch(financial_news['headline'].tolist())\n",
    "\n",
    "# Add expected sentiment for comparison\n",
    "results_df['expected'] = financial_news['expected_sentiment'].values\n",
    "results_df['correct'] = results_df['sentiment'] == results_df['expected']\n",
    "\n",
    "# Display results\n",
    "print(\"Sample Results:\")\n",
    "print(\"=\"*100)\n",
    "for _, row in results_df.head(6).iterrows():\n",
    "    status = \"âœ“\" if row['correct'] else \"âœ—\"\n",
    "    print(f\"{status} [{row['sentiment'].upper():8}] (conf: {row['confidence']:.2f}) {row['text'][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "accuracy = results_df['correct'].mean()\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3db063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment analysis results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = axes[0, 0]\n",
    "cm = confusion_matrix(\n",
    "    results_df['expected'], \n",
    "    results_df['sentiment'],\n",
    "    labels=['positive', 'negative', 'neutral']\n",
    ")\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Positive', 'Negative', 'Neutral'],\n",
    "            yticklabels=['Positive', 'Negative', 'Neutral'])\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title('Confusion Matrix')\n",
    "\n",
    "# 2. Confidence Distribution\n",
    "ax2 = axes[0, 1]\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    mask = results_df['sentiment'] == sentiment\n",
    "    ax2.hist(results_df.loc[mask, 'confidence'], alpha=0.6, \n",
    "             label=sentiment.capitalize(), bins=15)\n",
    "ax2.set_xlabel('Confidence')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Prediction Confidence Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Probability Distribution by Sentiment\n",
    "ax3 = axes[1, 0]\n",
    "prob_data = results_df.groupby('sentiment')[['positive_prob', 'negative_prob', 'neutral_prob']].mean()\n",
    "prob_data.plot(kind='bar', ax=ax3)\n",
    "ax3.set_xlabel('Predicted Sentiment')\n",
    "ax3.set_ylabel('Average Probability')\n",
    "ax3.set_title('Average Probability Distribution')\n",
    "ax3.legend(['Positive', 'Negative', 'Neutral'])\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 4. Sentiment Score Distribution\n",
    "ax4 = axes[1, 1]\n",
    "results_df['sentiment_score'] = results_df['positive_prob'] - results_df['negative_prob']\n",
    "colors = ['green' if x > 0.2 else 'red' if x < -0.2 else 'gray' \n",
    "          for x in results_df['sentiment_score']]\n",
    "ax4.bar(range(len(results_df)), results_df['sentiment_score'], color=colors, alpha=0.7)\n",
    "ax4.axhline(y=0.2, color='green', linestyle='--', alpha=0.5, label='Buy threshold')\n",
    "ax4.axhline(y=-0.2, color='red', linestyle='--', alpha=0.5, label='Sell threshold')\n",
    "ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax4.set_xlabel('Headline Index')\n",
    "ax4.set_ylabel('Sentiment Score')\n",
    "ax4.set_title('Sentiment Scores (Positive - Negative)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive sentiment analysis\n",
    "def analyze_custom_text(text):\n",
    "    \"\"\"Analyze custom financial text.\"\"\"\n",
    "    result = analyzer.analyze_single(text)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SENTIMENT ANALYSIS RESULT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nText: {text}\\n\")\n",
    "    print(f\"Sentiment: {result['sentiment'].upper()}\")\n",
    "    print(f\"Confidence: {result['confidence']:.1%}\\n\")\n",
    "    print(\"Probability Distribution:\")\n",
    "    \n",
    "    # Visual bar\n",
    "    for label, prob in result['probabilities'].items():\n",
    "        bar = 'â–ˆ' * int(prob * 40)\n",
    "        print(f\"  {label.capitalize():10} [{bar:40}] {prob:.1%}\")\n",
    "    \n",
    "    # Trading signal\n",
    "    score = result['probabilities']['positive'] - result['probabilities']['negative']\n",
    "    print(f\"\\nðŸ“Š Sentiment Score: {score:+.3f}\")\n",
    "    \n",
    "    if score > 0.3:\n",
    "        print(\"ðŸ“ˆ Trading Signal: STRONG BUY\")\n",
    "    elif score > 0.1:\n",
    "        print(\"ðŸ“ˆ Trading Signal: BUY\")\n",
    "    elif score < -0.3:\n",
    "        print(\"ðŸ“‰ Trading Signal: STRONG SELL\")\n",
    "    elif score < -0.1:\n",
    "        print(\"ðŸ“‰ Trading Signal: SELL\")\n",
    "    else:\n",
    "        print(\"âž¡ï¸  Trading Signal: HOLD\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with various headlines\n",
    "test_headlines = [\n",
    "    \"Company posts record profits as demand exceeds expectations\",\n",
    "    \"SEC launches investigation into accounting irregularities\",\n",
    "    \"Quarterly results in line with analyst estimates\",\n",
    "]\n",
    "\n",
    "for headline in test_headlines:\n",
    "    _ = analyze_custom_text(headline)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a2938",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Using Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "Embeddings are dense vector representations of text that capture semantic meaning. They can be used for:\n",
    "\n",
    "1. **Similarity search** - Finding similar documents/news\n",
    "2. **Clustering** - Grouping related content\n",
    "3. **Feature extraction** - Input to downstream models\n",
    "4. **Visualization** - Understanding text relationships\n",
    "\n",
    "### Extraction Methods\n",
    "\n",
    "| Method | Description | Use Case |\n",
    "|--------|-------------|----------|\n",
    "| **[CLS] Token** | First token output | Classification tasks |\n",
    "| **Mean Pooling** | Average of all tokens | General purpose |\n",
    "| **Max Pooling** | Max across tokens | Capturing key features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FinBERT base model for embeddings (without classification head)\n",
    "print(\"Loading FinBERT base model for embeddings...\")\n",
    "\n",
    "# Use base model (without classifier) for embeddings\n",
    "finbert_base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "finbert_base.to(device)\n",
    "finbert_base.eval()\n",
    "\n",
    "print(f\"âœ“ Base model loaded\")\n",
    "print(f\"âœ“ Hidden size: {finbert_base.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Extractor Class\n",
    "class FinBERTEmbedder:\n",
    "    \"\"\"\n",
    "    Extract embeddings from FinBERT for financial texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def get_embeddings(self, texts, pooling='cls', batch_size=8):\n",
    "        \"\"\"\n",
    "        Extract embeddings from texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts\n",
    "            pooling: 'cls', 'mean', or 'max'\n",
    "            batch_size: Batch size for processing\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings (n_texts, hidden_size)\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                attention_mask = inputs['attention_mask']\n",
    "            \n",
    "            # Apply pooling\n",
    "            if pooling == 'cls':\n",
    "                embeddings = hidden_states[:, 0, :]  # [CLS] token\n",
    "            elif pooling == 'mean':\n",
    "                # Mean pooling with attention mask\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                sum_embeddings = torch.sum(hidden_states * mask_expanded, 1)\n",
    "                sum_mask = mask_expanded.sum(1)\n",
    "                embeddings = sum_embeddings / sum_mask\n",
    "            elif pooling == 'max':\n",
    "                # Max pooling\n",
    "                mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "                hidden_states[mask_expanded == 0] = -1e9\n",
    "                embeddings = torch.max(hidden_states, 1)[0]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooling method: {pooling}\")\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(all_embeddings)\n",
    "    \n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two texts.\n",
    "        \"\"\"\n",
    "        emb1 = self.get_embeddings([text1])[0]\n",
    "        emb2 = self.get_embeddings([text2])[0]\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def find_similar(self, query, corpus, top_k=5):\n",
    "        \"\"\"\n",
    "        Find most similar texts in corpus.\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        query_emb = self.get_embeddings([query])[0]\n",
    "        corpus_embs = self.get_embeddings(corpus)\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = np.dot(corpus_embs, query_emb) / (\n",
    "            np.linalg.norm(corpus_embs, axis=1) * np.linalg.norm(query_emb)\n",
    "        )\n",
    "        \n",
    "        # Get top-k\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        return [(corpus[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# Initialize embedder\n",
    "embedder = FinBERTEmbedder(finbert_base, finbert_tokenizer, device)\n",
    "print(\"âœ“ FinBERT Embedder initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c231678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate embedding extraction\n",
    "print(\"Extracting embeddings...\\n\")\n",
    "\n",
    "# Get embeddings for all headlines\n",
    "headlines = financial_news['headline'].tolist()\n",
    "embeddings = embedder.get_embeddings(headlines, pooling='cls')\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  - {embeddings.shape[0]} headlines\")\n",
    "print(f\"  - {embeddings.shape[1]} dimensions per embedding\")\n",
    "\n",
    "# Show embedding statistics\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings.std():.4f}\")\n",
    "print(f\"  Min: {embeddings.min():.4f}\")\n",
    "print(f\"  Max: {embeddings.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9613146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search demonstration\n",
    "print(\"=\"*70)\n",
    "print(\"SEMANTIC SIMILARITY SEARCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Query\n",
    "query = \"Technology company reports strong quarterly earnings\"\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "print(\"Most similar headlines:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "similar = embedder.find_similar(query, headlines, top_k=5)\n",
    "for i, (text, score) in enumerate(similar, 1):\n",
    "    print(f\"{i}. (similarity: {score:.3f}) {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity between pairs\n",
    "print(\"=\"*70)\n",
    "print(\"PAIRWISE SIMILARITY COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pairs = [\n",
    "    (\"Stock price rises on earnings beat\", \"Company reports better than expected profits\"),\n",
    "    (\"Stock price rises on earnings beat\", \"Weather forecast predicts rain tomorrow\"),\n",
    "    (\"Fed raises interest rates\", \"Central bank tightens monetary policy\"),\n",
    "    (\"Fed raises interest rates\", \"Apple launches new iPhone\"),\n",
    "]\n",
    "\n",
    "for text1, text2 in pairs:\n",
    "    sim = embedder.compute_similarity(text1, text2)\n",
    "    print(f\"\\nSimilarity: {sim:.3f}\")\n",
    "    print(f\"  Text 1: {text1}\")\n",
    "    print(f\"  Text 2: {text2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ab9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with t-SNE\n",
    "print(\"Creating t-SNE visualization...\")\n",
    "\n",
    "# Reduce dimensions\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=8)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by sentiment\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'blue'}\n",
    "sentiments = financial_news['expected_sentiment'].values\n",
    "\n",
    "for sentiment in ['positive', 'negative', 'neutral']:\n",
    "    mask = sentiments == sentiment\n",
    "    ax.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=colors[sentiment],\n",
    "        label=sentiment.capitalize(),\n",
    "        alpha=0.7,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "# Add some labels\n",
    "for i in range(0, len(headlines), 3):\n",
    "    ax.annotate(\n",
    "        headlines[i][:25] + \"...\",\n",
    "        (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "        fontsize=8,\n",
    "        alpha=0.7\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('t-SNE Dimension 1')\n",
    "ax.set_ylabel('t-SNE Dimension 2')\n",
    "ax.set_title('Financial Headlines Embeddings (t-SNE Visualization)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Notice how headlines cluster by sentiment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using embeddings as features for classification\n",
    "print(\"=\"*70)\n",
    "print(\"EMBEDDINGS AS FEATURES FOR CLASSIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data\n",
    "X = embeddings\n",
    "y = financial_news['expected_sentiment'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "# Train logistic regression on embeddings\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining accuracy: {train_acc:.1%}\")\n",
    "print(f\"Testing accuracy: {test_acc:.1%}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28f162",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Trading Signal Generation\n",
    "\n",
    "### From Sentiment to Signals\n",
    "\n",
    "Converting sentiment analysis into actionable trading signals:\n",
    "\n",
    "```\n",
    "News/Text â†’ Sentiment Analysis â†’ Signal Score â†’ Trading Decision\n",
    "                    â†“                  â†“               â†“\n",
    "              FinBERT          -1 to +1 scale    Buy/Sell/Hold\n",
    "```\n",
    "\n",
    "### Signal Aggregation Methods\n",
    "\n",
    "| Method | Formula | Use Case |\n",
    "|--------|---------|----------|\n",
    "| Simple Average | $\\bar{s} = \\frac{1}{n}\\sum_{i=1}^{n} s_i$ | Equal weighting |\n",
    "| Time-Weighted | $s_t = \\sum_{i} w_i \\cdot s_i$ where $w_i \\propto e^{-\\lambda \\cdot age_i}$ | Recent news matters more |\n",
    "| Confidence-Weighted | $s_c = \\sum_{i} c_i \\cdot s_i / \\sum c_i$ | Trust confident predictions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Signal Generator\n",
    "class SentimentTradingSignal:\n",
    "    \"\"\"\n",
    "    Generate trading signals from sentiment analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer):\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def compute_sentiment_score(self, text):\n",
    "        \"\"\"\n",
    "        Compute sentiment score for single text.\n",
    "        Returns score between -1 (very negative) and +1 (very positive).\n",
    "        \"\"\"\n",
    "        result = self.analyzer.analyze_single(text)\n",
    "        probs = result['probabilities']\n",
    "        \n",
    "        # Weighted score: positive - negative, scaled by confidence\n",
    "        score = probs['positive'] - probs['negative']\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'score': score,\n",
    "            'confidence': result['confidence'],\n",
    "            'sentiment': result['sentiment']\n",
    "        }\n",
    "    \n",
    "    def aggregate_signals(self, texts, method='simple'):\n",
    "        \"\"\"\n",
    "        Aggregate multiple sentiment scores into single signal.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of (text, weight) tuples or just texts\n",
    "            method: 'simple', 'confidence_weighted'\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        confidences = []\n",
    "        \n",
    "        for text in texts:\n",
    "            result = self.compute_sentiment_score(text)\n",
    "            scores.append(result['score'])\n",
    "            confidences.append(result['confidence'])\n",
    "        \n",
    "        if method == 'simple':\n",
    "            agg_score = np.mean(scores)\n",
    "        elif method == 'confidence_weighted':\n",
    "            agg_score = np.average(scores, weights=confidences)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        return {\n",
    "            'aggregate_score': agg_score,\n",
    "            'individual_scores': scores,\n",
    "            'confidences': confidences,\n",
    "            'n_articles': len(texts)\n",
    "        }\n",
    "    \n",
    "    def generate_signal(self, score, thresholds=None):\n",
    "        \"\"\"\n",
    "        Generate trading signal from score.\n",
    "        \n",
    "        Args:\n",
    "            score: Sentiment score (-1 to 1)\n",
    "            thresholds: Dict with 'strong_buy', 'buy', 'sell', 'strong_sell'\n",
    "        \"\"\"\n",
    "        if thresholds is None:\n",
    "            thresholds = {\n",
    "                'strong_buy': 0.5,\n",
    "                'buy': 0.2,\n",
    "                'sell': -0.2,\n",
    "                'strong_sell': -0.5\n",
    "            }\n",
    "        \n",
    "        if score >= thresholds['strong_buy']:\n",
    "            return 'STRONG_BUY', 1.0\n",
    "        elif score >= thresholds['buy']:\n",
    "            return 'BUY', 0.5\n",
    "        elif score <= thresholds['strong_sell']:\n",
    "            return 'STRONG_SELL', -1.0\n",
    "        elif score <= thresholds['sell']:\n",
    "            return 'SELL', -0.5\n",
    "        else:\n",
    "            return 'HOLD', 0.0\n",
    "\n",
    "# Initialize signal generator\n",
    "signal_generator = SentimentTradingSignal(analyzer)\n",
    "print(\"âœ“ Trading Signal Generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c002ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate trading signal generation for a stock\n",
    "print(\"=\"*70)\n",
    "print(\"TRADING SIGNAL SIMULATION: Apple Inc. (AAPL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulated news feed for Apple\n",
    "apple_news = [\n",
    "    \"Apple reports record iPhone 15 Pro sales exceeding analyst estimates\",\n",
    "    \"Apple's services revenue continues strong growth trajectory\",\n",
    "    \"Warren Buffett increases Berkshire's stake in Apple\",\n",
    "    \"Apple announces expansion of AI features in upcoming iOS update\",\n",
    "    \"Analysts raise Apple price targets after strong guidance\",\n",
    "    \"Minor supply chain disruption reported for Apple Watch production\",\n",
    "]\n",
    "\n",
    "# Analyze each headline\n",
    "print(\"\\nIndividual Article Analysis:\")\n",
    "print(\"-\" * 70)\n",
    "for i, news in enumerate(apple_news, 1):\n",
    "    result = signal_generator.compute_sentiment_score(news)\n",
    "    signal, _ = signal_generator.generate_signal(result['score'])\n",
    "    print(f\"{i}. Score: {result['score']:+.3f} | {result['sentiment']:8} | {news[:50]}...\")\n",
    "\n",
    "# Aggregate signal\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATED SIGNAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "agg_result = signal_generator.aggregate_signals(apple_news, method='confidence_weighted')\n",
    "final_signal, position_size = signal_generator.generate_signal(agg_result['aggregate_score'])\n",
    "\n",
    "print(f\"\\nAggregate Sentiment Score: {agg_result['aggregate_score']:+.3f}\")\n",
    "print(f\"Number of Articles Analyzed: {agg_result['n_articles']}\")\n",
    "print(f\"\\nðŸŽ¯ TRADING SIGNAL: {final_signal}\")\n",
    "print(f\"ðŸ“Š Suggested Position Size: {abs(position_size)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize signal generation process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual scores\n",
    "ax1 = axes[0]\n",
    "scores = agg_result['individual_scores']\n",
    "colors = ['green' if s > 0 else 'red' for s in scores]\n",
    "bars = ax1.barh(range(len(scores)), scores, color=colors, alpha=0.7)\n",
    "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.axvline(x=0.2, color='green', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=-0.2, color='red', linestyle='--', alpha=0.5)\n",
    "ax1.set_yticks(range(len(scores)))\n",
    "ax1.set_yticklabels([f\"Article {i+1}\" for i in range(len(scores))])\n",
    "ax1.set_xlabel('Sentiment Score')\n",
    "ax1.set_title('Individual Article Sentiment Scores')\n",
    "ax1.set_xlim(-1, 1)\n",
    "\n",
    "# Signal gauge\n",
    "ax2 = axes[1]\n",
    "agg_score = agg_result['aggregate_score']\n",
    "\n",
    "# Create gauge-like visualization\n",
    "theta = np.linspace(0, np.pi, 100)\n",
    "r = 1\n",
    "\n",
    "# Background arcs\n",
    "ax2.fill_between(np.cos(theta[:25]), 0, np.sin(theta[:25]), alpha=0.3, color='darkred')\n",
    "ax2.fill_between(np.cos(theta[25:40]), 0, np.sin(theta[25:40]), alpha=0.3, color='red')\n",
    "ax2.fill_between(np.cos(theta[40:60]), 0, np.sin(theta[40:60]), alpha=0.3, color='gray')\n",
    "ax2.fill_between(np.cos(theta[60:75]), 0, np.sin(theta[60:75]), alpha=0.3, color='green')\n",
    "ax2.fill_between(np.cos(theta[75:]), 0, np.sin(theta[75:]), alpha=0.3, color='darkgreen')\n",
    "\n",
    "# Needle\n",
    "needle_angle = np.pi/2 + (agg_score * np.pi/2)\n",
    "ax2.arrow(0, 0, 0.8*np.cos(needle_angle), 0.8*np.sin(needle_angle),\n",
    "          head_width=0.1, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Labels\n",
    "ax2.text(-0.9, -0.2, 'Strong Sell', fontsize=10)\n",
    "ax2.text(-0.5, 0.7, 'Sell', fontsize=10)\n",
    "ax2.text(-0.1, 0.9, 'Hold', fontsize=10)\n",
    "ax2.text(0.4, 0.7, 'Buy', fontsize=10)\n",
    "ax2.text(0.7, -0.2, 'Strong Buy', fontsize=10)\n",
    "\n",
    "ax2.text(0, -0.4, f'Score: {agg_score:+.3f}\\nSignal: {final_signal}', \n",
    "         ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2.set_xlim(-1.2, 1.2)\n",
    "ax2.set_ylim(-0.6, 1.2)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title('Aggregate Trading Signal', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62928c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Exercises\n",
    "\n",
    "### Exercise 1: Fine-tune FinBERT\n",
    "Create a custom dataset and fine-tune FinBERT for a specific task (e.g., earnings call sentiment).\n",
    "\n",
    "### Exercise 2: Multi-stock Sentiment Dashboard\n",
    "Build a system that tracks sentiment for multiple stocks simultaneously.\n",
    "\n",
    "### Exercise 3: Sentiment-Based Strategy\n",
    "Backtest a trading strategy that uses sentiment signals as entry/exit triggers.\n",
    "\n",
    "### Exercise 4: Compare Models\n",
    "Compare FinBERT with other models (BERT, RoBERTa) on financial sentiment tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Transfer Learning** enables leveraging large pre-trained models for finance-specific tasks\n",
    "\n",
    "2. **FinBERT** is specifically trained on financial text and understands domain vocabulary\n",
    "\n",
    "3. **Sentiment Analysis** can be directly applied to:\n",
    "   - News headlines\n",
    "   - Earnings reports\n",
    "   - Social media\n",
    "   - Analyst reports\n",
    "\n",
    "4. **Embeddings** are versatile features that can be used for:\n",
    "   - Similarity search\n",
    "   - Clustering\n",
    "   - Classification\n",
    "   - Visualization\n",
    "\n",
    "5. **Trading Signals** require careful aggregation and threshold tuning\n",
    "\n",
    "### Next Steps\n",
    "- Day 6: Building Custom Transformers for Finance\n",
    "- Integrate sentiment signals with price data\n",
    "- Build production-ready sentiment pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d35add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference - Using FinBERT pipeline (simplest approach)\n",
    "print(\"QUICK REFERENCE: Using HuggingFace Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create sentiment pipeline\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Quick analysis\n",
    "test_texts = [\n",
    "    \"Revenue growth exceeded expectations with strong margins.\",\n",
    "    \"Company faces regulatory investigation over accounting practices.\",\n",
    "    \"Board approves quarterly dividend at unchanged rate.\"\n",
    "]\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for text, result in zip(test_texts, sentiment_pipeline(test_texts)):\n",
    "    print(f\"  [{result['label']:8}] ({result['score']:.2f}) {text[:50]}...\")\n",
    "\n",
    "print(\"\\nâœ“ Pipeline is the quickest way to use pre-trained models!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
