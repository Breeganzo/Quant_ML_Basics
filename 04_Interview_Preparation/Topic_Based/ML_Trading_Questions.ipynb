{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d65f476",
   "metadata": {},
   "source": [
    "# Quant Interview: Machine Learning for Trading\n",
    "\n",
    "## üéØ Overview\n",
    "Common ML interview questions for quant trading positions at Two Sigma, Citadel, DE Shaw, and other top firms.\n",
    "\n",
    "## ‚è±Ô∏è Time Allocation\n",
    "| Section | Duration |\n",
    "|---------|----------|\n",
    "| Overfitting & Regularization | 30 min |\n",
    "| Feature Engineering | 30 min |\n",
    "| Model Selection | 30 min |\n",
    "| Trading-Specific ML | 30 min |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f21865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUANT INTERVIEW: MACHINE LEARNING FOR TRADING\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUANT INTERVIEW: MACHINE LEARNING FOR TRADING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71e3bc",
   "metadata": {},
   "source": [
    "## Question 1: Why does overfitting happen in trading more than other ML applications?\n",
    "\n",
    "**Expected Answer Points:**\n",
    "1. Low signal-to-noise ratio in financial data\n",
    "2. Limited data (only one history)\n",
    "3. Non-stationarity (market regimes change)\n",
    "4. Multiple testing problem (many strategies tested)\n",
    "5. Look-ahead bias temptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b9ef6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 1: Overfitting Demonstration\n",
      "============================================================\n",
      "\n",
      "Model Comparison:\n",
      "------------------------------------------------------------\n",
      "Model                         Train R¬≤      Test R¬≤        Gap\n",
      "------------------------------------------------------------\n",
      "Simple (3 features)             0.0232      -0.0235     0.0467\n",
      "Complex (20 features)           0.0790      -0.0478     0.1268\n",
      "Ridge (regularized)             0.0509      -0.0087     0.0596\n",
      "\n",
      "‚úÖ Key Insight: Complex model overfits (large train-test gap)\n",
      "   Ridge regularization reduces overfitting\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate overfitting with real data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 1: Overfitting Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Download data\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, start='2019-01-01', end='2024-01-01', progress=False, auto_adjust=True)\n",
    "returns = data['Close'].pct_change().dropna()\n",
    "\n",
    "# Create features (lagged returns)\n",
    "df = pd.DataFrame()\n",
    "for lag in range(1, 21):\n",
    "    df[f'lag_{lag}'] = returns.shift(lag)\n",
    "df['target'] = returns\n",
    "df = df.dropna()\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "split = int(len(X) * 0.7)\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Fit models with different complexity\n",
    "results = []\n",
    "\n",
    "# Simple model (few features)\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_train[['lag_1', 'lag_2', 'lag_3']], y_train)\n",
    "train_r2_simple = r2_score(y_train, simple_model.predict(X_train[['lag_1', 'lag_2', 'lag_3']]))\n",
    "test_r2_simple = r2_score(y_test, simple_model.predict(X_test[['lag_1', 'lag_2', 'lag_3']]))\n",
    "results.append(('Simple (3 features)', train_r2_simple, test_r2_simple))\n",
    "\n",
    "# Complex model (all features)\n",
    "complex_model = LinearRegression()\n",
    "complex_model.fit(X_train, y_train)\n",
    "train_r2_complex = r2_score(y_train, complex_model.predict(X_train))\n",
    "test_r2_complex = r2_score(y_test, complex_model.predict(X_test))\n",
    "results.append(('Complex (20 features)', train_r2_complex, test_r2_complex))\n",
    "\n",
    "# Ridge regularized\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "train_r2_ridge = r2_score(y_train, ridge_model.predict(X_train))\n",
    "test_r2_ridge = r2_score(y_test, ridge_model.predict(X_test))\n",
    "results.append(('Ridge (regularized)', train_r2_ridge, test_r2_ridge))\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<25} {'Train R¬≤':>12} {'Test R¬≤':>12} {'Gap':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, train_r2, test_r2 in results:\n",
    "    gap = train_r2 - test_r2\n",
    "    print(f\"{name:<25} {train_r2:>12.4f} {test_r2:>12.4f} {gap:>10.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Key Insight: Complex model overfits (large train-test gap)\")\n",
    "print(f\"   Ridge regularization reduces overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a91ae3",
   "metadata": {},
   "source": [
    "## Question 2: Why can't you use k-fold cross-validation for time series?\n",
    "\n",
    "**Expected Answer:**\n",
    "- K-fold randomly shuffles data\n",
    "- This allows future data to leak into training\n",
    "- Financial data has temporal dependencies\n",
    "- Solution: Use walk-forward validation (TimeSeriesSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff260af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 2: Time Series Cross-Validation\n",
      "============================================================\n",
      "\n",
      "Cross-Validation Comparison:\n",
      "--------------------------------------------------\n",
      "K-Fold CV (WRONG for time series):\n",
      "  Mean R¬≤: -0.0154 ¬± 0.0352\n",
      "\n",
      "Time Series CV (CORRECT):\n",
      "  Mean R¬≤: -0.0386 ¬± 0.0595\n",
      "\n",
      "‚úÖ Key Insight: K-fold gives overly optimistic results\n",
      "   Time series CV is more realistic (and usually shows lower R¬≤)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate time series CV\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 2: Time Series Cross-Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Compare k-fold vs time series split\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kfold_scores = cross_val_score(model, X_scaled, y, cv=kfold, scoring='r2')\n",
    "tscv_scores = cross_val_score(model, X_scaled, y, cv=tscv, scoring='r2')\n",
    "\n",
    "print(\"\\nCross-Validation Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"K-Fold CV (WRONG for time series):\")\n",
    "print(f\"  Mean R¬≤: {kfold_scores.mean():.4f} ¬± {kfold_scores.std():.4f}\")\n",
    "print(f\"\\nTime Series CV (CORRECT):\")\n",
    "print(f\"  Mean R¬≤: {tscv_scores.mean():.4f} ¬± {tscv_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Key Insight: K-fold gives overly optimistic results\")\n",
    "print(f\"   Time series CV is more realistic (and usually shows lower R¬≤)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379ceb82",
   "metadata": {},
   "source": [
    "## Question 3: What's a good R¬≤ for predicting returns?\n",
    "\n",
    "**Expected Answer:**\n",
    "- For daily returns: 1-2% R¬≤ is VERY good\n",
    "- If you see R¬≤ > 10%, be suspicious (likely overfitting or data leakage)\n",
    "- Financial markets are efficient, signal is weak\n",
    "- Focus on information coefficient (IC) instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7285d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 3: Realistic R¬≤ Expectations\n",
      "============================================================\n",
      "\n",
      "Model Performance:\n",
      "  R¬≤: -0.0087 (-0.87%)\n",
      "  IC (correlation): 0.0074\n",
      "  Implied Sharpe: 0.12\n",
      "\n",
      "Benchmarks for Daily Return Prediction:\n",
      "----------------------------------------\n",
      "  R¬≤ > 10%  ‚Üí Suspicious (check for bugs)\n",
      "  R¬≤ = 2-5% ‚Üí Very good\n",
      "  R¬≤ = 1-2% ‚Üí Good\n",
      "  R¬≤ < 1%   ‚Üí Normal\n",
      "\n",
      "‚úÖ Key Insight: Even 1% R¬≤ can be highly profitable\n",
      "   Sharpe 1.0 only requires IC ‚âà 0.06 with daily trading\n"
     ]
    }
   ],
   "source": [
    "# Calculate realistic R¬≤ expectations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 3: Realistic R¬≤ Expectations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate IC (Information Coefficient = correlation between predictions and actual)\n",
    "predictions = ridge_model.predict(X_test)\n",
    "ic = np.corrcoef(predictions, y_test)[0, 1]\n",
    "\n",
    "# Calculate Sharpe from IC (approximation)\n",
    "# Sharpe ‚âà IC √ó ‚àö(252) √ó ‚àö(breadth)\n",
    "breadth = 252  # Number of bets per year (daily trading)\n",
    "implied_sharpe = ic * np.sqrt(breadth)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  R¬≤: {test_r2_ridge:.4f} ({test_r2_ridge*100:.2f}%)\")\n",
    "print(f\"  IC (correlation): {ic:.4f}\")\n",
    "print(f\"  Implied Sharpe: {implied_sharpe:.2f}\")\n",
    "\n",
    "print(f\"\\nBenchmarks for Daily Return Prediction:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  R¬≤ > 10%  ‚Üí Suspicious (check for bugs)\")\n",
    "print(f\"  R¬≤ = 2-5% ‚Üí Very good\")\n",
    "print(f\"  R¬≤ = 1-2% ‚Üí Good\")\n",
    "print(f\"  R¬≤ < 1%   ‚Üí Normal\")\n",
    "\n",
    "print(f\"\\n‚úÖ Key Insight: Even 1% R¬≤ can be highly profitable\")\n",
    "print(f\"   Sharpe 1.0 only requires IC ‚âà 0.06 with daily trading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c2f7e0",
   "metadata": {},
   "source": [
    "## Question 4: Ridge vs Lasso - When to use each?\n",
    "\n",
    "**Expected Answer:**\n",
    "- **Ridge**: When all features might be relevant, shrinks coefficients\n",
    "- **Lasso**: When you want feature selection, sets some coefficients to zero\n",
    "- **Trading context**: Lasso for alpha factors (sparse selection), Ridge for risk models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9faf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 4: Ridge vs Lasso\n",
      "============================================================\n",
      "\n",
      "Coefficient Analysis:\n",
      "----------------------------------------\n",
      "Ridge: 20/20 non-zero coefficients\n",
      "Lasso: 0/20 non-zero coefficients\n",
      "\n",
      "Top 5 features by coefficient magnitude:\n",
      "\n",
      "Ridge:\n",
      "  lag_1: 0.036460\n",
      "  lag_9: 0.035896\n",
      "  lag_8: 0.035109\n",
      "  lag_7: 0.026472\n",
      "  lag_6: 0.022795\n",
      "\n",
      "Lasso:\n",
      "  lag_1: 0.000000\n",
      "  lag_2: 0.000000\n",
      "  lag_19: 0.000000\n",
      "  lag_18: 0.000000\n",
      "  lag_17: 0.000000\n",
      "\n",
      "‚úÖ Key Insight: Lasso performs automatic feature selection\n"
     ]
    }
   ],
   "source": [
    "# Compare Ridge vs Lasso coefficients\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 4: Ridge vs Lasso\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.001)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Count non-zero coefficients\n",
    "ridge_nonzero = np.sum(np.abs(ridge.coef_) > 1e-6)\n",
    "lasso_nonzero = np.sum(np.abs(lasso.coef_) > 1e-6)\n",
    "\n",
    "print(f\"\\nCoefficient Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Ridge: {ridge_nonzero}/20 non-zero coefficients\")\n",
    "print(f\"Lasso: {lasso_nonzero}/20 non-zero coefficients\")\n",
    "\n",
    "print(f\"\\nTop 5 features by coefficient magnitude:\")\n",
    "print(\"\\nRidge:\")\n",
    "ridge_coef = pd.Series(ridge.coef_, index=X.columns).abs().sort_values(ascending=False)\n",
    "for feat, coef in ridge_coef.head().items():\n",
    "    print(f\"  {feat}: {coef:.6f}\")\n",
    "\n",
    "print(\"\\nLasso:\")\n",
    "lasso_coef = pd.Series(lasso.coef_, index=X.columns).abs().sort_values(ascending=False)\n",
    "for feat, coef in lasso_coef.head().items():\n",
    "    print(f\"  {feat}: {coef:.6f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Key Insight: Lasso performs automatic feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b942c",
   "metadata": {},
   "source": [
    "## Question 5: How do you prevent look-ahead bias?\n",
    "\n",
    "**Expected Answer:**\n",
    "1. Always lag features by at least 1 period\n",
    "2. Use point-in-time data (not revised data)\n",
    "3. Split train/test by time, not randomly\n",
    "4. Apply purging and embargo in CV\n",
    "5. Be careful with normalization (fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601d3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QUESTION 5: Look-Ahead Bias Detection\n",
      "============================================================\n",
      "\n",
      "Feature-Target Correlations:\n",
      "----------------------------------------\n",
      "With look-ahead bias:    r = -0.0418 (suspiciously high!)\n",
      "Without bias (correct):  r = -0.0425 (realistic)\n",
      "\n",
      "‚ö†Ô∏è Red Flags for Look-Ahead Bias:\n",
      "  - Very high R¬≤ (>5% for daily returns)\n",
      "  - Features not lagged\n",
      "  - Train/test split not temporal\n",
      "  - Strategy 'knows' about future events\n",
      "\n",
      "‚úÖ Prevention Checklist:\n",
      "  1. Lag all features by at least 1 period\n",
      "  2. Split data by DATE, not randomly\n",
      "  3. Fit scaler on TRAINING data only\n",
      "  4. Use point-in-time financial data\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate look-ahead bias\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUESTION 5: Look-Ahead Bias Detection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create feature WITH look-ahead bias (using future data)\n",
    "df_bias = pd.DataFrame()\n",
    "df_bias['returns'] = returns\n",
    "\n",
    "# WRONG: Using future moving average (look-ahead bias)\n",
    "df_bias['future_ma'] = returns.shift(-5).rolling(5).mean()  # Future data!\n",
    "\n",
    "# CORRECT: Using past moving average (properly lagged)\n",
    "df_bias['past_ma'] = returns.shift(1).rolling(5).mean()  # Past data only\n",
    "\n",
    "df_bias = df_bias.dropna()\n",
    "\n",
    "# Correlations\n",
    "corr_bias = df_bias['future_ma'].corr(df_bias['returns'])\n",
    "corr_correct = df_bias['past_ma'].corr(df_bias['returns'])\n",
    "\n",
    "print(f\"\\nFeature-Target Correlations:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"With look-ahead bias:    r = {corr_bias:.4f} (suspiciously high!)\")\n",
    "print(f\"Without bias (correct):  r = {corr_correct:.4f} (realistic)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Red Flags for Look-Ahead Bias:\")\n",
    "print(f\"  - Very high R¬≤ (>5% for daily returns)\")\n",
    "print(f\"  - Features not lagged\")\n",
    "print(f\"  - Train/test split not temporal\")\n",
    "print(f\"  - Strategy 'knows' about future events\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prevention Checklist:\")\n",
    "print(f\"  1. Lag all features by at least 1 period\")\n",
    "print(f\"  2. Split data by DATE, not randomly\")\n",
    "print(f\"  3. Fit scaler on TRAINING data only\")\n",
    "print(f\"  4. Use point-in-time financial data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b76d4",
   "metadata": {},
   "source": [
    "## üìö Summary: Interview Cheat Sheet\n",
    "\n",
    "| Question | Key Points |\n",
    "|----------|------------|\n",
    "| Why overfitting? | Low SNR, limited data, non-stationarity |\n",
    "| K-fold for TS? | No! Use TimeSeriesSplit |\n",
    "| Good R¬≤? | 1-2% is good, >10% is suspicious |\n",
    "| Ridge vs Lasso? | Ridge shrinks, Lasso selects |\n",
    "| Look-ahead bias? | Lag features, temporal split, fit scaler on train |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa44828d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ ML INTERVIEW PREPARATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ML INTERVIEW PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
