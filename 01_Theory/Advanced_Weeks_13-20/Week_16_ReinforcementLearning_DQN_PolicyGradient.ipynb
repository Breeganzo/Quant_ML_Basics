{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318bce23",
   "metadata": {},
   "source": [
    "# Week 16: Reinforcement Learning for Trading\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **RL Fundamentals**: States, Actions, Rewards\n",
    "- **Q-Learning**: Value-based learning\n",
    "- **Policy Gradient**: Direct policy optimization\n",
    "- **Trading Environment**: Formulating trading as RL\n",
    "\n",
    "---\n",
    "\n",
    "## Why RL for Trading?\n",
    "\n",
    "Trading is inherently sequential decision-making:\n",
    "- **State**: Current market conditions\n",
    "- **Action**: Buy, sell, hold\n",
    "- **Reward**: Profit/loss\n",
    "- **Goal**: Maximize cumulative reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ðŸ“š Week 16: Reinforcement Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1377cfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: RL Framework\n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "\n",
    "- **State space S**: Market conditions\n",
    "- **Action space A**: Trading decisions\n",
    "- **Transition P(s'|s,a)**: Market dynamics\n",
    "- **Reward R(s,a,s')**: Trading returns\n",
    "\n",
    "### Objective\n",
    "\n",
    "Maximize expected discounted return:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "RL is learning by doing. The agent takes actions, sees results, and adjusts strategy. Like a trader learning from P&L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bd222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment:\n",
    "    \"\"\"Simple trading environment\"\"\"\n",
    "    \n",
    "    def __init__(self, prices, transaction_cost=0.001):\n",
    "        self.prices = prices\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.n_steps = len(prices)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # -1, 0, or 1\n",
    "        self.portfolio_value = 1.0\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Simple state: past 5 returns + position\n",
    "        if self.current_step < 5:\n",
    "            returns = np.zeros(5)\n",
    "        else:\n",
    "            returns = np.diff(np.log(self.prices[self.current_step-5:self.current_step+1]))\n",
    "        return np.append(returns, self.position)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Action: 0=sell, 1=hold, 2=buy\"\"\"\n",
    "        new_position = action - 1  # Convert to -1, 0, 1\n",
    "        \n",
    "        # Calculate return\n",
    "        price_return = (self.prices[self.current_step + 1] - \n",
    "                       self.prices[self.current_step]) / self.prices[self.current_step]\n",
    "        \n",
    "        # Position return\n",
    "        pnl = self.position * price_return\n",
    "        \n",
    "        # Transaction cost if position changed\n",
    "        if new_position != self.position:\n",
    "            pnl -= self.transaction_cost\n",
    "        \n",
    "        self.portfolio_value *= (1 + pnl)\n",
    "        self.position = new_position\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= self.n_steps - 1\n",
    "        \n",
    "        return self._get_state(), pnl, done\n",
    "\n",
    "# Test environment\n",
    "prices = 100 * np.cumprod(1 + np.random.randn(100) * 0.02)\n",
    "env = TradingEnvironment(prices)\n",
    "state = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Random action\n",
    "next_state, reward, done = env.step(2)  # Buy\n",
    "print(f\"After BUY: reward={reward:.4f}, done={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e04ad8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Q-Learning\n",
    "\n",
    "### Value Function\n",
    "\n",
    "$$Q(s, a) = E[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "$$Q(s, a) = R + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "### Q-Learning Update\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, state_bins=10, n_actions=3, lr=0.1, gamma=0.95, epsilon=0.1):\n",
    "        self.state_bins = state_bins\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-table (simplified: discretize state space)\n",
    "        # State: 5 returns + position â†’ discretize to bins\n",
    "        self.q_table = {}\n",
    "    \n",
    "    def _discretize_state(self, state):\n",
    "        # Simple discretization\n",
    "        bins = np.linspace(-0.1, 0.1, self.state_bins)\n",
    "        discrete = tuple(np.digitize(state[:-1], bins).tolist() + [int(state[-1] + 1)])\n",
    "        return discrete\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state_d = self._discretize_state(state)\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_actions)\n",
    "        \n",
    "        # Greedy\n",
    "        if state_d not in self.q_table:\n",
    "            self.q_table[state_d] = np.zeros(self.n_actions)\n",
    "        return np.argmax(self.q_table[state_d])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_d = self._discretize_state(state)\n",
    "        next_state_d = self._discretize_state(next_state)\n",
    "        \n",
    "        if state_d not in self.q_table:\n",
    "            self.q_table[state_d] = np.zeros(self.n_actions)\n",
    "        if next_state_d not in self.q_table:\n",
    "            self.q_table[next_state_d] = np.zeros(self.n_actions)\n",
    "        \n",
    "        # Q-learning update\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * np.max(self.q_table[next_state_d])\n",
    "        \n",
    "        self.q_table[state_d][action] += self.lr * (target - self.q_table[state_d][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Q-learning agent\n",
    "def train_agent(env, agent, episodes=100):\n",
    "    rewards_history = []\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "# Generate training data\n",
    "prices_train = 100 * np.cumprod(1 + np.random.randn(500) * 0.02)\n",
    "env = TradingEnvironment(prices_train, transaction_cost=0.001)\n",
    "agent = QLearningAgent(epsilon=0.2)\n",
    "\n",
    "rewards = train_agent(env, agent, episodes=50)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(pd.Series(rewards).rolling(10).mean())\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (10-ep MA)')\n",
    "plt.title('Q-Learning Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3932d98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Deep Q-Network (DQN)\n",
    "\n",
    "### Neural Network for Q-Function\n",
    "\n",
    "Instead of a table, use a neural network:\n",
    "\n",
    "$$Q(s, a; \\theta) \\approx Q^*(s, a)$$\n",
    "\n",
    "### Key Improvements\n",
    "\n",
    "1. **Experience Replay**: Store transitions, sample randomly\n",
    "2. **Target Network**: Stable targets for training\n",
    "3. **Continuous states**: Handle high-dimensional states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035386d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from collections import deque\n",
    "    \n",
    "    class DQN(nn.Module):\n",
    "        def __init__(self, state_dim, n_actions):\n",
    "            super().__init__()\n",
    "            self.network = nn.Sequential(\n",
    "                nn.Linear(state_dim, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, n_actions)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    print(\"DQN Architecture:\")\n",
    "    model = DQN(state_dim=6, n_actions=3)\n",
    "    print(model)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cdd51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Policy Gradient\n",
    "\n",
    "### Direct Policy Optimization\n",
    "\n",
    "Instead of learning value, learn policy directly:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = P(A_t = a | S_t = s; \\theta)$$\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t]$$\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "Policy gradient directly learns \"what to do\" instead of \"how good is this state.\" Good outcomes increase the probability of the actions that led to them.\n",
    "\n",
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. Why is trading a good fit for RL?\n",
    "2. What's the difference between Q-learning and policy gradient?\n",
    "3. What is the exploration-exploitation trade-off?\n",
    "\n",
    "### Technical\n",
    "1. How do you define the reward function for trading?\n",
    "2. What are the challenges of applying RL to finance?\n",
    "3. How does experience replay help DQN?\n",
    "\n",
    "### Finance-Specific\n",
    "1. What state representation would you use for stocks?\n",
    "2. How do you handle transaction costs in RL?\n",
    "3. Can RL learn risk management?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Method | Learns | Best For |\n",
    "|--------|--------|----------|\n",
    "| Q-Learning | Value function | Discrete actions |\n",
    "| DQN | Q via neural net | Complex states |\n",
    "| Policy Gradient | Policy directly | Continuous actions |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
