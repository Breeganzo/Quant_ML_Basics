{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a097aa",
   "metadata": {},
   "source": [
    "# Week 13: Neural Networks - MLP for Finance\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Neural Network Fundamentals**: Architecture, activations, forward pass\n",
    "- **Backpropagation**: How neural networks learn\n",
    "- **MLP for Finance**: Regression and classification tasks\n",
    "- **Regularization**: Dropout, batch normalization, early stopping\n",
    "\n",
    "---\n",
    "\n",
    "## Why Neural Networks in Finance?\n",
    "\n",
    "- Capture complex non-linear relationships\n",
    "- Handle high-dimensional data\n",
    "- Learn feature interactions automatically\n",
    "- Foundation for advanced architectures (LSTM, Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries loaded!\")\n",
    "print(\"üìö Week 13: Neural Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1095f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Neural Network Architecture\n",
    "\n",
    "### Building Blocks\n",
    "\n",
    "**Neuron**: $y = \\sigma(w^T x + b)$\n",
    "\n",
    "**Layer**: Multiple neurons processing inputs\n",
    "\n",
    "**Network**: Stack of layers\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "$$h^{(l)} = \\sigma(W^{(l)} h^{(l-1)} + b^{(l)})$$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "A neural network is layers of simple functions stacked together. Each layer takes input, multiplies by weights, adds bias, and applies activation. The magic is in learning the right weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Visualize\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "axes[0].plot(x, relu(x), 'b-', linewidth=2)\n",
    "axes[0].set_title('ReLU')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(x, sigmoid(x), 'g-', linewidth=2)\n",
    "axes[1].set_title('Sigmoid')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(x, tanh(x), 'r-', linewidth=2)\n",
    "axes[2].set_title('Tanh')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c9622",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: MLP with PyTorch\n",
    "\n",
    "### Network Architecture for Returns Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    \n",
    "    # Define MLP\n",
    "    class FinanceMLP(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dims=[64, 32], dropout=0.2):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            prev_dim = input_dim\n",
    "            \n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                prev_dim = hidden_dim\n",
    "            \n",
    "            layers.append(nn.Linear(prev_dim, 1))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    print(\"PyTorch MLP Architecture\")\n",
    "    print(\"=\"*50)\n",
    "    model = FinanceMLP(input_dim=10)\n",
    "    print(model)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch not installed. Using sklearn MLP instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77181eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate financial data\n",
    "n = 2000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features\n",
    "momentum = np.random.randn(n)\n",
    "volatility = np.abs(np.random.randn(n))\n",
    "volume = np.random.exponential(1, n)\n",
    "rsi = np.random.uniform(20, 80, n)\n",
    "ma_ratio = 1 + np.random.randn(n) * 0.1\n",
    "\n",
    "# Non-linear target\n",
    "target = (\n",
    "    0.01 * momentum * (volatility < 0.5) +\n",
    "    0.005 * np.log(volume + 1) * (momentum > 0) +\n",
    "    0.002 * (rsi - 50) / 50 +\n",
    "    np.random.randn(n) * 0.01\n",
    ")\n",
    "\n",
    "X = np.column_stack([momentum, volatility, volume, rsi, ma_ratio])\n",
    "y = target\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1788df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with sklearn (fallback)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"MLP Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train R¬≤: {mlp.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test R¬≤:  {mlp.score(X_test, y_test):.4f}\")\n",
    "print(f\"Iterations: {mlp.n_iter_}\")\n",
    "\n",
    "# Learning curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MLP Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a10ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Regularization Techniques\n",
    "\n",
    "### Preventing Overfitting\n",
    "\n",
    "1. **Dropout**: Randomly zero neurons during training\n",
    "2. **Batch Normalization**: Normalize layer inputs\n",
    "3. **Early Stopping**: Stop when validation loss increases\n",
    "4. **L2 Regularization**: Penalize large weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ac81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regularization effects\n",
    "configs = [\n",
    "    {'alpha': 0.0001, 'early_stopping': False},  # Baseline\n",
    "    {'alpha': 0.01, 'early_stopping': False},    # L2 regularization\n",
    "    {'alpha': 0.0001, 'early_stopping': True},   # Early stopping\n",
    "]\n",
    "\n",
    "print(\"Regularization Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Config':<30} {'Train R¬≤':<12} {'Test R¬≤':<12}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for config in configs:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        **config\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    train_r2 = model.score(X_train, y_train)\n",
    "    test_r2 = model.score(X_test, y_test)\n",
    "    print(f\"{str(config):<30} {train_r2:<12.4f} {test_r2:<12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1849313",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. Why do we need non-linear activation functions?\n",
    "2. What problem does batch normalization solve?\n",
    "3. How does dropout prevent overfitting?\n",
    "\n",
    "### Technical\n",
    "1. Derive backpropagation for a simple 2-layer network.\n",
    "2. What's the vanishing gradient problem?\n",
    "3. Why is ReLU preferred over sigmoid in deep networks?\n",
    "\n",
    "### Finance-Specific\n",
    "1. Why might neural networks struggle with financial data?\n",
    "2. How would you prevent overfitting in a return prediction model?\n",
    "3. When would you choose MLP over tree-based models?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Architecture | Input ‚Üí Hidden ‚Üí Output with activations |\n",
    "| Training | Backpropagation + gradient descent |\n",
    "| Regularization | Dropout, early stopping, L2 |\n",
    "| Finance | Watch for overfitting on noisy data |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
