{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac980892",
   "metadata": {},
   "source": [
    "# Week 18: Alternative Data - Satellites, Geo, Web Scraping\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Alternative Data Types**: Non-traditional data sources\n",
    "- **Web Scraping**: Extracting data from websites\n",
    "- **Geolocation Data**: Foot traffic, shipping\n",
    "- **Data Quality**: Validation and cleaning\n",
    "\n",
    "---\n",
    "\n",
    "## Why Alternative Data?\n",
    "\n",
    "Traditional data (prices, financials) is crowded. Edge comes from:\n",
    "- **Timing**: Know before earnings\n",
    "- **Granularity**: Store-level vs. company-level\n",
    "- **Unique insights**: See what others can't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ðŸ“š Week 18: Alternative Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908d1d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Types of Alternative Data\n",
    "\n",
    "### Categories\n",
    "\n",
    "| Type | Examples | Use Case |\n",
    "|------|----------|----------|\n",
    "| Satellite | Parking lots, oil tanks | Retail, energy |\n",
    "| Geolocation | Foot traffic, ships | Retail, commodities |\n",
    "| Web/Social | Reviews, job postings | Sentiment, growth |\n",
    "| Transactions | Credit cards, receipts | Revenue nowcasting |\n",
    "| Sensor | IoT, weather | Agriculture, energy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate satellite parking lot data\n",
    "n_weeks = 52\n",
    "dates = pd.date_range('2023-01-01', periods=n_weeks, freq='W')\n",
    "\n",
    "# True company revenue (what we want to predict)\n",
    "true_revenue = 1000 + np.cumsum(np.random.randn(n_weeks) * 50) + 100 * np.sin(np.linspace(0, 4*np.pi, n_weeks))\n",
    "\n",
    "# Satellite-observed parking lot occupancy (leading indicator)\n",
    "parking_occupancy = 0.6 + 0.3 * (true_revenue - true_revenue.mean()) / true_revenue.std() + np.random.randn(n_weeks) * 0.1\n",
    "parking_occupancy = np.clip(parking_occupancy, 0.2, 0.95)\n",
    "\n",
    "# Quarterly reported revenue (lagged, what market sees)\n",
    "quarterly_revenue = pd.Series(true_revenue).rolling(13).mean().shift(4)  # 4 week reporting lag\n",
    "\n",
    "df_alt = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'true_revenue': true_revenue,\n",
    "    'parking_occupancy': parking_occupancy,\n",
    "    'reported_revenue': quarterly_revenue\n",
    "}).set_index('date')\n",
    "\n",
    "print(\"Satellite Data vs. Reported Financials\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Correlation (True Revenue vs Parking): {np.corrcoef(true_revenue, parking_occupancy)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(df_alt.index, df_alt['parking_occupancy'], 'b-', label='Parking Occupancy')\n",
    "axes[0].set_ylabel('Occupancy Rate')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('Satellite Data (Real-time)')\n",
    "\n",
    "axes[1].plot(df_alt.index, df_alt['true_revenue'], 'g-', label='True Revenue')\n",
    "axes[1].plot(df_alt.index, df_alt['reported_revenue'], 'r--', label='Reported (Lagged)', alpha=0.7)\n",
    "axes[1].set_ylabel('Revenue')\n",
    "axes[1].legend()\n",
    "axes[1].set_title('Revenue: True vs. Reported')\n",
    "\n",
    "# Information advantage\n",
    "info_advantage = df_alt['parking_occupancy'].rolling(4).mean() - df_alt['parking_occupancy'].rolling(4).mean().shift(4)\n",
    "axes[2].bar(df_alt.index, info_advantage.fillna(0), color='purple', alpha=0.6)\n",
    "axes[2].set_ylabel('Info Advantage')\n",
    "axes[2].set_title('Trading Signal (Satellite vs. Reported)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca4528",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Web Scraping Basics\n",
    "\n",
    "### Key Libraries\n",
    "\n",
    "- **requests**: HTTP requests\n",
    "- **BeautifulSoup**: HTML parsing\n",
    "- **Selenium**: Dynamic content\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "- Respect robots.txt\n",
    "- Rate limiting\n",
    "- Terms of service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c2a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping example (conceptual - won't run without actual URL)\n",
    "import json\n",
    "\n",
    "def scrape_job_postings(company_name):\n",
    "    \"\"\"Conceptual job posting scraper\"\"\"\n",
    "    # In practice, you would:\n",
    "    # 1. Use requests to get the page\n",
    "    # 2. Parse HTML with BeautifulSoup\n",
    "    # 3. Extract job counts by department\n",
    "    \n",
    "    # Simulated data\n",
    "    return {\n",
    "        'company': company_name,\n",
    "        'engineering': np.random.randint(50, 200),\n",
    "        'sales': np.random.randint(20, 100),\n",
    "        'research': np.random.randint(10, 50),\n",
    "        'timestamp': pd.Timestamp.now()\n",
    "    }\n",
    "\n",
    "# Simulate tracking job postings over time\n",
    "companies = ['TechCo', 'FinanceCorp', 'RetailInc']\n",
    "job_data = []\n",
    "\n",
    "for date in pd.date_range('2023-01-01', periods=12, freq='M'):\n",
    "    for company in companies:\n",
    "        data = scrape_job_postings(company)\n",
    "        data['date'] = date\n",
    "        job_data.append(data)\n",
    "\n",
    "df_jobs = pd.DataFrame(job_data)\n",
    "print(\"Job Posting Data Sample:\")\n",
    "print(df_jobs.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8842c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Data Quality Checks\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **Missing data**: Gaps in coverage\n",
    "2. **Survivorship**: Only see winners\n",
    "3. **Look-ahead**: Data available only in hindsight\n",
    "4. **Noise**: Signal-to-noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fce340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, time_col='date'):\n",
    "    \"\"\"Generate data quality report\"\"\"\n",
    "    report = {\n",
    "        'total_rows': len(df),\n",
    "        'date_range': f\"{df[time_col].min()} to {df[time_col].max()}\",\n",
    "        'missing_pct': df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100,\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "    }\n",
    "    \n",
    "    # Column-specific\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        report[f'{col}_missing'] = df[col].isnull().sum()\n",
    "        report[f'{col}_zeros'] = (df[col] == 0).sum()\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Example\n",
    "report = data_quality_report(df_alt.reset_index(), time_col='date')\n",
    "print(\"Data Quality Report\")\n",
    "print(\"=\"*50)\n",
    "for k, v in report.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0497ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Building Alt Data Trading Signal\n",
    "\n",
    "### Process\n",
    "\n",
    "1. **Acquire**: Get raw data\n",
    "2. **Clean**: Handle missing, outliers\n",
    "3. **Aggregate**: Combine to company level\n",
    "4. **Normalize**: Make comparable\n",
    "5. **Signal**: Convert to trading signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example: Satellite to trading signal\n",
    "def create_alt_data_signal(parking_data, price_data):\n",
    "    \"\"\"Create trading signal from satellite parking data\"\"\"\n",
    "    \n",
    "    # 1. Rolling average (smooth noise)\n",
    "    smooth_parking = parking_data.rolling(4).mean()\n",
    "    \n",
    "    # 2. Z-score normalization\n",
    "    zscore = (smooth_parking - smooth_parking.rolling(13).mean()) / smooth_parking.rolling(13).std()\n",
    "    \n",
    "    # 3. Generate signal\n",
    "    signal = np.sign(zscore)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "# Simulate price data\n",
    "price_returns = 0.001 * df_alt['true_revenue'].pct_change() + np.random.randn(len(df_alt)) * 0.02\n",
    "prices = 100 * (1 + price_returns).cumprod()\n",
    "\n",
    "# Create signal\n",
    "signal = create_alt_data_signal(df_alt['parking_occupancy'], prices)\n",
    "\n",
    "# Backtest\n",
    "strategy_returns = signal.shift(1) * price_returns\n",
    "strategy_returns = strategy_returns.dropna()\n",
    "\n",
    "sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(52)\n",
    "print(f\"\\nAlt Data Strategy Sharpe: {sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77689547",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What makes alternative data valuable?\n",
    "2. How do you evaluate alt data before buying?\n",
    "3. What are the risks of relying on alt data?\n",
    "\n",
    "### Technical\n",
    "1. How do you handle missing satellite images?\n",
    "2. What's the latency of different alt data sources?\n",
    "3. How do you validate alt data predictions?\n",
    "\n",
    "### Finance-Specific\n",
    "1. How do you prevent alpha decay with alt data?\n",
    "2. What's the typical cost structure for alt data?\n",
    "3. How do you combine multiple alt data sources?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Data Type | Latency | Cost | Alpha Potential |\n",
    "|-----------|---------|------|----------------|\n",
    "| Satellite | Days | High | High (if novel) |\n",
    "| Web Scrape | Hours | Low | Medium |\n",
    "| Transactions | Days | Very High | High |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
