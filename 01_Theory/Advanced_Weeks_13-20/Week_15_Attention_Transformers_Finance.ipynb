{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e135ea02",
   "metadata": {},
   "source": [
    "# Week 15: Attention & Transformers for Finance\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Attention Mechanism**: Focus on relevant parts of input\n",
    "- **Transformer Architecture**: Self-attention without recurrence\n",
    "- **Temporal Fusion Transformer**: State-of-the-art time series\n",
    "- **Finance Applications**: Multi-horizon forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## Why Transformers?\n",
    "\n",
    "- **Parallel processing**: No sequential bottleneck\n",
    "- **Long-range dependencies**: Direct connections\n",
    "- **Interpretable**: Attention weights show what matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries loaded!\")\n",
    "print(\"üìö Week 15: Attention & Transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078db54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Attention Mechanism\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### Components\n",
    "\n",
    "- **Query (Q)**: What am I looking for?\n",
    "- **Key (K)**: What do I contain?\n",
    "- **Value (V)**: What information do I provide?\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "Attention is like a search engine. The Query asks a question, Keys are indexed by their content, and Values are retrieved based on Query-Key match strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223324c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"Compute scaled dot-product attention\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Softmax\n",
    "    exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example: 5 time steps, 4 features\n",
    "seq_len = 5\n",
    "d_model = 4\n",
    "\n",
    "# In self-attention, Q, K, V come from the same input\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, attention = scaled_dot_product_attention(X, X, X)\n",
    "\n",
    "print(\"Self-Attention Example\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\")\n",
    "print(attention.round(3))\n",
    "\n",
    "# Visualize attention\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention, cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563efaf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Multi-Head Attention\n",
    "\n",
    "### Multiple Attention Heads\n",
    "\n",
    "Run attention multiple times with different projections:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$$\n",
    "\n",
    "Where: $head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "Each head looks at the data from a different \"perspective.\" One head might focus on short-term patterns, another on long-term trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b20110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Projection matrices\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Project\n",
    "        Q = X @ self.W_q\n",
    "        K = X @ self.W_k\n",
    "        V = X @ self.W_v\n",
    "        \n",
    "        # Split into heads\n",
    "        seq_len = X.shape[0]\n",
    "        Q = Q.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        K = K.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        V = V.reshape(seq_len, self.n_heads, self.d_k)\n",
    "        \n",
    "        # Attention for each head\n",
    "        heads = []\n",
    "        for h in range(self.n_heads):\n",
    "            out, _ = scaled_dot_product_attention(Q[:, h, :], K[:, h, :], V[:, h, :])\n",
    "            heads.append(out)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        concat = np.concatenate(heads, axis=-1)\n",
    "        output = concat @ self.W_o\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test\n",
    "mha = MultiHeadAttention(d_model=8, n_heads=2)\n",
    "X = np.random.randn(5, 8)\n",
    "output = mha.forward(X)\n",
    "print(f\"Multi-Head Attention output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba45734",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Transformer Architecture\n",
    "\n",
    "### Encoder Block\n",
    "\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Normalize (residual connection)\n",
    "3. Feed-Forward Network\n",
    "4. Add & Normalize\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since Transformers have no recurrence, we add position information:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1933f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"Generate positional encodings\"\"\"\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    position = np.arange(seq_len).reshape(-1, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    PE[:, 0::2] = np.sin(position * div_term)\n",
    "    PE[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return PE\n",
    "\n",
    "# Visualize\n",
    "PE = positional_encoding(50, 64)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(PE.T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Dimension')\n",
    "plt.title('Positional Encoding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744017fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Transformer for Time Series\n",
    "\n",
    "### Adaptations for Finance\n",
    "\n",
    "1. **Causal masking**: Prevent looking at future\n",
    "2. **Temporal features**: Time-aware embeddings\n",
    "3. **Multi-horizon output**: Predict multiple steps ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    class TimeSeriesTransformer(nn.Module):\n",
    "        def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, forecast_horizon=5):\n",
    "            super().__init__()\n",
    "            self.embedding = nn.Linear(input_dim, d_model)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.fc_out = nn.Linear(d_model, forecast_horizon)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x: (batch, seq_len, features)\n",
    "            x = self.embedding(x)\n",
    "            x = self.transformer(x)\n",
    "            return self.fc_out(x[:, -1, :])  # Use last position for forecast\n",
    "    \n",
    "    # Test\n",
    "    model = TimeSeriesTransformer(input_dim=5, d_model=32, forecast_horizon=3)\n",
    "    x = torch.randn(16, 20, 5)  # batch=16, seq_len=20, features=5\n",
    "    out = model(x)\n",
    "    print(f\"Transformer output shape: {out.shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d671c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What advantage does attention have over RNNs?\n",
    "2. Why do we need positional encoding?\n",
    "3. What do attention weights tell us about the model?\n",
    "\n",
    "### Technical\n",
    "1. Explain the computational complexity of self-attention.\n",
    "2. How does multi-head attention help?\n",
    "3. What is causal masking and why is it important?\n",
    "\n",
    "### Finance-Specific\n",
    "1. How would you interpret attention weights in a trading context?\n",
    "2. What are the challenges of using Transformers for financial data?\n",
    "3. How would you handle multiple assets with Transformers?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Attention | Focus on relevant inputs |\n",
    "| Transformers | Parallel, long-range dependencies |\n",
    "| Finance Use | Multi-horizon, interpretable forecasts |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
