{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f26f50",
   "metadata": {},
   "source": [
    "# Week 14: Recurrent Networks - LSTM, GRU for Sequences\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **RNN Fundamentals**: Sequential processing\n",
    "- **LSTM**: Long Short-Term Memory gates\n",
    "- **GRU**: Gated Recurrent Units (simplified LSTM)\n",
    "- **Finance Applications**: Time series forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## Why Recurrent Networks?\n",
    "\n",
    "Financial data is sequential - today depends on yesterday:\n",
    "- Price history matters\n",
    "- Patterns repeat at different time scales\n",
    "- Order of events is crucial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf5bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries loaded!\")\n",
    "print(\"üìö Week 14: Recurrent Networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb567b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: RNN Fundamentals\n",
    "\n",
    "### The Idea\n",
    "\n",
    "RNNs maintain a hidden state that carries information through time:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "An RNN has \"memory\" - it processes sequences one step at a time, updating its internal state. Think of it as reading a book word by word while remembering context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908266a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN cell (conceptual)\n",
    "class SimpleRNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Single step forward pass\"\"\"\n",
    "        h_new = np.tanh(self.Wxh @ x + self.Whh @ h_prev + self.bh)\n",
    "        return h_new\n",
    "\n",
    "# Demonstrate RNN processing\n",
    "rnn = SimpleRNNCell(input_size=3, hidden_size=4)\n",
    "sequence = np.random.randn(5, 3, 1)  # 5 time steps, 3 features\n",
    "h = np.zeros((4, 1))  # Initial hidden state\n",
    "\n",
    "print(\"RNN Forward Pass\")\n",
    "print(\"=\"*50)\n",
    "for t, x in enumerate(sequence):\n",
    "    h = rnn.forward(x, h)\n",
    "    print(f\"Time {t}: Hidden state shape = {h.shape}, values = {h.ravel()[:2]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c068e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: LSTM - Long Short-Term Memory\n",
    "\n",
    "### The Problem with Vanilla RNN\n",
    "\n",
    "- Vanishing gradients ‚Üí Can't learn long-term dependencies\n",
    "- Exploding gradients ‚Üí Unstable training\n",
    "\n",
    "### LSTM Gates\n",
    "\n",
    "1. **Forget Gate**: What to forget from cell state\n",
    "2. **Input Gate**: What new info to add\n",
    "3. **Output Gate**: What to output\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "LSTM has gates that control information flow:\n",
    "- Forget gate: \"Should I forget yesterday's news?\"\n",
    "- Input gate: \"Is today's info worth remembering?\"\n",
    "- Output gate: \"What should I report now?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM gates\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create a diagram\n",
    "ax.text(0.1, 0.5, 'x_t\\n(input)', ha='center', fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "ax.text(0.3, 0.8, 'Forget\\nGate', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightyellow'))\n",
    "ax.text(0.5, 0.8, 'Input\\nGate', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightgreen'))\n",
    "ax.text(0.7, 0.8, 'Output\\nGate', ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='lightcoral'))\n",
    "ax.text(0.5, 0.5, 'Cell State\\nC_t', ha='center', fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "ax.text(0.9, 0.5, 'h_t\\n(output)', ha='center', fontsize=12, bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(0.3, 0.6), xytext=(0.15, 0.5), arrowprops=dict(arrowstyle='->'))\n",
    "ax.annotate('', xy=(0.5, 0.6), xytext=(0.35, 0.75), arrowprops=dict(arrowstyle='->'))\n",
    "ax.annotate('', xy=(0.65, 0.5), xytext=(0.55, 0.5), arrowprops=dict(arrowstyle='->'))\n",
    "ax.annotate('', xy=(0.85, 0.5), xytext=(0.7, 0.65), arrowprops=dict(arrowstyle='->'))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('LSTM Cell Structure (Simplified)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95018f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LSTM for Price Prediction\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "For sequence models, we need to create sliding windows of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cce72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic price data\n",
    "n = 1000\n",
    "np.random.seed(42)\n",
    "\n",
    "# Random walk with trend and seasonality\n",
    "returns = np.random.randn(n) * 0.02 + 0.0001\n",
    "prices = 100 * np.cumprod(1 + returns)\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "prices_scaled = scaler.fit_transform(prices.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Create sequences\n",
    "seq_length = 20\n",
    "X, y = create_sequences(prices_scaled, seq_length)\n",
    "\n",
    "# Reshape for LSTM: (samples, time steps, features)\n",
    "X = X.reshape(-1, seq_length, 1)\n",
    "\n",
    "# Train/test split (no shuffle for time series!)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Sequence shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    class LSTMPredictor(nn.Module):\n",
    "        def __init__(self, input_size=1, hidden_size=50, num_layers=2):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            return self.fc(lstm_out[:, -1, :])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train_t = torch.FloatTensor(X_train)\n",
    "    y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_test_t = torch.FloatTensor(X_test)\n",
    "    y_test_t = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    \n",
    "    # Train\n",
    "    model = LSTMPredictor()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    epochs = 50\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_t)\n",
    "        loss = criterion(outputs, y_train_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_pred = model(X_train_t).numpy()\n",
    "        test_pred = model(X_test_t).numpy()\n",
    "    \n",
    "    print(f\"\\nTest MSE: {np.mean((test_pred.ravel() - y_test)**2):.6f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch not installed. Skipping LSTM implementation.\")\n",
    "    print(\"Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae982b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: GRU - Gated Recurrent Unit\n",
    "\n",
    "### Simplified LSTM\n",
    "\n",
    "GRU has only 2 gates (vs LSTM's 3):\n",
    "- **Reset Gate**: How much past to forget\n",
    "- **Update Gate**: How much new info to add\n",
    "\n",
    "### When to Use GRU vs LSTM\n",
    "\n",
    "- GRU: Faster training, fewer parameters, shorter sequences\n",
    "- LSTM: Longer sequences, more complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What problem does LSTM solve that vanilla RNN cannot?\n",
    "2. Explain the purpose of each gate in LSTM.\n",
    "3. Why use sequences for financial forecasting?\n",
    "\n",
    "### Technical\n",
    "1. How do you choose sequence length?\n",
    "2. What's the difference between stateful and stateless LSTM?\n",
    "3. How do you handle variable length sequences?\n",
    "\n",
    "### Finance-Specific\n",
    "1. Can LSTM capture mean reversion?\n",
    "2. How would you use LSTM for volatility forecasting?\n",
    "3. What are the risks of using LSTM for trading?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Model | Complexity | Best For |\n",
    "|-------|------------|----------|\n",
    "| RNN | Simple | Short sequences |\n",
    "| LSTM | Complex | Long-term dependencies |\n",
    "| GRU | Medium | Balance of speed/performance |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
