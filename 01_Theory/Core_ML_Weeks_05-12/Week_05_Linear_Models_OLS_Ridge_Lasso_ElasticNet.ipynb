{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b2a416",
   "metadata": {},
   "source": [
    "# Week 5: Linear Models - OLS, Ridge, Lasso, ElasticNet\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Ordinary Least Squares (OLS)**: The foundation of regression\n",
    "- **Ridge Regression (L2)**: Handling multicollinearity\n",
    "- **Lasso Regression (L1)**: Feature selection through regularization\n",
    "- **ElasticNet**: Combining L1 and L2 penalties\n",
    "- **Finance Applications**: Factor models, risk attribution\n",
    "\n",
    "---\n",
    "\n",
    "## Why Linear Models in Finance?\n",
    "\n",
    "Linear models are the **workhorses** of quantitative finance:\n",
    "- Simple, interpretable, fast\n",
    "- Foundation for factor models (Fama-French)\n",
    "- Risk attribution and decomposition\n",
    "- Baseline for comparing complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries loaded!\")\n",
    "print(\"üìö Week 5: Linear Models Theory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c785d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Ordinary Least Squares (OLS)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Given data $(X, y)$, find weights $\\beta$ that minimize:\n",
    "\n",
    "$$\\min_{\\beta} ||y - X\\beta||_2^2 = \\min_{\\beta} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2$$\n",
    "\n",
    "### Closed-Form Solution\n",
    "\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "OLS finds the line (or hyperplane) that minimizes the sum of squared errors. Think of it as finding the \"best fit\" line through your data points.\n",
    "\n",
    "### Finance Application: Factor Model\n",
    "\n",
    "$$R_i = \\alpha + \\beta_1 \\cdot MKT + \\beta_2 \\cdot SMB + \\beta_3 \\cdot HML + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e30f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic factor returns\n",
    "n_days = 252 * 5  # 5 years\n",
    "\n",
    "# Market factor (beta to market)\n",
    "mkt = np.random.normal(0.0004, 0.01, n_days)  # Market returns\n",
    "\n",
    "# Size factor (SMB)\n",
    "smb = np.random.normal(0.0001, 0.005, n_days)\n",
    "\n",
    "# Value factor (HML)\n",
    "hml = np.random.normal(0.0001, 0.005, n_days)\n",
    "\n",
    "# Stock return = alpha + factor exposures + noise\n",
    "true_alpha = 0.0002\n",
    "true_betas = [1.2, 0.3, -0.2]  # MKT, SMB, HML exposures\n",
    "\n",
    "stock_return = (true_alpha + \n",
    "                true_betas[0] * mkt + \n",
    "                true_betas[1] * smb + \n",
    "                true_betas[2] * hml + \n",
    "                np.random.normal(0, 0.008, n_days))\n",
    "\n",
    "# Fit OLS\n",
    "X = np.column_stack([mkt, smb, hml])\n",
    "model = LinearRegression()\n",
    "model.fit(X, stock_return)\n",
    "\n",
    "print(\"OLS Factor Model Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Alpha (daily): {model.intercept_:.6f} (true: {true_alpha})\")\n",
    "print(f\"Alpha (annual): {model.intercept_ * 252:.2%}\")\n",
    "print(f\"\\nBeta Exposures:\")\n",
    "print(f\"  MKT: {model.coef_[0]:.3f} (true: {true_betas[0]})\")\n",
    "print(f\"  SMB: {model.coef_[1]:.3f} (true: {true_betas[1]})\")\n",
    "print(f\"  HML: {model.coef_[2]:.3f} (true: {true_betas[2]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7fd21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Ridge Regression (L2 Regularization)\n",
    "\n",
    "### The Problem with OLS\n",
    "\n",
    "When features are correlated (multicollinearity), $(X^TX)^{-1}$ becomes unstable ‚Üí large coefficient variance.\n",
    "\n",
    "### Ridge Solution\n",
    "\n",
    "Add L2 penalty to shrink coefficients:\n",
    "\n",
    "$$\\min_{\\beta} ||y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2$$\n",
    "\n",
    "Closed-form: $\\hat{\\beta}_{ridge} = (X^TX + \\lambda I)^{-1}X^Ty$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "Ridge adds a \"penalty\" for large coefficients. It's like saying \"I want a good fit, but I also want small, stable coefficients.\" This prevents overfitting when features are correlated.\n",
    "\n",
    "### Finance Application\n",
    "\n",
    "- Many financial factors are correlated (momentum vs. reversal)\n",
    "- Ridge stabilizes factor loadings\n",
    "- More robust out-of-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlated features (simulating multicollinearity)\n",
    "n = 500\n",
    "X1 = np.random.randn(n)\n",
    "X2 = X1 + np.random.randn(n) * 0.1  # Highly correlated with X1\n",
    "X3 = np.random.randn(n)\n",
    "\n",
    "true_beta = [1, 1, 0.5]\n",
    "y = true_beta[0]*X1 + true_beta[1]*X2 + true_beta[2]*X3 + np.random.randn(n) * 0.5\n",
    "\n",
    "X = np.column_stack([X1, X2, X3])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compare OLS vs Ridge\n",
    "ols = LinearRegression().fit(X_scaled, y)\n",
    "ridge = Ridge(alpha=1.0).fit(X_scaled, y)\n",
    "\n",
    "print(\"Multicollinearity Example\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Correlation X1-X2: {np.corrcoef(X1, X2)[0,1]:.3f}\")\n",
    "print(f\"\\nCoefficients (true: {true_beta}):\")\n",
    "print(f\"  OLS:   [{ols.coef_[0]:.2f}, {ols.coef_[1]:.2f}, {ols.coef_[2]:.2f}]\")\n",
    "print(f\"  Ridge: [{ridge.coef_[0]:.2f}, {ridge.coef_[1]:.2f}, {ridge.coef_[2]:.2f}]\")\n",
    "print(f\"\\n‚ö†Ô∏è OLS has unstable coefficients due to multicollinearity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ddcf16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Lasso Regression (L1 Regularization)\n",
    "\n",
    "### The Lasso Objective\n",
    "\n",
    "$$\\min_{\\beta} ||y - X\\beta||_2^2 + \\lambda ||\\beta||_1$$\n",
    "\n",
    "### Key Property: Sparsity\n",
    "\n",
    "Lasso drives coefficients **exactly to zero** ‚Üí automatic feature selection!\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "Lasso is like Ridge, but instead of shrinking coefficients, it eliminates them entirely. If a feature isn't important, Lasso sets its coefficient to zero.\n",
    "\n",
    "### Finance Application\n",
    "\n",
    "- Select which factors actually matter\n",
    "- Sparse portfolios (fewer positions)\n",
    "- Interpretable models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with many features, only some relevant\n",
    "n, p = 200, 50  # 200 samples, 50 features\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "# Only first 5 features matter\n",
    "true_beta = np.zeros(p)\n",
    "true_beta[:5] = [2, -1.5, 1, -0.5, 0.8]\n",
    "\n",
    "y = X @ true_beta + np.random.randn(n) * 0.5\n",
    "\n",
    "# Fit Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(\"Lasso Feature Selection\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True non-zero features: 5\")\n",
    "print(f\"Lasso non-zero features: {np.sum(lasso.coef_ != 0)}\")\n",
    "print(f\"\\nTrue coefficients (first 5): {true_beta[:5]}\")\n",
    "print(f\"Lasso coefficients (first 5): {np.round(lasso.coef_[:5], 2)}\")\n",
    "print(f\"\\n‚úÖ Lasso correctly identified the important features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64c3fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: ElasticNet (L1 + L2)\n",
    "\n",
    "### Best of Both Worlds\n",
    "\n",
    "$$\\min_{\\beta} ||y - X\\beta||_2^2 + \\lambda_1 ||\\beta||_1 + \\lambda_2 ||\\beta||_2^2$$\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Correlated features (Ridge helps)\n",
    "- Want sparsity (Lasso helps)\n",
    "- Many features with groups of correlated ones\n",
    "\n",
    "### Finance Application\n",
    "\n",
    "Factor models with correlated factors where you want to select the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae630893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = {\n",
    "    'OLS': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "print(\"Model Comparison (Cross-Validation R¬≤)\")\n",
    "print(\"=\"*50)\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    print(f\"{name:12} R¬≤ = {scores.mean():.3f} ¬± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ab984",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. When would you choose Ridge over Lasso?\n",
    "2. What happens to Lasso coefficients as Œª increases?\n",
    "3. How do you interpret a negative beta in a factor model?\n",
    "\n",
    "### Technical\n",
    "1. Derive the Ridge regression closed-form solution\n",
    "2. Why doesn't Lasso have a closed-form solution?\n",
    "3. How do you select the regularization parameter?\n",
    "\n",
    "### Finance-Specific\n",
    "1. Your factor model has 50 factors. How do you reduce it?\n",
    "2. How would you test if alpha is statistically significant?\n",
    "3. What's the difference between realized and predicted beta?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Model | Penalty | Sparsity | When to Use |\n",
    "|-------|---------|----------|-------------|\n",
    "| OLS | None | No | Simple problems, no collinearity |\n",
    "| Ridge | L2 | No | Multicollinearity, all features matter |\n",
    "| Lasso | L1 | Yes | Feature selection needed |\n",
    "| ElasticNet | L1+L2 | Yes | Correlated features + sparsity |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
