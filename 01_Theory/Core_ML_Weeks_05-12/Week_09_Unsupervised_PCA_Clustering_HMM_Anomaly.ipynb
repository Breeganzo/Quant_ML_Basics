{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57453df8",
   "metadata": {},
   "source": [
    "# Week 9: Unsupervised Learning - PCA, Clustering, HMM, Anomaly Detection\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **PCA**: Dimensionality reduction for factors\n",
    "- **Clustering**: Market regime identification\n",
    "- **Hidden Markov Models (HMM)**: Regime switching\n",
    "- **Anomaly Detection**: Unusual market conditions\n",
    "\n",
    "---\n",
    "\n",
    "## Why Unsupervised Learning in Finance?\n",
    "\n",
    "- **No labels needed**: Markets don't come with \"regime\" labels\n",
    "- **Pattern discovery**: Find structure in data\n",
    "- **Risk management**: Detect unusual conditions\n",
    "- **Feature engineering**: Create new factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries loaded!\")\n",
    "print(\"üìö Week 9: Unsupervised Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df6dd7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Principal Component Analysis (PCA)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Many features, but they're correlated. Can we reduce dimensions while preserving information?\n",
    "\n",
    "### The Math\n",
    "\n",
    "1. Center the data: $\\tilde{X} = X - \\bar{X}$\n",
    "2. Compute covariance: $\\Sigma = \\frac{1}{n}\\tilde{X}^T\\tilde{X}$\n",
    "3. Eigendecomposition: $\\Sigma = V\\Lambda V^T$\n",
    "4. Project: $Z = XV_k$ (keep top k eigenvectors)\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "PCA finds the directions in your data with the most variation. Instead of 100 correlated features, you might get 5 uncorrelated \"principal components\" that explain 95% of the variance.\n",
    "\n",
    "### Finance Application: Statistical Factor Model\n",
    "\n",
    "PCA on stock returns reveals \"statistical factors\" - latent drivers of returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd3bb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stock returns with underlying factors\n",
    "n_days = 252\n",
    "n_stocks = 50\n",
    "\n",
    "# True underlying factors\n",
    "market_factor = np.random.randn(n_days) * 0.01\n",
    "sector_factor = np.random.randn(n_days) * 0.008\n",
    "momentum_factor = np.random.randn(n_days) * 0.006\n",
    "\n",
    "# Stock loadings on factors\n",
    "market_beta = np.random.uniform(0.5, 1.5, n_stocks)\n",
    "sector_beta = np.random.randn(n_stocks) * 0.5\n",
    "momentum_beta = np.random.randn(n_stocks) * 0.3\n",
    "\n",
    "# Generate returns\n",
    "returns = np.zeros((n_days, n_stocks))\n",
    "for i in range(n_stocks):\n",
    "    returns[:, i] = (\n",
    "        market_beta[i] * market_factor +\n",
    "        sector_beta[i] * sector_factor +\n",
    "        momentum_beta[i] * momentum_factor +\n",
    "        np.random.randn(n_days) * 0.02  # Idiosyncratic\n",
    "    )\n",
    "\n",
    "# Apply PCA\n",
    "scaler = StandardScaler()\n",
    "returns_scaled = scaler.fit_transform(returns)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(returns_scaled)\n",
    "\n",
    "# Explained variance\n",
    "exp_var = pca.explained_variance_ratio_\n",
    "cum_var = np.cumsum(exp_var)\n",
    "\n",
    "print(\"PCA Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Variance explained by top 5 PCs:\")\n",
    "for i in range(5):\n",
    "    print(f\"  PC{i+1}: {exp_var[i]:.1%} (cumulative: {cum_var[i]:.1%})\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 11), exp_var[:10])\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.title('Scree Plot')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), cum_var[:10], 'bo-')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66093a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Clustering - K-Means\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "1. Initialize k cluster centers\n",
    "2. Assign points to nearest center\n",
    "3. Update centers as mean of assigned points\n",
    "4. Repeat until convergence\n",
    "\n",
    "### Objective\n",
    "\n",
    "$$\\min \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "K-Means groups similar data points together. Think of it as sorting market days into \"buckets\" based on their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create market regime features\n",
    "n_days = 1000\n",
    "vix = np.random.exponential(20, n_days)\n",
    "volume_ratio = np.random.lognormal(0, 0.5, n_days)\n",
    "momentum = np.random.randn(n_days) * 10\n",
    "\n",
    "X = np.column_stack([vix, volume_ratio, momentum])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Find optimal k using elbow method\n",
    "inertias = []\n",
    "K_range = range(2, 10)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertias, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.axvline(x=4, color='r', linestyle='--', label='Elbow')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with k=4\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"Market Regime Clusters\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "regime_names = {0: 'Normal', 1: 'High Vol', 2: 'Low Vol', 3: 'Trending'}\n",
    "\n",
    "for c in range(4):\n",
    "    mask = clusters == c\n",
    "    print(f\"\\nCluster {c} ({mask.sum()} days):\")\n",
    "    print(f\"  Avg VIX: {vix[mask].mean():.1f}\")\n",
    "    print(f\"  Avg Volume Ratio: {volume_ratio[mask].mean():.2f}\")\n",
    "    print(f\"  Avg Momentum: {momentum[mask].mean():.1f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "scatter = axes[0].scatter(vix, momentum, c=clusters, cmap='viridis', alpha=0.5)\n",
    "axes[0].set_xlabel('VIX')\n",
    "axes[0].set_ylabel('Momentum')\n",
    "axes[0].set_title('Market Regimes')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "axes[1].bar(cluster_counts.index, cluster_counts.values)\n",
    "axes[1].set_xlabel('Cluster')\n",
    "axes[1].set_ylabel('Days')\n",
    "axes[1].set_title('Cluster Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c73ad5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Hidden Markov Models (HMM)\n",
    "\n",
    "### The Concept\n",
    "\n",
    "Markets switch between hidden \"regimes\" (bull, bear, sideways). We observe returns but not the regime.\n",
    "\n",
    "### Model Components\n",
    "\n",
    "- **Hidden states**: S = {Bull, Bear, Neutral}\n",
    "- **Transition matrix**: P(state_t | state_{t-1})\n",
    "- **Emission**: P(return | state)\n",
    "\n",
    "### ü§î Simple Explanation\n",
    "\n",
    "HMM assumes the market is always in one of several \"modes\" (regimes), but we can't directly see which one. We can only observe returns and infer the regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from hmmlearn import hmm\n",
    "    \n",
    "    # Simulate regime-switching returns\n",
    "    n_days = 2000\n",
    "    true_states = np.zeros(n_days, dtype=int)\n",
    "    \n",
    "    # Simple regime generation\n",
    "    for i in range(1, n_days):\n",
    "        if true_states[i-1] == 0:  # Bull\n",
    "            true_states[i] = np.random.choice([0, 1], p=[0.95, 0.05])\n",
    "        else:  # Bear\n",
    "            true_states[i] = np.random.choice([0, 1], p=[0.1, 0.9])\n",
    "    \n",
    "    # Regime-dependent returns\n",
    "    returns_hmm = np.where(\n",
    "        true_states == 0,\n",
    "        np.random.normal(0.001, 0.01, n_days),  # Bull\n",
    "        np.random.normal(-0.002, 0.025, n_days)  # Bear\n",
    "    )\n",
    "    \n",
    "    # Fit HMM\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"full\", n_iter=100)\n",
    "    model.fit(returns_hmm.reshape(-1, 1))\n",
    "    \n",
    "    # Predict states\n",
    "    predicted_states = model.predict(returns_hmm.reshape(-1, 1))\n",
    "    \n",
    "    print(\"Hidden Markov Model Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nLearned means: {model.means_.ravel()}\")\n",
    "    print(f\"Learned variances: {np.sqrt(model.covars_.ravel())}\")\n",
    "    print(f\"\\nTransition matrix:\")\n",
    "    print(model.transmat_.round(3))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "    \n",
    "    axes[0].plot(np.cumsum(returns_hmm))\n",
    "    axes[0].set_ylabel('Cumulative Return')\n",
    "    axes[0].set_title('Returns')\n",
    "    \n",
    "    axes[1].fill_between(range(len(predicted_states)), 0, predicted_states, alpha=0.5, label='Predicted')\n",
    "    axes[1].set_ylabel('Regime')\n",
    "    axes[1].set_xlabel('Day')\n",
    "    axes[1].set_title('Regime States')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è hmmlearn not installed. Install with: pip install hmmlearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149a8be9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Anomaly Detection\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **Isolation Forest**: Isolate anomalies using random trees\n",
    "2. **DBSCAN**: Density-based clustering (outliers = no cluster)\n",
    "3. **Statistical**: Z-score, Mahalanobis distance\n",
    "\n",
    "### Finance Applications\n",
    "\n",
    "- Flash crash detection\n",
    "- Unusual trading patterns\n",
    "- Market manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73495f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection with Isolation Forest\n",
    "n_normal = 950\n",
    "n_anomaly = 50\n",
    "\n",
    "# Normal market conditions\n",
    "normal_vix = np.random.exponential(15, n_normal)\n",
    "normal_volume = np.random.lognormal(0, 0.3, n_normal)\n",
    "\n",
    "# Anomalous conditions (flash crashes, unusual activity)\n",
    "anomaly_vix = np.random.uniform(50, 80, n_anomaly)\n",
    "anomaly_volume = np.random.uniform(3, 6, n_anomaly)\n",
    "\n",
    "# Combine\n",
    "X_all = np.vstack([\n",
    "    np.column_stack([normal_vix, normal_volume]),\n",
    "    np.column_stack([anomaly_vix, anomaly_volume])\n",
    "])\n",
    "labels_true = np.array([0]*n_normal + [1]*n_anomaly)\n",
    "\n",
    "# Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "predictions = iso_forest.fit_predict(X_all)\n",
    "anomaly_pred = predictions == -1  # -1 indicates anomaly\n",
    "\n",
    "print(\"Isolation Forest Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True anomalies: {labels_true.sum()}\")\n",
    "print(f\"Detected anomalies: {anomaly_pred.sum()}\")\n",
    "print(f\"True positives: {(anomaly_pred & (labels_true==1)).sum()}\")\n",
    "print(f\"False positives: {(anomaly_pred & (labels_true==0)).sum()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.scatter(X_all[~anomaly_pred, 0], X_all[~anomaly_pred, 1], c='blue', alpha=0.5, label='Normal')\n",
    "plt.scatter(X_all[anomaly_pred, 0], X_all[anomaly_pred, 1], c='red', s=100, marker='x', label='Anomaly')\n",
    "plt.xlabel('VIX')\n",
    "plt.ylabel('Volume Ratio')\n",
    "plt.title('Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f21671",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What's the difference between PCA and factor analysis?\n",
    "2. How do you choose the number of clusters in K-Means?\n",
    "3. When would you use HMM over K-Means for regime detection?\n",
    "\n",
    "### Technical\n",
    "1. Derive the PCA solution using SVD.\n",
    "2. What are the limitations of K-Means for financial data?\n",
    "3. Explain the Viterbi algorithm for HMM.\n",
    "\n",
    "### Finance-Specific\n",
    "1. How would you use PCA factors in a trading strategy?\n",
    "2. What market events would you expect anomaly detection to catch?\n",
    "3. How often should regime models be retrained?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Method | Use Case | Output |\n",
    "|--------|----------|--------|\n",
    "| PCA | Dimension reduction | Uncorrelated factors |\n",
    "| K-Means | Regime clustering | Discrete labels |\n",
    "| HMM | Regime switching | Probabilistic states |\n",
    "| Isolation Forest | Anomaly detection | Outlier scores |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
