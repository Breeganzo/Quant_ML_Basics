{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2686337",
   "metadata": {},
   "source": [
    "# Week 7: Tree-Based Models - RF, GBM, XGBoost, LightGBM\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Decision Trees**: The building block\n",
    "- **Random Forest**: Bagging ensemble\n",
    "- **Gradient Boosting (GBM)**: Sequential learning\n",
    "- **XGBoost & LightGBM**: Production-grade implementations\n",
    "- **Finance Applications**: Non-linear patterns, feature importance\n",
    "\n",
    "---\n",
    "\n",
    "## Why Trees in Finance?\n",
    "\n",
    "- Capture non-linear relationships\n",
    "- Handle mixed feature types\n",
    "- Built-in feature importance\n",
    "- No scaling required\n",
    "- State-of-the-art for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2105d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ðŸ“š Week 7: Tree-Based Models Theory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3469f5be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Decision Trees\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "Recursively split data to minimize impurity:\n",
    "\n",
    "**For Regression (MSE):**\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\bar{y})^2$$\n",
    "\n",
    "**For Classification (Gini):**\n",
    "$$Gini = 1 - \\sum_{k=1}^{K}p_k^2$$\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "A decision tree is like a game of 20 questions. At each node, it asks a yes/no question about a feature. Based on the answer, it goes left or right until it reaches a leaf with a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear financial data\n",
    "n = 1000\n",
    "momentum = np.random.randn(n) * 2\n",
    "volatility = np.abs(np.random.randn(n))\n",
    "volume = np.random.exponential(1, n)\n",
    "\n",
    "# Non-linear relationship: returns depend on regimes\n",
    "returns = np.where(\n",
    "    (momentum > 0) & (volatility < 0.5),\n",
    "    0.002 + 0.001 * momentum,  # Good regime: momentum works\n",
    "    np.where(\n",
    "        volatility > 1.5,\n",
    "        -0.001,  # High vol regime: negative\n",
    "        0.0001 * volume  # Normal: volume matters\n",
    "    )\n",
    ") + np.random.randn(n) * 0.005\n",
    "\n",
    "X = np.column_stack([momentum, volatility, volume])\n",
    "feature_names = ['Momentum', 'Volatility', 'Volume']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, returns, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit decision tree\n",
    "tree = DecisionTreeRegressor(max_depth=4, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Decision Tree Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train RÂ²: {tree.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test RÂ²:  {tree.score(X_test, y_test):.3f}\")\n",
    "print(f\"\\nFeature Importance:\")\n",
    "for name, imp in zip(feature_names, tree.feature_importances_):\n",
    "    print(f\"  {name}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d130a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Random Forest (Bagging)\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Build many trees on bootstrapped samples and average predictions.\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{B}\\sum_{b=1}^{B}T_b(x)$$\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **n_estimators**: Number of trees\n",
    "- **max_depth**: Tree depth (controls overfitting)\n",
    "- **max_features**: Features per split (adds randomness)\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "Random Forest = \"Wisdom of the crowd\"\n",
    "- Build 100+ different trees\n",
    "- Each sees different data samples\n",
    "- Average their predictions\n",
    "- Result: Lower variance, more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d441269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Compare single tree vs forest\n",
    "tree_single = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "forest = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "tree_single.fit(X_train, y_train)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Single Tree vs Random Forest\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Model':<20} {'Train RÂ²':<12} {'Test RÂ²':<12}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Single Tree':<20} {tree_single.score(X_train, y_train):<12.3f} {tree_single.score(X_test, y_test):<12.3f}\")\n",
    "print(f\"{'Random Forest':<20} {forest.score(X_train, y_train):<12.3f} {forest.score(X_test, y_test):<12.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… Random Forest has better generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb6ccc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gradient Boosting\n",
    "\n",
    "### The Idea: Learn from Mistakes\n",
    "\n",
    "Build trees sequentially, each correcting the errors of the previous:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
    "\n",
    "Where $h_m$ is trained on the **residuals** of $F_{m-1}$.\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "- **learning_rate** (Î·): Step size (smaller = more trees needed)\n",
    "- **n_estimators**: Number of boosting stages\n",
    "- **max_depth**: Usually shallow (3-8)\n",
    "\n",
    "### ðŸ¤” Simple Explanation\n",
    "\n",
    "Gradient Boosting is like hiring consultants:\n",
    "1. First consultant makes predictions\n",
    "2. Second consultant focuses on what the first got wrong\n",
    "3. Third consultant fixes what the first two missed\n",
    "4. Final prediction = sum of all consultants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213eae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Gradient Boosting with different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.3]\n",
    "\n",
    "print(\"Gradient Boosting: Learning Rate Effect\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbm = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    )\n",
    "    gbm.fit(X_train, y_train)\n",
    "    print(f\"LR={lr}: Train RÂ²={gbm.score(X_train, y_train):.3f}, Test RÂ²={gbm.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd8711",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: XGBoost & LightGBM\n",
    "\n",
    "### XGBoost Innovations\n",
    "\n",
    "- **Regularization**: L1/L2 on leaf weights\n",
    "- **Second-order gradients**: Better optimization\n",
    "- **Missing value handling**: Built-in\n",
    "- **Parallel learning**: Fast training\n",
    "\n",
    "### LightGBM Innovations\n",
    "\n",
    "- **Leaf-wise growth**: Grows deepest leaf first\n",
    "- **Histogram binning**: Faster splits\n",
    "- **GOSS**: Gradient-based One-Side Sampling\n",
    "- **Even faster** than XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"XGBoost vs LightGBM\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"XGBoost:  Test RÂ² = {xgb_model.score(X_test, y_test):.3f}\")\n",
    "    print(f\"LightGBM: Test RÂ² = {lgb_model.score(X_test, y_test):.3f}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ XGBoost or LightGBM not installed.\")\n",
    "    print(\"Install with: pip install xgboost lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b998d07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Feature Importance\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. **Impurity-based**: How much does splitting on this feature reduce MSE/Gini?\n",
    "2. **Permutation-based**: How much does performance drop if we shuffle this feature?\n",
    "3. **SHAP**: Game-theoretic approach (more in Week 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d7a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compare importance methods\n",
    "impurity_imp = forest.feature_importances_\n",
    "\n",
    "perm_result = permutation_importance(forest, X_test, y_test, n_repeats=10, random_state=42)\n",
    "perm_imp = perm_result.importances_mean\n",
    "\n",
    "print(\"Feature Importance Comparison\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Feature':<15} {'Impurity':<15} {'Permutation':<15}\")\n",
    "print(\"-\"*50)\n",
    "for name, imp, perm in zip(feature_names, impurity_imp, perm_imp):\n",
    "    print(f\"{name:<15} {imp:<15.3f} {perm:<15.3f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].barh(feature_names, impurity_imp)\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Impurity-based Importance')\n",
    "\n",
    "axes[1].barh(feature_names, perm_imp)\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Permutation Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f90179",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What's the difference between bagging and boosting?\n",
    "2. Why are Random Forests less prone to overfitting than single trees?\n",
    "3. How does XGBoost handle missing values?\n",
    "\n",
    "### Technical\n",
    "1. Derive the gradient boosting update rule.\n",
    "2. What is the bias-variance tradeoff for ensembles?\n",
    "3. How would you tune a gradient boosting model?\n",
    "\n",
    "### Finance-Specific\n",
    "1. Why might tree models work well for regime detection?\n",
    "2. How would you prevent lookahead bias with tree models?\n",
    "3. When would you prefer linear models over trees?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Model | Ensemble Type | Strengths | Weaknesses |\n",
    "|-------|--------------|-----------|------------|\n",
    "| Random Forest | Bagging | Robust, parallel | Can't extrapolate |\n",
    "| GBM | Boosting | Accurate, handles bias | Slower training |\n",
    "| XGBoost | Boosting | Regularized, fast | Complex tuning |\n",
    "| LightGBM | Boosting | Fastest, large data | Memory for large trees |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
