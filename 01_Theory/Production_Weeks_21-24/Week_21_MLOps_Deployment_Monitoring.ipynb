{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0133c496",
   "metadata": {},
   "source": [
    "# Week 21: MLOps - Model Deployment & Monitoring\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this week, you will understand:\n",
    "- **Model Versioning**: Tracking experiments and models\n",
    "- **Deployment Patterns**: Batch, real-time, edge\n",
    "- **Monitoring**: Drift detection, performance tracking\n",
    "- **CI/CD for ML**: Automated pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## Why MLOps?\n",
    "\n",
    "Research â†’ Production gap:\n",
    "- 87% of ML projects never reach production\n",
    "- Models degrade over time\n",
    "- Reproducibility is critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"âœ… Libraries loaded!\")\n",
    "print(\"ğŸ“š Week 21: MLOps - Deployment & Monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff3335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Model Versioning\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Experiment tracking**: Log parameters, metrics, artifacts\n",
    "- **Model registry**: Store and version models\n",
    "- **Data versioning**: Track training data\n",
    "\n",
    "### Tools\n",
    "\n",
    "- MLflow, Weights & Biases, DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74631dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleExperimentTracker:\n",
    "    \"\"\"Basic experiment tracking (conceptual MLflow-like)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = []\n",
    "        self.current_run = None\n",
    "    \n",
    "    def start_run(self, name):\n",
    "        self.current_run = {\n",
    "            'name': name,\n",
    "            'run_id': hashlib.md5(f\"{name}{datetime.now()}\".encode()).hexdigest()[:8],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'params': {},\n",
    "            'metrics': {},\n",
    "            'artifacts': []\n",
    "        }\n",
    "        return self\n",
    "    \n",
    "    def log_param(self, key, value):\n",
    "        self.current_run['params'][key] = value\n",
    "    \n",
    "    def log_metric(self, key, value):\n",
    "        self.current_run['metrics'][key] = value\n",
    "    \n",
    "    def end_run(self):\n",
    "        self.experiments.append(self.current_run)\n",
    "        run_id = self.current_run['run_id']\n",
    "        self.current_run = None\n",
    "        return run_id\n",
    "    \n",
    "    def get_best_run(self, metric, maximize=True):\n",
    "        valid_runs = [e for e in self.experiments if metric in e['metrics']]\n",
    "        if not valid_runs:\n",
    "            return None\n",
    "        key_func = lambda x: x['metrics'][metric]\n",
    "        return max(valid_runs, key=key_func) if maximize else min(valid_runs, key=key_func)\n",
    "\n",
    "# Usage example\n",
    "tracker = SimpleExperimentTracker()\n",
    "\n",
    "# Simulate multiple experiments\n",
    "for lr in [0.001, 0.01, 0.1]:\n",
    "    for depth in [3, 5, 7]:\n",
    "        tracker.start_run(f\"xgboost_lr{lr}_d{depth}\")\n",
    "        tracker.log_param('learning_rate', lr)\n",
    "        tracker.log_param('max_depth', depth)\n",
    "        \n",
    "        # Simulated metrics\n",
    "        sharpe = 1.2 + np.random.randn() * 0.3 - 0.1 * abs(np.log10(lr) + 2)\n",
    "        tracker.log_metric('sharpe', sharpe)\n",
    "        tracker.log_metric('max_drawdown', -np.random.uniform(0.05, 0.15))\n",
    "        \n",
    "        tracker.end_run()\n",
    "\n",
    "# Find best model\n",
    "best = tracker.get_best_run('sharpe', maximize=True)\n",
    "print(\"Best Experiment:\")\n",
    "print(f\"  Run: {best['name']}\")\n",
    "print(f\"  Params: {best['params']}\")\n",
    "print(f\"  Sharpe: {best['metrics']['sharpe']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380e6c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Deployment Patterns\n",
    "\n",
    "### Batch Prediction\n",
    "\n",
    "- Run predictions on schedule (daily, hourly)\n",
    "- Good for: Portfolio rebalancing, end-of-day signals\n",
    "\n",
    "### Real-time Prediction\n",
    "\n",
    "- REST API / gRPC serving\n",
    "- Good for: HFT, immediate execution\n",
    "\n",
    "### Streaming\n",
    "\n",
    "- Process data as it arrives (Kafka, Kinesis)\n",
    "- Good for: Continuous monitoring, alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea100b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelServingSimulator:\n",
    "    \"\"\"Simulate different serving patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, latency_ms=10):\n",
    "        self.model_name = model_name\n",
    "        self.latency_ms = latency_ms\n",
    "        self.prediction_count = 0\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"Real-time prediction\"\"\"\n",
    "        import time\n",
    "        time.sleep(self.latency_ms / 1000)  # Simulate latency\n",
    "        self.prediction_count += 1\n",
    "        # Dummy prediction\n",
    "        return np.random.choice([-1, 0, 1], p=[0.3, 0.4, 0.3])\n",
    "    \n",
    "    def batch_predict(self, features_batch):\n",
    "        \"\"\"Batch prediction\"\"\"\n",
    "        predictions = []\n",
    "        for features in features_batch:\n",
    "            predictions.append(self.predict(features))\n",
    "        return predictions\n",
    "\n",
    "# Architecture diagram (conceptual)\n",
    "print(\"Typical ML Deployment Architecture\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Market    â”‚â”€â”€â”€â–¶â”‚   Feature   â”‚â”€â”€â”€â–¶â”‚   Model     â”‚\n",
    "â”‚   Data      â”‚    â”‚   Pipeline  â”‚    â”‚   Serving   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                            â”‚\n",
    "                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "                   â”‚   Order     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚   Manager   â”‚\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cace9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Model Monitoring\n",
    "\n",
    "### Types of Drift\n",
    "\n",
    "1. **Data Drift**: Input distribution changes\n",
    "2. **Concept Drift**: Relationship between X and Y changes\n",
    "3. **Prediction Drift**: Model outputs change\n",
    "\n",
    "### Detection Methods\n",
    "\n",
    "- Statistical tests (KS, PSI)\n",
    "- Moving average comparisons\n",
    "- Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_stability_index(expected, actual, buckets=10):\n",
    "    \"\"\"Calculate PSI for drift detection\"\"\"\n",
    "    # Create buckets based on expected distribution\n",
    "    breakpoints = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
    "    breakpoints[0] = -np.inf\n",
    "    breakpoints[-1] = np.inf\n",
    "    \n",
    "    expected_counts = np.histogram(expected, breakpoints)[0]\n",
    "    actual_counts = np.histogram(actual, breakpoints)[0]\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    expected_pct = (expected_counts + 0.001) / len(expected)\n",
    "    actual_pct = (actual_counts + 0.001) / len(actual)\n",
    "    \n",
    "    psi = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "    return psi\n",
    "\n",
    "# Simulate drift\n",
    "n_samples = 1000\n",
    "\n",
    "# Training distribution\n",
    "train_feature = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "# Production distributions\n",
    "prod_no_drift = np.random.normal(0, 1, n_samples)\n",
    "prod_mild_drift = np.random.normal(0.3, 1.1, n_samples)\n",
    "prod_severe_drift = np.random.normal(1.0, 1.5, n_samples)\n",
    "\n",
    "print(\"PSI Drift Detection\")\n",
    "print(\"=\"*50)\n",
    "print(f\"No drift:     PSI = {population_stability_index(train_feature, prod_no_drift):.4f}\")\n",
    "print(f\"Mild drift:   PSI = {population_stability_index(train_feature, prod_mild_drift):.4f}\")\n",
    "print(f\"Severe drift: PSI = {population_stability_index(train_feature, prod_severe_drift):.4f}\")\n",
    "print(\"\\nInterpretation: PSI < 0.1 (no drift), 0.1-0.2 (moderate), > 0.2 (significant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2434cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, (name, data) in zip(axes, [\n",
    "    ('No Drift', prod_no_drift),\n",
    "    ('Mild Drift', prod_mild_drift),\n",
    "    ('Severe Drift', prod_severe_drift)\n",
    "]):\n",
    "    ax.hist(train_feature, bins=30, alpha=0.5, label='Training', density=True)\n",
    "    ax.hist(data, bins=30, alpha=0.5, label='Production', density=True)\n",
    "    psi = population_stability_index(train_feature, data)\n",
    "    ax.set_title(f'{name}\\nPSI = {psi:.4f}')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Feature Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18eb977",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Performance Monitoring Dashboard\n",
    "\n",
    "### Key Metrics to Track\n",
    "\n",
    "| Metric | Purpose | Alert Threshold |\n",
    "|--------|---------|----------------|\n",
    "| Sharpe (rolling) | Performance | < 0.5 |\n",
    "| Max Drawdown | Risk | > 10% |\n",
    "| Hit Rate | Signal quality | < 45% |\n",
    "| Feature PSI | Data drift | > 0.2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor model performance in production\"\"\"\n",
    "    \n",
    "    def __init__(self, sharpe_threshold=0.5, drawdown_threshold=0.10):\n",
    "        self.sharpe_threshold = sharpe_threshold\n",
    "        self.drawdown_threshold = drawdown_threshold\n",
    "        self.returns = []\n",
    "        self.alerts = []\n",
    "    \n",
    "    def add_return(self, ret):\n",
    "        self.returns.append(ret)\n",
    "        self._check_alerts()\n",
    "    \n",
    "    def _check_alerts(self):\n",
    "        if len(self.returns) < 20:\n",
    "            return\n",
    "        \n",
    "        recent = np.array(self.returns[-60:])  # Last 60 periods\n",
    "        \n",
    "        # Rolling Sharpe\n",
    "        sharpe = recent.mean() / recent.std() * np.sqrt(252)\n",
    "        if sharpe < self.sharpe_threshold:\n",
    "            self.alerts.append(f\"LOW SHARPE: {sharpe:.2f}\")\n",
    "        \n",
    "        # Max Drawdown\n",
    "        cum = (1 + recent).cumprod()\n",
    "        peak = np.maximum.accumulate(cum)\n",
    "        dd = (cum - peak) / peak\n",
    "        max_dd = dd.min()\n",
    "        if abs(max_dd) > self.drawdown_threshold:\n",
    "            self.alerts.append(f\"HIGH DRAWDOWN: {max_dd:.1%}\")\n",
    "    \n",
    "    def get_status(self):\n",
    "        if len(self.returns) < 20:\n",
    "            return \"INSUFFICIENT DATA\"\n",
    "        if self.alerts:\n",
    "            return f\"WARNING: {len(self.alerts)} alerts\"\n",
    "        return \"HEALTHY\"\n",
    "\n",
    "# Simulate monitoring\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "# Add daily returns\n",
    "for _ in range(100):\n",
    "    ret = np.random.normal(0.0003, 0.015)  # Slightly positive expected return\n",
    "    monitor.add_return(ret)\n",
    "\n",
    "print(f\"Model Status: {monitor.get_status()}\")\n",
    "print(f\"Alerts: {monitor.alerts[-3:] if monitor.alerts else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfc16c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interview Questions\n",
    "\n",
    "### Conceptual\n",
    "1. What's the difference between data drift and concept drift?\n",
    "2. Why do most ML projects fail to reach production?\n",
    "3. How often should you retrain models?\n",
    "\n",
    "### Technical\n",
    "1. How do you detect model degradation?\n",
    "2. What's your approach to A/B testing trading models?\n",
    "3. How do you handle model rollback?\n",
    "\n",
    "### Finance-Specific\n",
    "1. What's unique about deploying trading models vs. other ML?\n",
    "2. How do you handle regime changes?\n",
    "3. What metrics do you monitor in production?\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Component | Tool | Purpose |\n",
    "|-----------|------|--------|\n",
    "| Tracking | MLflow | Experiment management |\n",
    "| Serving | FastAPI/Flask | Model inference |\n",
    "| Monitoring | Grafana/Custom | Performance tracking |\n",
    "| Pipeline | Airflow/Prefect | Orchestration |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
